@main () -> () {
    prog.fragment @deinit <CPU> <data> {
        addr:0xffffffffffffffff prog.label[symbol:deinit.__entry]
    }
    prog.fragment @__MLLM_JIT_PACKAGE_CODE_SEGMENT <CPU> <code> {
        addr:0x00000000000000 prog.kernel_launch(%582:tensor<[1, 192], Int64, CPU>) -> (%583:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.embed_tokens, op_type:Embedding, op_options:{"hidden_size":1536,"vocab_size":151936}]
        addr:0x00000000000001 prog.jump visual.__entry[offset:1]
        addr:0x00000000000002 prog.label[symbol:visual.__entry]
        addr:0x00000000000003 prog.jump visual.patch_embed.__entry[offset:35]
        addr:0x00000000000004 prog.jump visual.blocks.0.__entry[offset:39]
        addr:0x00000000000005 prog.jump visual.blocks.1.__entry[offset:69]
        addr:0x00000000000006 prog.jump visual.blocks.2.__entry[offset:99]
        addr:0x00000000000007 prog.jump visual.blocks.3.__entry[offset:129]
        addr:0x00000000000008 prog.jump visual.blocks.4.__entry[offset:159]
        addr:0x00000000000009 prog.jump visual.blocks.5.__entry[offset:189]
        addr:0x0000000000000a prog.jump visual.blocks.6.__entry[offset:219]
        addr:0x0000000000000b prog.jump visual.blocks.7.__entry[offset:249]
        addr:0x0000000000000c prog.jump visual.blocks.8.__entry[offset:279]
        addr:0x0000000000000d prog.jump visual.blocks.9.__entry[offset:309]
        addr:0x0000000000000e prog.jump visual.blocks.10.__entry[offset:339]
        addr:0x0000000000000f prog.jump visual.blocks.11.__entry[offset:369]
        addr:0x00000000000010 prog.jump visual.blocks.12.__entry[offset:399]
        addr:0x00000000000011 prog.jump visual.blocks.13.__entry[offset:429]
        addr:0x00000000000012 prog.jump visual.blocks.14.__entry[offset:459]
        addr:0x00000000000013 prog.jump visual.blocks.15.__entry[offset:489]
        addr:0x00000000000014 prog.jump visual.blocks.16.__entry[offset:519]
        addr:0x00000000000015 prog.jump visual.blocks.17.__entry[offset:549]
        addr:0x00000000000016 prog.jump visual.blocks.18.__entry[offset:579]
        addr:0x00000000000017 prog.jump visual.blocks.19.__entry[offset:609]
        addr:0x00000000000018 prog.jump visual.blocks.20.__entry[offset:639]
        addr:0x00000000000019 prog.jump visual.blocks.21.__entry[offset:669]
        addr:0x0000000000001a prog.jump visual.blocks.22.__entry[offset:699]
        addr:0x0000000000001b prog.jump visual.blocks.23.__entry[offset:729]
        addr:0x0000000000001c prog.jump visual.blocks.24.__entry[offset:759]
        addr:0x0000000000001d prog.jump visual.blocks.25.__entry[offset:789]
        addr:0x0000000000001e prog.jump visual.blocks.26.__entry[offset:819]
        addr:0x0000000000001f prog.jump visual.blocks.27.__entry[offset:849]
        addr:0x00000000000020 prog.jump visual.blocks.28.__entry[offset:879]
        addr:0x00000000000021 prog.jump visual.blocks.29.__entry[offset:909]
        addr:0x00000000000022 prog.jump visual.blocks.30.__entry[offset:939]
        addr:0x00000000000023 prog.jump visual.blocks.31.__entry[offset:969]
        addr:0x00000000000024 prog.jump visual.merger.__entry[offset:999]
        addr:0x00000000000025 prog.ret
        addr:0x00000000000026 prog.label[symbol:visual.patch_embed.__entry]
        addr:0x00000000000027 prog.kernel_launch(%580:tensor<[680, 1176], Float32, CPU>) -> (%580:tensor<[680, 3, 2, 14, 14], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,3,2,14,14]}]
        addr:0x00000000000028 prog.kernel_launch(%580:tensor<[680, 3, 2, 14, 14], Float32, CPU>) -> (%590:tensor<[680, 1280, 1, 1, 1], Float32, CPU>)[symbol_name:visual.patch_embed.proj, op_type:Conv3D, op_options:null]
        addr:0x00000000000029 prog.kernel_launch(%590:tensor<[680, 1280, 1, 1, 1], Float32, CPU>) -> (%590:tensor<[680, 1280], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,1280]}]
        addr:0x0000000000002a prog.ret
        addr:0x0000000000002b prog.label[symbol:visual.blocks.0.__entry]
        addr:0x0000000000002c prog.kernel_launch(%590:tensor<[680, 1280], Float32, CPU>) -> (%591:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.0.norm1, op_type:LayerNorm, op_options:null]
        addr:0x0000000000002d prog.jump visual.blocks.0.attn.__entry[offset:6]
        addr:0x0000000000002e prog.kernel_launch(%590:tensor<[680, 1280], Float32, CPU>, %605:tensor<[680, 1280], Float32, CPU>) -> (%606:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x0000000000002f prog.kernel_launch(%606:tensor<[680, 1280], Float32, CPU>) -> (%607:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.0.norm2, op_type:LayerNorm, op_options:null]
        addr:0x00000000000030 prog.jump visual.blocks.0.mlp.__entry[offset:21]
        addr:0x00000000000031 prog.kernel_launch(%606:tensor<[680, 1280], Float32, CPU>, %610:tensor<[680, 1280], Float32, CPU>) -> (%611:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x00000000000032 prog.ret
        addr:0x00000000000033 prog.label[symbol:visual.blocks.0.attn.__entry]
        addr:0x00000000000034 prog.kernel_launch(%591:tensor<[680, 1280], Float32, CPU>) -> (%592:tensor<[680, 3840], Float32, CPU>)[symbol_name:visual.blocks.0.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x00000000000035 prog.kernel_launch(%592:tensor<[680, 3840], Float32, CPU>) -> (%592:tensor<[680, 3, 16, 80], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,3,16,80]}]
        addr:0x00000000000036 prog.kernel_launch(%592:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%593:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Permute, op_options:{"axis":[1,0,2,3]}]
        addr:0x00000000000037 prog.kernel_launch(%593:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%593:tensor<[3, 680, 16, 80], Float32, CPU>, %593:tensor<[3, 680, 16, 80], Float32, CPU>, %593:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Split, op_options:{"dim":0,"split_size_or_sections":[1]}]
        addr:0x00000000000038 prog.kernel_launch(%593:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%594:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.0.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x00000000000039 prog.kernel_launch(%593:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%595:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.0.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x0000000000003a prog.kernel_launch(%594:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%596:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000003b prog.kernel_launch(%595:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%597:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000003c prog.kernel_launch(%593:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%598:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000003d prog.kernel_launch(%596:tensor<[1, 16, 680, 80], Float32, CPU>, %597:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%599:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x0000000000003e prog.kernel_launch(%599:tensor<[1, 16, 680, 680], Float32, CPU>, %600:tensor<[1], Float32, CPU>) -> (%601:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x0000000000003f prog.kernel_launch(%601:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%602:tensor<[1, 16, 680, 680], Float32, CPU>)[symbol_name:visual.blocks.0.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x00000000000040 prog.kernel_launch(%602:tensor<[1, 16, 680, 680], Float32, CPU>, %598:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%603:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x00000000000041 prog.kernel_launch(%603:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%604:tensor<[1, 680, 16, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000042 prog.kernel_launch(%604:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%604:tensor<[680, 1280], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,1280]}]
        addr:0x00000000000043 prog.kernel_launch(%604:tensor<[680, 1280], Float32, CPU>) -> (%605:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.0.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x00000000000044 prog.ret
        addr:0x00000000000045 prog.label[symbol:visual.blocks.0.mlp.__entry]
        addr:0x00000000000046 prog.kernel_launch(%607:tensor<[680, 1280], Float32, CPU>) -> (%608:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.0.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x00000000000047 prog.kernel_launch(%608:tensor<[680, 5120], Float32, CPU>) -> (%609:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.0.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x00000000000048 prog.kernel_launch(%609:tensor<[680, 5120], Float32, CPU>) -> (%610:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.0.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x00000000000049 prog.ret
        addr:0x0000000000004a prog.label[symbol:visual.blocks.1.__entry]
        addr:0x0000000000004b prog.kernel_launch(%611:tensor<[680, 1280], Float32, CPU>) -> (%612:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.1.norm1, op_type:LayerNorm, op_options:null]
        addr:0x0000000000004c prog.jump visual.blocks.1.attn.__entry[offset:6]
        addr:0x0000000000004d prog.kernel_launch(%611:tensor<[680, 1280], Float32, CPU>, %626:tensor<[680, 1280], Float32, CPU>) -> (%627:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x0000000000004e prog.kernel_launch(%627:tensor<[680, 1280], Float32, CPU>) -> (%628:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.1.norm2, op_type:LayerNorm, op_options:null]
        addr:0x0000000000004f prog.jump visual.blocks.1.mlp.__entry[offset:21]
        addr:0x00000000000050 prog.kernel_launch(%627:tensor<[680, 1280], Float32, CPU>, %631:tensor<[680, 1280], Float32, CPU>) -> (%632:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x00000000000051 prog.ret
        addr:0x00000000000052 prog.label[symbol:visual.blocks.1.attn.__entry]
        addr:0x00000000000053 prog.kernel_launch(%612:tensor<[680, 1280], Float32, CPU>) -> (%613:tensor<[680, 3840], Float32, CPU>)[symbol_name:visual.blocks.1.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x00000000000054 prog.kernel_launch(%613:tensor<[680, 3840], Float32, CPU>) -> (%613:tensor<[680, 3, 16, 80], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,3,16,80]}]
        addr:0x00000000000055 prog.kernel_launch(%613:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%614:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Permute, op_options:{"axis":[1,0,2,3]}]
        addr:0x00000000000056 prog.kernel_launch(%614:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%614:tensor<[3, 680, 16, 80], Float32, CPU>, %614:tensor<[3, 680, 16, 80], Float32, CPU>, %614:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Split, op_options:{"dim":0,"split_size_or_sections":[1]}]
        addr:0x00000000000057 prog.kernel_launch(%614:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%615:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.1.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x00000000000058 prog.kernel_launch(%614:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%616:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.1.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x00000000000059 prog.kernel_launch(%615:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%617:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000005a prog.kernel_launch(%616:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%618:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000005b prog.kernel_launch(%614:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%619:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000005c prog.kernel_launch(%617:tensor<[1, 16, 680, 80], Float32, CPU>, %618:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%620:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x0000000000005d prog.kernel_launch(%620:tensor<[1, 16, 680, 680], Float32, CPU>, %621:tensor<[1], Float32, CPU>) -> (%622:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x0000000000005e prog.kernel_launch(%622:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%623:tensor<[1, 16, 680, 680], Float32, CPU>)[symbol_name:visual.blocks.1.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x0000000000005f prog.kernel_launch(%623:tensor<[1, 16, 680, 680], Float32, CPU>, %619:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%624:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x00000000000060 prog.kernel_launch(%624:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%625:tensor<[1, 680, 16, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000061 prog.kernel_launch(%625:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%625:tensor<[680, 1280], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,1280]}]
        addr:0x00000000000062 prog.kernel_launch(%625:tensor<[680, 1280], Float32, CPU>) -> (%626:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.1.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x00000000000063 prog.ret
        addr:0x00000000000064 prog.label[symbol:visual.blocks.1.mlp.__entry]
        addr:0x00000000000065 prog.kernel_launch(%628:tensor<[680, 1280], Float32, CPU>) -> (%629:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.1.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x00000000000066 prog.kernel_launch(%629:tensor<[680, 5120], Float32, CPU>) -> (%630:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.1.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x00000000000067 prog.kernel_launch(%630:tensor<[680, 5120], Float32, CPU>) -> (%631:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.1.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x00000000000068 prog.ret
        addr:0x00000000000069 prog.label[symbol:visual.blocks.2.__entry]
        addr:0x0000000000006a prog.kernel_launch(%632:tensor<[680, 1280], Float32, CPU>) -> (%633:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.2.norm1, op_type:LayerNorm, op_options:null]
        addr:0x0000000000006b prog.jump visual.blocks.2.attn.__entry[offset:6]
        addr:0x0000000000006c prog.kernel_launch(%632:tensor<[680, 1280], Float32, CPU>, %647:tensor<[680, 1280], Float32, CPU>) -> (%648:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x0000000000006d prog.kernel_launch(%648:tensor<[680, 1280], Float32, CPU>) -> (%649:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.2.norm2, op_type:LayerNorm, op_options:null]
        addr:0x0000000000006e prog.jump visual.blocks.2.mlp.__entry[offset:21]
        addr:0x0000000000006f prog.kernel_launch(%648:tensor<[680, 1280], Float32, CPU>, %652:tensor<[680, 1280], Float32, CPU>) -> (%653:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x00000000000070 prog.ret
        addr:0x00000000000071 prog.label[symbol:visual.blocks.2.attn.__entry]
        addr:0x00000000000072 prog.kernel_launch(%633:tensor<[680, 1280], Float32, CPU>) -> (%634:tensor<[680, 3840], Float32, CPU>)[symbol_name:visual.blocks.2.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x00000000000073 prog.kernel_launch(%634:tensor<[680, 3840], Float32, CPU>) -> (%634:tensor<[680, 3, 16, 80], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,3,16,80]}]
        addr:0x00000000000074 prog.kernel_launch(%634:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%635:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Permute, op_options:{"axis":[1,0,2,3]}]
        addr:0x00000000000075 prog.kernel_launch(%635:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%635:tensor<[3, 680, 16, 80], Float32, CPU>, %635:tensor<[3, 680, 16, 80], Float32, CPU>, %635:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Split, op_options:{"dim":0,"split_size_or_sections":[1]}]
        addr:0x00000000000076 prog.kernel_launch(%635:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%636:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.2.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x00000000000077 prog.kernel_launch(%635:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%637:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.2.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x00000000000078 prog.kernel_launch(%636:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%638:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000079 prog.kernel_launch(%637:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%639:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000007a prog.kernel_launch(%635:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%640:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000007b prog.kernel_launch(%638:tensor<[1, 16, 680, 80], Float32, CPU>, %639:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%641:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x0000000000007c prog.kernel_launch(%641:tensor<[1, 16, 680, 680], Float32, CPU>, %642:tensor<[1], Float32, CPU>) -> (%643:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x0000000000007d prog.kernel_launch(%643:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%644:tensor<[1, 16, 680, 680], Float32, CPU>)[symbol_name:visual.blocks.2.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x0000000000007e prog.kernel_launch(%644:tensor<[1, 16, 680, 680], Float32, CPU>, %640:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%645:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x0000000000007f prog.kernel_launch(%645:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%646:tensor<[1, 680, 16, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000080 prog.kernel_launch(%646:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%646:tensor<[680, 1280], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,1280]}]
        addr:0x00000000000081 prog.kernel_launch(%646:tensor<[680, 1280], Float32, CPU>) -> (%647:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.2.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x00000000000082 prog.ret
        addr:0x00000000000083 prog.label[symbol:visual.blocks.2.mlp.__entry]
        addr:0x00000000000084 prog.kernel_launch(%649:tensor<[680, 1280], Float32, CPU>) -> (%650:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.2.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x00000000000085 prog.kernel_launch(%650:tensor<[680, 5120], Float32, CPU>) -> (%651:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.2.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x00000000000086 prog.kernel_launch(%651:tensor<[680, 5120], Float32, CPU>) -> (%652:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.2.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x00000000000087 prog.ret
        addr:0x00000000000088 prog.label[symbol:visual.blocks.3.__entry]
        addr:0x00000000000089 prog.kernel_launch(%653:tensor<[680, 1280], Float32, CPU>) -> (%654:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.3.norm1, op_type:LayerNorm, op_options:null]
        addr:0x0000000000008a prog.jump visual.blocks.3.attn.__entry[offset:6]
        addr:0x0000000000008b prog.kernel_launch(%653:tensor<[680, 1280], Float32, CPU>, %668:tensor<[680, 1280], Float32, CPU>) -> (%669:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x0000000000008c prog.kernel_launch(%669:tensor<[680, 1280], Float32, CPU>) -> (%670:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.3.norm2, op_type:LayerNorm, op_options:null]
        addr:0x0000000000008d prog.jump visual.blocks.3.mlp.__entry[offset:21]
        addr:0x0000000000008e prog.kernel_launch(%669:tensor<[680, 1280], Float32, CPU>, %673:tensor<[680, 1280], Float32, CPU>) -> (%674:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x0000000000008f prog.ret
        addr:0x00000000000090 prog.label[symbol:visual.blocks.3.attn.__entry]
        addr:0x00000000000091 prog.kernel_launch(%654:tensor<[680, 1280], Float32, CPU>) -> (%655:tensor<[680, 3840], Float32, CPU>)[symbol_name:visual.blocks.3.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x00000000000092 prog.kernel_launch(%655:tensor<[680, 3840], Float32, CPU>) -> (%655:tensor<[680, 3, 16, 80], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,3,16,80]}]
        addr:0x00000000000093 prog.kernel_launch(%655:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%656:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Permute, op_options:{"axis":[1,0,2,3]}]
        addr:0x00000000000094 prog.kernel_launch(%656:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%656:tensor<[3, 680, 16, 80], Float32, CPU>, %656:tensor<[3, 680, 16, 80], Float32, CPU>, %656:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Split, op_options:{"dim":0,"split_size_or_sections":[1]}]
        addr:0x00000000000095 prog.kernel_launch(%656:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%657:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.3.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x00000000000096 prog.kernel_launch(%656:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%658:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.3.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x00000000000097 prog.kernel_launch(%657:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%659:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000098 prog.kernel_launch(%658:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%660:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000099 prog.kernel_launch(%656:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%661:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000009a prog.kernel_launch(%659:tensor<[1, 16, 680, 80], Float32, CPU>, %660:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%662:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x0000000000009b prog.kernel_launch(%662:tensor<[1, 16, 680, 680], Float32, CPU>, %663:tensor<[1], Float32, CPU>) -> (%664:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x0000000000009c prog.kernel_launch(%664:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%665:tensor<[1, 16, 680, 680], Float32, CPU>)[symbol_name:visual.blocks.3.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x0000000000009d prog.kernel_launch(%665:tensor<[1, 16, 680, 680], Float32, CPU>, %661:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%666:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x0000000000009e prog.kernel_launch(%666:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%667:tensor<[1, 680, 16, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000009f prog.kernel_launch(%667:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%667:tensor<[680, 1280], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,1280]}]
        addr:0x000000000000a0 prog.kernel_launch(%667:tensor<[680, 1280], Float32, CPU>) -> (%668:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.3.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x000000000000a1 prog.ret
        addr:0x000000000000a2 prog.label[symbol:visual.blocks.3.mlp.__entry]
        addr:0x000000000000a3 prog.kernel_launch(%670:tensor<[680, 1280], Float32, CPU>) -> (%671:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.3.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x000000000000a4 prog.kernel_launch(%671:tensor<[680, 5120], Float32, CPU>) -> (%672:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.3.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x000000000000a5 prog.kernel_launch(%672:tensor<[680, 5120], Float32, CPU>) -> (%673:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.3.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x000000000000a6 prog.ret
        addr:0x000000000000a7 prog.label[symbol:visual.blocks.4.__entry]
        addr:0x000000000000a8 prog.kernel_launch(%674:tensor<[680, 1280], Float32, CPU>) -> (%675:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.4.norm1, op_type:LayerNorm, op_options:null]
        addr:0x000000000000a9 prog.jump visual.blocks.4.attn.__entry[offset:6]
        addr:0x000000000000aa prog.kernel_launch(%674:tensor<[680, 1280], Float32, CPU>, %689:tensor<[680, 1280], Float32, CPU>) -> (%690:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000000ab prog.kernel_launch(%690:tensor<[680, 1280], Float32, CPU>) -> (%691:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.4.norm2, op_type:LayerNorm, op_options:null]
        addr:0x000000000000ac prog.jump visual.blocks.4.mlp.__entry[offset:21]
        addr:0x000000000000ad prog.kernel_launch(%690:tensor<[680, 1280], Float32, CPU>, %694:tensor<[680, 1280], Float32, CPU>) -> (%695:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000000ae prog.ret
        addr:0x000000000000af prog.label[symbol:visual.blocks.4.attn.__entry]
        addr:0x000000000000b0 prog.kernel_launch(%675:tensor<[680, 1280], Float32, CPU>) -> (%676:tensor<[680, 3840], Float32, CPU>)[symbol_name:visual.blocks.4.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x000000000000b1 prog.kernel_launch(%676:tensor<[680, 3840], Float32, CPU>) -> (%676:tensor<[680, 3, 16, 80], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,3,16,80]}]
        addr:0x000000000000b2 prog.kernel_launch(%676:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%677:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Permute, op_options:{"axis":[1,0,2,3]}]
        addr:0x000000000000b3 prog.kernel_launch(%677:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%677:tensor<[3, 680, 16, 80], Float32, CPU>, %677:tensor<[3, 680, 16, 80], Float32, CPU>, %677:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Split, op_options:{"dim":0,"split_size_or_sections":[1]}]
        addr:0x000000000000b4 prog.kernel_launch(%677:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%678:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.4.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x000000000000b5 prog.kernel_launch(%677:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%679:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.4.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x000000000000b6 prog.kernel_launch(%678:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%680:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000000b7 prog.kernel_launch(%679:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%681:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000000b8 prog.kernel_launch(%677:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%682:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000000b9 prog.kernel_launch(%680:tensor<[1, 16, 680, 80], Float32, CPU>, %681:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%683:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x000000000000ba prog.kernel_launch(%683:tensor<[1, 16, 680, 680], Float32, CPU>, %684:tensor<[1], Float32, CPU>) -> (%685:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x000000000000bb prog.kernel_launch(%685:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%686:tensor<[1, 16, 680, 680], Float32, CPU>)[symbol_name:visual.blocks.4.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x000000000000bc prog.kernel_launch(%686:tensor<[1, 16, 680, 680], Float32, CPU>, %682:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%687:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x000000000000bd prog.kernel_launch(%687:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%688:tensor<[1, 680, 16, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000000be prog.kernel_launch(%688:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%688:tensor<[680, 1280], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,1280]}]
        addr:0x000000000000bf prog.kernel_launch(%688:tensor<[680, 1280], Float32, CPU>) -> (%689:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.4.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x000000000000c0 prog.ret
        addr:0x000000000000c1 prog.label[symbol:visual.blocks.4.mlp.__entry]
        addr:0x000000000000c2 prog.kernel_launch(%691:tensor<[680, 1280], Float32, CPU>) -> (%692:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.4.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x000000000000c3 prog.kernel_launch(%692:tensor<[680, 5120], Float32, CPU>) -> (%693:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.4.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x000000000000c4 prog.kernel_launch(%693:tensor<[680, 5120], Float32, CPU>) -> (%694:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.4.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x000000000000c5 prog.ret
        addr:0x000000000000c6 prog.label[symbol:visual.blocks.5.__entry]
        addr:0x000000000000c7 prog.kernel_launch(%695:tensor<[680, 1280], Float32, CPU>) -> (%696:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.5.norm1, op_type:LayerNorm, op_options:null]
        addr:0x000000000000c8 prog.jump visual.blocks.5.attn.__entry[offset:6]
        addr:0x000000000000c9 prog.kernel_launch(%695:tensor<[680, 1280], Float32, CPU>, %710:tensor<[680, 1280], Float32, CPU>) -> (%711:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000000ca prog.kernel_launch(%711:tensor<[680, 1280], Float32, CPU>) -> (%712:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.5.norm2, op_type:LayerNorm, op_options:null]
        addr:0x000000000000cb prog.jump visual.blocks.5.mlp.__entry[offset:21]
        addr:0x000000000000cc prog.kernel_launch(%711:tensor<[680, 1280], Float32, CPU>, %715:tensor<[680, 1280], Float32, CPU>) -> (%716:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000000cd prog.ret
        addr:0x000000000000ce prog.label[symbol:visual.blocks.5.attn.__entry]
        addr:0x000000000000cf prog.kernel_launch(%696:tensor<[680, 1280], Float32, CPU>) -> (%697:tensor<[680, 3840], Float32, CPU>)[symbol_name:visual.blocks.5.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x000000000000d0 prog.kernel_launch(%697:tensor<[680, 3840], Float32, CPU>) -> (%697:tensor<[680, 3, 16, 80], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,3,16,80]}]
        addr:0x000000000000d1 prog.kernel_launch(%697:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%698:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Permute, op_options:{"axis":[1,0,2,3]}]
        addr:0x000000000000d2 prog.kernel_launch(%698:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%698:tensor<[3, 680, 16, 80], Float32, CPU>, %698:tensor<[3, 680, 16, 80], Float32, CPU>, %698:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Split, op_options:{"dim":0,"split_size_or_sections":[1]}]
        addr:0x000000000000d3 prog.kernel_launch(%698:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%699:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.5.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x000000000000d4 prog.kernel_launch(%698:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%700:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.5.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x000000000000d5 prog.kernel_launch(%699:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%701:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000000d6 prog.kernel_launch(%700:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%702:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000000d7 prog.kernel_launch(%698:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%703:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000000d8 prog.kernel_launch(%701:tensor<[1, 16, 680, 80], Float32, CPU>, %702:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%704:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x000000000000d9 prog.kernel_launch(%704:tensor<[1, 16, 680, 680], Float32, CPU>, %705:tensor<[1], Float32, CPU>) -> (%706:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x000000000000da prog.kernel_launch(%706:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%707:tensor<[1, 16, 680, 680], Float32, CPU>)[symbol_name:visual.blocks.5.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x000000000000db prog.kernel_launch(%707:tensor<[1, 16, 680, 680], Float32, CPU>, %703:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%708:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x000000000000dc prog.kernel_launch(%708:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%709:tensor<[1, 680, 16, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000000dd prog.kernel_launch(%709:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%709:tensor<[680, 1280], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,1280]}]
        addr:0x000000000000de prog.kernel_launch(%709:tensor<[680, 1280], Float32, CPU>) -> (%710:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.5.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x000000000000df prog.ret
        addr:0x000000000000e0 prog.label[symbol:visual.blocks.5.mlp.__entry]
        addr:0x000000000000e1 prog.kernel_launch(%712:tensor<[680, 1280], Float32, CPU>) -> (%713:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.5.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x000000000000e2 prog.kernel_launch(%713:tensor<[680, 5120], Float32, CPU>) -> (%714:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.5.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x000000000000e3 prog.kernel_launch(%714:tensor<[680, 5120], Float32, CPU>) -> (%715:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.5.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x000000000000e4 prog.ret
        addr:0x000000000000e5 prog.label[symbol:visual.blocks.6.__entry]
        addr:0x000000000000e6 prog.kernel_launch(%716:tensor<[680, 1280], Float32, CPU>) -> (%717:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.6.norm1, op_type:LayerNorm, op_options:null]
        addr:0x000000000000e7 prog.jump visual.blocks.6.attn.__entry[offset:6]
        addr:0x000000000000e8 prog.kernel_launch(%716:tensor<[680, 1280], Float32, CPU>, %731:tensor<[680, 1280], Float32, CPU>) -> (%732:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000000e9 prog.kernel_launch(%732:tensor<[680, 1280], Float32, CPU>) -> (%733:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.6.norm2, op_type:LayerNorm, op_options:null]
        addr:0x000000000000ea prog.jump visual.blocks.6.mlp.__entry[offset:21]
        addr:0x000000000000eb prog.kernel_launch(%732:tensor<[680, 1280], Float32, CPU>, %736:tensor<[680, 1280], Float32, CPU>) -> (%737:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000000ec prog.ret
        addr:0x000000000000ed prog.label[symbol:visual.blocks.6.attn.__entry]
        addr:0x000000000000ee prog.kernel_launch(%717:tensor<[680, 1280], Float32, CPU>) -> (%718:tensor<[680, 3840], Float32, CPU>)[symbol_name:visual.blocks.6.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x000000000000ef prog.kernel_launch(%718:tensor<[680, 3840], Float32, CPU>) -> (%718:tensor<[680, 3, 16, 80], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,3,16,80]}]
        addr:0x000000000000f0 prog.kernel_launch(%718:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%719:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Permute, op_options:{"axis":[1,0,2,3]}]
        addr:0x000000000000f1 prog.kernel_launch(%719:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%719:tensor<[3, 680, 16, 80], Float32, CPU>, %719:tensor<[3, 680, 16, 80], Float32, CPU>, %719:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Split, op_options:{"dim":0,"split_size_or_sections":[1]}]
        addr:0x000000000000f2 prog.kernel_launch(%719:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%720:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.6.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x000000000000f3 prog.kernel_launch(%719:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%721:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.6.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x000000000000f4 prog.kernel_launch(%720:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%722:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000000f5 prog.kernel_launch(%721:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%723:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000000f6 prog.kernel_launch(%719:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%724:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000000f7 prog.kernel_launch(%722:tensor<[1, 16, 680, 80], Float32, CPU>, %723:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%725:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x000000000000f8 prog.kernel_launch(%725:tensor<[1, 16, 680, 680], Float32, CPU>, %726:tensor<[1], Float32, CPU>) -> (%727:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x000000000000f9 prog.kernel_launch(%727:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%728:tensor<[1, 16, 680, 680], Float32, CPU>)[symbol_name:visual.blocks.6.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x000000000000fa prog.kernel_launch(%728:tensor<[1, 16, 680, 680], Float32, CPU>, %724:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%729:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x000000000000fb prog.kernel_launch(%729:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%730:tensor<[1, 680, 16, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000000fc prog.kernel_launch(%730:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%730:tensor<[680, 1280], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,1280]}]
        addr:0x000000000000fd prog.kernel_launch(%730:tensor<[680, 1280], Float32, CPU>) -> (%731:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.6.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x000000000000fe prog.ret
        addr:0x000000000000ff prog.label[symbol:visual.blocks.6.mlp.__entry]
        addr:0x00000000000100 prog.kernel_launch(%733:tensor<[680, 1280], Float32, CPU>) -> (%734:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.6.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x00000000000101 prog.kernel_launch(%734:tensor<[680, 5120], Float32, CPU>) -> (%735:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.6.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x00000000000102 prog.kernel_launch(%735:tensor<[680, 5120], Float32, CPU>) -> (%736:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.6.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x00000000000103 prog.ret
        addr:0x00000000000104 prog.label[symbol:visual.blocks.7.__entry]
        addr:0x00000000000105 prog.kernel_launch(%737:tensor<[680, 1280], Float32, CPU>) -> (%738:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.7.norm1, op_type:LayerNorm, op_options:null]
        addr:0x00000000000106 prog.jump visual.blocks.7.attn.__entry[offset:6]
        addr:0x00000000000107 prog.kernel_launch(%737:tensor<[680, 1280], Float32, CPU>, %752:tensor<[680, 1280], Float32, CPU>) -> (%753:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x00000000000108 prog.kernel_launch(%753:tensor<[680, 1280], Float32, CPU>) -> (%754:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.7.norm2, op_type:LayerNorm, op_options:null]
        addr:0x00000000000109 prog.jump visual.blocks.7.mlp.__entry[offset:21]
        addr:0x0000000000010a prog.kernel_launch(%753:tensor<[680, 1280], Float32, CPU>, %757:tensor<[680, 1280], Float32, CPU>) -> (%758:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x0000000000010b prog.ret
        addr:0x0000000000010c prog.label[symbol:visual.blocks.7.attn.__entry]
        addr:0x0000000000010d prog.kernel_launch(%738:tensor<[680, 1280], Float32, CPU>) -> (%739:tensor<[680, 3840], Float32, CPU>)[symbol_name:visual.blocks.7.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x0000000000010e prog.kernel_launch(%739:tensor<[680, 3840], Float32, CPU>) -> (%739:tensor<[680, 3, 16, 80], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,3,16,80]}]
        addr:0x0000000000010f prog.kernel_launch(%739:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%740:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Permute, op_options:{"axis":[1,0,2,3]}]
        addr:0x00000000000110 prog.kernel_launch(%740:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%740:tensor<[3, 680, 16, 80], Float32, CPU>, %740:tensor<[3, 680, 16, 80], Float32, CPU>, %740:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Split, op_options:{"dim":0,"split_size_or_sections":[1]}]
        addr:0x00000000000111 prog.kernel_launch(%740:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%741:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.7.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x00000000000112 prog.kernel_launch(%740:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%742:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.7.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x00000000000113 prog.kernel_launch(%741:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%743:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000114 prog.kernel_launch(%742:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%744:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000115 prog.kernel_launch(%740:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%745:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000116 prog.kernel_launch(%743:tensor<[1, 16, 680, 80], Float32, CPU>, %744:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%746:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x00000000000117 prog.kernel_launch(%746:tensor<[1, 16, 680, 680], Float32, CPU>, %747:tensor<[1], Float32, CPU>) -> (%748:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x00000000000118 prog.kernel_launch(%748:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%749:tensor<[1, 16, 680, 680], Float32, CPU>)[symbol_name:visual.blocks.7.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x00000000000119 prog.kernel_launch(%749:tensor<[1, 16, 680, 680], Float32, CPU>, %745:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%750:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x0000000000011a prog.kernel_launch(%750:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%751:tensor<[1, 680, 16, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000011b prog.kernel_launch(%751:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%751:tensor<[680, 1280], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,1280]}]
        addr:0x0000000000011c prog.kernel_launch(%751:tensor<[680, 1280], Float32, CPU>) -> (%752:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.7.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x0000000000011d prog.ret
        addr:0x0000000000011e prog.label[symbol:visual.blocks.7.mlp.__entry]
        addr:0x0000000000011f prog.kernel_launch(%754:tensor<[680, 1280], Float32, CPU>) -> (%755:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.7.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x00000000000120 prog.kernel_launch(%755:tensor<[680, 5120], Float32, CPU>) -> (%756:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.7.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x00000000000121 prog.kernel_launch(%756:tensor<[680, 5120], Float32, CPU>) -> (%757:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.7.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x00000000000122 prog.ret
        addr:0x00000000000123 prog.label[symbol:visual.blocks.8.__entry]
        addr:0x00000000000124 prog.kernel_launch(%758:tensor<[680, 1280], Float32, CPU>) -> (%759:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.8.norm1, op_type:LayerNorm, op_options:null]
        addr:0x00000000000125 prog.jump visual.blocks.8.attn.__entry[offset:6]
        addr:0x00000000000126 prog.kernel_launch(%758:tensor<[680, 1280], Float32, CPU>, %773:tensor<[680, 1280], Float32, CPU>) -> (%774:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x00000000000127 prog.kernel_launch(%774:tensor<[680, 1280], Float32, CPU>) -> (%775:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.8.norm2, op_type:LayerNorm, op_options:null]
        addr:0x00000000000128 prog.jump visual.blocks.8.mlp.__entry[offset:21]
        addr:0x00000000000129 prog.kernel_launch(%774:tensor<[680, 1280], Float32, CPU>, %778:tensor<[680, 1280], Float32, CPU>) -> (%779:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x0000000000012a prog.ret
        addr:0x0000000000012b prog.label[symbol:visual.blocks.8.attn.__entry]
        addr:0x0000000000012c prog.kernel_launch(%759:tensor<[680, 1280], Float32, CPU>) -> (%760:tensor<[680, 3840], Float32, CPU>)[symbol_name:visual.blocks.8.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x0000000000012d prog.kernel_launch(%760:tensor<[680, 3840], Float32, CPU>) -> (%760:tensor<[680, 3, 16, 80], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,3,16,80]}]
        addr:0x0000000000012e prog.kernel_launch(%760:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%761:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Permute, op_options:{"axis":[1,0,2,3]}]
        addr:0x0000000000012f prog.kernel_launch(%761:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%761:tensor<[3, 680, 16, 80], Float32, CPU>, %761:tensor<[3, 680, 16, 80], Float32, CPU>, %761:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Split, op_options:{"dim":0,"split_size_or_sections":[1]}]
        addr:0x00000000000130 prog.kernel_launch(%761:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%762:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.8.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x00000000000131 prog.kernel_launch(%761:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%763:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.8.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x00000000000132 prog.kernel_launch(%762:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%764:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000133 prog.kernel_launch(%763:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%765:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000134 prog.kernel_launch(%761:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%766:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000135 prog.kernel_launch(%764:tensor<[1, 16, 680, 80], Float32, CPU>, %765:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%767:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x00000000000136 prog.kernel_launch(%767:tensor<[1, 16, 680, 680], Float32, CPU>, %768:tensor<[1], Float32, CPU>) -> (%769:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x00000000000137 prog.kernel_launch(%769:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%770:tensor<[1, 16, 680, 680], Float32, CPU>)[symbol_name:visual.blocks.8.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x00000000000138 prog.kernel_launch(%770:tensor<[1, 16, 680, 680], Float32, CPU>, %766:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%771:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x00000000000139 prog.kernel_launch(%771:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%772:tensor<[1, 680, 16, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000013a prog.kernel_launch(%772:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%772:tensor<[680, 1280], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,1280]}]
        addr:0x0000000000013b prog.kernel_launch(%772:tensor<[680, 1280], Float32, CPU>) -> (%773:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.8.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x0000000000013c prog.ret
        addr:0x0000000000013d prog.label[symbol:visual.blocks.8.mlp.__entry]
        addr:0x0000000000013e prog.kernel_launch(%775:tensor<[680, 1280], Float32, CPU>) -> (%776:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.8.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x0000000000013f prog.kernel_launch(%776:tensor<[680, 5120], Float32, CPU>) -> (%777:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.8.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x00000000000140 prog.kernel_launch(%777:tensor<[680, 5120], Float32, CPU>) -> (%778:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.8.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x00000000000141 prog.ret
        addr:0x00000000000142 prog.label[symbol:visual.blocks.9.__entry]
        addr:0x00000000000143 prog.kernel_launch(%779:tensor<[680, 1280], Float32, CPU>) -> (%780:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.9.norm1, op_type:LayerNorm, op_options:null]
        addr:0x00000000000144 prog.jump visual.blocks.9.attn.__entry[offset:6]
        addr:0x00000000000145 prog.kernel_launch(%779:tensor<[680, 1280], Float32, CPU>, %794:tensor<[680, 1280], Float32, CPU>) -> (%795:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x00000000000146 prog.kernel_launch(%795:tensor<[680, 1280], Float32, CPU>) -> (%796:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.9.norm2, op_type:LayerNorm, op_options:null]
        addr:0x00000000000147 prog.jump visual.blocks.9.mlp.__entry[offset:21]
        addr:0x00000000000148 prog.kernel_launch(%795:tensor<[680, 1280], Float32, CPU>, %799:tensor<[680, 1280], Float32, CPU>) -> (%800:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x00000000000149 prog.ret
        addr:0x0000000000014a prog.label[symbol:visual.blocks.9.attn.__entry]
        addr:0x0000000000014b prog.kernel_launch(%780:tensor<[680, 1280], Float32, CPU>) -> (%781:tensor<[680, 3840], Float32, CPU>)[symbol_name:visual.blocks.9.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x0000000000014c prog.kernel_launch(%781:tensor<[680, 3840], Float32, CPU>) -> (%781:tensor<[680, 3, 16, 80], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,3,16,80]}]
        addr:0x0000000000014d prog.kernel_launch(%781:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%782:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Permute, op_options:{"axis":[1,0,2,3]}]
        addr:0x0000000000014e prog.kernel_launch(%782:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%782:tensor<[3, 680, 16, 80], Float32, CPU>, %782:tensor<[3, 680, 16, 80], Float32, CPU>, %782:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Split, op_options:{"dim":0,"split_size_or_sections":[1]}]
        addr:0x0000000000014f prog.kernel_launch(%782:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%783:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.9.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x00000000000150 prog.kernel_launch(%782:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%784:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.9.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x00000000000151 prog.kernel_launch(%783:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%785:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000152 prog.kernel_launch(%784:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%786:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000153 prog.kernel_launch(%782:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%787:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000154 prog.kernel_launch(%785:tensor<[1, 16, 680, 80], Float32, CPU>, %786:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%788:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x00000000000155 prog.kernel_launch(%788:tensor<[1, 16, 680, 680], Float32, CPU>, %789:tensor<[1], Float32, CPU>) -> (%790:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x00000000000156 prog.kernel_launch(%790:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%791:tensor<[1, 16, 680, 680], Float32, CPU>)[symbol_name:visual.blocks.9.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x00000000000157 prog.kernel_launch(%791:tensor<[1, 16, 680, 680], Float32, CPU>, %787:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%792:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x00000000000158 prog.kernel_launch(%792:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%793:tensor<[1, 680, 16, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000159 prog.kernel_launch(%793:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%793:tensor<[680, 1280], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,1280]}]
        addr:0x0000000000015a prog.kernel_launch(%793:tensor<[680, 1280], Float32, CPU>) -> (%794:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.9.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x0000000000015b prog.ret
        addr:0x0000000000015c prog.label[symbol:visual.blocks.9.mlp.__entry]
        addr:0x0000000000015d prog.kernel_launch(%796:tensor<[680, 1280], Float32, CPU>) -> (%797:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.9.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x0000000000015e prog.kernel_launch(%797:tensor<[680, 5120], Float32, CPU>) -> (%798:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.9.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x0000000000015f prog.kernel_launch(%798:tensor<[680, 5120], Float32, CPU>) -> (%799:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.9.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x00000000000160 prog.ret
        addr:0x00000000000161 prog.label[symbol:visual.blocks.10.__entry]
        addr:0x00000000000162 prog.kernel_launch(%800:tensor<[680, 1280], Float32, CPU>) -> (%801:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.10.norm1, op_type:LayerNorm, op_options:null]
        addr:0x00000000000163 prog.jump visual.blocks.10.attn.__entry[offset:6]
        addr:0x00000000000164 prog.kernel_launch(%800:tensor<[680, 1280], Float32, CPU>, %815:tensor<[680, 1280], Float32, CPU>) -> (%816:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x00000000000165 prog.kernel_launch(%816:tensor<[680, 1280], Float32, CPU>) -> (%817:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.10.norm2, op_type:LayerNorm, op_options:null]
        addr:0x00000000000166 prog.jump visual.blocks.10.mlp.__entry[offset:21]
        addr:0x00000000000167 prog.kernel_launch(%816:tensor<[680, 1280], Float32, CPU>, %820:tensor<[680, 1280], Float32, CPU>) -> (%821:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x00000000000168 prog.ret
        addr:0x00000000000169 prog.label[symbol:visual.blocks.10.attn.__entry]
        addr:0x0000000000016a prog.kernel_launch(%801:tensor<[680, 1280], Float32, CPU>) -> (%802:tensor<[680, 3840], Float32, CPU>)[symbol_name:visual.blocks.10.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x0000000000016b prog.kernel_launch(%802:tensor<[680, 3840], Float32, CPU>) -> (%802:tensor<[680, 3, 16, 80], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,3,16,80]}]
        addr:0x0000000000016c prog.kernel_launch(%802:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%803:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Permute, op_options:{"axis":[1,0,2,3]}]
        addr:0x0000000000016d prog.kernel_launch(%803:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%803:tensor<[3, 680, 16, 80], Float32, CPU>, %803:tensor<[3, 680, 16, 80], Float32, CPU>, %803:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Split, op_options:{"dim":0,"split_size_or_sections":[1]}]
        addr:0x0000000000016e prog.kernel_launch(%803:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%804:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.10.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x0000000000016f prog.kernel_launch(%803:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%805:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.10.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x00000000000170 prog.kernel_launch(%804:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%806:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000171 prog.kernel_launch(%805:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%807:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000172 prog.kernel_launch(%803:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%808:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000173 prog.kernel_launch(%806:tensor<[1, 16, 680, 80], Float32, CPU>, %807:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%809:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x00000000000174 prog.kernel_launch(%809:tensor<[1, 16, 680, 680], Float32, CPU>, %810:tensor<[1], Float32, CPU>) -> (%811:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x00000000000175 prog.kernel_launch(%811:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%812:tensor<[1, 16, 680, 680], Float32, CPU>)[symbol_name:visual.blocks.10.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x00000000000176 prog.kernel_launch(%812:tensor<[1, 16, 680, 680], Float32, CPU>, %808:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%813:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x00000000000177 prog.kernel_launch(%813:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%814:tensor<[1, 680, 16, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000178 prog.kernel_launch(%814:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%814:tensor<[680, 1280], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,1280]}]
        addr:0x00000000000179 prog.kernel_launch(%814:tensor<[680, 1280], Float32, CPU>) -> (%815:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.10.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x0000000000017a prog.ret
        addr:0x0000000000017b prog.label[symbol:visual.blocks.10.mlp.__entry]
        addr:0x0000000000017c prog.kernel_launch(%817:tensor<[680, 1280], Float32, CPU>) -> (%818:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.10.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x0000000000017d prog.kernel_launch(%818:tensor<[680, 5120], Float32, CPU>) -> (%819:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.10.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x0000000000017e prog.kernel_launch(%819:tensor<[680, 5120], Float32, CPU>) -> (%820:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.10.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x0000000000017f prog.ret
        addr:0x00000000000180 prog.label[symbol:visual.blocks.11.__entry]
        addr:0x00000000000181 prog.kernel_launch(%821:tensor<[680, 1280], Float32, CPU>) -> (%822:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.11.norm1, op_type:LayerNorm, op_options:null]
        addr:0x00000000000182 prog.jump visual.blocks.11.attn.__entry[offset:6]
        addr:0x00000000000183 prog.kernel_launch(%821:tensor<[680, 1280], Float32, CPU>, %836:tensor<[680, 1280], Float32, CPU>) -> (%837:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x00000000000184 prog.kernel_launch(%837:tensor<[680, 1280], Float32, CPU>) -> (%838:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.11.norm2, op_type:LayerNorm, op_options:null]
        addr:0x00000000000185 prog.jump visual.blocks.11.mlp.__entry[offset:21]
        addr:0x00000000000186 prog.kernel_launch(%837:tensor<[680, 1280], Float32, CPU>, %841:tensor<[680, 1280], Float32, CPU>) -> (%842:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x00000000000187 prog.ret
        addr:0x00000000000188 prog.label[symbol:visual.blocks.11.attn.__entry]
        addr:0x00000000000189 prog.kernel_launch(%822:tensor<[680, 1280], Float32, CPU>) -> (%823:tensor<[680, 3840], Float32, CPU>)[symbol_name:visual.blocks.11.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x0000000000018a prog.kernel_launch(%823:tensor<[680, 3840], Float32, CPU>) -> (%823:tensor<[680, 3, 16, 80], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,3,16,80]}]
        addr:0x0000000000018b prog.kernel_launch(%823:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%824:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Permute, op_options:{"axis":[1,0,2,3]}]
        addr:0x0000000000018c prog.kernel_launch(%824:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%824:tensor<[3, 680, 16, 80], Float32, CPU>, %824:tensor<[3, 680, 16, 80], Float32, CPU>, %824:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Split, op_options:{"dim":0,"split_size_or_sections":[1]}]
        addr:0x0000000000018d prog.kernel_launch(%824:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%825:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.11.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x0000000000018e prog.kernel_launch(%824:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%826:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.11.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x0000000000018f prog.kernel_launch(%825:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%827:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000190 prog.kernel_launch(%826:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%828:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000191 prog.kernel_launch(%824:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%829:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000192 prog.kernel_launch(%827:tensor<[1, 16, 680, 80], Float32, CPU>, %828:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%830:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x00000000000193 prog.kernel_launch(%830:tensor<[1, 16, 680, 680], Float32, CPU>, %831:tensor<[1], Float32, CPU>) -> (%832:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x00000000000194 prog.kernel_launch(%832:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%833:tensor<[1, 16, 680, 680], Float32, CPU>)[symbol_name:visual.blocks.11.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x00000000000195 prog.kernel_launch(%833:tensor<[1, 16, 680, 680], Float32, CPU>, %829:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%834:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x00000000000196 prog.kernel_launch(%834:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%835:tensor<[1, 680, 16, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000197 prog.kernel_launch(%835:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%835:tensor<[680, 1280], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,1280]}]
        addr:0x00000000000198 prog.kernel_launch(%835:tensor<[680, 1280], Float32, CPU>) -> (%836:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.11.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x00000000000199 prog.ret
        addr:0x0000000000019a prog.label[symbol:visual.blocks.11.mlp.__entry]
        addr:0x0000000000019b prog.kernel_launch(%838:tensor<[680, 1280], Float32, CPU>) -> (%839:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.11.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x0000000000019c prog.kernel_launch(%839:tensor<[680, 5120], Float32, CPU>) -> (%840:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.11.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x0000000000019d prog.kernel_launch(%840:tensor<[680, 5120], Float32, CPU>) -> (%841:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.11.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x0000000000019e prog.ret
        addr:0x0000000000019f prog.label[symbol:visual.blocks.12.__entry]
        addr:0x000000000001a0 prog.kernel_launch(%842:tensor<[680, 1280], Float32, CPU>) -> (%843:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.12.norm1, op_type:LayerNorm, op_options:null]
        addr:0x000000000001a1 prog.jump visual.blocks.12.attn.__entry[offset:6]
        addr:0x000000000001a2 prog.kernel_launch(%842:tensor<[680, 1280], Float32, CPU>, %857:tensor<[680, 1280], Float32, CPU>) -> (%858:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000001a3 prog.kernel_launch(%858:tensor<[680, 1280], Float32, CPU>) -> (%859:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.12.norm2, op_type:LayerNorm, op_options:null]
        addr:0x000000000001a4 prog.jump visual.blocks.12.mlp.__entry[offset:21]
        addr:0x000000000001a5 prog.kernel_launch(%858:tensor<[680, 1280], Float32, CPU>, %862:tensor<[680, 1280], Float32, CPU>) -> (%863:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000001a6 prog.ret
        addr:0x000000000001a7 prog.label[symbol:visual.blocks.12.attn.__entry]
        addr:0x000000000001a8 prog.kernel_launch(%843:tensor<[680, 1280], Float32, CPU>) -> (%844:tensor<[680, 3840], Float32, CPU>)[symbol_name:visual.blocks.12.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x000000000001a9 prog.kernel_launch(%844:tensor<[680, 3840], Float32, CPU>) -> (%844:tensor<[680, 3, 16, 80], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,3,16,80]}]
        addr:0x000000000001aa prog.kernel_launch(%844:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%845:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Permute, op_options:{"axis":[1,0,2,3]}]
        addr:0x000000000001ab prog.kernel_launch(%845:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%845:tensor<[3, 680, 16, 80], Float32, CPU>, %845:tensor<[3, 680, 16, 80], Float32, CPU>, %845:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Split, op_options:{"dim":0,"split_size_or_sections":[1]}]
        addr:0x000000000001ac prog.kernel_launch(%845:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%846:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.12.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x000000000001ad prog.kernel_launch(%845:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%847:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.12.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x000000000001ae prog.kernel_launch(%846:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%848:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000001af prog.kernel_launch(%847:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%849:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000001b0 prog.kernel_launch(%845:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%850:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000001b1 prog.kernel_launch(%848:tensor<[1, 16, 680, 80], Float32, CPU>, %849:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%851:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x000000000001b2 prog.kernel_launch(%851:tensor<[1, 16, 680, 680], Float32, CPU>, %852:tensor<[1], Float32, CPU>) -> (%853:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x000000000001b3 prog.kernel_launch(%853:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%854:tensor<[1, 16, 680, 680], Float32, CPU>)[symbol_name:visual.blocks.12.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x000000000001b4 prog.kernel_launch(%854:tensor<[1, 16, 680, 680], Float32, CPU>, %850:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%855:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x000000000001b5 prog.kernel_launch(%855:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%856:tensor<[1, 680, 16, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000001b6 prog.kernel_launch(%856:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%856:tensor<[680, 1280], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,1280]}]
        addr:0x000000000001b7 prog.kernel_launch(%856:tensor<[680, 1280], Float32, CPU>) -> (%857:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.12.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x000000000001b8 prog.ret
        addr:0x000000000001b9 prog.label[symbol:visual.blocks.12.mlp.__entry]
        addr:0x000000000001ba prog.kernel_launch(%859:tensor<[680, 1280], Float32, CPU>) -> (%860:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.12.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x000000000001bb prog.kernel_launch(%860:tensor<[680, 5120], Float32, CPU>) -> (%861:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.12.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x000000000001bc prog.kernel_launch(%861:tensor<[680, 5120], Float32, CPU>) -> (%862:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.12.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x000000000001bd prog.ret
        addr:0x000000000001be prog.label[symbol:visual.blocks.13.__entry]
        addr:0x000000000001bf prog.kernel_launch(%863:tensor<[680, 1280], Float32, CPU>) -> (%864:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.13.norm1, op_type:LayerNorm, op_options:null]
        addr:0x000000000001c0 prog.jump visual.blocks.13.attn.__entry[offset:6]
        addr:0x000000000001c1 prog.kernel_launch(%863:tensor<[680, 1280], Float32, CPU>, %878:tensor<[680, 1280], Float32, CPU>) -> (%879:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000001c2 prog.kernel_launch(%879:tensor<[680, 1280], Float32, CPU>) -> (%880:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.13.norm2, op_type:LayerNorm, op_options:null]
        addr:0x000000000001c3 prog.jump visual.blocks.13.mlp.__entry[offset:21]
        addr:0x000000000001c4 prog.kernel_launch(%879:tensor<[680, 1280], Float32, CPU>, %883:tensor<[680, 1280], Float32, CPU>) -> (%884:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000001c5 prog.ret
        addr:0x000000000001c6 prog.label[symbol:visual.blocks.13.attn.__entry]
        addr:0x000000000001c7 prog.kernel_launch(%864:tensor<[680, 1280], Float32, CPU>) -> (%865:tensor<[680, 3840], Float32, CPU>)[symbol_name:visual.blocks.13.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x000000000001c8 prog.kernel_launch(%865:tensor<[680, 3840], Float32, CPU>) -> (%865:tensor<[680, 3, 16, 80], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,3,16,80]}]
        addr:0x000000000001c9 prog.kernel_launch(%865:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%866:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Permute, op_options:{"axis":[1,0,2,3]}]
        addr:0x000000000001ca prog.kernel_launch(%866:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%866:tensor<[3, 680, 16, 80], Float32, CPU>, %866:tensor<[3, 680, 16, 80], Float32, CPU>, %866:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Split, op_options:{"dim":0,"split_size_or_sections":[1]}]
        addr:0x000000000001cb prog.kernel_launch(%866:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%867:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.13.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x000000000001cc prog.kernel_launch(%866:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%868:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.13.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x000000000001cd prog.kernel_launch(%867:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%869:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000001ce prog.kernel_launch(%868:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%870:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000001cf prog.kernel_launch(%866:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%871:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000001d0 prog.kernel_launch(%869:tensor<[1, 16, 680, 80], Float32, CPU>, %870:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%872:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x000000000001d1 prog.kernel_launch(%872:tensor<[1, 16, 680, 680], Float32, CPU>, %873:tensor<[1], Float32, CPU>) -> (%874:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x000000000001d2 prog.kernel_launch(%874:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%875:tensor<[1, 16, 680, 680], Float32, CPU>)[symbol_name:visual.blocks.13.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x000000000001d3 prog.kernel_launch(%875:tensor<[1, 16, 680, 680], Float32, CPU>, %871:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%876:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x000000000001d4 prog.kernel_launch(%876:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%877:tensor<[1, 680, 16, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000001d5 prog.kernel_launch(%877:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%877:tensor<[680, 1280], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,1280]}]
        addr:0x000000000001d6 prog.kernel_launch(%877:tensor<[680, 1280], Float32, CPU>) -> (%878:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.13.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x000000000001d7 prog.ret
        addr:0x000000000001d8 prog.label[symbol:visual.blocks.13.mlp.__entry]
        addr:0x000000000001d9 prog.kernel_launch(%880:tensor<[680, 1280], Float32, CPU>) -> (%881:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.13.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x000000000001da prog.kernel_launch(%881:tensor<[680, 5120], Float32, CPU>) -> (%882:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.13.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x000000000001db prog.kernel_launch(%882:tensor<[680, 5120], Float32, CPU>) -> (%883:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.13.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x000000000001dc prog.ret
        addr:0x000000000001dd prog.label[symbol:visual.blocks.14.__entry]
        addr:0x000000000001de prog.kernel_launch(%884:tensor<[680, 1280], Float32, CPU>) -> (%885:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.14.norm1, op_type:LayerNorm, op_options:null]
        addr:0x000000000001df prog.jump visual.blocks.14.attn.__entry[offset:6]
        addr:0x000000000001e0 prog.kernel_launch(%884:tensor<[680, 1280], Float32, CPU>, %899:tensor<[680, 1280], Float32, CPU>) -> (%900:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000001e1 prog.kernel_launch(%900:tensor<[680, 1280], Float32, CPU>) -> (%901:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.14.norm2, op_type:LayerNorm, op_options:null]
        addr:0x000000000001e2 prog.jump visual.blocks.14.mlp.__entry[offset:21]
        addr:0x000000000001e3 prog.kernel_launch(%900:tensor<[680, 1280], Float32, CPU>, %904:tensor<[680, 1280], Float32, CPU>) -> (%905:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000001e4 prog.ret
        addr:0x000000000001e5 prog.label[symbol:visual.blocks.14.attn.__entry]
        addr:0x000000000001e6 prog.kernel_launch(%885:tensor<[680, 1280], Float32, CPU>) -> (%886:tensor<[680, 3840], Float32, CPU>)[symbol_name:visual.blocks.14.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x000000000001e7 prog.kernel_launch(%886:tensor<[680, 3840], Float32, CPU>) -> (%886:tensor<[680, 3, 16, 80], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,3,16,80]}]
        addr:0x000000000001e8 prog.kernel_launch(%886:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%887:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Permute, op_options:{"axis":[1,0,2,3]}]
        addr:0x000000000001e9 prog.kernel_launch(%887:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%887:tensor<[3, 680, 16, 80], Float32, CPU>, %887:tensor<[3, 680, 16, 80], Float32, CPU>, %887:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Split, op_options:{"dim":0,"split_size_or_sections":[1]}]
        addr:0x000000000001ea prog.kernel_launch(%887:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%888:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.14.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x000000000001eb prog.kernel_launch(%887:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%889:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.14.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x000000000001ec prog.kernel_launch(%888:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%890:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000001ed prog.kernel_launch(%889:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%891:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000001ee prog.kernel_launch(%887:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%892:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000001ef prog.kernel_launch(%890:tensor<[1, 16, 680, 80], Float32, CPU>, %891:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%893:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x000000000001f0 prog.kernel_launch(%893:tensor<[1, 16, 680, 680], Float32, CPU>, %894:tensor<[1], Float32, CPU>) -> (%895:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x000000000001f1 prog.kernel_launch(%895:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%896:tensor<[1, 16, 680, 680], Float32, CPU>)[symbol_name:visual.blocks.14.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x000000000001f2 prog.kernel_launch(%896:tensor<[1, 16, 680, 680], Float32, CPU>, %892:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%897:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x000000000001f3 prog.kernel_launch(%897:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%898:tensor<[1, 680, 16, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000001f4 prog.kernel_launch(%898:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%898:tensor<[680, 1280], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,1280]}]
        addr:0x000000000001f5 prog.kernel_launch(%898:tensor<[680, 1280], Float32, CPU>) -> (%899:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.14.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x000000000001f6 prog.ret
        addr:0x000000000001f7 prog.label[symbol:visual.blocks.14.mlp.__entry]
        addr:0x000000000001f8 prog.kernel_launch(%901:tensor<[680, 1280], Float32, CPU>) -> (%902:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.14.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x000000000001f9 prog.kernel_launch(%902:tensor<[680, 5120], Float32, CPU>) -> (%903:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.14.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x000000000001fa prog.kernel_launch(%903:tensor<[680, 5120], Float32, CPU>) -> (%904:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.14.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x000000000001fb prog.ret
        addr:0x000000000001fc prog.label[symbol:visual.blocks.15.__entry]
        addr:0x000000000001fd prog.kernel_launch(%905:tensor<[680, 1280], Float32, CPU>) -> (%906:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.15.norm1, op_type:LayerNorm, op_options:null]
        addr:0x000000000001fe prog.jump visual.blocks.15.attn.__entry[offset:6]
        addr:0x000000000001ff prog.kernel_launch(%905:tensor<[680, 1280], Float32, CPU>, %920:tensor<[680, 1280], Float32, CPU>) -> (%921:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x00000000000200 prog.kernel_launch(%921:tensor<[680, 1280], Float32, CPU>) -> (%922:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.15.norm2, op_type:LayerNorm, op_options:null]
        addr:0x00000000000201 prog.jump visual.blocks.15.mlp.__entry[offset:21]
        addr:0x00000000000202 prog.kernel_launch(%921:tensor<[680, 1280], Float32, CPU>, %925:tensor<[680, 1280], Float32, CPU>) -> (%926:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x00000000000203 prog.ret
        addr:0x00000000000204 prog.label[symbol:visual.blocks.15.attn.__entry]
        addr:0x00000000000205 prog.kernel_launch(%906:tensor<[680, 1280], Float32, CPU>) -> (%907:tensor<[680, 3840], Float32, CPU>)[symbol_name:visual.blocks.15.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x00000000000206 prog.kernel_launch(%907:tensor<[680, 3840], Float32, CPU>) -> (%907:tensor<[680, 3, 16, 80], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,3,16,80]}]
        addr:0x00000000000207 prog.kernel_launch(%907:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%908:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Permute, op_options:{"axis":[1,0,2,3]}]
        addr:0x00000000000208 prog.kernel_launch(%908:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%908:tensor<[3, 680, 16, 80], Float32, CPU>, %908:tensor<[3, 680, 16, 80], Float32, CPU>, %908:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Split, op_options:{"dim":0,"split_size_or_sections":[1]}]
        addr:0x00000000000209 prog.kernel_launch(%908:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%909:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.15.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x0000000000020a prog.kernel_launch(%908:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%910:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.15.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x0000000000020b prog.kernel_launch(%909:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%911:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000020c prog.kernel_launch(%910:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%912:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000020d prog.kernel_launch(%908:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%913:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000020e prog.kernel_launch(%911:tensor<[1, 16, 680, 80], Float32, CPU>, %912:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%914:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x0000000000020f prog.kernel_launch(%914:tensor<[1, 16, 680, 680], Float32, CPU>, %915:tensor<[1], Float32, CPU>) -> (%916:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x00000000000210 prog.kernel_launch(%916:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%917:tensor<[1, 16, 680, 680], Float32, CPU>)[symbol_name:visual.blocks.15.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x00000000000211 prog.kernel_launch(%917:tensor<[1, 16, 680, 680], Float32, CPU>, %913:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%918:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x00000000000212 prog.kernel_launch(%918:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%919:tensor<[1, 680, 16, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000213 prog.kernel_launch(%919:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%919:tensor<[680, 1280], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,1280]}]
        addr:0x00000000000214 prog.kernel_launch(%919:tensor<[680, 1280], Float32, CPU>) -> (%920:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.15.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x00000000000215 prog.ret
        addr:0x00000000000216 prog.label[symbol:visual.blocks.15.mlp.__entry]
        addr:0x00000000000217 prog.kernel_launch(%922:tensor<[680, 1280], Float32, CPU>) -> (%923:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.15.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x00000000000218 prog.kernel_launch(%923:tensor<[680, 5120], Float32, CPU>) -> (%924:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.15.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x00000000000219 prog.kernel_launch(%924:tensor<[680, 5120], Float32, CPU>) -> (%925:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.15.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x0000000000021a prog.ret
        addr:0x0000000000021b prog.label[symbol:visual.blocks.16.__entry]
        addr:0x0000000000021c prog.kernel_launch(%926:tensor<[680, 1280], Float32, CPU>) -> (%927:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.16.norm1, op_type:LayerNorm, op_options:null]
        addr:0x0000000000021d prog.jump visual.blocks.16.attn.__entry[offset:6]
        addr:0x0000000000021e prog.kernel_launch(%926:tensor<[680, 1280], Float32, CPU>, %941:tensor<[680, 1280], Float32, CPU>) -> (%942:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x0000000000021f prog.kernel_launch(%942:tensor<[680, 1280], Float32, CPU>) -> (%943:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.16.norm2, op_type:LayerNorm, op_options:null]
        addr:0x00000000000220 prog.jump visual.blocks.16.mlp.__entry[offset:21]
        addr:0x00000000000221 prog.kernel_launch(%942:tensor<[680, 1280], Float32, CPU>, %946:tensor<[680, 1280], Float32, CPU>) -> (%947:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x00000000000222 prog.ret
        addr:0x00000000000223 prog.label[symbol:visual.blocks.16.attn.__entry]
        addr:0x00000000000224 prog.kernel_launch(%927:tensor<[680, 1280], Float32, CPU>) -> (%928:tensor<[680, 3840], Float32, CPU>)[symbol_name:visual.blocks.16.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x00000000000225 prog.kernel_launch(%928:tensor<[680, 3840], Float32, CPU>) -> (%928:tensor<[680, 3, 16, 80], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,3,16,80]}]
        addr:0x00000000000226 prog.kernel_launch(%928:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%929:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Permute, op_options:{"axis":[1,0,2,3]}]
        addr:0x00000000000227 prog.kernel_launch(%929:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%929:tensor<[3, 680, 16, 80], Float32, CPU>, %929:tensor<[3, 680, 16, 80], Float32, CPU>, %929:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Split, op_options:{"dim":0,"split_size_or_sections":[1]}]
        addr:0x00000000000228 prog.kernel_launch(%929:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%930:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.16.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x00000000000229 prog.kernel_launch(%929:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%931:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.16.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x0000000000022a prog.kernel_launch(%930:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%932:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000022b prog.kernel_launch(%931:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%933:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000022c prog.kernel_launch(%929:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%934:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000022d prog.kernel_launch(%932:tensor<[1, 16, 680, 80], Float32, CPU>, %933:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%935:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x0000000000022e prog.kernel_launch(%935:tensor<[1, 16, 680, 680], Float32, CPU>, %936:tensor<[1], Float32, CPU>) -> (%937:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x0000000000022f prog.kernel_launch(%937:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%938:tensor<[1, 16, 680, 680], Float32, CPU>)[symbol_name:visual.blocks.16.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x00000000000230 prog.kernel_launch(%938:tensor<[1, 16, 680, 680], Float32, CPU>, %934:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%939:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x00000000000231 prog.kernel_launch(%939:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%940:tensor<[1, 680, 16, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000232 prog.kernel_launch(%940:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%940:tensor<[680, 1280], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,1280]}]
        addr:0x00000000000233 prog.kernel_launch(%940:tensor<[680, 1280], Float32, CPU>) -> (%941:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.16.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x00000000000234 prog.ret
        addr:0x00000000000235 prog.label[symbol:visual.blocks.16.mlp.__entry]
        addr:0x00000000000236 prog.kernel_launch(%943:tensor<[680, 1280], Float32, CPU>) -> (%944:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.16.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x00000000000237 prog.kernel_launch(%944:tensor<[680, 5120], Float32, CPU>) -> (%945:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.16.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x00000000000238 prog.kernel_launch(%945:tensor<[680, 5120], Float32, CPU>) -> (%946:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.16.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x00000000000239 prog.ret
        addr:0x0000000000023a prog.label[symbol:visual.blocks.17.__entry]
        addr:0x0000000000023b prog.kernel_launch(%947:tensor<[680, 1280], Float32, CPU>) -> (%948:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.17.norm1, op_type:LayerNorm, op_options:null]
        addr:0x0000000000023c prog.jump visual.blocks.17.attn.__entry[offset:6]
        addr:0x0000000000023d prog.kernel_launch(%947:tensor<[680, 1280], Float32, CPU>, %962:tensor<[680, 1280], Float32, CPU>) -> (%963:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x0000000000023e prog.kernel_launch(%963:tensor<[680, 1280], Float32, CPU>) -> (%964:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.17.norm2, op_type:LayerNorm, op_options:null]
        addr:0x0000000000023f prog.jump visual.blocks.17.mlp.__entry[offset:21]
        addr:0x00000000000240 prog.kernel_launch(%963:tensor<[680, 1280], Float32, CPU>, %967:tensor<[680, 1280], Float32, CPU>) -> (%968:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x00000000000241 prog.ret
        addr:0x00000000000242 prog.label[symbol:visual.blocks.17.attn.__entry]
        addr:0x00000000000243 prog.kernel_launch(%948:tensor<[680, 1280], Float32, CPU>) -> (%949:tensor<[680, 3840], Float32, CPU>)[symbol_name:visual.blocks.17.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x00000000000244 prog.kernel_launch(%949:tensor<[680, 3840], Float32, CPU>) -> (%949:tensor<[680, 3, 16, 80], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,3,16,80]}]
        addr:0x00000000000245 prog.kernel_launch(%949:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%950:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Permute, op_options:{"axis":[1,0,2,3]}]
        addr:0x00000000000246 prog.kernel_launch(%950:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%950:tensor<[3, 680, 16, 80], Float32, CPU>, %950:tensor<[3, 680, 16, 80], Float32, CPU>, %950:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Split, op_options:{"dim":0,"split_size_or_sections":[1]}]
        addr:0x00000000000247 prog.kernel_launch(%950:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%951:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.17.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x00000000000248 prog.kernel_launch(%950:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%952:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.17.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x00000000000249 prog.kernel_launch(%951:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%953:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000024a prog.kernel_launch(%952:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%954:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000024b prog.kernel_launch(%950:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%955:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000024c prog.kernel_launch(%953:tensor<[1, 16, 680, 80], Float32, CPU>, %954:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%956:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x0000000000024d prog.kernel_launch(%956:tensor<[1, 16, 680, 680], Float32, CPU>, %957:tensor<[1], Float32, CPU>) -> (%958:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x0000000000024e prog.kernel_launch(%958:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%959:tensor<[1, 16, 680, 680], Float32, CPU>)[symbol_name:visual.blocks.17.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x0000000000024f prog.kernel_launch(%959:tensor<[1, 16, 680, 680], Float32, CPU>, %955:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%960:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x00000000000250 prog.kernel_launch(%960:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%961:tensor<[1, 680, 16, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000251 prog.kernel_launch(%961:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%961:tensor<[680, 1280], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,1280]}]
        addr:0x00000000000252 prog.kernel_launch(%961:tensor<[680, 1280], Float32, CPU>) -> (%962:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.17.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x00000000000253 prog.ret
        addr:0x00000000000254 prog.label[symbol:visual.blocks.17.mlp.__entry]
        addr:0x00000000000255 prog.kernel_launch(%964:tensor<[680, 1280], Float32, CPU>) -> (%965:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.17.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x00000000000256 prog.kernel_launch(%965:tensor<[680, 5120], Float32, CPU>) -> (%966:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.17.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x00000000000257 prog.kernel_launch(%966:tensor<[680, 5120], Float32, CPU>) -> (%967:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.17.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x00000000000258 prog.ret
        addr:0x00000000000259 prog.label[symbol:visual.blocks.18.__entry]
        addr:0x0000000000025a prog.kernel_launch(%968:tensor<[680, 1280], Float32, CPU>) -> (%969:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.18.norm1, op_type:LayerNorm, op_options:null]
        addr:0x0000000000025b prog.jump visual.blocks.18.attn.__entry[offset:6]
        addr:0x0000000000025c prog.kernel_launch(%968:tensor<[680, 1280], Float32, CPU>, %983:tensor<[680, 1280], Float32, CPU>) -> (%984:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x0000000000025d prog.kernel_launch(%984:tensor<[680, 1280], Float32, CPU>) -> (%985:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.18.norm2, op_type:LayerNorm, op_options:null]
        addr:0x0000000000025e prog.jump visual.blocks.18.mlp.__entry[offset:21]
        addr:0x0000000000025f prog.kernel_launch(%984:tensor<[680, 1280], Float32, CPU>, %988:tensor<[680, 1280], Float32, CPU>) -> (%989:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x00000000000260 prog.ret
        addr:0x00000000000261 prog.label[symbol:visual.blocks.18.attn.__entry]
        addr:0x00000000000262 prog.kernel_launch(%969:tensor<[680, 1280], Float32, CPU>) -> (%970:tensor<[680, 3840], Float32, CPU>)[symbol_name:visual.blocks.18.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x00000000000263 prog.kernel_launch(%970:tensor<[680, 3840], Float32, CPU>) -> (%970:tensor<[680, 3, 16, 80], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,3,16,80]}]
        addr:0x00000000000264 prog.kernel_launch(%970:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%971:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Permute, op_options:{"axis":[1,0,2,3]}]
        addr:0x00000000000265 prog.kernel_launch(%971:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%971:tensor<[3, 680, 16, 80], Float32, CPU>, %971:tensor<[3, 680, 16, 80], Float32, CPU>, %971:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Split, op_options:{"dim":0,"split_size_or_sections":[1]}]
        addr:0x00000000000266 prog.kernel_launch(%971:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%972:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.18.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x00000000000267 prog.kernel_launch(%971:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%973:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.18.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x00000000000268 prog.kernel_launch(%972:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%974:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000269 prog.kernel_launch(%973:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%975:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000026a prog.kernel_launch(%971:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%976:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000026b prog.kernel_launch(%974:tensor<[1, 16, 680, 80], Float32, CPU>, %975:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%977:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x0000000000026c prog.kernel_launch(%977:tensor<[1, 16, 680, 680], Float32, CPU>, %978:tensor<[1], Float32, CPU>) -> (%979:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x0000000000026d prog.kernel_launch(%979:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%980:tensor<[1, 16, 680, 680], Float32, CPU>)[symbol_name:visual.blocks.18.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x0000000000026e prog.kernel_launch(%980:tensor<[1, 16, 680, 680], Float32, CPU>, %976:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%981:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x0000000000026f prog.kernel_launch(%981:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%982:tensor<[1, 680, 16, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000270 prog.kernel_launch(%982:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%982:tensor<[680, 1280], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,1280]}]
        addr:0x00000000000271 prog.kernel_launch(%982:tensor<[680, 1280], Float32, CPU>) -> (%983:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.18.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x00000000000272 prog.ret
        addr:0x00000000000273 prog.label[symbol:visual.blocks.18.mlp.__entry]
        addr:0x00000000000274 prog.kernel_launch(%985:tensor<[680, 1280], Float32, CPU>) -> (%986:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.18.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x00000000000275 prog.kernel_launch(%986:tensor<[680, 5120], Float32, CPU>) -> (%987:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.18.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x00000000000276 prog.kernel_launch(%987:tensor<[680, 5120], Float32, CPU>) -> (%988:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.18.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x00000000000277 prog.ret
        addr:0x00000000000278 prog.label[symbol:visual.blocks.19.__entry]
        addr:0x00000000000279 prog.kernel_launch(%989:tensor<[680, 1280], Float32, CPU>) -> (%990:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.19.norm1, op_type:LayerNorm, op_options:null]
        addr:0x0000000000027a prog.jump visual.blocks.19.attn.__entry[offset:6]
        addr:0x0000000000027b prog.kernel_launch(%989:tensor<[680, 1280], Float32, CPU>, %1004:tensor<[680, 1280], Float32, CPU>) -> (%1005:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x0000000000027c prog.kernel_launch(%1005:tensor<[680, 1280], Float32, CPU>) -> (%1006:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.19.norm2, op_type:LayerNorm, op_options:null]
        addr:0x0000000000027d prog.jump visual.blocks.19.mlp.__entry[offset:21]
        addr:0x0000000000027e prog.kernel_launch(%1005:tensor<[680, 1280], Float32, CPU>, %1009:tensor<[680, 1280], Float32, CPU>) -> (%1010:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x0000000000027f prog.ret
        addr:0x00000000000280 prog.label[symbol:visual.blocks.19.attn.__entry]
        addr:0x00000000000281 prog.kernel_launch(%990:tensor<[680, 1280], Float32, CPU>) -> (%991:tensor<[680, 3840], Float32, CPU>)[symbol_name:visual.blocks.19.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x00000000000282 prog.kernel_launch(%991:tensor<[680, 3840], Float32, CPU>) -> (%991:tensor<[680, 3, 16, 80], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,3,16,80]}]
        addr:0x00000000000283 prog.kernel_launch(%991:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%992:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Permute, op_options:{"axis":[1,0,2,3]}]
        addr:0x00000000000284 prog.kernel_launch(%992:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%992:tensor<[3, 680, 16, 80], Float32, CPU>, %992:tensor<[3, 680, 16, 80], Float32, CPU>, %992:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Split, op_options:{"dim":0,"split_size_or_sections":[1]}]
        addr:0x00000000000285 prog.kernel_launch(%992:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%993:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.19.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x00000000000286 prog.kernel_launch(%992:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%994:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.19.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x00000000000287 prog.kernel_launch(%993:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%995:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000288 prog.kernel_launch(%994:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%996:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000289 prog.kernel_launch(%992:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%997:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000028a prog.kernel_launch(%995:tensor<[1, 16, 680, 80], Float32, CPU>, %996:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%998:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x0000000000028b prog.kernel_launch(%998:tensor<[1, 16, 680, 680], Float32, CPU>, %999:tensor<[1], Float32, CPU>) -> (%1000:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x0000000000028c prog.kernel_launch(%1000:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%1001:tensor<[1, 16, 680, 680], Float32, CPU>)[symbol_name:visual.blocks.19.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x0000000000028d prog.kernel_launch(%1001:tensor<[1, 16, 680, 680], Float32, CPU>, %997:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1002:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x0000000000028e prog.kernel_launch(%1002:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1003:tensor<[1, 680, 16, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000028f prog.kernel_launch(%1003:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1003:tensor<[680, 1280], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,1280]}]
        addr:0x00000000000290 prog.kernel_launch(%1003:tensor<[680, 1280], Float32, CPU>) -> (%1004:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.19.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x00000000000291 prog.ret
        addr:0x00000000000292 prog.label[symbol:visual.blocks.19.mlp.__entry]
        addr:0x00000000000293 prog.kernel_launch(%1006:tensor<[680, 1280], Float32, CPU>) -> (%1007:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.19.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x00000000000294 prog.kernel_launch(%1007:tensor<[680, 5120], Float32, CPU>) -> (%1008:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.19.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x00000000000295 prog.kernel_launch(%1008:tensor<[680, 5120], Float32, CPU>) -> (%1009:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.19.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x00000000000296 prog.ret
        addr:0x00000000000297 prog.label[symbol:visual.blocks.20.__entry]
        addr:0x00000000000298 prog.kernel_launch(%1010:tensor<[680, 1280], Float32, CPU>) -> (%1011:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.20.norm1, op_type:LayerNorm, op_options:null]
        addr:0x00000000000299 prog.jump visual.blocks.20.attn.__entry[offset:6]
        addr:0x0000000000029a prog.kernel_launch(%1010:tensor<[680, 1280], Float32, CPU>, %1025:tensor<[680, 1280], Float32, CPU>) -> (%1026:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x0000000000029b prog.kernel_launch(%1026:tensor<[680, 1280], Float32, CPU>) -> (%1027:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.20.norm2, op_type:LayerNorm, op_options:null]
        addr:0x0000000000029c prog.jump visual.blocks.20.mlp.__entry[offset:21]
        addr:0x0000000000029d prog.kernel_launch(%1026:tensor<[680, 1280], Float32, CPU>, %1030:tensor<[680, 1280], Float32, CPU>) -> (%1031:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x0000000000029e prog.ret
        addr:0x0000000000029f prog.label[symbol:visual.blocks.20.attn.__entry]
        addr:0x000000000002a0 prog.kernel_launch(%1011:tensor<[680, 1280], Float32, CPU>) -> (%1012:tensor<[680, 3840], Float32, CPU>)[symbol_name:visual.blocks.20.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x000000000002a1 prog.kernel_launch(%1012:tensor<[680, 3840], Float32, CPU>) -> (%1012:tensor<[680, 3, 16, 80], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,3,16,80]}]
        addr:0x000000000002a2 prog.kernel_launch(%1012:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%1013:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Permute, op_options:{"axis":[1,0,2,3]}]
        addr:0x000000000002a3 prog.kernel_launch(%1013:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%1013:tensor<[3, 680, 16, 80], Float32, CPU>, %1013:tensor<[3, 680, 16, 80], Float32, CPU>, %1013:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Split, op_options:{"dim":0,"split_size_or_sections":[1]}]
        addr:0x000000000002a4 prog.kernel_launch(%1013:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1014:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.20.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x000000000002a5 prog.kernel_launch(%1013:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1015:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.20.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x000000000002a6 prog.kernel_launch(%1014:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1016:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000002a7 prog.kernel_launch(%1015:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1017:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000002a8 prog.kernel_launch(%1013:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%1018:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000002a9 prog.kernel_launch(%1016:tensor<[1, 16, 680, 80], Float32, CPU>, %1017:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1019:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x000000000002aa prog.kernel_launch(%1019:tensor<[1, 16, 680, 680], Float32, CPU>, %1020:tensor<[1], Float32, CPU>) -> (%1021:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x000000000002ab prog.kernel_launch(%1021:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%1022:tensor<[1, 16, 680, 680], Float32, CPU>)[symbol_name:visual.blocks.20.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x000000000002ac prog.kernel_launch(%1022:tensor<[1, 16, 680, 680], Float32, CPU>, %1018:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1023:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x000000000002ad prog.kernel_launch(%1023:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1024:tensor<[1, 680, 16, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000002ae prog.kernel_launch(%1024:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1024:tensor<[680, 1280], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,1280]}]
        addr:0x000000000002af prog.kernel_launch(%1024:tensor<[680, 1280], Float32, CPU>) -> (%1025:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.20.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x000000000002b0 prog.ret
        addr:0x000000000002b1 prog.label[symbol:visual.blocks.20.mlp.__entry]
        addr:0x000000000002b2 prog.kernel_launch(%1027:tensor<[680, 1280], Float32, CPU>) -> (%1028:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.20.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x000000000002b3 prog.kernel_launch(%1028:tensor<[680, 5120], Float32, CPU>) -> (%1029:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.20.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x000000000002b4 prog.kernel_launch(%1029:tensor<[680, 5120], Float32, CPU>) -> (%1030:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.20.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x000000000002b5 prog.ret
        addr:0x000000000002b6 prog.label[symbol:visual.blocks.21.__entry]
        addr:0x000000000002b7 prog.kernel_launch(%1031:tensor<[680, 1280], Float32, CPU>) -> (%1032:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.21.norm1, op_type:LayerNorm, op_options:null]
        addr:0x000000000002b8 prog.jump visual.blocks.21.attn.__entry[offset:6]
        addr:0x000000000002b9 prog.kernel_launch(%1031:tensor<[680, 1280], Float32, CPU>, %1046:tensor<[680, 1280], Float32, CPU>) -> (%1047:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000002ba prog.kernel_launch(%1047:tensor<[680, 1280], Float32, CPU>) -> (%1048:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.21.norm2, op_type:LayerNorm, op_options:null]
        addr:0x000000000002bb prog.jump visual.blocks.21.mlp.__entry[offset:21]
        addr:0x000000000002bc prog.kernel_launch(%1047:tensor<[680, 1280], Float32, CPU>, %1051:tensor<[680, 1280], Float32, CPU>) -> (%1052:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000002bd prog.ret
        addr:0x000000000002be prog.label[symbol:visual.blocks.21.attn.__entry]
        addr:0x000000000002bf prog.kernel_launch(%1032:tensor<[680, 1280], Float32, CPU>) -> (%1033:tensor<[680, 3840], Float32, CPU>)[symbol_name:visual.blocks.21.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x000000000002c0 prog.kernel_launch(%1033:tensor<[680, 3840], Float32, CPU>) -> (%1033:tensor<[680, 3, 16, 80], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,3,16,80]}]
        addr:0x000000000002c1 prog.kernel_launch(%1033:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%1034:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Permute, op_options:{"axis":[1,0,2,3]}]
        addr:0x000000000002c2 prog.kernel_launch(%1034:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%1034:tensor<[3, 680, 16, 80], Float32, CPU>, %1034:tensor<[3, 680, 16, 80], Float32, CPU>, %1034:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Split, op_options:{"dim":0,"split_size_or_sections":[1]}]
        addr:0x000000000002c3 prog.kernel_launch(%1034:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1035:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.21.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x000000000002c4 prog.kernel_launch(%1034:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1036:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.21.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x000000000002c5 prog.kernel_launch(%1035:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1037:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000002c6 prog.kernel_launch(%1036:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1038:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000002c7 prog.kernel_launch(%1034:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%1039:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000002c8 prog.kernel_launch(%1037:tensor<[1, 16, 680, 80], Float32, CPU>, %1038:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1040:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x000000000002c9 prog.kernel_launch(%1040:tensor<[1, 16, 680, 680], Float32, CPU>, %1041:tensor<[1], Float32, CPU>) -> (%1042:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x000000000002ca prog.kernel_launch(%1042:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%1043:tensor<[1, 16, 680, 680], Float32, CPU>)[symbol_name:visual.blocks.21.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x000000000002cb prog.kernel_launch(%1043:tensor<[1, 16, 680, 680], Float32, CPU>, %1039:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1044:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x000000000002cc prog.kernel_launch(%1044:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1045:tensor<[1, 680, 16, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000002cd prog.kernel_launch(%1045:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1045:tensor<[680, 1280], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,1280]}]
        addr:0x000000000002ce prog.kernel_launch(%1045:tensor<[680, 1280], Float32, CPU>) -> (%1046:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.21.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x000000000002cf prog.ret
        addr:0x000000000002d0 prog.label[symbol:visual.blocks.21.mlp.__entry]
        addr:0x000000000002d1 prog.kernel_launch(%1048:tensor<[680, 1280], Float32, CPU>) -> (%1049:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.21.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x000000000002d2 prog.kernel_launch(%1049:tensor<[680, 5120], Float32, CPU>) -> (%1050:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.21.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x000000000002d3 prog.kernel_launch(%1050:tensor<[680, 5120], Float32, CPU>) -> (%1051:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.21.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x000000000002d4 prog.ret
        addr:0x000000000002d5 prog.label[symbol:visual.blocks.22.__entry]
        addr:0x000000000002d6 prog.kernel_launch(%1052:tensor<[680, 1280], Float32, CPU>) -> (%1053:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.22.norm1, op_type:LayerNorm, op_options:null]
        addr:0x000000000002d7 prog.jump visual.blocks.22.attn.__entry[offset:6]
        addr:0x000000000002d8 prog.kernel_launch(%1052:tensor<[680, 1280], Float32, CPU>, %1067:tensor<[680, 1280], Float32, CPU>) -> (%1068:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000002d9 prog.kernel_launch(%1068:tensor<[680, 1280], Float32, CPU>) -> (%1069:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.22.norm2, op_type:LayerNorm, op_options:null]
        addr:0x000000000002da prog.jump visual.blocks.22.mlp.__entry[offset:21]
        addr:0x000000000002db prog.kernel_launch(%1068:tensor<[680, 1280], Float32, CPU>, %1072:tensor<[680, 1280], Float32, CPU>) -> (%1073:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000002dc prog.ret
        addr:0x000000000002dd prog.label[symbol:visual.blocks.22.attn.__entry]
        addr:0x000000000002de prog.kernel_launch(%1053:tensor<[680, 1280], Float32, CPU>) -> (%1054:tensor<[680, 3840], Float32, CPU>)[symbol_name:visual.blocks.22.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x000000000002df prog.kernel_launch(%1054:tensor<[680, 3840], Float32, CPU>) -> (%1054:tensor<[680, 3, 16, 80], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,3,16,80]}]
        addr:0x000000000002e0 prog.kernel_launch(%1054:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%1055:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Permute, op_options:{"axis":[1,0,2,3]}]
        addr:0x000000000002e1 prog.kernel_launch(%1055:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%1055:tensor<[3, 680, 16, 80], Float32, CPU>, %1055:tensor<[3, 680, 16, 80], Float32, CPU>, %1055:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Split, op_options:{"dim":0,"split_size_or_sections":[1]}]
        addr:0x000000000002e2 prog.kernel_launch(%1055:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1056:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.22.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x000000000002e3 prog.kernel_launch(%1055:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1057:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.22.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x000000000002e4 prog.kernel_launch(%1056:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1058:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000002e5 prog.kernel_launch(%1057:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1059:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000002e6 prog.kernel_launch(%1055:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%1060:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000002e7 prog.kernel_launch(%1058:tensor<[1, 16, 680, 80], Float32, CPU>, %1059:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1061:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x000000000002e8 prog.kernel_launch(%1061:tensor<[1, 16, 680, 680], Float32, CPU>, %1062:tensor<[1], Float32, CPU>) -> (%1063:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x000000000002e9 prog.kernel_launch(%1063:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%1064:tensor<[1, 16, 680, 680], Float32, CPU>)[symbol_name:visual.blocks.22.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x000000000002ea prog.kernel_launch(%1064:tensor<[1, 16, 680, 680], Float32, CPU>, %1060:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1065:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x000000000002eb prog.kernel_launch(%1065:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1066:tensor<[1, 680, 16, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000002ec prog.kernel_launch(%1066:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1066:tensor<[680, 1280], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,1280]}]
        addr:0x000000000002ed prog.kernel_launch(%1066:tensor<[680, 1280], Float32, CPU>) -> (%1067:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.22.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x000000000002ee prog.ret
        addr:0x000000000002ef prog.label[symbol:visual.blocks.22.mlp.__entry]
        addr:0x000000000002f0 prog.kernel_launch(%1069:tensor<[680, 1280], Float32, CPU>) -> (%1070:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.22.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x000000000002f1 prog.kernel_launch(%1070:tensor<[680, 5120], Float32, CPU>) -> (%1071:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.22.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x000000000002f2 prog.kernel_launch(%1071:tensor<[680, 5120], Float32, CPU>) -> (%1072:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.22.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x000000000002f3 prog.ret
        addr:0x000000000002f4 prog.label[symbol:visual.blocks.23.__entry]
        addr:0x000000000002f5 prog.kernel_launch(%1073:tensor<[680, 1280], Float32, CPU>) -> (%1074:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.23.norm1, op_type:LayerNorm, op_options:null]
        addr:0x000000000002f6 prog.jump visual.blocks.23.attn.__entry[offset:6]
        addr:0x000000000002f7 prog.kernel_launch(%1073:tensor<[680, 1280], Float32, CPU>, %1088:tensor<[680, 1280], Float32, CPU>) -> (%1089:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000002f8 prog.kernel_launch(%1089:tensor<[680, 1280], Float32, CPU>) -> (%1090:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.23.norm2, op_type:LayerNorm, op_options:null]
        addr:0x000000000002f9 prog.jump visual.blocks.23.mlp.__entry[offset:21]
        addr:0x000000000002fa prog.kernel_launch(%1089:tensor<[680, 1280], Float32, CPU>, %1093:tensor<[680, 1280], Float32, CPU>) -> (%1094:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000002fb prog.ret
        addr:0x000000000002fc prog.label[symbol:visual.blocks.23.attn.__entry]
        addr:0x000000000002fd prog.kernel_launch(%1074:tensor<[680, 1280], Float32, CPU>) -> (%1075:tensor<[680, 3840], Float32, CPU>)[symbol_name:visual.blocks.23.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x000000000002fe prog.kernel_launch(%1075:tensor<[680, 3840], Float32, CPU>) -> (%1075:tensor<[680, 3, 16, 80], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,3,16,80]}]
        addr:0x000000000002ff prog.kernel_launch(%1075:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%1076:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Permute, op_options:{"axis":[1,0,2,3]}]
        addr:0x00000000000300 prog.kernel_launch(%1076:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%1076:tensor<[3, 680, 16, 80], Float32, CPU>, %1076:tensor<[3, 680, 16, 80], Float32, CPU>, %1076:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Split, op_options:{"dim":0,"split_size_or_sections":[1]}]
        addr:0x00000000000301 prog.kernel_launch(%1076:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1077:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.23.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x00000000000302 prog.kernel_launch(%1076:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1078:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.23.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x00000000000303 prog.kernel_launch(%1077:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1079:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000304 prog.kernel_launch(%1078:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1080:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000305 prog.kernel_launch(%1076:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%1081:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000306 prog.kernel_launch(%1079:tensor<[1, 16, 680, 80], Float32, CPU>, %1080:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1082:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x00000000000307 prog.kernel_launch(%1082:tensor<[1, 16, 680, 680], Float32, CPU>, %1083:tensor<[1], Float32, CPU>) -> (%1084:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x00000000000308 prog.kernel_launch(%1084:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%1085:tensor<[1, 16, 680, 680], Float32, CPU>)[symbol_name:visual.blocks.23.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x00000000000309 prog.kernel_launch(%1085:tensor<[1, 16, 680, 680], Float32, CPU>, %1081:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1086:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x0000000000030a prog.kernel_launch(%1086:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1087:tensor<[1, 680, 16, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000030b prog.kernel_launch(%1087:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1087:tensor<[680, 1280], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,1280]}]
        addr:0x0000000000030c prog.kernel_launch(%1087:tensor<[680, 1280], Float32, CPU>) -> (%1088:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.23.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x0000000000030d prog.ret
        addr:0x0000000000030e prog.label[symbol:visual.blocks.23.mlp.__entry]
        addr:0x0000000000030f prog.kernel_launch(%1090:tensor<[680, 1280], Float32, CPU>) -> (%1091:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.23.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x00000000000310 prog.kernel_launch(%1091:tensor<[680, 5120], Float32, CPU>) -> (%1092:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.23.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x00000000000311 prog.kernel_launch(%1092:tensor<[680, 5120], Float32, CPU>) -> (%1093:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.23.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x00000000000312 prog.ret
        addr:0x00000000000313 prog.label[symbol:visual.blocks.24.__entry]
        addr:0x00000000000314 prog.kernel_launch(%1094:tensor<[680, 1280], Float32, CPU>) -> (%1095:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.24.norm1, op_type:LayerNorm, op_options:null]
        addr:0x00000000000315 prog.jump visual.blocks.24.attn.__entry[offset:6]
        addr:0x00000000000316 prog.kernel_launch(%1094:tensor<[680, 1280], Float32, CPU>, %1109:tensor<[680, 1280], Float32, CPU>) -> (%1110:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x00000000000317 prog.kernel_launch(%1110:tensor<[680, 1280], Float32, CPU>) -> (%1111:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.24.norm2, op_type:LayerNorm, op_options:null]
        addr:0x00000000000318 prog.jump visual.blocks.24.mlp.__entry[offset:21]
        addr:0x00000000000319 prog.kernel_launch(%1110:tensor<[680, 1280], Float32, CPU>, %1114:tensor<[680, 1280], Float32, CPU>) -> (%1115:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x0000000000031a prog.ret
        addr:0x0000000000031b prog.label[symbol:visual.blocks.24.attn.__entry]
        addr:0x0000000000031c prog.kernel_launch(%1095:tensor<[680, 1280], Float32, CPU>) -> (%1096:tensor<[680, 3840], Float32, CPU>)[symbol_name:visual.blocks.24.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x0000000000031d prog.kernel_launch(%1096:tensor<[680, 3840], Float32, CPU>) -> (%1096:tensor<[680, 3, 16, 80], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,3,16,80]}]
        addr:0x0000000000031e prog.kernel_launch(%1096:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%1097:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Permute, op_options:{"axis":[1,0,2,3]}]
        addr:0x0000000000031f prog.kernel_launch(%1097:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%1097:tensor<[3, 680, 16, 80], Float32, CPU>, %1097:tensor<[3, 680, 16, 80], Float32, CPU>, %1097:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Split, op_options:{"dim":0,"split_size_or_sections":[1]}]
        addr:0x00000000000320 prog.kernel_launch(%1097:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1098:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.24.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x00000000000321 prog.kernel_launch(%1097:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1099:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.24.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x00000000000322 prog.kernel_launch(%1098:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1100:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000323 prog.kernel_launch(%1099:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1101:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000324 prog.kernel_launch(%1097:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%1102:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000325 prog.kernel_launch(%1100:tensor<[1, 16, 680, 80], Float32, CPU>, %1101:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1103:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x00000000000326 prog.kernel_launch(%1103:tensor<[1, 16, 680, 680], Float32, CPU>, %1104:tensor<[1], Float32, CPU>) -> (%1105:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x00000000000327 prog.kernel_launch(%1105:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%1106:tensor<[1, 16, 680, 680], Float32, CPU>)[symbol_name:visual.blocks.24.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x00000000000328 prog.kernel_launch(%1106:tensor<[1, 16, 680, 680], Float32, CPU>, %1102:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1107:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x00000000000329 prog.kernel_launch(%1107:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1108:tensor<[1, 680, 16, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000032a prog.kernel_launch(%1108:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1108:tensor<[680, 1280], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,1280]}]
        addr:0x0000000000032b prog.kernel_launch(%1108:tensor<[680, 1280], Float32, CPU>) -> (%1109:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.24.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x0000000000032c prog.ret
        addr:0x0000000000032d prog.label[symbol:visual.blocks.24.mlp.__entry]
        addr:0x0000000000032e prog.kernel_launch(%1111:tensor<[680, 1280], Float32, CPU>) -> (%1112:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.24.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x0000000000032f prog.kernel_launch(%1112:tensor<[680, 5120], Float32, CPU>) -> (%1113:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.24.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x00000000000330 prog.kernel_launch(%1113:tensor<[680, 5120], Float32, CPU>) -> (%1114:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.24.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x00000000000331 prog.ret
        addr:0x00000000000332 prog.label[symbol:visual.blocks.25.__entry]
        addr:0x00000000000333 prog.kernel_launch(%1115:tensor<[680, 1280], Float32, CPU>) -> (%1116:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.25.norm1, op_type:LayerNorm, op_options:null]
        addr:0x00000000000334 prog.jump visual.blocks.25.attn.__entry[offset:6]
        addr:0x00000000000335 prog.kernel_launch(%1115:tensor<[680, 1280], Float32, CPU>, %1130:tensor<[680, 1280], Float32, CPU>) -> (%1131:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x00000000000336 prog.kernel_launch(%1131:tensor<[680, 1280], Float32, CPU>) -> (%1132:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.25.norm2, op_type:LayerNorm, op_options:null]
        addr:0x00000000000337 prog.jump visual.blocks.25.mlp.__entry[offset:21]
        addr:0x00000000000338 prog.kernel_launch(%1131:tensor<[680, 1280], Float32, CPU>, %1135:tensor<[680, 1280], Float32, CPU>) -> (%1136:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x00000000000339 prog.ret
        addr:0x0000000000033a prog.label[symbol:visual.blocks.25.attn.__entry]
        addr:0x0000000000033b prog.kernel_launch(%1116:tensor<[680, 1280], Float32, CPU>) -> (%1117:tensor<[680, 3840], Float32, CPU>)[symbol_name:visual.blocks.25.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x0000000000033c prog.kernel_launch(%1117:tensor<[680, 3840], Float32, CPU>) -> (%1117:tensor<[680, 3, 16, 80], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,3,16,80]}]
        addr:0x0000000000033d prog.kernel_launch(%1117:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%1118:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Permute, op_options:{"axis":[1,0,2,3]}]
        addr:0x0000000000033e prog.kernel_launch(%1118:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%1118:tensor<[3, 680, 16, 80], Float32, CPU>, %1118:tensor<[3, 680, 16, 80], Float32, CPU>, %1118:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Split, op_options:{"dim":0,"split_size_or_sections":[1]}]
        addr:0x0000000000033f prog.kernel_launch(%1118:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1119:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.25.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x00000000000340 prog.kernel_launch(%1118:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1120:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.25.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x00000000000341 prog.kernel_launch(%1119:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1121:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000342 prog.kernel_launch(%1120:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1122:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000343 prog.kernel_launch(%1118:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%1123:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000344 prog.kernel_launch(%1121:tensor<[1, 16, 680, 80], Float32, CPU>, %1122:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1124:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x00000000000345 prog.kernel_launch(%1124:tensor<[1, 16, 680, 680], Float32, CPU>, %1125:tensor<[1], Float32, CPU>) -> (%1126:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x00000000000346 prog.kernel_launch(%1126:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%1127:tensor<[1, 16, 680, 680], Float32, CPU>)[symbol_name:visual.blocks.25.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x00000000000347 prog.kernel_launch(%1127:tensor<[1, 16, 680, 680], Float32, CPU>, %1123:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1128:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x00000000000348 prog.kernel_launch(%1128:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1129:tensor<[1, 680, 16, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000349 prog.kernel_launch(%1129:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1129:tensor<[680, 1280], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,1280]}]
        addr:0x0000000000034a prog.kernel_launch(%1129:tensor<[680, 1280], Float32, CPU>) -> (%1130:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.25.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x0000000000034b prog.ret
        addr:0x0000000000034c prog.label[symbol:visual.blocks.25.mlp.__entry]
        addr:0x0000000000034d prog.kernel_launch(%1132:tensor<[680, 1280], Float32, CPU>) -> (%1133:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.25.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x0000000000034e prog.kernel_launch(%1133:tensor<[680, 5120], Float32, CPU>) -> (%1134:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.25.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x0000000000034f prog.kernel_launch(%1134:tensor<[680, 5120], Float32, CPU>) -> (%1135:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.25.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x00000000000350 prog.ret
        addr:0x00000000000351 prog.label[symbol:visual.blocks.26.__entry]
        addr:0x00000000000352 prog.kernel_launch(%1136:tensor<[680, 1280], Float32, CPU>) -> (%1137:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.26.norm1, op_type:LayerNorm, op_options:null]
        addr:0x00000000000353 prog.jump visual.blocks.26.attn.__entry[offset:6]
        addr:0x00000000000354 prog.kernel_launch(%1136:tensor<[680, 1280], Float32, CPU>, %1151:tensor<[680, 1280], Float32, CPU>) -> (%1152:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x00000000000355 prog.kernel_launch(%1152:tensor<[680, 1280], Float32, CPU>) -> (%1153:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.26.norm2, op_type:LayerNorm, op_options:null]
        addr:0x00000000000356 prog.jump visual.blocks.26.mlp.__entry[offset:21]
        addr:0x00000000000357 prog.kernel_launch(%1152:tensor<[680, 1280], Float32, CPU>, %1156:tensor<[680, 1280], Float32, CPU>) -> (%1157:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x00000000000358 prog.ret
        addr:0x00000000000359 prog.label[symbol:visual.blocks.26.attn.__entry]
        addr:0x0000000000035a prog.kernel_launch(%1137:tensor<[680, 1280], Float32, CPU>) -> (%1138:tensor<[680, 3840], Float32, CPU>)[symbol_name:visual.blocks.26.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x0000000000035b prog.kernel_launch(%1138:tensor<[680, 3840], Float32, CPU>) -> (%1138:tensor<[680, 3, 16, 80], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,3,16,80]}]
        addr:0x0000000000035c prog.kernel_launch(%1138:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%1139:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Permute, op_options:{"axis":[1,0,2,3]}]
        addr:0x0000000000035d prog.kernel_launch(%1139:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%1139:tensor<[3, 680, 16, 80], Float32, CPU>, %1139:tensor<[3, 680, 16, 80], Float32, CPU>, %1139:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Split, op_options:{"dim":0,"split_size_or_sections":[1]}]
        addr:0x0000000000035e prog.kernel_launch(%1139:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1140:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.26.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x0000000000035f prog.kernel_launch(%1139:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1141:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.26.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x00000000000360 prog.kernel_launch(%1140:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1142:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000361 prog.kernel_launch(%1141:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1143:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000362 prog.kernel_launch(%1139:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%1144:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000363 prog.kernel_launch(%1142:tensor<[1, 16, 680, 80], Float32, CPU>, %1143:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1145:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x00000000000364 prog.kernel_launch(%1145:tensor<[1, 16, 680, 680], Float32, CPU>, %1146:tensor<[1], Float32, CPU>) -> (%1147:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x00000000000365 prog.kernel_launch(%1147:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%1148:tensor<[1, 16, 680, 680], Float32, CPU>)[symbol_name:visual.blocks.26.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x00000000000366 prog.kernel_launch(%1148:tensor<[1, 16, 680, 680], Float32, CPU>, %1144:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1149:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x00000000000367 prog.kernel_launch(%1149:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1150:tensor<[1, 680, 16, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000368 prog.kernel_launch(%1150:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1150:tensor<[680, 1280], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,1280]}]
        addr:0x00000000000369 prog.kernel_launch(%1150:tensor<[680, 1280], Float32, CPU>) -> (%1151:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.26.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x0000000000036a prog.ret
        addr:0x0000000000036b prog.label[symbol:visual.blocks.26.mlp.__entry]
        addr:0x0000000000036c prog.kernel_launch(%1153:tensor<[680, 1280], Float32, CPU>) -> (%1154:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.26.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x0000000000036d prog.kernel_launch(%1154:tensor<[680, 5120], Float32, CPU>) -> (%1155:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.26.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x0000000000036e prog.kernel_launch(%1155:tensor<[680, 5120], Float32, CPU>) -> (%1156:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.26.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x0000000000036f prog.ret
        addr:0x00000000000370 prog.label[symbol:visual.blocks.27.__entry]
        addr:0x00000000000371 prog.kernel_launch(%1157:tensor<[680, 1280], Float32, CPU>) -> (%1158:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.27.norm1, op_type:LayerNorm, op_options:null]
        addr:0x00000000000372 prog.jump visual.blocks.27.attn.__entry[offset:6]
        addr:0x00000000000373 prog.kernel_launch(%1157:tensor<[680, 1280], Float32, CPU>, %1172:tensor<[680, 1280], Float32, CPU>) -> (%1173:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x00000000000374 prog.kernel_launch(%1173:tensor<[680, 1280], Float32, CPU>) -> (%1174:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.27.norm2, op_type:LayerNorm, op_options:null]
        addr:0x00000000000375 prog.jump visual.blocks.27.mlp.__entry[offset:21]
        addr:0x00000000000376 prog.kernel_launch(%1173:tensor<[680, 1280], Float32, CPU>, %1177:tensor<[680, 1280], Float32, CPU>) -> (%1178:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x00000000000377 prog.ret
        addr:0x00000000000378 prog.label[symbol:visual.blocks.27.attn.__entry]
        addr:0x00000000000379 prog.kernel_launch(%1158:tensor<[680, 1280], Float32, CPU>) -> (%1159:tensor<[680, 3840], Float32, CPU>)[symbol_name:visual.blocks.27.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x0000000000037a prog.kernel_launch(%1159:tensor<[680, 3840], Float32, CPU>) -> (%1159:tensor<[680, 3, 16, 80], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,3,16,80]}]
        addr:0x0000000000037b prog.kernel_launch(%1159:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%1160:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Permute, op_options:{"axis":[1,0,2,3]}]
        addr:0x0000000000037c prog.kernel_launch(%1160:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%1160:tensor<[3, 680, 16, 80], Float32, CPU>, %1160:tensor<[3, 680, 16, 80], Float32, CPU>, %1160:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Split, op_options:{"dim":0,"split_size_or_sections":[1]}]
        addr:0x0000000000037d prog.kernel_launch(%1160:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1161:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.27.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x0000000000037e prog.kernel_launch(%1160:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1162:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.27.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x0000000000037f prog.kernel_launch(%1161:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1163:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000380 prog.kernel_launch(%1162:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1164:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000381 prog.kernel_launch(%1160:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%1165:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000382 prog.kernel_launch(%1163:tensor<[1, 16, 680, 80], Float32, CPU>, %1164:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1166:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x00000000000383 prog.kernel_launch(%1166:tensor<[1, 16, 680, 680], Float32, CPU>, %1167:tensor<[1], Float32, CPU>) -> (%1168:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x00000000000384 prog.kernel_launch(%1168:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%1169:tensor<[1, 16, 680, 680], Float32, CPU>)[symbol_name:visual.blocks.27.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x00000000000385 prog.kernel_launch(%1169:tensor<[1, 16, 680, 680], Float32, CPU>, %1165:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1170:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x00000000000386 prog.kernel_launch(%1170:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1171:tensor<[1, 680, 16, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000387 prog.kernel_launch(%1171:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1171:tensor<[680, 1280], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,1280]}]
        addr:0x00000000000388 prog.kernel_launch(%1171:tensor<[680, 1280], Float32, CPU>) -> (%1172:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.27.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x00000000000389 prog.ret
        addr:0x0000000000038a prog.label[symbol:visual.blocks.27.mlp.__entry]
        addr:0x0000000000038b prog.kernel_launch(%1174:tensor<[680, 1280], Float32, CPU>) -> (%1175:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.27.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x0000000000038c prog.kernel_launch(%1175:tensor<[680, 5120], Float32, CPU>) -> (%1176:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.27.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x0000000000038d prog.kernel_launch(%1176:tensor<[680, 5120], Float32, CPU>) -> (%1177:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.27.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x0000000000038e prog.ret
        addr:0x0000000000038f prog.label[symbol:visual.blocks.28.__entry]
        addr:0x00000000000390 prog.kernel_launch(%1178:tensor<[680, 1280], Float32, CPU>) -> (%1179:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.28.norm1, op_type:LayerNorm, op_options:null]
        addr:0x00000000000391 prog.jump visual.blocks.28.attn.__entry[offset:6]
        addr:0x00000000000392 prog.kernel_launch(%1178:tensor<[680, 1280], Float32, CPU>, %1193:tensor<[680, 1280], Float32, CPU>) -> (%1194:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x00000000000393 prog.kernel_launch(%1194:tensor<[680, 1280], Float32, CPU>) -> (%1195:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.28.norm2, op_type:LayerNorm, op_options:null]
        addr:0x00000000000394 prog.jump visual.blocks.28.mlp.__entry[offset:21]
        addr:0x00000000000395 prog.kernel_launch(%1194:tensor<[680, 1280], Float32, CPU>, %1198:tensor<[680, 1280], Float32, CPU>) -> (%1199:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x00000000000396 prog.ret
        addr:0x00000000000397 prog.label[symbol:visual.blocks.28.attn.__entry]
        addr:0x00000000000398 prog.kernel_launch(%1179:tensor<[680, 1280], Float32, CPU>) -> (%1180:tensor<[680, 3840], Float32, CPU>)[symbol_name:visual.blocks.28.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x00000000000399 prog.kernel_launch(%1180:tensor<[680, 3840], Float32, CPU>) -> (%1180:tensor<[680, 3, 16, 80], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,3,16,80]}]
        addr:0x0000000000039a prog.kernel_launch(%1180:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%1181:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Permute, op_options:{"axis":[1,0,2,3]}]
        addr:0x0000000000039b prog.kernel_launch(%1181:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%1181:tensor<[3, 680, 16, 80], Float32, CPU>, %1181:tensor<[3, 680, 16, 80], Float32, CPU>, %1181:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Split, op_options:{"dim":0,"split_size_or_sections":[1]}]
        addr:0x0000000000039c prog.kernel_launch(%1181:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1182:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.28.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x0000000000039d prog.kernel_launch(%1181:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1183:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.28.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x0000000000039e prog.kernel_launch(%1182:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1184:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000039f prog.kernel_launch(%1183:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1185:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000003a0 prog.kernel_launch(%1181:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%1186:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000003a1 prog.kernel_launch(%1184:tensor<[1, 16, 680, 80], Float32, CPU>, %1185:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1187:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x000000000003a2 prog.kernel_launch(%1187:tensor<[1, 16, 680, 680], Float32, CPU>, %1188:tensor<[1], Float32, CPU>) -> (%1189:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x000000000003a3 prog.kernel_launch(%1189:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%1190:tensor<[1, 16, 680, 680], Float32, CPU>)[symbol_name:visual.blocks.28.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x000000000003a4 prog.kernel_launch(%1190:tensor<[1, 16, 680, 680], Float32, CPU>, %1186:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1191:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x000000000003a5 prog.kernel_launch(%1191:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1192:tensor<[1, 680, 16, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000003a6 prog.kernel_launch(%1192:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1192:tensor<[680, 1280], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,1280]}]
        addr:0x000000000003a7 prog.kernel_launch(%1192:tensor<[680, 1280], Float32, CPU>) -> (%1193:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.28.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x000000000003a8 prog.ret
        addr:0x000000000003a9 prog.label[symbol:visual.blocks.28.mlp.__entry]
        addr:0x000000000003aa prog.kernel_launch(%1195:tensor<[680, 1280], Float32, CPU>) -> (%1196:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.28.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x000000000003ab prog.kernel_launch(%1196:tensor<[680, 5120], Float32, CPU>) -> (%1197:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.28.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x000000000003ac prog.kernel_launch(%1197:tensor<[680, 5120], Float32, CPU>) -> (%1198:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.28.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x000000000003ad prog.ret
        addr:0x000000000003ae prog.label[symbol:visual.blocks.29.__entry]
        addr:0x000000000003af prog.kernel_launch(%1199:tensor<[680, 1280], Float32, CPU>) -> (%1200:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.29.norm1, op_type:LayerNorm, op_options:null]
        addr:0x000000000003b0 prog.jump visual.blocks.29.attn.__entry[offset:6]
        addr:0x000000000003b1 prog.kernel_launch(%1199:tensor<[680, 1280], Float32, CPU>, %1214:tensor<[680, 1280], Float32, CPU>) -> (%1215:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000003b2 prog.kernel_launch(%1215:tensor<[680, 1280], Float32, CPU>) -> (%1216:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.29.norm2, op_type:LayerNorm, op_options:null]
        addr:0x000000000003b3 prog.jump visual.blocks.29.mlp.__entry[offset:21]
        addr:0x000000000003b4 prog.kernel_launch(%1215:tensor<[680, 1280], Float32, CPU>, %1219:tensor<[680, 1280], Float32, CPU>) -> (%1220:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000003b5 prog.ret
        addr:0x000000000003b6 prog.label[symbol:visual.blocks.29.attn.__entry]
        addr:0x000000000003b7 prog.kernel_launch(%1200:tensor<[680, 1280], Float32, CPU>) -> (%1201:tensor<[680, 3840], Float32, CPU>)[symbol_name:visual.blocks.29.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x000000000003b8 prog.kernel_launch(%1201:tensor<[680, 3840], Float32, CPU>) -> (%1201:tensor<[680, 3, 16, 80], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,3,16,80]}]
        addr:0x000000000003b9 prog.kernel_launch(%1201:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%1202:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Permute, op_options:{"axis":[1,0,2,3]}]
        addr:0x000000000003ba prog.kernel_launch(%1202:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%1202:tensor<[3, 680, 16, 80], Float32, CPU>, %1202:tensor<[3, 680, 16, 80], Float32, CPU>, %1202:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Split, op_options:{"dim":0,"split_size_or_sections":[1]}]
        addr:0x000000000003bb prog.kernel_launch(%1202:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1203:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.29.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x000000000003bc prog.kernel_launch(%1202:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1204:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.29.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x000000000003bd prog.kernel_launch(%1203:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1205:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000003be prog.kernel_launch(%1204:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1206:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000003bf prog.kernel_launch(%1202:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%1207:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000003c0 prog.kernel_launch(%1205:tensor<[1, 16, 680, 80], Float32, CPU>, %1206:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1208:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x000000000003c1 prog.kernel_launch(%1208:tensor<[1, 16, 680, 680], Float32, CPU>, %1209:tensor<[1], Float32, CPU>) -> (%1210:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x000000000003c2 prog.kernel_launch(%1210:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%1211:tensor<[1, 16, 680, 680], Float32, CPU>)[symbol_name:visual.blocks.29.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x000000000003c3 prog.kernel_launch(%1211:tensor<[1, 16, 680, 680], Float32, CPU>, %1207:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1212:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x000000000003c4 prog.kernel_launch(%1212:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1213:tensor<[1, 680, 16, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000003c5 prog.kernel_launch(%1213:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1213:tensor<[680, 1280], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,1280]}]
        addr:0x000000000003c6 prog.kernel_launch(%1213:tensor<[680, 1280], Float32, CPU>) -> (%1214:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.29.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x000000000003c7 prog.ret
        addr:0x000000000003c8 prog.label[symbol:visual.blocks.29.mlp.__entry]
        addr:0x000000000003c9 prog.kernel_launch(%1216:tensor<[680, 1280], Float32, CPU>) -> (%1217:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.29.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x000000000003ca prog.kernel_launch(%1217:tensor<[680, 5120], Float32, CPU>) -> (%1218:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.29.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x000000000003cb prog.kernel_launch(%1218:tensor<[680, 5120], Float32, CPU>) -> (%1219:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.29.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x000000000003cc prog.ret
        addr:0x000000000003cd prog.label[symbol:visual.blocks.30.__entry]
        addr:0x000000000003ce prog.kernel_launch(%1220:tensor<[680, 1280], Float32, CPU>) -> (%1221:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.30.norm1, op_type:LayerNorm, op_options:null]
        addr:0x000000000003cf prog.jump visual.blocks.30.attn.__entry[offset:6]
        addr:0x000000000003d0 prog.kernel_launch(%1220:tensor<[680, 1280], Float32, CPU>, %1235:tensor<[680, 1280], Float32, CPU>) -> (%1236:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000003d1 prog.kernel_launch(%1236:tensor<[680, 1280], Float32, CPU>) -> (%1237:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.30.norm2, op_type:LayerNorm, op_options:null]
        addr:0x000000000003d2 prog.jump visual.blocks.30.mlp.__entry[offset:21]
        addr:0x000000000003d3 prog.kernel_launch(%1236:tensor<[680, 1280], Float32, CPU>, %1240:tensor<[680, 1280], Float32, CPU>) -> (%1241:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000003d4 prog.ret
        addr:0x000000000003d5 prog.label[symbol:visual.blocks.30.attn.__entry]
        addr:0x000000000003d6 prog.kernel_launch(%1221:tensor<[680, 1280], Float32, CPU>) -> (%1222:tensor<[680, 3840], Float32, CPU>)[symbol_name:visual.blocks.30.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x000000000003d7 prog.kernel_launch(%1222:tensor<[680, 3840], Float32, CPU>) -> (%1222:tensor<[680, 3, 16, 80], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,3,16,80]}]
        addr:0x000000000003d8 prog.kernel_launch(%1222:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%1223:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Permute, op_options:{"axis":[1,0,2,3]}]
        addr:0x000000000003d9 prog.kernel_launch(%1223:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%1223:tensor<[3, 680, 16, 80], Float32, CPU>, %1223:tensor<[3, 680, 16, 80], Float32, CPU>, %1223:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Split, op_options:{"dim":0,"split_size_or_sections":[1]}]
        addr:0x000000000003da prog.kernel_launch(%1223:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1224:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.30.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x000000000003db prog.kernel_launch(%1223:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1225:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.30.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x000000000003dc prog.kernel_launch(%1224:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1226:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000003dd prog.kernel_launch(%1225:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1227:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000003de prog.kernel_launch(%1223:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%1228:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000003df prog.kernel_launch(%1226:tensor<[1, 16, 680, 80], Float32, CPU>, %1227:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1229:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x000000000003e0 prog.kernel_launch(%1229:tensor<[1, 16, 680, 680], Float32, CPU>, %1230:tensor<[1], Float32, CPU>) -> (%1231:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x000000000003e1 prog.kernel_launch(%1231:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%1232:tensor<[1, 16, 680, 680], Float32, CPU>)[symbol_name:visual.blocks.30.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x000000000003e2 prog.kernel_launch(%1232:tensor<[1, 16, 680, 680], Float32, CPU>, %1228:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1233:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x000000000003e3 prog.kernel_launch(%1233:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1234:tensor<[1, 680, 16, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000003e4 prog.kernel_launch(%1234:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1234:tensor<[680, 1280], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,1280]}]
        addr:0x000000000003e5 prog.kernel_launch(%1234:tensor<[680, 1280], Float32, CPU>) -> (%1235:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.30.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x000000000003e6 prog.ret
        addr:0x000000000003e7 prog.label[symbol:visual.blocks.30.mlp.__entry]
        addr:0x000000000003e8 prog.kernel_launch(%1237:tensor<[680, 1280], Float32, CPU>) -> (%1238:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.30.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x000000000003e9 prog.kernel_launch(%1238:tensor<[680, 5120], Float32, CPU>) -> (%1239:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.30.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x000000000003ea prog.kernel_launch(%1239:tensor<[680, 5120], Float32, CPU>) -> (%1240:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.30.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x000000000003eb prog.ret
        addr:0x000000000003ec prog.label[symbol:visual.blocks.31.__entry]
        addr:0x000000000003ed prog.kernel_launch(%1241:tensor<[680, 1280], Float32, CPU>) -> (%1242:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.31.norm1, op_type:LayerNorm, op_options:null]
        addr:0x000000000003ee prog.jump visual.blocks.31.attn.__entry[offset:6]
        addr:0x000000000003ef prog.kernel_launch(%1241:tensor<[680, 1280], Float32, CPU>, %1256:tensor<[680, 1280], Float32, CPU>) -> (%1257:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000003f0 prog.kernel_launch(%1257:tensor<[680, 1280], Float32, CPU>) -> (%1258:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.31.norm2, op_type:LayerNorm, op_options:null]
        addr:0x000000000003f1 prog.jump visual.blocks.31.mlp.__entry[offset:21]
        addr:0x000000000003f2 prog.kernel_launch(%1257:tensor<[680, 1280], Float32, CPU>, %1261:tensor<[680, 1280], Float32, CPU>) -> (%1262:tensor<[680, 1280], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000003f3 prog.ret
        addr:0x000000000003f4 prog.label[symbol:visual.blocks.31.attn.__entry]
        addr:0x000000000003f5 prog.kernel_launch(%1242:tensor<[680, 1280], Float32, CPU>) -> (%1243:tensor<[680, 3840], Float32, CPU>)[symbol_name:visual.blocks.31.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x000000000003f6 prog.kernel_launch(%1243:tensor<[680, 3840], Float32, CPU>) -> (%1243:tensor<[680, 3, 16, 80], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,3,16,80]}]
        addr:0x000000000003f7 prog.kernel_launch(%1243:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%1244:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Permute, op_options:{"axis":[1,0,2,3]}]
        addr:0x000000000003f8 prog.kernel_launch(%1244:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%1244:tensor<[3, 680, 16, 80], Float32, CPU>, %1244:tensor<[3, 680, 16, 80], Float32, CPU>, %1244:tensor<[3, 680, 16, 80], Float32, CPU>)[op_type:Split, op_options:{"dim":0,"split_size_or_sections":[1]}]
        addr:0x000000000003f9 prog.kernel_launch(%1244:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1245:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.31.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x000000000003fa prog.kernel_launch(%1244:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1246:tensor<[1, 680, 16, 80], Float32, CPU>)[symbol_name:visual.blocks.31.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x000000000003fb prog.kernel_launch(%1245:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1247:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000003fc prog.kernel_launch(%1246:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1248:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000003fd prog.kernel_launch(%1244:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%1249:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000003fe prog.kernel_launch(%1247:tensor<[1, 16, 680, 80], Float32, CPU>, %1248:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1250:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x000000000003ff prog.kernel_launch(%1250:tensor<[1, 16, 680, 680], Float32, CPU>, %1251:tensor<[1], Float32, CPU>) -> (%1252:tensor<[1, 16, 680, 680], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x00000000000400 prog.kernel_launch(%1252:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%1253:tensor<[1, 16, 680, 680], Float32, CPU>)[symbol_name:visual.blocks.31.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x00000000000401 prog.kernel_launch(%1253:tensor<[1, 16, 680, 680], Float32, CPU>, %1249:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1254:tensor<[1, 16, 680, 80], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x00000000000402 prog.kernel_launch(%1254:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1255:tensor<[1, 680, 16, 80], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000403 prog.kernel_launch(%1255:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1255:tensor<[680, 1280], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,1280]}]
        addr:0x00000000000404 prog.kernel_launch(%1255:tensor<[680, 1280], Float32, CPU>) -> (%1256:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.31.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x00000000000405 prog.ret
        addr:0x00000000000406 prog.label[symbol:visual.blocks.31.mlp.__entry]
        addr:0x00000000000407 prog.kernel_launch(%1258:tensor<[680, 1280], Float32, CPU>) -> (%1259:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.31.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x00000000000408 prog.kernel_launch(%1259:tensor<[680, 5120], Float32, CPU>) -> (%1260:tensor<[680, 5120], Float32, CPU>)[symbol_name:visual.blocks.31.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x00000000000409 prog.kernel_launch(%1260:tensor<[680, 5120], Float32, CPU>) -> (%1261:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.blocks.31.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x0000000000040a prog.ret
        addr:0x0000000000040b prog.label[symbol:visual.merger.__entry]
        addr:0x0000000000040c prog.kernel_launch(%1262:tensor<[680, 1280], Float32, CPU>) -> (%1263:tensor<[680, 1280], Float32, CPU>)[symbol_name:visual.merger.ln_q, op_type:LayerNorm, op_options:null]
        addr:0x0000000000040d prog.kernel_launch(%1263:tensor<[680, 1280], Float32, CPU>) -> (%1263:tensor<[170, 5120], Float32, CPU>)[op_type:View, op_options:{"to_shape":[-1,5120]}]
        addr:0x0000000000040e prog.kernel_launch(%1263:tensor<[170, 5120], Float32, CPU>) -> (%1264:tensor<[170, 5120], Float32, CPU>)[symbol_name:visual.merger.mlp.0, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":5120}]
        addr:0x0000000000040f prog.kernel_launch(%1264:tensor<[170, 5120], Float32, CPU>) -> (%1265:tensor<[170, 5120], Float32, CPU>)[symbol_name:visual.merger.mlp.gelu, op_type:GELU, op_options:null]
        addr:0x00000000000410 prog.kernel_launch(%1265:tensor<[170, 5120], Float32, CPU>) -> (%1266:tensor<[170, 1536], Float32, CPU>)[symbol_name:visual.merger.mlp.2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1536}]
        addr:0x00000000000411 prog.ret
        addr:0x00000000000412 prog.kernel_launch(%583:tensor<[1, 192, 1536], Float32, CPU>) -> (%583:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Slice, op_options:{"indices":[{"end":2147483633,"start":2147483633,"step":1},{"end":185,"start":15,"step":1},{"end":2147483633,"start":2147483633,"step":1}]}]
        addr:0x00000000000413 prog.kernel_launch(%1266:tensor<[170, 1536], Float32, CPU>, %583:tensor<[1, 192, 1536], Float32, CPU>) -> ()[op_type:Copy, op_options:null]
        addr:0x00000000000414 prog.jump model.__entry[offset:1]
        addr:0x00000000000415 prog.label[symbol:model.__entry]
        addr:0x00000000000416 prog.jump model.layers.0.__entry[offset:32]
        addr:0x00000000000417 prog.jump model.layers.1.__entry[offset:68]
        addr:0x00000000000418 prog.jump model.layers.2.__entry[offset:104]
        addr:0x00000000000419 prog.jump model.layers.3.__entry[offset:140]
        addr:0x0000000000041a prog.jump model.layers.4.__entry[offset:176]
        addr:0x0000000000041b prog.jump model.layers.5.__entry[offset:212]
        addr:0x0000000000041c prog.jump model.layers.6.__entry[offset:248]
        addr:0x0000000000041d prog.jump model.layers.7.__entry[offset:284]
        addr:0x0000000000041e prog.jump model.layers.8.__entry[offset:320]
        addr:0x0000000000041f prog.jump model.layers.9.__entry[offset:356]
        addr:0x00000000000420 prog.jump model.layers.10.__entry[offset:392]
        addr:0x00000000000421 prog.jump model.layers.11.__entry[offset:428]
        addr:0x00000000000422 prog.jump model.layers.12.__entry[offset:464]
        addr:0x00000000000423 prog.jump model.layers.13.__entry[offset:500]
        addr:0x00000000000424 prog.jump model.layers.14.__entry[offset:536]
        addr:0x00000000000425 prog.jump model.layers.15.__entry[offset:572]
        addr:0x00000000000426 prog.jump model.layers.16.__entry[offset:608]
        addr:0x00000000000427 prog.jump model.layers.17.__entry[offset:644]
        addr:0x00000000000428 prog.jump model.layers.18.__entry[offset:680]
        addr:0x00000000000429 prog.jump model.layers.19.__entry[offset:716]
        addr:0x0000000000042a prog.jump model.layers.20.__entry[offset:752]
        addr:0x0000000000042b prog.jump model.layers.21.__entry[offset:788]
        addr:0x0000000000042c prog.jump model.layers.22.__entry[offset:824]
        addr:0x0000000000042d prog.jump model.layers.23.__entry[offset:860]
        addr:0x0000000000042e prog.jump model.layers.24.__entry[offset:896]
        addr:0x0000000000042f prog.jump model.layers.25.__entry[offset:932]
        addr:0x00000000000430 prog.jump model.layers.26.__entry[offset:968]
        addr:0x00000000000431 prog.jump model.layers.27.__entry[offset:1004]
        addr:0x00000000000432 prog.kernel_launch(%2027:tensor<[1, 192, 1536], Float32, CPU>) -> (%2028:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.norm, op_type:RMSNorm, op_options:null]
        addr:0x00000000000433 prog.kernel_launch(%2028:tensor<[1, 192, 1536], Float32, CPU>) -> (%2028:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Slice, op_options:{"indices":[{"end":2147483633,"start":2147483633,"step":1},{"end":192,"start":191,"step":1},{"end":2147483633,"start":2147483633,"step":1}]}]
        addr:0x00000000000434 prog.kernel_launch(%2028:tensor<[1, 192, 1536], Float32, CPU>) -> (%2029:tensor<[1, 1, 151936], Float32, CPU>)[symbol_name:model.lm_head, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":151936}]
        addr:0x00000000000435 prog.ret
        addr:0x00000000000436 prog.label[symbol:model.layers.0.__entry]
        addr:0x00000000000437 prog.kernel_launch(%583:tensor<[1, 192, 1536], Float32, CPU>) -> (%1272:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.0.input_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x00000000000438 prog.jump model.layers.0.self_attn.__entry[offset:6]
        addr:0x00000000000439 prog.kernel_launch(%1290:tensor<[1, 192, 1536], Float32, CPU>, %583:tensor<[1, 192, 1536], Float32, CPU>) -> (%1291:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x0000000000043a prog.kernel_launch(%1291:tensor<[1, 192, 1536], Float32, CPU>) -> (%1292:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.0.post_attention_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x0000000000043b prog.jump model.layers.0.mlp.__entry[offset:25]
        addr:0x0000000000043c prog.kernel_launch(%1297:tensor<[1, 192, 1536], Float32, CPU>, %1291:tensor<[1, 192, 1536], Float32, CPU>) -> (%1298:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x0000000000043d prog.ret
        addr:0x0000000000043e prog.label[symbol:model.layers.0.self_attn.__entry]
        addr:0x0000000000043f prog.kernel_launch(%1272:tensor<[1, 192, 1536], Float32, CPU>) -> (%1273:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.0.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000440 prog.kernel_launch(%1272:tensor<[1, 192, 1536], Float32, CPU>) -> (%1274:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.0.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000441 prog.kernel_launch(%1272:tensor<[1, 192, 1536], Float32, CPU>) -> (%1275:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.0.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000442 prog.kernel_launch(%1273:tensor<[1, 192, 1536], Float32, CPU>) -> (%1273:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,12,128]}]
        addr:0x00000000000443 prog.kernel_launch(%1274:tensor<[1, 192, 256], Float32, CPU>) -> (%1274:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x00000000000444 prog.kernel_launch(%1275:tensor<[1, 192, 256], Float32, CPU>) -> (%1275:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x00000000000445 prog.kernel_launch(%1273:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1276:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000446 prog.kernel_launch(%1274:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1277:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000447 prog.kernel_launch(%1275:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1278:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000448 prog.kernel_launch(%1276:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1279:tensor<[1, 12, 192, 128], Float32, CPU>)[symbol_name:model.layers.0.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000449 prog.kernel_launch(%1277:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1280:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.0.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x0000000000044a prog.kernel_launch(%1280:tensor<[1, 2, 192, 128], Float32, CPU>, %1278:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1281:tensor<[1, 2, 192, 128], Float32, CPU>, %1282:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.0.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x0000000000044b prog.kernel_launch(%1279:tensor<[1, 12, 192, 128], Float32, CPU>, %1281:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1283:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x0000000000044c prog.kernel_launch(%1283:tensor<[1, 12, 192, 192], Float32, CPU>, %1284:tensor<[1], Float32, CPU>) -> (%1285:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x0000000000044d prog.kernel_launch(%1285:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1286:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.0.self_attn.mask, op_type:CausalMask, op_options:null]
        addr:0x0000000000044e prog.kernel_launch(%1286:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1287:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.0.self_attn.softmax, op_type:Softmax, op_options:null]
        addr:0x0000000000044f prog.kernel_launch(%1287:tensor<[1, 12, 192, 192], Float32, CPU>, %1282:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1288:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x00000000000450 prog.kernel_launch(%1288:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1289:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000451 prog.kernel_launch(%1289:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1289:tensor<[1, 192, 1536], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,1536]}]
        addr:0x00000000000452 prog.kernel_launch(%1289:tensor<[1, 192, 1536], Float32, CPU>) -> (%1290:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.0.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000453 prog.ret
        addr:0x00000000000454 prog.label[symbol:model.layers.0.mlp.__entry]
        addr:0x00000000000455 prog.kernel_launch(%1292:tensor<[1, 192, 1536], Float32, CPU>) -> (%1293:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.0.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000456 prog.kernel_launch(%1293:tensor<[1, 192, 8960], Float32, CPU>) -> (%1294:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.0.mlp.act, op_type:SiLU, op_options:null]
        addr:0x00000000000457 prog.kernel_launch(%1292:tensor<[1, 192, 1536], Float32, CPU>) -> (%1295:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.0.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000458 prog.kernel_launch(%1294:tensor<[1, 192, 8960], Float32, CPU>, %1295:tensor<[1, 192, 8960], Float32, CPU>) -> (%1296:tensor<[1, 192, 8960], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x00000000000459 prog.kernel_launch(%1296:tensor<[1, 192, 8960], Float32, CPU>) -> (%1297:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.0.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x0000000000045a prog.ret
        addr:0x0000000000045b prog.label[symbol:model.layers.1.__entry]
        addr:0x0000000000045c prog.kernel_launch(%1298:tensor<[1, 192, 1536], Float32, CPU>) -> (%1299:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.1.input_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x0000000000045d prog.jump model.layers.1.self_attn.__entry[offset:6]
        addr:0x0000000000045e prog.kernel_launch(%1317:tensor<[1, 192, 1536], Float32, CPU>, %1298:tensor<[1, 192, 1536], Float32, CPU>) -> (%1318:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x0000000000045f prog.kernel_launch(%1318:tensor<[1, 192, 1536], Float32, CPU>) -> (%1319:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.1.post_attention_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x00000000000460 prog.jump model.layers.1.mlp.__entry[offset:25]
        addr:0x00000000000461 prog.kernel_launch(%1324:tensor<[1, 192, 1536], Float32, CPU>, %1318:tensor<[1, 192, 1536], Float32, CPU>) -> (%1325:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x00000000000462 prog.ret
        addr:0x00000000000463 prog.label[symbol:model.layers.1.self_attn.__entry]
        addr:0x00000000000464 prog.kernel_launch(%1299:tensor<[1, 192, 1536], Float32, CPU>) -> (%1300:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.1.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000465 prog.kernel_launch(%1299:tensor<[1, 192, 1536], Float32, CPU>) -> (%1301:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.1.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000466 prog.kernel_launch(%1299:tensor<[1, 192, 1536], Float32, CPU>) -> (%1302:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.1.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000467 prog.kernel_launch(%1300:tensor<[1, 192, 1536], Float32, CPU>) -> (%1300:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,12,128]}]
        addr:0x00000000000468 prog.kernel_launch(%1301:tensor<[1, 192, 256], Float32, CPU>) -> (%1301:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x00000000000469 prog.kernel_launch(%1302:tensor<[1, 192, 256], Float32, CPU>) -> (%1302:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x0000000000046a prog.kernel_launch(%1300:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1303:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000046b prog.kernel_launch(%1301:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1304:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000046c prog.kernel_launch(%1302:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1305:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000046d prog.kernel_launch(%1303:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1306:tensor<[1, 12, 192, 128], Float32, CPU>)[symbol_name:model.layers.1.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x0000000000046e prog.kernel_launch(%1304:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1307:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.1.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x0000000000046f prog.kernel_launch(%1307:tensor<[1, 2, 192, 128], Float32, CPU>, %1305:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1308:tensor<[1, 2, 192, 128], Float32, CPU>, %1309:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.1.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000470 prog.kernel_launch(%1306:tensor<[1, 12, 192, 128], Float32, CPU>, %1308:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1310:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x00000000000471 prog.kernel_launch(%1310:tensor<[1, 12, 192, 192], Float32, CPU>, %1311:tensor<[1], Float32, CPU>) -> (%1312:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x00000000000472 prog.kernel_launch(%1312:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1313:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.1.self_attn.mask, op_type:CausalMask, op_options:null]
        addr:0x00000000000473 prog.kernel_launch(%1313:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1314:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.1.self_attn.softmax, op_type:Softmax, op_options:null]
        addr:0x00000000000474 prog.kernel_launch(%1314:tensor<[1, 12, 192, 192], Float32, CPU>, %1309:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1315:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x00000000000475 prog.kernel_launch(%1315:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1316:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000476 prog.kernel_launch(%1316:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1316:tensor<[1, 192, 1536], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,1536]}]
        addr:0x00000000000477 prog.kernel_launch(%1316:tensor<[1, 192, 1536], Float32, CPU>) -> (%1317:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.1.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000478 prog.ret
        addr:0x00000000000479 prog.label[symbol:model.layers.1.mlp.__entry]
        addr:0x0000000000047a prog.kernel_launch(%1319:tensor<[1, 192, 1536], Float32, CPU>) -> (%1320:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.1.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x0000000000047b prog.kernel_launch(%1320:tensor<[1, 192, 8960], Float32, CPU>) -> (%1321:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.1.mlp.act, op_type:SiLU, op_options:null]
        addr:0x0000000000047c prog.kernel_launch(%1319:tensor<[1, 192, 1536], Float32, CPU>) -> (%1322:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.1.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x0000000000047d prog.kernel_launch(%1321:tensor<[1, 192, 8960], Float32, CPU>, %1322:tensor<[1, 192, 8960], Float32, CPU>) -> (%1323:tensor<[1, 192, 8960], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x0000000000047e prog.kernel_launch(%1323:tensor<[1, 192, 8960], Float32, CPU>) -> (%1324:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.1.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x0000000000047f prog.ret
        addr:0x00000000000480 prog.label[symbol:model.layers.2.__entry]
        addr:0x00000000000481 prog.kernel_launch(%1325:tensor<[1, 192, 1536], Float32, CPU>) -> (%1326:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.2.input_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x00000000000482 prog.jump model.layers.2.self_attn.__entry[offset:6]
        addr:0x00000000000483 prog.kernel_launch(%1344:tensor<[1, 192, 1536], Float32, CPU>, %1325:tensor<[1, 192, 1536], Float32, CPU>) -> (%1345:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x00000000000484 prog.kernel_launch(%1345:tensor<[1, 192, 1536], Float32, CPU>) -> (%1346:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.2.post_attention_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x00000000000485 prog.jump model.layers.2.mlp.__entry[offset:25]
        addr:0x00000000000486 prog.kernel_launch(%1351:tensor<[1, 192, 1536], Float32, CPU>, %1345:tensor<[1, 192, 1536], Float32, CPU>) -> (%1352:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x00000000000487 prog.ret
        addr:0x00000000000488 prog.label[symbol:model.layers.2.self_attn.__entry]
        addr:0x00000000000489 prog.kernel_launch(%1326:tensor<[1, 192, 1536], Float32, CPU>) -> (%1327:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.2.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000048a prog.kernel_launch(%1326:tensor<[1, 192, 1536], Float32, CPU>) -> (%1328:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.2.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x0000000000048b prog.kernel_launch(%1326:tensor<[1, 192, 1536], Float32, CPU>) -> (%1329:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.2.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x0000000000048c prog.kernel_launch(%1327:tensor<[1, 192, 1536], Float32, CPU>) -> (%1327:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,12,128]}]
        addr:0x0000000000048d prog.kernel_launch(%1328:tensor<[1, 192, 256], Float32, CPU>) -> (%1328:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x0000000000048e prog.kernel_launch(%1329:tensor<[1, 192, 256], Float32, CPU>) -> (%1329:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x0000000000048f prog.kernel_launch(%1327:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1330:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000490 prog.kernel_launch(%1328:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1331:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000491 prog.kernel_launch(%1329:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1332:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000492 prog.kernel_launch(%1330:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1333:tensor<[1, 12, 192, 128], Float32, CPU>)[symbol_name:model.layers.2.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000493 prog.kernel_launch(%1331:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1334:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.2.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000494 prog.kernel_launch(%1334:tensor<[1, 2, 192, 128], Float32, CPU>, %1332:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1335:tensor<[1, 2, 192, 128], Float32, CPU>, %1336:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.2.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000495 prog.kernel_launch(%1333:tensor<[1, 12, 192, 128], Float32, CPU>, %1335:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1337:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x00000000000496 prog.kernel_launch(%1337:tensor<[1, 12, 192, 192], Float32, CPU>, %1338:tensor<[1], Float32, CPU>) -> (%1339:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x00000000000497 prog.kernel_launch(%1339:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1340:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.2.self_attn.mask, op_type:CausalMask, op_options:null]
        addr:0x00000000000498 prog.kernel_launch(%1340:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1341:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.2.self_attn.softmax, op_type:Softmax, op_options:null]
        addr:0x00000000000499 prog.kernel_launch(%1341:tensor<[1, 12, 192, 192], Float32, CPU>, %1336:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1342:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x0000000000049a prog.kernel_launch(%1342:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1343:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000049b prog.kernel_launch(%1343:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1343:tensor<[1, 192, 1536], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,1536]}]
        addr:0x0000000000049c prog.kernel_launch(%1343:tensor<[1, 192, 1536], Float32, CPU>) -> (%1344:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.2.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000049d prog.ret
        addr:0x0000000000049e prog.label[symbol:model.layers.2.mlp.__entry]
        addr:0x0000000000049f prog.kernel_launch(%1346:tensor<[1, 192, 1536], Float32, CPU>) -> (%1347:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.2.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000004a0 prog.kernel_launch(%1347:tensor<[1, 192, 8960], Float32, CPU>) -> (%1348:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.2.mlp.act, op_type:SiLU, op_options:null]
        addr:0x000000000004a1 prog.kernel_launch(%1346:tensor<[1, 192, 1536], Float32, CPU>) -> (%1349:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.2.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000004a2 prog.kernel_launch(%1348:tensor<[1, 192, 8960], Float32, CPU>, %1349:tensor<[1, 192, 8960], Float32, CPU>) -> (%1350:tensor<[1, 192, 8960], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x000000000004a3 prog.kernel_launch(%1350:tensor<[1, 192, 8960], Float32, CPU>) -> (%1351:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.2.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x000000000004a4 prog.ret
        addr:0x000000000004a5 prog.label[symbol:model.layers.3.__entry]
        addr:0x000000000004a6 prog.kernel_launch(%1352:tensor<[1, 192, 1536], Float32, CPU>) -> (%1353:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.3.input_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x000000000004a7 prog.jump model.layers.3.self_attn.__entry[offset:6]
        addr:0x000000000004a8 prog.kernel_launch(%1371:tensor<[1, 192, 1536], Float32, CPU>, %1352:tensor<[1, 192, 1536], Float32, CPU>) -> (%1372:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000004a9 prog.kernel_launch(%1372:tensor<[1, 192, 1536], Float32, CPU>) -> (%1373:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.3.post_attention_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x000000000004aa prog.jump model.layers.3.mlp.__entry[offset:25]
        addr:0x000000000004ab prog.kernel_launch(%1378:tensor<[1, 192, 1536], Float32, CPU>, %1372:tensor<[1, 192, 1536], Float32, CPU>) -> (%1379:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000004ac prog.ret
        addr:0x000000000004ad prog.label[symbol:model.layers.3.self_attn.__entry]
        addr:0x000000000004ae prog.kernel_launch(%1353:tensor<[1, 192, 1536], Float32, CPU>) -> (%1354:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.3.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000004af prog.kernel_launch(%1353:tensor<[1, 192, 1536], Float32, CPU>) -> (%1355:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.3.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000004b0 prog.kernel_launch(%1353:tensor<[1, 192, 1536], Float32, CPU>) -> (%1356:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.3.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000004b1 prog.kernel_launch(%1354:tensor<[1, 192, 1536], Float32, CPU>) -> (%1354:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,12,128]}]
        addr:0x000000000004b2 prog.kernel_launch(%1355:tensor<[1, 192, 256], Float32, CPU>) -> (%1355:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x000000000004b3 prog.kernel_launch(%1356:tensor<[1, 192, 256], Float32, CPU>) -> (%1356:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x000000000004b4 prog.kernel_launch(%1354:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1357:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000004b5 prog.kernel_launch(%1355:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1358:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000004b6 prog.kernel_launch(%1356:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1359:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000004b7 prog.kernel_launch(%1357:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1360:tensor<[1, 12, 192, 128], Float32, CPU>)[symbol_name:model.layers.3.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000004b8 prog.kernel_launch(%1358:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1361:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.3.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000004b9 prog.kernel_launch(%1361:tensor<[1, 2, 192, 128], Float32, CPU>, %1359:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1362:tensor<[1, 2, 192, 128], Float32, CPU>, %1363:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.3.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x000000000004ba prog.kernel_launch(%1360:tensor<[1, 12, 192, 128], Float32, CPU>, %1362:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1364:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x000000000004bb prog.kernel_launch(%1364:tensor<[1, 12, 192, 192], Float32, CPU>, %1365:tensor<[1], Float32, CPU>) -> (%1366:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x000000000004bc prog.kernel_launch(%1366:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1367:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.3.self_attn.mask, op_type:CausalMask, op_options:null]
        addr:0x000000000004bd prog.kernel_launch(%1367:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1368:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.3.self_attn.softmax, op_type:Softmax, op_options:null]
        addr:0x000000000004be prog.kernel_launch(%1368:tensor<[1, 12, 192, 192], Float32, CPU>, %1363:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1369:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x000000000004bf prog.kernel_launch(%1369:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1370:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000004c0 prog.kernel_launch(%1370:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1370:tensor<[1, 192, 1536], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,1536]}]
        addr:0x000000000004c1 prog.kernel_launch(%1370:tensor<[1, 192, 1536], Float32, CPU>) -> (%1371:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.3.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000004c2 prog.ret
        addr:0x000000000004c3 prog.label[symbol:model.layers.3.mlp.__entry]
        addr:0x000000000004c4 prog.kernel_launch(%1373:tensor<[1, 192, 1536], Float32, CPU>) -> (%1374:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.3.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000004c5 prog.kernel_launch(%1374:tensor<[1, 192, 8960], Float32, CPU>) -> (%1375:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.3.mlp.act, op_type:SiLU, op_options:null]
        addr:0x000000000004c6 prog.kernel_launch(%1373:tensor<[1, 192, 1536], Float32, CPU>) -> (%1376:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.3.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000004c7 prog.kernel_launch(%1375:tensor<[1, 192, 8960], Float32, CPU>, %1376:tensor<[1, 192, 8960], Float32, CPU>) -> (%1377:tensor<[1, 192, 8960], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x000000000004c8 prog.kernel_launch(%1377:tensor<[1, 192, 8960], Float32, CPU>) -> (%1378:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.3.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x000000000004c9 prog.ret
        addr:0x000000000004ca prog.label[symbol:model.layers.4.__entry]
        addr:0x000000000004cb prog.kernel_launch(%1379:tensor<[1, 192, 1536], Float32, CPU>) -> (%1380:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.4.input_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x000000000004cc prog.jump model.layers.4.self_attn.__entry[offset:6]
        addr:0x000000000004cd prog.kernel_launch(%1398:tensor<[1, 192, 1536], Float32, CPU>, %1379:tensor<[1, 192, 1536], Float32, CPU>) -> (%1399:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000004ce prog.kernel_launch(%1399:tensor<[1, 192, 1536], Float32, CPU>) -> (%1400:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.4.post_attention_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x000000000004cf prog.jump model.layers.4.mlp.__entry[offset:25]
        addr:0x000000000004d0 prog.kernel_launch(%1405:tensor<[1, 192, 1536], Float32, CPU>, %1399:tensor<[1, 192, 1536], Float32, CPU>) -> (%1406:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000004d1 prog.ret
        addr:0x000000000004d2 prog.label[symbol:model.layers.4.self_attn.__entry]
        addr:0x000000000004d3 prog.kernel_launch(%1380:tensor<[1, 192, 1536], Float32, CPU>) -> (%1381:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.4.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000004d4 prog.kernel_launch(%1380:tensor<[1, 192, 1536], Float32, CPU>) -> (%1382:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.4.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000004d5 prog.kernel_launch(%1380:tensor<[1, 192, 1536], Float32, CPU>) -> (%1383:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.4.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000004d6 prog.kernel_launch(%1381:tensor<[1, 192, 1536], Float32, CPU>) -> (%1381:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,12,128]}]
        addr:0x000000000004d7 prog.kernel_launch(%1382:tensor<[1, 192, 256], Float32, CPU>) -> (%1382:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x000000000004d8 prog.kernel_launch(%1383:tensor<[1, 192, 256], Float32, CPU>) -> (%1383:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x000000000004d9 prog.kernel_launch(%1381:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1384:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000004da prog.kernel_launch(%1382:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1385:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000004db prog.kernel_launch(%1383:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1386:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000004dc prog.kernel_launch(%1384:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1387:tensor<[1, 12, 192, 128], Float32, CPU>)[symbol_name:model.layers.4.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000004dd prog.kernel_launch(%1385:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1388:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.4.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000004de prog.kernel_launch(%1388:tensor<[1, 2, 192, 128], Float32, CPU>, %1386:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1389:tensor<[1, 2, 192, 128], Float32, CPU>, %1390:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.4.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x000000000004df prog.kernel_launch(%1387:tensor<[1, 12, 192, 128], Float32, CPU>, %1389:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1391:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x000000000004e0 prog.kernel_launch(%1391:tensor<[1, 12, 192, 192], Float32, CPU>, %1392:tensor<[1], Float32, CPU>) -> (%1393:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x000000000004e1 prog.kernel_launch(%1393:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1394:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.4.self_attn.mask, op_type:CausalMask, op_options:null]
        addr:0x000000000004e2 prog.kernel_launch(%1394:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1395:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.4.self_attn.softmax, op_type:Softmax, op_options:null]
        addr:0x000000000004e3 prog.kernel_launch(%1395:tensor<[1, 12, 192, 192], Float32, CPU>, %1390:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1396:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x000000000004e4 prog.kernel_launch(%1396:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1397:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000004e5 prog.kernel_launch(%1397:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1397:tensor<[1, 192, 1536], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,1536]}]
        addr:0x000000000004e6 prog.kernel_launch(%1397:tensor<[1, 192, 1536], Float32, CPU>) -> (%1398:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.4.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000004e7 prog.ret
        addr:0x000000000004e8 prog.label[symbol:model.layers.4.mlp.__entry]
        addr:0x000000000004e9 prog.kernel_launch(%1400:tensor<[1, 192, 1536], Float32, CPU>) -> (%1401:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.4.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000004ea prog.kernel_launch(%1401:tensor<[1, 192, 8960], Float32, CPU>) -> (%1402:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.4.mlp.act, op_type:SiLU, op_options:null]
        addr:0x000000000004eb prog.kernel_launch(%1400:tensor<[1, 192, 1536], Float32, CPU>) -> (%1403:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.4.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000004ec prog.kernel_launch(%1402:tensor<[1, 192, 8960], Float32, CPU>, %1403:tensor<[1, 192, 8960], Float32, CPU>) -> (%1404:tensor<[1, 192, 8960], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x000000000004ed prog.kernel_launch(%1404:tensor<[1, 192, 8960], Float32, CPU>) -> (%1405:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.4.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x000000000004ee prog.ret
        addr:0x000000000004ef prog.label[symbol:model.layers.5.__entry]
        addr:0x000000000004f0 prog.kernel_launch(%1406:tensor<[1, 192, 1536], Float32, CPU>) -> (%1407:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.5.input_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x000000000004f1 prog.jump model.layers.5.self_attn.__entry[offset:6]
        addr:0x000000000004f2 prog.kernel_launch(%1425:tensor<[1, 192, 1536], Float32, CPU>, %1406:tensor<[1, 192, 1536], Float32, CPU>) -> (%1426:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000004f3 prog.kernel_launch(%1426:tensor<[1, 192, 1536], Float32, CPU>) -> (%1427:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.5.post_attention_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x000000000004f4 prog.jump model.layers.5.mlp.__entry[offset:25]
        addr:0x000000000004f5 prog.kernel_launch(%1432:tensor<[1, 192, 1536], Float32, CPU>, %1426:tensor<[1, 192, 1536], Float32, CPU>) -> (%1433:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000004f6 prog.ret
        addr:0x000000000004f7 prog.label[symbol:model.layers.5.self_attn.__entry]
        addr:0x000000000004f8 prog.kernel_launch(%1407:tensor<[1, 192, 1536], Float32, CPU>) -> (%1408:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.5.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000004f9 prog.kernel_launch(%1407:tensor<[1, 192, 1536], Float32, CPU>) -> (%1409:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.5.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000004fa prog.kernel_launch(%1407:tensor<[1, 192, 1536], Float32, CPU>) -> (%1410:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.5.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000004fb prog.kernel_launch(%1408:tensor<[1, 192, 1536], Float32, CPU>) -> (%1408:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,12,128]}]
        addr:0x000000000004fc prog.kernel_launch(%1409:tensor<[1, 192, 256], Float32, CPU>) -> (%1409:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x000000000004fd prog.kernel_launch(%1410:tensor<[1, 192, 256], Float32, CPU>) -> (%1410:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x000000000004fe prog.kernel_launch(%1408:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1411:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000004ff prog.kernel_launch(%1409:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1412:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000500 prog.kernel_launch(%1410:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1413:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000501 prog.kernel_launch(%1411:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1414:tensor<[1, 12, 192, 128], Float32, CPU>)[symbol_name:model.layers.5.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000502 prog.kernel_launch(%1412:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1415:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.5.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000503 prog.kernel_launch(%1415:tensor<[1, 2, 192, 128], Float32, CPU>, %1413:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1416:tensor<[1, 2, 192, 128], Float32, CPU>, %1417:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.5.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000504 prog.kernel_launch(%1414:tensor<[1, 12, 192, 128], Float32, CPU>, %1416:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1418:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x00000000000505 prog.kernel_launch(%1418:tensor<[1, 12, 192, 192], Float32, CPU>, %1419:tensor<[1], Float32, CPU>) -> (%1420:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x00000000000506 prog.kernel_launch(%1420:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1421:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.5.self_attn.mask, op_type:CausalMask, op_options:null]
        addr:0x00000000000507 prog.kernel_launch(%1421:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1422:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.5.self_attn.softmax, op_type:Softmax, op_options:null]
        addr:0x00000000000508 prog.kernel_launch(%1422:tensor<[1, 12, 192, 192], Float32, CPU>, %1417:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1423:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x00000000000509 prog.kernel_launch(%1423:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1424:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000050a prog.kernel_launch(%1424:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1424:tensor<[1, 192, 1536], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,1536]}]
        addr:0x0000000000050b prog.kernel_launch(%1424:tensor<[1, 192, 1536], Float32, CPU>) -> (%1425:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.5.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000050c prog.ret
        addr:0x0000000000050d prog.label[symbol:model.layers.5.mlp.__entry]
        addr:0x0000000000050e prog.kernel_launch(%1427:tensor<[1, 192, 1536], Float32, CPU>) -> (%1428:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.5.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x0000000000050f prog.kernel_launch(%1428:tensor<[1, 192, 8960], Float32, CPU>) -> (%1429:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.5.mlp.act, op_type:SiLU, op_options:null]
        addr:0x00000000000510 prog.kernel_launch(%1427:tensor<[1, 192, 1536], Float32, CPU>) -> (%1430:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.5.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000511 prog.kernel_launch(%1429:tensor<[1, 192, 8960], Float32, CPU>, %1430:tensor<[1, 192, 8960], Float32, CPU>) -> (%1431:tensor<[1, 192, 8960], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x00000000000512 prog.kernel_launch(%1431:tensor<[1, 192, 8960], Float32, CPU>) -> (%1432:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.5.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x00000000000513 prog.ret
        addr:0x00000000000514 prog.label[symbol:model.layers.6.__entry]
        addr:0x00000000000515 prog.kernel_launch(%1433:tensor<[1, 192, 1536], Float32, CPU>) -> (%1434:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.6.input_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x00000000000516 prog.jump model.layers.6.self_attn.__entry[offset:6]
        addr:0x00000000000517 prog.kernel_launch(%1452:tensor<[1, 192, 1536], Float32, CPU>, %1433:tensor<[1, 192, 1536], Float32, CPU>) -> (%1453:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x00000000000518 prog.kernel_launch(%1453:tensor<[1, 192, 1536], Float32, CPU>) -> (%1454:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.6.post_attention_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x00000000000519 prog.jump model.layers.6.mlp.__entry[offset:25]
        addr:0x0000000000051a prog.kernel_launch(%1459:tensor<[1, 192, 1536], Float32, CPU>, %1453:tensor<[1, 192, 1536], Float32, CPU>) -> (%1460:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x0000000000051b prog.ret
        addr:0x0000000000051c prog.label[symbol:model.layers.6.self_attn.__entry]
        addr:0x0000000000051d prog.kernel_launch(%1434:tensor<[1, 192, 1536], Float32, CPU>) -> (%1435:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.6.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000051e prog.kernel_launch(%1434:tensor<[1, 192, 1536], Float32, CPU>) -> (%1436:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.6.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x0000000000051f prog.kernel_launch(%1434:tensor<[1, 192, 1536], Float32, CPU>) -> (%1437:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.6.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000520 prog.kernel_launch(%1435:tensor<[1, 192, 1536], Float32, CPU>) -> (%1435:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,12,128]}]
        addr:0x00000000000521 prog.kernel_launch(%1436:tensor<[1, 192, 256], Float32, CPU>) -> (%1436:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x00000000000522 prog.kernel_launch(%1437:tensor<[1, 192, 256], Float32, CPU>) -> (%1437:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x00000000000523 prog.kernel_launch(%1435:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1438:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000524 prog.kernel_launch(%1436:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1439:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000525 prog.kernel_launch(%1437:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1440:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000526 prog.kernel_launch(%1438:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1441:tensor<[1, 12, 192, 128], Float32, CPU>)[symbol_name:model.layers.6.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000527 prog.kernel_launch(%1439:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1442:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.6.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000528 prog.kernel_launch(%1442:tensor<[1, 2, 192, 128], Float32, CPU>, %1440:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1443:tensor<[1, 2, 192, 128], Float32, CPU>, %1444:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.6.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000529 prog.kernel_launch(%1441:tensor<[1, 12, 192, 128], Float32, CPU>, %1443:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1445:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x0000000000052a prog.kernel_launch(%1445:tensor<[1, 12, 192, 192], Float32, CPU>, %1446:tensor<[1], Float32, CPU>) -> (%1447:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x0000000000052b prog.kernel_launch(%1447:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1448:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.6.self_attn.mask, op_type:CausalMask, op_options:null]
        addr:0x0000000000052c prog.kernel_launch(%1448:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1449:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.6.self_attn.softmax, op_type:Softmax, op_options:null]
        addr:0x0000000000052d prog.kernel_launch(%1449:tensor<[1, 12, 192, 192], Float32, CPU>, %1444:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1450:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x0000000000052e prog.kernel_launch(%1450:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1451:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000052f prog.kernel_launch(%1451:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1451:tensor<[1, 192, 1536], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,1536]}]
        addr:0x00000000000530 prog.kernel_launch(%1451:tensor<[1, 192, 1536], Float32, CPU>) -> (%1452:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.6.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000531 prog.ret
        addr:0x00000000000532 prog.label[symbol:model.layers.6.mlp.__entry]
        addr:0x00000000000533 prog.kernel_launch(%1454:tensor<[1, 192, 1536], Float32, CPU>) -> (%1455:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.6.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000534 prog.kernel_launch(%1455:tensor<[1, 192, 8960], Float32, CPU>) -> (%1456:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.6.mlp.act, op_type:SiLU, op_options:null]
        addr:0x00000000000535 prog.kernel_launch(%1454:tensor<[1, 192, 1536], Float32, CPU>) -> (%1457:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.6.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000536 prog.kernel_launch(%1456:tensor<[1, 192, 8960], Float32, CPU>, %1457:tensor<[1, 192, 8960], Float32, CPU>) -> (%1458:tensor<[1, 192, 8960], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x00000000000537 prog.kernel_launch(%1458:tensor<[1, 192, 8960], Float32, CPU>) -> (%1459:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.6.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x00000000000538 prog.ret
        addr:0x00000000000539 prog.label[symbol:model.layers.7.__entry]
        addr:0x0000000000053a prog.kernel_launch(%1460:tensor<[1, 192, 1536], Float32, CPU>) -> (%1461:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.7.input_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x0000000000053b prog.jump model.layers.7.self_attn.__entry[offset:6]
        addr:0x0000000000053c prog.kernel_launch(%1479:tensor<[1, 192, 1536], Float32, CPU>, %1460:tensor<[1, 192, 1536], Float32, CPU>) -> (%1480:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x0000000000053d prog.kernel_launch(%1480:tensor<[1, 192, 1536], Float32, CPU>) -> (%1481:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.7.post_attention_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x0000000000053e prog.jump model.layers.7.mlp.__entry[offset:25]
        addr:0x0000000000053f prog.kernel_launch(%1486:tensor<[1, 192, 1536], Float32, CPU>, %1480:tensor<[1, 192, 1536], Float32, CPU>) -> (%1487:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x00000000000540 prog.ret
        addr:0x00000000000541 prog.label[symbol:model.layers.7.self_attn.__entry]
        addr:0x00000000000542 prog.kernel_launch(%1461:tensor<[1, 192, 1536], Float32, CPU>) -> (%1462:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.7.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000543 prog.kernel_launch(%1461:tensor<[1, 192, 1536], Float32, CPU>) -> (%1463:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.7.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000544 prog.kernel_launch(%1461:tensor<[1, 192, 1536], Float32, CPU>) -> (%1464:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.7.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000545 prog.kernel_launch(%1462:tensor<[1, 192, 1536], Float32, CPU>) -> (%1462:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,12,128]}]
        addr:0x00000000000546 prog.kernel_launch(%1463:tensor<[1, 192, 256], Float32, CPU>) -> (%1463:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x00000000000547 prog.kernel_launch(%1464:tensor<[1, 192, 256], Float32, CPU>) -> (%1464:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x00000000000548 prog.kernel_launch(%1462:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1465:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000549 prog.kernel_launch(%1463:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1466:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000054a prog.kernel_launch(%1464:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1467:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000054b prog.kernel_launch(%1465:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1468:tensor<[1, 12, 192, 128], Float32, CPU>)[symbol_name:model.layers.7.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x0000000000054c prog.kernel_launch(%1466:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1469:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.7.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x0000000000054d prog.kernel_launch(%1469:tensor<[1, 2, 192, 128], Float32, CPU>, %1467:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1470:tensor<[1, 2, 192, 128], Float32, CPU>, %1471:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.7.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x0000000000054e prog.kernel_launch(%1468:tensor<[1, 12, 192, 128], Float32, CPU>, %1470:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1472:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x0000000000054f prog.kernel_launch(%1472:tensor<[1, 12, 192, 192], Float32, CPU>, %1473:tensor<[1], Float32, CPU>) -> (%1474:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x00000000000550 prog.kernel_launch(%1474:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1475:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.7.self_attn.mask, op_type:CausalMask, op_options:null]
        addr:0x00000000000551 prog.kernel_launch(%1475:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1476:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.7.self_attn.softmax, op_type:Softmax, op_options:null]
        addr:0x00000000000552 prog.kernel_launch(%1476:tensor<[1, 12, 192, 192], Float32, CPU>, %1471:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1477:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x00000000000553 prog.kernel_launch(%1477:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1478:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000554 prog.kernel_launch(%1478:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1478:tensor<[1, 192, 1536], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,1536]}]
        addr:0x00000000000555 prog.kernel_launch(%1478:tensor<[1, 192, 1536], Float32, CPU>) -> (%1479:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.7.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000556 prog.ret
        addr:0x00000000000557 prog.label[symbol:model.layers.7.mlp.__entry]
        addr:0x00000000000558 prog.kernel_launch(%1481:tensor<[1, 192, 1536], Float32, CPU>) -> (%1482:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.7.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000559 prog.kernel_launch(%1482:tensor<[1, 192, 8960], Float32, CPU>) -> (%1483:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.7.mlp.act, op_type:SiLU, op_options:null]
        addr:0x0000000000055a prog.kernel_launch(%1481:tensor<[1, 192, 1536], Float32, CPU>) -> (%1484:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.7.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x0000000000055b prog.kernel_launch(%1483:tensor<[1, 192, 8960], Float32, CPU>, %1484:tensor<[1, 192, 8960], Float32, CPU>) -> (%1485:tensor<[1, 192, 8960], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x0000000000055c prog.kernel_launch(%1485:tensor<[1, 192, 8960], Float32, CPU>) -> (%1486:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.7.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x0000000000055d prog.ret
        addr:0x0000000000055e prog.label[symbol:model.layers.8.__entry]
        addr:0x0000000000055f prog.kernel_launch(%1487:tensor<[1, 192, 1536], Float32, CPU>) -> (%1488:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.8.input_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x00000000000560 prog.jump model.layers.8.self_attn.__entry[offset:6]
        addr:0x00000000000561 prog.kernel_launch(%1506:tensor<[1, 192, 1536], Float32, CPU>, %1487:tensor<[1, 192, 1536], Float32, CPU>) -> (%1507:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x00000000000562 prog.kernel_launch(%1507:tensor<[1, 192, 1536], Float32, CPU>) -> (%1508:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.8.post_attention_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x00000000000563 prog.jump model.layers.8.mlp.__entry[offset:25]
        addr:0x00000000000564 prog.kernel_launch(%1513:tensor<[1, 192, 1536], Float32, CPU>, %1507:tensor<[1, 192, 1536], Float32, CPU>) -> (%1514:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x00000000000565 prog.ret
        addr:0x00000000000566 prog.label[symbol:model.layers.8.self_attn.__entry]
        addr:0x00000000000567 prog.kernel_launch(%1488:tensor<[1, 192, 1536], Float32, CPU>) -> (%1489:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.8.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000568 prog.kernel_launch(%1488:tensor<[1, 192, 1536], Float32, CPU>) -> (%1490:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.8.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000569 prog.kernel_launch(%1488:tensor<[1, 192, 1536], Float32, CPU>) -> (%1491:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.8.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x0000000000056a prog.kernel_launch(%1489:tensor<[1, 192, 1536], Float32, CPU>) -> (%1489:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,12,128]}]
        addr:0x0000000000056b prog.kernel_launch(%1490:tensor<[1, 192, 256], Float32, CPU>) -> (%1490:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x0000000000056c prog.kernel_launch(%1491:tensor<[1, 192, 256], Float32, CPU>) -> (%1491:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x0000000000056d prog.kernel_launch(%1489:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1492:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000056e prog.kernel_launch(%1490:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1493:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000056f prog.kernel_launch(%1491:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1494:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000570 prog.kernel_launch(%1492:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1495:tensor<[1, 12, 192, 128], Float32, CPU>)[symbol_name:model.layers.8.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000571 prog.kernel_launch(%1493:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1496:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.8.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000572 prog.kernel_launch(%1496:tensor<[1, 2, 192, 128], Float32, CPU>, %1494:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1497:tensor<[1, 2, 192, 128], Float32, CPU>, %1498:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.8.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000573 prog.kernel_launch(%1495:tensor<[1, 12, 192, 128], Float32, CPU>, %1497:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1499:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x00000000000574 prog.kernel_launch(%1499:tensor<[1, 12, 192, 192], Float32, CPU>, %1500:tensor<[1], Float32, CPU>) -> (%1501:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x00000000000575 prog.kernel_launch(%1501:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1502:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.8.self_attn.mask, op_type:CausalMask, op_options:null]
        addr:0x00000000000576 prog.kernel_launch(%1502:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1503:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.8.self_attn.softmax, op_type:Softmax, op_options:null]
        addr:0x00000000000577 prog.kernel_launch(%1503:tensor<[1, 12, 192, 192], Float32, CPU>, %1498:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1504:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x00000000000578 prog.kernel_launch(%1504:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1505:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000579 prog.kernel_launch(%1505:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1505:tensor<[1, 192, 1536], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,1536]}]
        addr:0x0000000000057a prog.kernel_launch(%1505:tensor<[1, 192, 1536], Float32, CPU>) -> (%1506:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.8.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000057b prog.ret
        addr:0x0000000000057c prog.label[symbol:model.layers.8.mlp.__entry]
        addr:0x0000000000057d prog.kernel_launch(%1508:tensor<[1, 192, 1536], Float32, CPU>) -> (%1509:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.8.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x0000000000057e prog.kernel_launch(%1509:tensor<[1, 192, 8960], Float32, CPU>) -> (%1510:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.8.mlp.act, op_type:SiLU, op_options:null]
        addr:0x0000000000057f prog.kernel_launch(%1508:tensor<[1, 192, 1536], Float32, CPU>) -> (%1511:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.8.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000580 prog.kernel_launch(%1510:tensor<[1, 192, 8960], Float32, CPU>, %1511:tensor<[1, 192, 8960], Float32, CPU>) -> (%1512:tensor<[1, 192, 8960], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x00000000000581 prog.kernel_launch(%1512:tensor<[1, 192, 8960], Float32, CPU>) -> (%1513:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.8.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x00000000000582 prog.ret
        addr:0x00000000000583 prog.label[symbol:model.layers.9.__entry]
        addr:0x00000000000584 prog.kernel_launch(%1514:tensor<[1, 192, 1536], Float32, CPU>) -> (%1515:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.9.input_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x00000000000585 prog.jump model.layers.9.self_attn.__entry[offset:6]
        addr:0x00000000000586 prog.kernel_launch(%1533:tensor<[1, 192, 1536], Float32, CPU>, %1514:tensor<[1, 192, 1536], Float32, CPU>) -> (%1534:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x00000000000587 prog.kernel_launch(%1534:tensor<[1, 192, 1536], Float32, CPU>) -> (%1535:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.9.post_attention_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x00000000000588 prog.jump model.layers.9.mlp.__entry[offset:25]
        addr:0x00000000000589 prog.kernel_launch(%1540:tensor<[1, 192, 1536], Float32, CPU>, %1534:tensor<[1, 192, 1536], Float32, CPU>) -> (%1541:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x0000000000058a prog.ret
        addr:0x0000000000058b prog.label[symbol:model.layers.9.self_attn.__entry]
        addr:0x0000000000058c prog.kernel_launch(%1515:tensor<[1, 192, 1536], Float32, CPU>) -> (%1516:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.9.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000058d prog.kernel_launch(%1515:tensor<[1, 192, 1536], Float32, CPU>) -> (%1517:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.9.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x0000000000058e prog.kernel_launch(%1515:tensor<[1, 192, 1536], Float32, CPU>) -> (%1518:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.9.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x0000000000058f prog.kernel_launch(%1516:tensor<[1, 192, 1536], Float32, CPU>) -> (%1516:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,12,128]}]
        addr:0x00000000000590 prog.kernel_launch(%1517:tensor<[1, 192, 256], Float32, CPU>) -> (%1517:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x00000000000591 prog.kernel_launch(%1518:tensor<[1, 192, 256], Float32, CPU>) -> (%1518:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x00000000000592 prog.kernel_launch(%1516:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1519:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000593 prog.kernel_launch(%1517:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1520:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000594 prog.kernel_launch(%1518:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1521:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000595 prog.kernel_launch(%1519:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1522:tensor<[1, 12, 192, 128], Float32, CPU>)[symbol_name:model.layers.9.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000596 prog.kernel_launch(%1520:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1523:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.9.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000597 prog.kernel_launch(%1523:tensor<[1, 2, 192, 128], Float32, CPU>, %1521:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1524:tensor<[1, 2, 192, 128], Float32, CPU>, %1525:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.9.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000598 prog.kernel_launch(%1522:tensor<[1, 12, 192, 128], Float32, CPU>, %1524:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1526:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x00000000000599 prog.kernel_launch(%1526:tensor<[1, 12, 192, 192], Float32, CPU>, %1527:tensor<[1], Float32, CPU>) -> (%1528:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x0000000000059a prog.kernel_launch(%1528:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1529:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.9.self_attn.mask, op_type:CausalMask, op_options:null]
        addr:0x0000000000059b prog.kernel_launch(%1529:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1530:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.9.self_attn.softmax, op_type:Softmax, op_options:null]
        addr:0x0000000000059c prog.kernel_launch(%1530:tensor<[1, 12, 192, 192], Float32, CPU>, %1525:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1531:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x0000000000059d prog.kernel_launch(%1531:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1532:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000059e prog.kernel_launch(%1532:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1532:tensor<[1, 192, 1536], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,1536]}]
        addr:0x0000000000059f prog.kernel_launch(%1532:tensor<[1, 192, 1536], Float32, CPU>) -> (%1533:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.9.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000005a0 prog.ret
        addr:0x000000000005a1 prog.label[symbol:model.layers.9.mlp.__entry]
        addr:0x000000000005a2 prog.kernel_launch(%1535:tensor<[1, 192, 1536], Float32, CPU>) -> (%1536:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.9.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000005a3 prog.kernel_launch(%1536:tensor<[1, 192, 8960], Float32, CPU>) -> (%1537:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.9.mlp.act, op_type:SiLU, op_options:null]
        addr:0x000000000005a4 prog.kernel_launch(%1535:tensor<[1, 192, 1536], Float32, CPU>) -> (%1538:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.9.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000005a5 prog.kernel_launch(%1537:tensor<[1, 192, 8960], Float32, CPU>, %1538:tensor<[1, 192, 8960], Float32, CPU>) -> (%1539:tensor<[1, 192, 8960], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x000000000005a6 prog.kernel_launch(%1539:tensor<[1, 192, 8960], Float32, CPU>) -> (%1540:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.9.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x000000000005a7 prog.ret
        addr:0x000000000005a8 prog.label[symbol:model.layers.10.__entry]
        addr:0x000000000005a9 prog.kernel_launch(%1541:tensor<[1, 192, 1536], Float32, CPU>) -> (%1542:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.10.input_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x000000000005aa prog.jump model.layers.10.self_attn.__entry[offset:6]
        addr:0x000000000005ab prog.kernel_launch(%1560:tensor<[1, 192, 1536], Float32, CPU>, %1541:tensor<[1, 192, 1536], Float32, CPU>) -> (%1561:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000005ac prog.kernel_launch(%1561:tensor<[1, 192, 1536], Float32, CPU>) -> (%1562:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.10.post_attention_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x000000000005ad prog.jump model.layers.10.mlp.__entry[offset:25]
        addr:0x000000000005ae prog.kernel_launch(%1567:tensor<[1, 192, 1536], Float32, CPU>, %1561:tensor<[1, 192, 1536], Float32, CPU>) -> (%1568:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000005af prog.ret
        addr:0x000000000005b0 prog.label[symbol:model.layers.10.self_attn.__entry]
        addr:0x000000000005b1 prog.kernel_launch(%1542:tensor<[1, 192, 1536], Float32, CPU>) -> (%1543:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.10.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000005b2 prog.kernel_launch(%1542:tensor<[1, 192, 1536], Float32, CPU>) -> (%1544:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.10.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000005b3 prog.kernel_launch(%1542:tensor<[1, 192, 1536], Float32, CPU>) -> (%1545:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.10.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000005b4 prog.kernel_launch(%1543:tensor<[1, 192, 1536], Float32, CPU>) -> (%1543:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,12,128]}]
        addr:0x000000000005b5 prog.kernel_launch(%1544:tensor<[1, 192, 256], Float32, CPU>) -> (%1544:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x000000000005b6 prog.kernel_launch(%1545:tensor<[1, 192, 256], Float32, CPU>) -> (%1545:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x000000000005b7 prog.kernel_launch(%1543:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1546:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000005b8 prog.kernel_launch(%1544:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1547:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000005b9 prog.kernel_launch(%1545:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1548:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000005ba prog.kernel_launch(%1546:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1549:tensor<[1, 12, 192, 128], Float32, CPU>)[symbol_name:model.layers.10.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000005bb prog.kernel_launch(%1547:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1550:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.10.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000005bc prog.kernel_launch(%1550:tensor<[1, 2, 192, 128], Float32, CPU>, %1548:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1551:tensor<[1, 2, 192, 128], Float32, CPU>, %1552:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.10.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x000000000005bd prog.kernel_launch(%1549:tensor<[1, 12, 192, 128], Float32, CPU>, %1551:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1553:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x000000000005be prog.kernel_launch(%1553:tensor<[1, 12, 192, 192], Float32, CPU>, %1554:tensor<[1], Float32, CPU>) -> (%1555:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x000000000005bf prog.kernel_launch(%1555:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1556:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.10.self_attn.mask, op_type:CausalMask, op_options:null]
        addr:0x000000000005c0 prog.kernel_launch(%1556:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1557:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.10.self_attn.softmax, op_type:Softmax, op_options:null]
        addr:0x000000000005c1 prog.kernel_launch(%1557:tensor<[1, 12, 192, 192], Float32, CPU>, %1552:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1558:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x000000000005c2 prog.kernel_launch(%1558:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1559:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000005c3 prog.kernel_launch(%1559:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1559:tensor<[1, 192, 1536], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,1536]}]
        addr:0x000000000005c4 prog.kernel_launch(%1559:tensor<[1, 192, 1536], Float32, CPU>) -> (%1560:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.10.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000005c5 prog.ret
        addr:0x000000000005c6 prog.label[symbol:model.layers.10.mlp.__entry]
        addr:0x000000000005c7 prog.kernel_launch(%1562:tensor<[1, 192, 1536], Float32, CPU>) -> (%1563:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.10.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000005c8 prog.kernel_launch(%1563:tensor<[1, 192, 8960], Float32, CPU>) -> (%1564:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.10.mlp.act, op_type:SiLU, op_options:null]
        addr:0x000000000005c9 prog.kernel_launch(%1562:tensor<[1, 192, 1536], Float32, CPU>) -> (%1565:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.10.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000005ca prog.kernel_launch(%1564:tensor<[1, 192, 8960], Float32, CPU>, %1565:tensor<[1, 192, 8960], Float32, CPU>) -> (%1566:tensor<[1, 192, 8960], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x000000000005cb prog.kernel_launch(%1566:tensor<[1, 192, 8960], Float32, CPU>) -> (%1567:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.10.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x000000000005cc prog.ret
        addr:0x000000000005cd prog.label[symbol:model.layers.11.__entry]
        addr:0x000000000005ce prog.kernel_launch(%1568:tensor<[1, 192, 1536], Float32, CPU>) -> (%1569:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.11.input_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x000000000005cf prog.jump model.layers.11.self_attn.__entry[offset:6]
        addr:0x000000000005d0 prog.kernel_launch(%1587:tensor<[1, 192, 1536], Float32, CPU>, %1568:tensor<[1, 192, 1536], Float32, CPU>) -> (%1588:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000005d1 prog.kernel_launch(%1588:tensor<[1, 192, 1536], Float32, CPU>) -> (%1589:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.11.post_attention_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x000000000005d2 prog.jump model.layers.11.mlp.__entry[offset:25]
        addr:0x000000000005d3 prog.kernel_launch(%1594:tensor<[1, 192, 1536], Float32, CPU>, %1588:tensor<[1, 192, 1536], Float32, CPU>) -> (%1595:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000005d4 prog.ret
        addr:0x000000000005d5 prog.label[symbol:model.layers.11.self_attn.__entry]
        addr:0x000000000005d6 prog.kernel_launch(%1569:tensor<[1, 192, 1536], Float32, CPU>) -> (%1570:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.11.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000005d7 prog.kernel_launch(%1569:tensor<[1, 192, 1536], Float32, CPU>) -> (%1571:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.11.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000005d8 prog.kernel_launch(%1569:tensor<[1, 192, 1536], Float32, CPU>) -> (%1572:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.11.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000005d9 prog.kernel_launch(%1570:tensor<[1, 192, 1536], Float32, CPU>) -> (%1570:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,12,128]}]
        addr:0x000000000005da prog.kernel_launch(%1571:tensor<[1, 192, 256], Float32, CPU>) -> (%1571:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x000000000005db prog.kernel_launch(%1572:tensor<[1, 192, 256], Float32, CPU>) -> (%1572:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x000000000005dc prog.kernel_launch(%1570:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1573:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000005dd prog.kernel_launch(%1571:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1574:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000005de prog.kernel_launch(%1572:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1575:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000005df prog.kernel_launch(%1573:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1576:tensor<[1, 12, 192, 128], Float32, CPU>)[symbol_name:model.layers.11.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000005e0 prog.kernel_launch(%1574:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1577:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.11.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000005e1 prog.kernel_launch(%1577:tensor<[1, 2, 192, 128], Float32, CPU>, %1575:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1578:tensor<[1, 2, 192, 128], Float32, CPU>, %1579:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.11.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x000000000005e2 prog.kernel_launch(%1576:tensor<[1, 12, 192, 128], Float32, CPU>, %1578:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1580:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x000000000005e3 prog.kernel_launch(%1580:tensor<[1, 12, 192, 192], Float32, CPU>, %1581:tensor<[1], Float32, CPU>) -> (%1582:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x000000000005e4 prog.kernel_launch(%1582:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1583:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.11.self_attn.mask, op_type:CausalMask, op_options:null]
        addr:0x000000000005e5 prog.kernel_launch(%1583:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1584:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.11.self_attn.softmax, op_type:Softmax, op_options:null]
        addr:0x000000000005e6 prog.kernel_launch(%1584:tensor<[1, 12, 192, 192], Float32, CPU>, %1579:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1585:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x000000000005e7 prog.kernel_launch(%1585:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1586:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000005e8 prog.kernel_launch(%1586:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1586:tensor<[1, 192, 1536], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,1536]}]
        addr:0x000000000005e9 prog.kernel_launch(%1586:tensor<[1, 192, 1536], Float32, CPU>) -> (%1587:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.11.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000005ea prog.ret
        addr:0x000000000005eb prog.label[symbol:model.layers.11.mlp.__entry]
        addr:0x000000000005ec prog.kernel_launch(%1589:tensor<[1, 192, 1536], Float32, CPU>) -> (%1590:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.11.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000005ed prog.kernel_launch(%1590:tensor<[1, 192, 8960], Float32, CPU>) -> (%1591:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.11.mlp.act, op_type:SiLU, op_options:null]
        addr:0x000000000005ee prog.kernel_launch(%1589:tensor<[1, 192, 1536], Float32, CPU>) -> (%1592:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.11.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000005ef prog.kernel_launch(%1591:tensor<[1, 192, 8960], Float32, CPU>, %1592:tensor<[1, 192, 8960], Float32, CPU>) -> (%1593:tensor<[1, 192, 8960], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x000000000005f0 prog.kernel_launch(%1593:tensor<[1, 192, 8960], Float32, CPU>) -> (%1594:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.11.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x000000000005f1 prog.ret
        addr:0x000000000005f2 prog.label[symbol:model.layers.12.__entry]
        addr:0x000000000005f3 prog.kernel_launch(%1595:tensor<[1, 192, 1536], Float32, CPU>) -> (%1596:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.12.input_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x000000000005f4 prog.jump model.layers.12.self_attn.__entry[offset:6]
        addr:0x000000000005f5 prog.kernel_launch(%1614:tensor<[1, 192, 1536], Float32, CPU>, %1595:tensor<[1, 192, 1536], Float32, CPU>) -> (%1615:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000005f6 prog.kernel_launch(%1615:tensor<[1, 192, 1536], Float32, CPU>) -> (%1616:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.12.post_attention_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x000000000005f7 prog.jump model.layers.12.mlp.__entry[offset:25]
        addr:0x000000000005f8 prog.kernel_launch(%1621:tensor<[1, 192, 1536], Float32, CPU>, %1615:tensor<[1, 192, 1536], Float32, CPU>) -> (%1622:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000005f9 prog.ret
        addr:0x000000000005fa prog.label[symbol:model.layers.12.self_attn.__entry]
        addr:0x000000000005fb prog.kernel_launch(%1596:tensor<[1, 192, 1536], Float32, CPU>) -> (%1597:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.12.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000005fc prog.kernel_launch(%1596:tensor<[1, 192, 1536], Float32, CPU>) -> (%1598:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.12.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000005fd prog.kernel_launch(%1596:tensor<[1, 192, 1536], Float32, CPU>) -> (%1599:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.12.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000005fe prog.kernel_launch(%1597:tensor<[1, 192, 1536], Float32, CPU>) -> (%1597:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,12,128]}]
        addr:0x000000000005ff prog.kernel_launch(%1598:tensor<[1, 192, 256], Float32, CPU>) -> (%1598:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x00000000000600 prog.kernel_launch(%1599:tensor<[1, 192, 256], Float32, CPU>) -> (%1599:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x00000000000601 prog.kernel_launch(%1597:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1600:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000602 prog.kernel_launch(%1598:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1601:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000603 prog.kernel_launch(%1599:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1602:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000604 prog.kernel_launch(%1600:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1603:tensor<[1, 12, 192, 128], Float32, CPU>)[symbol_name:model.layers.12.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000605 prog.kernel_launch(%1601:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1604:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.12.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000606 prog.kernel_launch(%1604:tensor<[1, 2, 192, 128], Float32, CPU>, %1602:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1605:tensor<[1, 2, 192, 128], Float32, CPU>, %1606:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.12.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000607 prog.kernel_launch(%1603:tensor<[1, 12, 192, 128], Float32, CPU>, %1605:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1607:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x00000000000608 prog.kernel_launch(%1607:tensor<[1, 12, 192, 192], Float32, CPU>, %1608:tensor<[1], Float32, CPU>) -> (%1609:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x00000000000609 prog.kernel_launch(%1609:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1610:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.12.self_attn.mask, op_type:CausalMask, op_options:null]
        addr:0x0000000000060a prog.kernel_launch(%1610:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1611:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.12.self_attn.softmax, op_type:Softmax, op_options:null]
        addr:0x0000000000060b prog.kernel_launch(%1611:tensor<[1, 12, 192, 192], Float32, CPU>, %1606:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1612:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x0000000000060c prog.kernel_launch(%1612:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1613:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000060d prog.kernel_launch(%1613:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1613:tensor<[1, 192, 1536], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,1536]}]
        addr:0x0000000000060e prog.kernel_launch(%1613:tensor<[1, 192, 1536], Float32, CPU>) -> (%1614:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.12.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000060f prog.ret
        addr:0x00000000000610 prog.label[symbol:model.layers.12.mlp.__entry]
        addr:0x00000000000611 prog.kernel_launch(%1616:tensor<[1, 192, 1536], Float32, CPU>) -> (%1617:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.12.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000612 prog.kernel_launch(%1617:tensor<[1, 192, 8960], Float32, CPU>) -> (%1618:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.12.mlp.act, op_type:SiLU, op_options:null]
        addr:0x00000000000613 prog.kernel_launch(%1616:tensor<[1, 192, 1536], Float32, CPU>) -> (%1619:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.12.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000614 prog.kernel_launch(%1618:tensor<[1, 192, 8960], Float32, CPU>, %1619:tensor<[1, 192, 8960], Float32, CPU>) -> (%1620:tensor<[1, 192, 8960], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x00000000000615 prog.kernel_launch(%1620:tensor<[1, 192, 8960], Float32, CPU>) -> (%1621:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.12.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x00000000000616 prog.ret
        addr:0x00000000000617 prog.label[symbol:model.layers.13.__entry]
        addr:0x00000000000618 prog.kernel_launch(%1622:tensor<[1, 192, 1536], Float32, CPU>) -> (%1623:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.13.input_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x00000000000619 prog.jump model.layers.13.self_attn.__entry[offset:6]
        addr:0x0000000000061a prog.kernel_launch(%1641:tensor<[1, 192, 1536], Float32, CPU>, %1622:tensor<[1, 192, 1536], Float32, CPU>) -> (%1642:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x0000000000061b prog.kernel_launch(%1642:tensor<[1, 192, 1536], Float32, CPU>) -> (%1643:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.13.post_attention_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x0000000000061c prog.jump model.layers.13.mlp.__entry[offset:25]
        addr:0x0000000000061d prog.kernel_launch(%1648:tensor<[1, 192, 1536], Float32, CPU>, %1642:tensor<[1, 192, 1536], Float32, CPU>) -> (%1649:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x0000000000061e prog.ret
        addr:0x0000000000061f prog.label[symbol:model.layers.13.self_attn.__entry]
        addr:0x00000000000620 prog.kernel_launch(%1623:tensor<[1, 192, 1536], Float32, CPU>) -> (%1624:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.13.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000621 prog.kernel_launch(%1623:tensor<[1, 192, 1536], Float32, CPU>) -> (%1625:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.13.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000622 prog.kernel_launch(%1623:tensor<[1, 192, 1536], Float32, CPU>) -> (%1626:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.13.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000623 prog.kernel_launch(%1624:tensor<[1, 192, 1536], Float32, CPU>) -> (%1624:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,12,128]}]
        addr:0x00000000000624 prog.kernel_launch(%1625:tensor<[1, 192, 256], Float32, CPU>) -> (%1625:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x00000000000625 prog.kernel_launch(%1626:tensor<[1, 192, 256], Float32, CPU>) -> (%1626:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x00000000000626 prog.kernel_launch(%1624:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1627:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000627 prog.kernel_launch(%1625:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1628:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000628 prog.kernel_launch(%1626:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1629:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000629 prog.kernel_launch(%1627:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1630:tensor<[1, 12, 192, 128], Float32, CPU>)[symbol_name:model.layers.13.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x0000000000062a prog.kernel_launch(%1628:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1631:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.13.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x0000000000062b prog.kernel_launch(%1631:tensor<[1, 2, 192, 128], Float32, CPU>, %1629:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1632:tensor<[1, 2, 192, 128], Float32, CPU>, %1633:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.13.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x0000000000062c prog.kernel_launch(%1630:tensor<[1, 12, 192, 128], Float32, CPU>, %1632:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1634:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x0000000000062d prog.kernel_launch(%1634:tensor<[1, 12, 192, 192], Float32, CPU>, %1635:tensor<[1], Float32, CPU>) -> (%1636:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x0000000000062e prog.kernel_launch(%1636:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1637:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.13.self_attn.mask, op_type:CausalMask, op_options:null]
        addr:0x0000000000062f prog.kernel_launch(%1637:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1638:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.13.self_attn.softmax, op_type:Softmax, op_options:null]
        addr:0x00000000000630 prog.kernel_launch(%1638:tensor<[1, 12, 192, 192], Float32, CPU>, %1633:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1639:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x00000000000631 prog.kernel_launch(%1639:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1640:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000632 prog.kernel_launch(%1640:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1640:tensor<[1, 192, 1536], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,1536]}]
        addr:0x00000000000633 prog.kernel_launch(%1640:tensor<[1, 192, 1536], Float32, CPU>) -> (%1641:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.13.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000634 prog.ret
        addr:0x00000000000635 prog.label[symbol:model.layers.13.mlp.__entry]
        addr:0x00000000000636 prog.kernel_launch(%1643:tensor<[1, 192, 1536], Float32, CPU>) -> (%1644:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.13.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000637 prog.kernel_launch(%1644:tensor<[1, 192, 8960], Float32, CPU>) -> (%1645:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.13.mlp.act, op_type:SiLU, op_options:null]
        addr:0x00000000000638 prog.kernel_launch(%1643:tensor<[1, 192, 1536], Float32, CPU>) -> (%1646:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.13.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000639 prog.kernel_launch(%1645:tensor<[1, 192, 8960], Float32, CPU>, %1646:tensor<[1, 192, 8960], Float32, CPU>) -> (%1647:tensor<[1, 192, 8960], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x0000000000063a prog.kernel_launch(%1647:tensor<[1, 192, 8960], Float32, CPU>) -> (%1648:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.13.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x0000000000063b prog.ret
        addr:0x0000000000063c prog.label[symbol:model.layers.14.__entry]
        addr:0x0000000000063d prog.kernel_launch(%1649:tensor<[1, 192, 1536], Float32, CPU>) -> (%1650:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.14.input_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x0000000000063e prog.jump model.layers.14.self_attn.__entry[offset:6]
        addr:0x0000000000063f prog.kernel_launch(%1668:tensor<[1, 192, 1536], Float32, CPU>, %1649:tensor<[1, 192, 1536], Float32, CPU>) -> (%1669:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x00000000000640 prog.kernel_launch(%1669:tensor<[1, 192, 1536], Float32, CPU>) -> (%1670:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.14.post_attention_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x00000000000641 prog.jump model.layers.14.mlp.__entry[offset:25]
        addr:0x00000000000642 prog.kernel_launch(%1675:tensor<[1, 192, 1536], Float32, CPU>, %1669:tensor<[1, 192, 1536], Float32, CPU>) -> (%1676:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x00000000000643 prog.ret
        addr:0x00000000000644 prog.label[symbol:model.layers.14.self_attn.__entry]
        addr:0x00000000000645 prog.kernel_launch(%1650:tensor<[1, 192, 1536], Float32, CPU>) -> (%1651:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.14.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000646 prog.kernel_launch(%1650:tensor<[1, 192, 1536], Float32, CPU>) -> (%1652:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.14.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000647 prog.kernel_launch(%1650:tensor<[1, 192, 1536], Float32, CPU>) -> (%1653:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.14.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000648 prog.kernel_launch(%1651:tensor<[1, 192, 1536], Float32, CPU>) -> (%1651:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,12,128]}]
        addr:0x00000000000649 prog.kernel_launch(%1652:tensor<[1, 192, 256], Float32, CPU>) -> (%1652:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x0000000000064a prog.kernel_launch(%1653:tensor<[1, 192, 256], Float32, CPU>) -> (%1653:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x0000000000064b prog.kernel_launch(%1651:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1654:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000064c prog.kernel_launch(%1652:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1655:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000064d prog.kernel_launch(%1653:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1656:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000064e prog.kernel_launch(%1654:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1657:tensor<[1, 12, 192, 128], Float32, CPU>)[symbol_name:model.layers.14.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x0000000000064f prog.kernel_launch(%1655:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1658:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.14.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000650 prog.kernel_launch(%1658:tensor<[1, 2, 192, 128], Float32, CPU>, %1656:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1659:tensor<[1, 2, 192, 128], Float32, CPU>, %1660:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.14.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000651 prog.kernel_launch(%1657:tensor<[1, 12, 192, 128], Float32, CPU>, %1659:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1661:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x00000000000652 prog.kernel_launch(%1661:tensor<[1, 12, 192, 192], Float32, CPU>, %1662:tensor<[1], Float32, CPU>) -> (%1663:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x00000000000653 prog.kernel_launch(%1663:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1664:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.14.self_attn.mask, op_type:CausalMask, op_options:null]
        addr:0x00000000000654 prog.kernel_launch(%1664:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1665:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.14.self_attn.softmax, op_type:Softmax, op_options:null]
        addr:0x00000000000655 prog.kernel_launch(%1665:tensor<[1, 12, 192, 192], Float32, CPU>, %1660:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1666:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x00000000000656 prog.kernel_launch(%1666:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1667:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000657 prog.kernel_launch(%1667:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1667:tensor<[1, 192, 1536], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,1536]}]
        addr:0x00000000000658 prog.kernel_launch(%1667:tensor<[1, 192, 1536], Float32, CPU>) -> (%1668:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.14.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000659 prog.ret
        addr:0x0000000000065a prog.label[symbol:model.layers.14.mlp.__entry]
        addr:0x0000000000065b prog.kernel_launch(%1670:tensor<[1, 192, 1536], Float32, CPU>) -> (%1671:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.14.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x0000000000065c prog.kernel_launch(%1671:tensor<[1, 192, 8960], Float32, CPU>) -> (%1672:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.14.mlp.act, op_type:SiLU, op_options:null]
        addr:0x0000000000065d prog.kernel_launch(%1670:tensor<[1, 192, 1536], Float32, CPU>) -> (%1673:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.14.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x0000000000065e prog.kernel_launch(%1672:tensor<[1, 192, 8960], Float32, CPU>, %1673:tensor<[1, 192, 8960], Float32, CPU>) -> (%1674:tensor<[1, 192, 8960], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x0000000000065f prog.kernel_launch(%1674:tensor<[1, 192, 8960], Float32, CPU>) -> (%1675:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.14.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x00000000000660 prog.ret
        addr:0x00000000000661 prog.label[symbol:model.layers.15.__entry]
        addr:0x00000000000662 prog.kernel_launch(%1676:tensor<[1, 192, 1536], Float32, CPU>) -> (%1677:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.15.input_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x00000000000663 prog.jump model.layers.15.self_attn.__entry[offset:6]
        addr:0x00000000000664 prog.kernel_launch(%1695:tensor<[1, 192, 1536], Float32, CPU>, %1676:tensor<[1, 192, 1536], Float32, CPU>) -> (%1696:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x00000000000665 prog.kernel_launch(%1696:tensor<[1, 192, 1536], Float32, CPU>) -> (%1697:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.15.post_attention_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x00000000000666 prog.jump model.layers.15.mlp.__entry[offset:25]
        addr:0x00000000000667 prog.kernel_launch(%1702:tensor<[1, 192, 1536], Float32, CPU>, %1696:tensor<[1, 192, 1536], Float32, CPU>) -> (%1703:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x00000000000668 prog.ret
        addr:0x00000000000669 prog.label[symbol:model.layers.15.self_attn.__entry]
        addr:0x0000000000066a prog.kernel_launch(%1677:tensor<[1, 192, 1536], Float32, CPU>) -> (%1678:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.15.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000066b prog.kernel_launch(%1677:tensor<[1, 192, 1536], Float32, CPU>) -> (%1679:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.15.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x0000000000066c prog.kernel_launch(%1677:tensor<[1, 192, 1536], Float32, CPU>) -> (%1680:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.15.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x0000000000066d prog.kernel_launch(%1678:tensor<[1, 192, 1536], Float32, CPU>) -> (%1678:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,12,128]}]
        addr:0x0000000000066e prog.kernel_launch(%1679:tensor<[1, 192, 256], Float32, CPU>) -> (%1679:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x0000000000066f prog.kernel_launch(%1680:tensor<[1, 192, 256], Float32, CPU>) -> (%1680:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x00000000000670 prog.kernel_launch(%1678:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1681:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000671 prog.kernel_launch(%1679:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1682:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000672 prog.kernel_launch(%1680:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1683:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000673 prog.kernel_launch(%1681:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1684:tensor<[1, 12, 192, 128], Float32, CPU>)[symbol_name:model.layers.15.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000674 prog.kernel_launch(%1682:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1685:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.15.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000675 prog.kernel_launch(%1685:tensor<[1, 2, 192, 128], Float32, CPU>, %1683:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1686:tensor<[1, 2, 192, 128], Float32, CPU>, %1687:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.15.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000676 prog.kernel_launch(%1684:tensor<[1, 12, 192, 128], Float32, CPU>, %1686:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1688:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x00000000000677 prog.kernel_launch(%1688:tensor<[1, 12, 192, 192], Float32, CPU>, %1689:tensor<[1], Float32, CPU>) -> (%1690:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x00000000000678 prog.kernel_launch(%1690:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1691:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.15.self_attn.mask, op_type:CausalMask, op_options:null]
        addr:0x00000000000679 prog.kernel_launch(%1691:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1692:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.15.self_attn.softmax, op_type:Softmax, op_options:null]
        addr:0x0000000000067a prog.kernel_launch(%1692:tensor<[1, 12, 192, 192], Float32, CPU>, %1687:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1693:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x0000000000067b prog.kernel_launch(%1693:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1694:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000067c prog.kernel_launch(%1694:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1694:tensor<[1, 192, 1536], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,1536]}]
        addr:0x0000000000067d prog.kernel_launch(%1694:tensor<[1, 192, 1536], Float32, CPU>) -> (%1695:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.15.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000067e prog.ret
        addr:0x0000000000067f prog.label[symbol:model.layers.15.mlp.__entry]
        addr:0x00000000000680 prog.kernel_launch(%1697:tensor<[1, 192, 1536], Float32, CPU>) -> (%1698:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.15.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000681 prog.kernel_launch(%1698:tensor<[1, 192, 8960], Float32, CPU>) -> (%1699:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.15.mlp.act, op_type:SiLU, op_options:null]
        addr:0x00000000000682 prog.kernel_launch(%1697:tensor<[1, 192, 1536], Float32, CPU>) -> (%1700:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.15.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000683 prog.kernel_launch(%1699:tensor<[1, 192, 8960], Float32, CPU>, %1700:tensor<[1, 192, 8960], Float32, CPU>) -> (%1701:tensor<[1, 192, 8960], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x00000000000684 prog.kernel_launch(%1701:tensor<[1, 192, 8960], Float32, CPU>) -> (%1702:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.15.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x00000000000685 prog.ret
        addr:0x00000000000686 prog.label[symbol:model.layers.16.__entry]
        addr:0x00000000000687 prog.kernel_launch(%1703:tensor<[1, 192, 1536], Float32, CPU>) -> (%1704:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.16.input_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x00000000000688 prog.jump model.layers.16.self_attn.__entry[offset:6]
        addr:0x00000000000689 prog.kernel_launch(%1722:tensor<[1, 192, 1536], Float32, CPU>, %1703:tensor<[1, 192, 1536], Float32, CPU>) -> (%1723:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x0000000000068a prog.kernel_launch(%1723:tensor<[1, 192, 1536], Float32, CPU>) -> (%1724:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.16.post_attention_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x0000000000068b prog.jump model.layers.16.mlp.__entry[offset:25]
        addr:0x0000000000068c prog.kernel_launch(%1729:tensor<[1, 192, 1536], Float32, CPU>, %1723:tensor<[1, 192, 1536], Float32, CPU>) -> (%1730:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x0000000000068d prog.ret
        addr:0x0000000000068e prog.label[symbol:model.layers.16.self_attn.__entry]
        addr:0x0000000000068f prog.kernel_launch(%1704:tensor<[1, 192, 1536], Float32, CPU>) -> (%1705:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.16.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000690 prog.kernel_launch(%1704:tensor<[1, 192, 1536], Float32, CPU>) -> (%1706:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.16.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000691 prog.kernel_launch(%1704:tensor<[1, 192, 1536], Float32, CPU>) -> (%1707:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.16.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000692 prog.kernel_launch(%1705:tensor<[1, 192, 1536], Float32, CPU>) -> (%1705:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,12,128]}]
        addr:0x00000000000693 prog.kernel_launch(%1706:tensor<[1, 192, 256], Float32, CPU>) -> (%1706:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x00000000000694 prog.kernel_launch(%1707:tensor<[1, 192, 256], Float32, CPU>) -> (%1707:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x00000000000695 prog.kernel_launch(%1705:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1708:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000696 prog.kernel_launch(%1706:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1709:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000697 prog.kernel_launch(%1707:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1710:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000698 prog.kernel_launch(%1708:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1711:tensor<[1, 12, 192, 128], Float32, CPU>)[symbol_name:model.layers.16.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000699 prog.kernel_launch(%1709:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1712:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.16.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x0000000000069a prog.kernel_launch(%1712:tensor<[1, 2, 192, 128], Float32, CPU>, %1710:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1713:tensor<[1, 2, 192, 128], Float32, CPU>, %1714:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.16.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x0000000000069b prog.kernel_launch(%1711:tensor<[1, 12, 192, 128], Float32, CPU>, %1713:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1715:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x0000000000069c prog.kernel_launch(%1715:tensor<[1, 12, 192, 192], Float32, CPU>, %1716:tensor<[1], Float32, CPU>) -> (%1717:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x0000000000069d prog.kernel_launch(%1717:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1718:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.16.self_attn.mask, op_type:CausalMask, op_options:null]
        addr:0x0000000000069e prog.kernel_launch(%1718:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1719:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.16.self_attn.softmax, op_type:Softmax, op_options:null]
        addr:0x0000000000069f prog.kernel_launch(%1719:tensor<[1, 12, 192, 192], Float32, CPU>, %1714:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1720:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x000000000006a0 prog.kernel_launch(%1720:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1721:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000006a1 prog.kernel_launch(%1721:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1721:tensor<[1, 192, 1536], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,1536]}]
        addr:0x000000000006a2 prog.kernel_launch(%1721:tensor<[1, 192, 1536], Float32, CPU>) -> (%1722:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.16.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000006a3 prog.ret
        addr:0x000000000006a4 prog.label[symbol:model.layers.16.mlp.__entry]
        addr:0x000000000006a5 prog.kernel_launch(%1724:tensor<[1, 192, 1536], Float32, CPU>) -> (%1725:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.16.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000006a6 prog.kernel_launch(%1725:tensor<[1, 192, 8960], Float32, CPU>) -> (%1726:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.16.mlp.act, op_type:SiLU, op_options:null]
        addr:0x000000000006a7 prog.kernel_launch(%1724:tensor<[1, 192, 1536], Float32, CPU>) -> (%1727:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.16.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000006a8 prog.kernel_launch(%1726:tensor<[1, 192, 8960], Float32, CPU>, %1727:tensor<[1, 192, 8960], Float32, CPU>) -> (%1728:tensor<[1, 192, 8960], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x000000000006a9 prog.kernel_launch(%1728:tensor<[1, 192, 8960], Float32, CPU>) -> (%1729:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.16.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x000000000006aa prog.ret
        addr:0x000000000006ab prog.label[symbol:model.layers.17.__entry]
        addr:0x000000000006ac prog.kernel_launch(%1730:tensor<[1, 192, 1536], Float32, CPU>) -> (%1731:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.17.input_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x000000000006ad prog.jump model.layers.17.self_attn.__entry[offset:6]
        addr:0x000000000006ae prog.kernel_launch(%1749:tensor<[1, 192, 1536], Float32, CPU>, %1730:tensor<[1, 192, 1536], Float32, CPU>) -> (%1750:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000006af prog.kernel_launch(%1750:tensor<[1, 192, 1536], Float32, CPU>) -> (%1751:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.17.post_attention_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x000000000006b0 prog.jump model.layers.17.mlp.__entry[offset:25]
        addr:0x000000000006b1 prog.kernel_launch(%1756:tensor<[1, 192, 1536], Float32, CPU>, %1750:tensor<[1, 192, 1536], Float32, CPU>) -> (%1757:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000006b2 prog.ret
        addr:0x000000000006b3 prog.label[symbol:model.layers.17.self_attn.__entry]
        addr:0x000000000006b4 prog.kernel_launch(%1731:tensor<[1, 192, 1536], Float32, CPU>) -> (%1732:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.17.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000006b5 prog.kernel_launch(%1731:tensor<[1, 192, 1536], Float32, CPU>) -> (%1733:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.17.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000006b6 prog.kernel_launch(%1731:tensor<[1, 192, 1536], Float32, CPU>) -> (%1734:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.17.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000006b7 prog.kernel_launch(%1732:tensor<[1, 192, 1536], Float32, CPU>) -> (%1732:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,12,128]}]
        addr:0x000000000006b8 prog.kernel_launch(%1733:tensor<[1, 192, 256], Float32, CPU>) -> (%1733:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x000000000006b9 prog.kernel_launch(%1734:tensor<[1, 192, 256], Float32, CPU>) -> (%1734:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x000000000006ba prog.kernel_launch(%1732:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1735:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000006bb prog.kernel_launch(%1733:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1736:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000006bc prog.kernel_launch(%1734:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1737:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000006bd prog.kernel_launch(%1735:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1738:tensor<[1, 12, 192, 128], Float32, CPU>)[symbol_name:model.layers.17.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000006be prog.kernel_launch(%1736:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1739:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.17.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000006bf prog.kernel_launch(%1739:tensor<[1, 2, 192, 128], Float32, CPU>, %1737:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1740:tensor<[1, 2, 192, 128], Float32, CPU>, %1741:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.17.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x000000000006c0 prog.kernel_launch(%1738:tensor<[1, 12, 192, 128], Float32, CPU>, %1740:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1742:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x000000000006c1 prog.kernel_launch(%1742:tensor<[1, 12, 192, 192], Float32, CPU>, %1743:tensor<[1], Float32, CPU>) -> (%1744:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x000000000006c2 prog.kernel_launch(%1744:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1745:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.17.self_attn.mask, op_type:CausalMask, op_options:null]
        addr:0x000000000006c3 prog.kernel_launch(%1745:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1746:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.17.self_attn.softmax, op_type:Softmax, op_options:null]
        addr:0x000000000006c4 prog.kernel_launch(%1746:tensor<[1, 12, 192, 192], Float32, CPU>, %1741:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1747:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x000000000006c5 prog.kernel_launch(%1747:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1748:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000006c6 prog.kernel_launch(%1748:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1748:tensor<[1, 192, 1536], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,1536]}]
        addr:0x000000000006c7 prog.kernel_launch(%1748:tensor<[1, 192, 1536], Float32, CPU>) -> (%1749:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.17.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000006c8 prog.ret
        addr:0x000000000006c9 prog.label[symbol:model.layers.17.mlp.__entry]
        addr:0x000000000006ca prog.kernel_launch(%1751:tensor<[1, 192, 1536], Float32, CPU>) -> (%1752:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.17.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000006cb prog.kernel_launch(%1752:tensor<[1, 192, 8960], Float32, CPU>) -> (%1753:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.17.mlp.act, op_type:SiLU, op_options:null]
        addr:0x000000000006cc prog.kernel_launch(%1751:tensor<[1, 192, 1536], Float32, CPU>) -> (%1754:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.17.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000006cd prog.kernel_launch(%1753:tensor<[1, 192, 8960], Float32, CPU>, %1754:tensor<[1, 192, 8960], Float32, CPU>) -> (%1755:tensor<[1, 192, 8960], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x000000000006ce prog.kernel_launch(%1755:tensor<[1, 192, 8960], Float32, CPU>) -> (%1756:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.17.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x000000000006cf prog.ret
        addr:0x000000000006d0 prog.label[symbol:model.layers.18.__entry]
        addr:0x000000000006d1 prog.kernel_launch(%1757:tensor<[1, 192, 1536], Float32, CPU>) -> (%1758:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.18.input_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x000000000006d2 prog.jump model.layers.18.self_attn.__entry[offset:6]
        addr:0x000000000006d3 prog.kernel_launch(%1776:tensor<[1, 192, 1536], Float32, CPU>, %1757:tensor<[1, 192, 1536], Float32, CPU>) -> (%1777:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000006d4 prog.kernel_launch(%1777:tensor<[1, 192, 1536], Float32, CPU>) -> (%1778:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.18.post_attention_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x000000000006d5 prog.jump model.layers.18.mlp.__entry[offset:25]
        addr:0x000000000006d6 prog.kernel_launch(%1783:tensor<[1, 192, 1536], Float32, CPU>, %1777:tensor<[1, 192, 1536], Float32, CPU>) -> (%1784:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000006d7 prog.ret
        addr:0x000000000006d8 prog.label[symbol:model.layers.18.self_attn.__entry]
        addr:0x000000000006d9 prog.kernel_launch(%1758:tensor<[1, 192, 1536], Float32, CPU>) -> (%1759:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.18.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000006da prog.kernel_launch(%1758:tensor<[1, 192, 1536], Float32, CPU>) -> (%1760:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.18.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000006db prog.kernel_launch(%1758:tensor<[1, 192, 1536], Float32, CPU>) -> (%1761:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.18.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000006dc prog.kernel_launch(%1759:tensor<[1, 192, 1536], Float32, CPU>) -> (%1759:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,12,128]}]
        addr:0x000000000006dd prog.kernel_launch(%1760:tensor<[1, 192, 256], Float32, CPU>) -> (%1760:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x000000000006de prog.kernel_launch(%1761:tensor<[1, 192, 256], Float32, CPU>) -> (%1761:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x000000000006df prog.kernel_launch(%1759:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1762:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000006e0 prog.kernel_launch(%1760:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1763:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000006e1 prog.kernel_launch(%1761:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1764:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000006e2 prog.kernel_launch(%1762:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1765:tensor<[1, 12, 192, 128], Float32, CPU>)[symbol_name:model.layers.18.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000006e3 prog.kernel_launch(%1763:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1766:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.18.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000006e4 prog.kernel_launch(%1766:tensor<[1, 2, 192, 128], Float32, CPU>, %1764:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1767:tensor<[1, 2, 192, 128], Float32, CPU>, %1768:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.18.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x000000000006e5 prog.kernel_launch(%1765:tensor<[1, 12, 192, 128], Float32, CPU>, %1767:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1769:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x000000000006e6 prog.kernel_launch(%1769:tensor<[1, 12, 192, 192], Float32, CPU>, %1770:tensor<[1], Float32, CPU>) -> (%1771:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x000000000006e7 prog.kernel_launch(%1771:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1772:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.18.self_attn.mask, op_type:CausalMask, op_options:null]
        addr:0x000000000006e8 prog.kernel_launch(%1772:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1773:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.18.self_attn.softmax, op_type:Softmax, op_options:null]
        addr:0x000000000006e9 prog.kernel_launch(%1773:tensor<[1, 12, 192, 192], Float32, CPU>, %1768:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1774:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x000000000006ea prog.kernel_launch(%1774:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1775:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000006eb prog.kernel_launch(%1775:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1775:tensor<[1, 192, 1536], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,1536]}]
        addr:0x000000000006ec prog.kernel_launch(%1775:tensor<[1, 192, 1536], Float32, CPU>) -> (%1776:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.18.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000006ed prog.ret
        addr:0x000000000006ee prog.label[symbol:model.layers.18.mlp.__entry]
        addr:0x000000000006ef prog.kernel_launch(%1778:tensor<[1, 192, 1536], Float32, CPU>) -> (%1779:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.18.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000006f0 prog.kernel_launch(%1779:tensor<[1, 192, 8960], Float32, CPU>) -> (%1780:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.18.mlp.act, op_type:SiLU, op_options:null]
        addr:0x000000000006f1 prog.kernel_launch(%1778:tensor<[1, 192, 1536], Float32, CPU>) -> (%1781:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.18.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000006f2 prog.kernel_launch(%1780:tensor<[1, 192, 8960], Float32, CPU>, %1781:tensor<[1, 192, 8960], Float32, CPU>) -> (%1782:tensor<[1, 192, 8960], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x000000000006f3 prog.kernel_launch(%1782:tensor<[1, 192, 8960], Float32, CPU>) -> (%1783:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.18.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x000000000006f4 prog.ret
        addr:0x000000000006f5 prog.label[symbol:model.layers.19.__entry]
        addr:0x000000000006f6 prog.kernel_launch(%1784:tensor<[1, 192, 1536], Float32, CPU>) -> (%1785:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.19.input_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x000000000006f7 prog.jump model.layers.19.self_attn.__entry[offset:6]
        addr:0x000000000006f8 prog.kernel_launch(%1803:tensor<[1, 192, 1536], Float32, CPU>, %1784:tensor<[1, 192, 1536], Float32, CPU>) -> (%1804:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000006f9 prog.kernel_launch(%1804:tensor<[1, 192, 1536], Float32, CPU>) -> (%1805:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.19.post_attention_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x000000000006fa prog.jump model.layers.19.mlp.__entry[offset:25]
        addr:0x000000000006fb prog.kernel_launch(%1810:tensor<[1, 192, 1536], Float32, CPU>, %1804:tensor<[1, 192, 1536], Float32, CPU>) -> (%1811:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000006fc prog.ret
        addr:0x000000000006fd prog.label[symbol:model.layers.19.self_attn.__entry]
        addr:0x000000000006fe prog.kernel_launch(%1785:tensor<[1, 192, 1536], Float32, CPU>) -> (%1786:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.19.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000006ff prog.kernel_launch(%1785:tensor<[1, 192, 1536], Float32, CPU>) -> (%1787:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.19.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000700 prog.kernel_launch(%1785:tensor<[1, 192, 1536], Float32, CPU>) -> (%1788:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.19.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000701 prog.kernel_launch(%1786:tensor<[1, 192, 1536], Float32, CPU>) -> (%1786:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,12,128]}]
        addr:0x00000000000702 prog.kernel_launch(%1787:tensor<[1, 192, 256], Float32, CPU>) -> (%1787:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x00000000000703 prog.kernel_launch(%1788:tensor<[1, 192, 256], Float32, CPU>) -> (%1788:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x00000000000704 prog.kernel_launch(%1786:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1789:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000705 prog.kernel_launch(%1787:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1790:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000706 prog.kernel_launch(%1788:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1791:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000707 prog.kernel_launch(%1789:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1792:tensor<[1, 12, 192, 128], Float32, CPU>)[symbol_name:model.layers.19.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000708 prog.kernel_launch(%1790:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1793:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.19.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000709 prog.kernel_launch(%1793:tensor<[1, 2, 192, 128], Float32, CPU>, %1791:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1794:tensor<[1, 2, 192, 128], Float32, CPU>, %1795:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.19.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x0000000000070a prog.kernel_launch(%1792:tensor<[1, 12, 192, 128], Float32, CPU>, %1794:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1796:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x0000000000070b prog.kernel_launch(%1796:tensor<[1, 12, 192, 192], Float32, CPU>, %1797:tensor<[1], Float32, CPU>) -> (%1798:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x0000000000070c prog.kernel_launch(%1798:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1799:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.19.self_attn.mask, op_type:CausalMask, op_options:null]
        addr:0x0000000000070d prog.kernel_launch(%1799:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1800:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.19.self_attn.softmax, op_type:Softmax, op_options:null]
        addr:0x0000000000070e prog.kernel_launch(%1800:tensor<[1, 12, 192, 192], Float32, CPU>, %1795:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1801:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x0000000000070f prog.kernel_launch(%1801:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1802:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000710 prog.kernel_launch(%1802:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1802:tensor<[1, 192, 1536], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,1536]}]
        addr:0x00000000000711 prog.kernel_launch(%1802:tensor<[1, 192, 1536], Float32, CPU>) -> (%1803:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.19.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000712 prog.ret
        addr:0x00000000000713 prog.label[symbol:model.layers.19.mlp.__entry]
        addr:0x00000000000714 prog.kernel_launch(%1805:tensor<[1, 192, 1536], Float32, CPU>) -> (%1806:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.19.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000715 prog.kernel_launch(%1806:tensor<[1, 192, 8960], Float32, CPU>) -> (%1807:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.19.mlp.act, op_type:SiLU, op_options:null]
        addr:0x00000000000716 prog.kernel_launch(%1805:tensor<[1, 192, 1536], Float32, CPU>) -> (%1808:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.19.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000717 prog.kernel_launch(%1807:tensor<[1, 192, 8960], Float32, CPU>, %1808:tensor<[1, 192, 8960], Float32, CPU>) -> (%1809:tensor<[1, 192, 8960], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x00000000000718 prog.kernel_launch(%1809:tensor<[1, 192, 8960], Float32, CPU>) -> (%1810:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.19.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x00000000000719 prog.ret
        addr:0x0000000000071a prog.label[symbol:model.layers.20.__entry]
        addr:0x0000000000071b prog.kernel_launch(%1811:tensor<[1, 192, 1536], Float32, CPU>) -> (%1812:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.20.input_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x0000000000071c prog.jump model.layers.20.self_attn.__entry[offset:6]
        addr:0x0000000000071d prog.kernel_launch(%1830:tensor<[1, 192, 1536], Float32, CPU>, %1811:tensor<[1, 192, 1536], Float32, CPU>) -> (%1831:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x0000000000071e prog.kernel_launch(%1831:tensor<[1, 192, 1536], Float32, CPU>) -> (%1832:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.20.post_attention_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x0000000000071f prog.jump model.layers.20.mlp.__entry[offset:25]
        addr:0x00000000000720 prog.kernel_launch(%1837:tensor<[1, 192, 1536], Float32, CPU>, %1831:tensor<[1, 192, 1536], Float32, CPU>) -> (%1838:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x00000000000721 prog.ret
        addr:0x00000000000722 prog.label[symbol:model.layers.20.self_attn.__entry]
        addr:0x00000000000723 prog.kernel_launch(%1812:tensor<[1, 192, 1536], Float32, CPU>) -> (%1813:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.20.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000724 prog.kernel_launch(%1812:tensor<[1, 192, 1536], Float32, CPU>) -> (%1814:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.20.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000725 prog.kernel_launch(%1812:tensor<[1, 192, 1536], Float32, CPU>) -> (%1815:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.20.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000726 prog.kernel_launch(%1813:tensor<[1, 192, 1536], Float32, CPU>) -> (%1813:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,12,128]}]
        addr:0x00000000000727 prog.kernel_launch(%1814:tensor<[1, 192, 256], Float32, CPU>) -> (%1814:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x00000000000728 prog.kernel_launch(%1815:tensor<[1, 192, 256], Float32, CPU>) -> (%1815:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x00000000000729 prog.kernel_launch(%1813:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1816:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000072a prog.kernel_launch(%1814:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1817:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000072b prog.kernel_launch(%1815:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1818:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000072c prog.kernel_launch(%1816:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1819:tensor<[1, 12, 192, 128], Float32, CPU>)[symbol_name:model.layers.20.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x0000000000072d prog.kernel_launch(%1817:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1820:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.20.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x0000000000072e prog.kernel_launch(%1820:tensor<[1, 2, 192, 128], Float32, CPU>, %1818:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1821:tensor<[1, 2, 192, 128], Float32, CPU>, %1822:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.20.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x0000000000072f prog.kernel_launch(%1819:tensor<[1, 12, 192, 128], Float32, CPU>, %1821:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1823:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x00000000000730 prog.kernel_launch(%1823:tensor<[1, 12, 192, 192], Float32, CPU>, %1824:tensor<[1], Float32, CPU>) -> (%1825:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x00000000000731 prog.kernel_launch(%1825:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1826:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.20.self_attn.mask, op_type:CausalMask, op_options:null]
        addr:0x00000000000732 prog.kernel_launch(%1826:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1827:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.20.self_attn.softmax, op_type:Softmax, op_options:null]
        addr:0x00000000000733 prog.kernel_launch(%1827:tensor<[1, 12, 192, 192], Float32, CPU>, %1822:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1828:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x00000000000734 prog.kernel_launch(%1828:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1829:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000735 prog.kernel_launch(%1829:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1829:tensor<[1, 192, 1536], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,1536]}]
        addr:0x00000000000736 prog.kernel_launch(%1829:tensor<[1, 192, 1536], Float32, CPU>) -> (%1830:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.20.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000737 prog.ret
        addr:0x00000000000738 prog.label[symbol:model.layers.20.mlp.__entry]
        addr:0x00000000000739 prog.kernel_launch(%1832:tensor<[1, 192, 1536], Float32, CPU>) -> (%1833:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.20.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x0000000000073a prog.kernel_launch(%1833:tensor<[1, 192, 8960], Float32, CPU>) -> (%1834:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.20.mlp.act, op_type:SiLU, op_options:null]
        addr:0x0000000000073b prog.kernel_launch(%1832:tensor<[1, 192, 1536], Float32, CPU>) -> (%1835:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.20.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x0000000000073c prog.kernel_launch(%1834:tensor<[1, 192, 8960], Float32, CPU>, %1835:tensor<[1, 192, 8960], Float32, CPU>) -> (%1836:tensor<[1, 192, 8960], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x0000000000073d prog.kernel_launch(%1836:tensor<[1, 192, 8960], Float32, CPU>) -> (%1837:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.20.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x0000000000073e prog.ret
        addr:0x0000000000073f prog.label[symbol:model.layers.21.__entry]
        addr:0x00000000000740 prog.kernel_launch(%1838:tensor<[1, 192, 1536], Float32, CPU>) -> (%1839:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.21.input_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x00000000000741 prog.jump model.layers.21.self_attn.__entry[offset:6]
        addr:0x00000000000742 prog.kernel_launch(%1857:tensor<[1, 192, 1536], Float32, CPU>, %1838:tensor<[1, 192, 1536], Float32, CPU>) -> (%1858:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x00000000000743 prog.kernel_launch(%1858:tensor<[1, 192, 1536], Float32, CPU>) -> (%1859:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.21.post_attention_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x00000000000744 prog.jump model.layers.21.mlp.__entry[offset:25]
        addr:0x00000000000745 prog.kernel_launch(%1864:tensor<[1, 192, 1536], Float32, CPU>, %1858:tensor<[1, 192, 1536], Float32, CPU>) -> (%1865:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x00000000000746 prog.ret
        addr:0x00000000000747 prog.label[symbol:model.layers.21.self_attn.__entry]
        addr:0x00000000000748 prog.kernel_launch(%1839:tensor<[1, 192, 1536], Float32, CPU>) -> (%1840:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.21.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000749 prog.kernel_launch(%1839:tensor<[1, 192, 1536], Float32, CPU>) -> (%1841:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.21.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x0000000000074a prog.kernel_launch(%1839:tensor<[1, 192, 1536], Float32, CPU>) -> (%1842:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.21.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x0000000000074b prog.kernel_launch(%1840:tensor<[1, 192, 1536], Float32, CPU>) -> (%1840:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,12,128]}]
        addr:0x0000000000074c prog.kernel_launch(%1841:tensor<[1, 192, 256], Float32, CPU>) -> (%1841:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x0000000000074d prog.kernel_launch(%1842:tensor<[1, 192, 256], Float32, CPU>) -> (%1842:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x0000000000074e prog.kernel_launch(%1840:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1843:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000074f prog.kernel_launch(%1841:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1844:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000750 prog.kernel_launch(%1842:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1845:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000751 prog.kernel_launch(%1843:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1846:tensor<[1, 12, 192, 128], Float32, CPU>)[symbol_name:model.layers.21.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000752 prog.kernel_launch(%1844:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1847:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.21.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000753 prog.kernel_launch(%1847:tensor<[1, 2, 192, 128], Float32, CPU>, %1845:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1848:tensor<[1, 2, 192, 128], Float32, CPU>, %1849:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.21.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000754 prog.kernel_launch(%1846:tensor<[1, 12, 192, 128], Float32, CPU>, %1848:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1850:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x00000000000755 prog.kernel_launch(%1850:tensor<[1, 12, 192, 192], Float32, CPU>, %1851:tensor<[1], Float32, CPU>) -> (%1852:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x00000000000756 prog.kernel_launch(%1852:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1853:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.21.self_attn.mask, op_type:CausalMask, op_options:null]
        addr:0x00000000000757 prog.kernel_launch(%1853:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1854:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.21.self_attn.softmax, op_type:Softmax, op_options:null]
        addr:0x00000000000758 prog.kernel_launch(%1854:tensor<[1, 12, 192, 192], Float32, CPU>, %1849:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1855:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x00000000000759 prog.kernel_launch(%1855:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1856:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000075a prog.kernel_launch(%1856:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1856:tensor<[1, 192, 1536], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,1536]}]
        addr:0x0000000000075b prog.kernel_launch(%1856:tensor<[1, 192, 1536], Float32, CPU>) -> (%1857:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.21.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000075c prog.ret
        addr:0x0000000000075d prog.label[symbol:model.layers.21.mlp.__entry]
        addr:0x0000000000075e prog.kernel_launch(%1859:tensor<[1, 192, 1536], Float32, CPU>) -> (%1860:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.21.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x0000000000075f prog.kernel_launch(%1860:tensor<[1, 192, 8960], Float32, CPU>) -> (%1861:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.21.mlp.act, op_type:SiLU, op_options:null]
        addr:0x00000000000760 prog.kernel_launch(%1859:tensor<[1, 192, 1536], Float32, CPU>) -> (%1862:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.21.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000761 prog.kernel_launch(%1861:tensor<[1, 192, 8960], Float32, CPU>, %1862:tensor<[1, 192, 8960], Float32, CPU>) -> (%1863:tensor<[1, 192, 8960], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x00000000000762 prog.kernel_launch(%1863:tensor<[1, 192, 8960], Float32, CPU>) -> (%1864:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.21.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x00000000000763 prog.ret
        addr:0x00000000000764 prog.label[symbol:model.layers.22.__entry]
        addr:0x00000000000765 prog.kernel_launch(%1865:tensor<[1, 192, 1536], Float32, CPU>) -> (%1866:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.22.input_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x00000000000766 prog.jump model.layers.22.self_attn.__entry[offset:6]
        addr:0x00000000000767 prog.kernel_launch(%1884:tensor<[1, 192, 1536], Float32, CPU>, %1865:tensor<[1, 192, 1536], Float32, CPU>) -> (%1885:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x00000000000768 prog.kernel_launch(%1885:tensor<[1, 192, 1536], Float32, CPU>) -> (%1886:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.22.post_attention_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x00000000000769 prog.jump model.layers.22.mlp.__entry[offset:25]
        addr:0x0000000000076a prog.kernel_launch(%1891:tensor<[1, 192, 1536], Float32, CPU>, %1885:tensor<[1, 192, 1536], Float32, CPU>) -> (%1892:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x0000000000076b prog.ret
        addr:0x0000000000076c prog.label[symbol:model.layers.22.self_attn.__entry]
        addr:0x0000000000076d prog.kernel_launch(%1866:tensor<[1, 192, 1536], Float32, CPU>) -> (%1867:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.22.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000076e prog.kernel_launch(%1866:tensor<[1, 192, 1536], Float32, CPU>) -> (%1868:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.22.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x0000000000076f prog.kernel_launch(%1866:tensor<[1, 192, 1536], Float32, CPU>) -> (%1869:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.22.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000770 prog.kernel_launch(%1867:tensor<[1, 192, 1536], Float32, CPU>) -> (%1867:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,12,128]}]
        addr:0x00000000000771 prog.kernel_launch(%1868:tensor<[1, 192, 256], Float32, CPU>) -> (%1868:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x00000000000772 prog.kernel_launch(%1869:tensor<[1, 192, 256], Float32, CPU>) -> (%1869:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x00000000000773 prog.kernel_launch(%1867:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1870:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000774 prog.kernel_launch(%1868:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1871:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000775 prog.kernel_launch(%1869:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1872:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000776 prog.kernel_launch(%1870:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1873:tensor<[1, 12, 192, 128], Float32, CPU>)[symbol_name:model.layers.22.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000777 prog.kernel_launch(%1871:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1874:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.22.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000778 prog.kernel_launch(%1874:tensor<[1, 2, 192, 128], Float32, CPU>, %1872:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1875:tensor<[1, 2, 192, 128], Float32, CPU>, %1876:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.22.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000779 prog.kernel_launch(%1873:tensor<[1, 12, 192, 128], Float32, CPU>, %1875:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1877:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x0000000000077a prog.kernel_launch(%1877:tensor<[1, 12, 192, 192], Float32, CPU>, %1878:tensor<[1], Float32, CPU>) -> (%1879:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x0000000000077b prog.kernel_launch(%1879:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1880:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.22.self_attn.mask, op_type:CausalMask, op_options:null]
        addr:0x0000000000077c prog.kernel_launch(%1880:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1881:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.22.self_attn.softmax, op_type:Softmax, op_options:null]
        addr:0x0000000000077d prog.kernel_launch(%1881:tensor<[1, 12, 192, 192], Float32, CPU>, %1876:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1882:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x0000000000077e prog.kernel_launch(%1882:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1883:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000077f prog.kernel_launch(%1883:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1883:tensor<[1, 192, 1536], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,1536]}]
        addr:0x00000000000780 prog.kernel_launch(%1883:tensor<[1, 192, 1536], Float32, CPU>) -> (%1884:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.22.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000781 prog.ret
        addr:0x00000000000782 prog.label[symbol:model.layers.22.mlp.__entry]
        addr:0x00000000000783 prog.kernel_launch(%1886:tensor<[1, 192, 1536], Float32, CPU>) -> (%1887:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.22.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000784 prog.kernel_launch(%1887:tensor<[1, 192, 8960], Float32, CPU>) -> (%1888:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.22.mlp.act, op_type:SiLU, op_options:null]
        addr:0x00000000000785 prog.kernel_launch(%1886:tensor<[1, 192, 1536], Float32, CPU>) -> (%1889:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.22.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000786 prog.kernel_launch(%1888:tensor<[1, 192, 8960], Float32, CPU>, %1889:tensor<[1, 192, 8960], Float32, CPU>) -> (%1890:tensor<[1, 192, 8960], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x00000000000787 prog.kernel_launch(%1890:tensor<[1, 192, 8960], Float32, CPU>) -> (%1891:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.22.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x00000000000788 prog.ret
        addr:0x00000000000789 prog.label[symbol:model.layers.23.__entry]
        addr:0x0000000000078a prog.kernel_launch(%1892:tensor<[1, 192, 1536], Float32, CPU>) -> (%1893:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.23.input_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x0000000000078b prog.jump model.layers.23.self_attn.__entry[offset:6]
        addr:0x0000000000078c prog.kernel_launch(%1911:tensor<[1, 192, 1536], Float32, CPU>, %1892:tensor<[1, 192, 1536], Float32, CPU>) -> (%1912:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x0000000000078d prog.kernel_launch(%1912:tensor<[1, 192, 1536], Float32, CPU>) -> (%1913:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.23.post_attention_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x0000000000078e prog.jump model.layers.23.mlp.__entry[offset:25]
        addr:0x0000000000078f prog.kernel_launch(%1918:tensor<[1, 192, 1536], Float32, CPU>, %1912:tensor<[1, 192, 1536], Float32, CPU>) -> (%1919:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x00000000000790 prog.ret
        addr:0x00000000000791 prog.label[symbol:model.layers.23.self_attn.__entry]
        addr:0x00000000000792 prog.kernel_launch(%1893:tensor<[1, 192, 1536], Float32, CPU>) -> (%1894:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.23.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000793 prog.kernel_launch(%1893:tensor<[1, 192, 1536], Float32, CPU>) -> (%1895:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.23.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000794 prog.kernel_launch(%1893:tensor<[1, 192, 1536], Float32, CPU>) -> (%1896:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.23.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000795 prog.kernel_launch(%1894:tensor<[1, 192, 1536], Float32, CPU>) -> (%1894:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,12,128]}]
        addr:0x00000000000796 prog.kernel_launch(%1895:tensor<[1, 192, 256], Float32, CPU>) -> (%1895:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x00000000000797 prog.kernel_launch(%1896:tensor<[1, 192, 256], Float32, CPU>) -> (%1896:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x00000000000798 prog.kernel_launch(%1894:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1897:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000799 prog.kernel_launch(%1895:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1898:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000079a prog.kernel_launch(%1896:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1899:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000079b prog.kernel_launch(%1897:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1900:tensor<[1, 12, 192, 128], Float32, CPU>)[symbol_name:model.layers.23.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x0000000000079c prog.kernel_launch(%1898:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1901:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.23.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x0000000000079d prog.kernel_launch(%1901:tensor<[1, 2, 192, 128], Float32, CPU>, %1899:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1902:tensor<[1, 2, 192, 128], Float32, CPU>, %1903:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.23.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x0000000000079e prog.kernel_launch(%1900:tensor<[1, 12, 192, 128], Float32, CPU>, %1902:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1904:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x0000000000079f prog.kernel_launch(%1904:tensor<[1, 12, 192, 192], Float32, CPU>, %1905:tensor<[1], Float32, CPU>) -> (%1906:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x000000000007a0 prog.kernel_launch(%1906:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1907:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.23.self_attn.mask, op_type:CausalMask, op_options:null]
        addr:0x000000000007a1 prog.kernel_launch(%1907:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1908:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.23.self_attn.softmax, op_type:Softmax, op_options:null]
        addr:0x000000000007a2 prog.kernel_launch(%1908:tensor<[1, 12, 192, 192], Float32, CPU>, %1903:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1909:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x000000000007a3 prog.kernel_launch(%1909:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1910:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000007a4 prog.kernel_launch(%1910:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1910:tensor<[1, 192, 1536], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,1536]}]
        addr:0x000000000007a5 prog.kernel_launch(%1910:tensor<[1, 192, 1536], Float32, CPU>) -> (%1911:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.23.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000007a6 prog.ret
        addr:0x000000000007a7 prog.label[symbol:model.layers.23.mlp.__entry]
        addr:0x000000000007a8 prog.kernel_launch(%1913:tensor<[1, 192, 1536], Float32, CPU>) -> (%1914:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.23.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000007a9 prog.kernel_launch(%1914:tensor<[1, 192, 8960], Float32, CPU>) -> (%1915:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.23.mlp.act, op_type:SiLU, op_options:null]
        addr:0x000000000007aa prog.kernel_launch(%1913:tensor<[1, 192, 1536], Float32, CPU>) -> (%1916:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.23.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000007ab prog.kernel_launch(%1915:tensor<[1, 192, 8960], Float32, CPU>, %1916:tensor<[1, 192, 8960], Float32, CPU>) -> (%1917:tensor<[1, 192, 8960], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x000000000007ac prog.kernel_launch(%1917:tensor<[1, 192, 8960], Float32, CPU>) -> (%1918:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.23.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x000000000007ad prog.ret
        addr:0x000000000007ae prog.label[symbol:model.layers.24.__entry]
        addr:0x000000000007af prog.kernel_launch(%1919:tensor<[1, 192, 1536], Float32, CPU>) -> (%1920:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.24.input_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x000000000007b0 prog.jump model.layers.24.self_attn.__entry[offset:6]
        addr:0x000000000007b1 prog.kernel_launch(%1938:tensor<[1, 192, 1536], Float32, CPU>, %1919:tensor<[1, 192, 1536], Float32, CPU>) -> (%1939:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000007b2 prog.kernel_launch(%1939:tensor<[1, 192, 1536], Float32, CPU>) -> (%1940:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.24.post_attention_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x000000000007b3 prog.jump model.layers.24.mlp.__entry[offset:25]
        addr:0x000000000007b4 prog.kernel_launch(%1945:tensor<[1, 192, 1536], Float32, CPU>, %1939:tensor<[1, 192, 1536], Float32, CPU>) -> (%1946:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000007b5 prog.ret
        addr:0x000000000007b6 prog.label[symbol:model.layers.24.self_attn.__entry]
        addr:0x000000000007b7 prog.kernel_launch(%1920:tensor<[1, 192, 1536], Float32, CPU>) -> (%1921:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.24.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000007b8 prog.kernel_launch(%1920:tensor<[1, 192, 1536], Float32, CPU>) -> (%1922:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.24.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000007b9 prog.kernel_launch(%1920:tensor<[1, 192, 1536], Float32, CPU>) -> (%1923:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.24.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000007ba prog.kernel_launch(%1921:tensor<[1, 192, 1536], Float32, CPU>) -> (%1921:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,12,128]}]
        addr:0x000000000007bb prog.kernel_launch(%1922:tensor<[1, 192, 256], Float32, CPU>) -> (%1922:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x000000000007bc prog.kernel_launch(%1923:tensor<[1, 192, 256], Float32, CPU>) -> (%1923:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x000000000007bd prog.kernel_launch(%1921:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1924:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000007be prog.kernel_launch(%1922:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1925:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000007bf prog.kernel_launch(%1923:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1926:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000007c0 prog.kernel_launch(%1924:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1927:tensor<[1, 12, 192, 128], Float32, CPU>)[symbol_name:model.layers.24.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000007c1 prog.kernel_launch(%1925:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1928:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.24.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000007c2 prog.kernel_launch(%1928:tensor<[1, 2, 192, 128], Float32, CPU>, %1926:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1929:tensor<[1, 2, 192, 128], Float32, CPU>, %1930:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.24.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x000000000007c3 prog.kernel_launch(%1927:tensor<[1, 12, 192, 128], Float32, CPU>, %1929:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1931:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x000000000007c4 prog.kernel_launch(%1931:tensor<[1, 12, 192, 192], Float32, CPU>, %1932:tensor<[1], Float32, CPU>) -> (%1933:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x000000000007c5 prog.kernel_launch(%1933:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1934:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.24.self_attn.mask, op_type:CausalMask, op_options:null]
        addr:0x000000000007c6 prog.kernel_launch(%1934:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1935:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.24.self_attn.softmax, op_type:Softmax, op_options:null]
        addr:0x000000000007c7 prog.kernel_launch(%1935:tensor<[1, 12, 192, 192], Float32, CPU>, %1930:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1936:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x000000000007c8 prog.kernel_launch(%1936:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1937:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000007c9 prog.kernel_launch(%1937:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1937:tensor<[1, 192, 1536], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,1536]}]
        addr:0x000000000007ca prog.kernel_launch(%1937:tensor<[1, 192, 1536], Float32, CPU>) -> (%1938:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.24.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000007cb prog.ret
        addr:0x000000000007cc prog.label[symbol:model.layers.24.mlp.__entry]
        addr:0x000000000007cd prog.kernel_launch(%1940:tensor<[1, 192, 1536], Float32, CPU>) -> (%1941:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.24.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000007ce prog.kernel_launch(%1941:tensor<[1, 192, 8960], Float32, CPU>) -> (%1942:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.24.mlp.act, op_type:SiLU, op_options:null]
        addr:0x000000000007cf prog.kernel_launch(%1940:tensor<[1, 192, 1536], Float32, CPU>) -> (%1943:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.24.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000007d0 prog.kernel_launch(%1942:tensor<[1, 192, 8960], Float32, CPU>, %1943:tensor<[1, 192, 8960], Float32, CPU>) -> (%1944:tensor<[1, 192, 8960], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x000000000007d1 prog.kernel_launch(%1944:tensor<[1, 192, 8960], Float32, CPU>) -> (%1945:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.24.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x000000000007d2 prog.ret
        addr:0x000000000007d3 prog.label[symbol:model.layers.25.__entry]
        addr:0x000000000007d4 prog.kernel_launch(%1946:tensor<[1, 192, 1536], Float32, CPU>) -> (%1947:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.25.input_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x000000000007d5 prog.jump model.layers.25.self_attn.__entry[offset:6]
        addr:0x000000000007d6 prog.kernel_launch(%1965:tensor<[1, 192, 1536], Float32, CPU>, %1946:tensor<[1, 192, 1536], Float32, CPU>) -> (%1966:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000007d7 prog.kernel_launch(%1966:tensor<[1, 192, 1536], Float32, CPU>) -> (%1967:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.25.post_attention_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x000000000007d8 prog.jump model.layers.25.mlp.__entry[offset:25]
        addr:0x000000000007d9 prog.kernel_launch(%1972:tensor<[1, 192, 1536], Float32, CPU>, %1966:tensor<[1, 192, 1536], Float32, CPU>) -> (%1973:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000007da prog.ret
        addr:0x000000000007db prog.label[symbol:model.layers.25.self_attn.__entry]
        addr:0x000000000007dc prog.kernel_launch(%1947:tensor<[1, 192, 1536], Float32, CPU>) -> (%1948:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.25.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000007dd prog.kernel_launch(%1947:tensor<[1, 192, 1536], Float32, CPU>) -> (%1949:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.25.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000007de prog.kernel_launch(%1947:tensor<[1, 192, 1536], Float32, CPU>) -> (%1950:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.25.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000007df prog.kernel_launch(%1948:tensor<[1, 192, 1536], Float32, CPU>) -> (%1948:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,12,128]}]
        addr:0x000000000007e0 prog.kernel_launch(%1949:tensor<[1, 192, 256], Float32, CPU>) -> (%1949:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x000000000007e1 prog.kernel_launch(%1950:tensor<[1, 192, 256], Float32, CPU>) -> (%1950:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x000000000007e2 prog.kernel_launch(%1948:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1951:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000007e3 prog.kernel_launch(%1949:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1952:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000007e4 prog.kernel_launch(%1950:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1953:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000007e5 prog.kernel_launch(%1951:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1954:tensor<[1, 12, 192, 128], Float32, CPU>)[symbol_name:model.layers.25.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000007e6 prog.kernel_launch(%1952:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1955:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.25.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000007e7 prog.kernel_launch(%1955:tensor<[1, 2, 192, 128], Float32, CPU>, %1953:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1956:tensor<[1, 2, 192, 128], Float32, CPU>, %1957:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.25.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x000000000007e8 prog.kernel_launch(%1954:tensor<[1, 12, 192, 128], Float32, CPU>, %1956:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1958:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x000000000007e9 prog.kernel_launch(%1958:tensor<[1, 12, 192, 192], Float32, CPU>, %1959:tensor<[1], Float32, CPU>) -> (%1960:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x000000000007ea prog.kernel_launch(%1960:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1961:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.25.self_attn.mask, op_type:CausalMask, op_options:null]
        addr:0x000000000007eb prog.kernel_launch(%1961:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1962:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.25.self_attn.softmax, op_type:Softmax, op_options:null]
        addr:0x000000000007ec prog.kernel_launch(%1962:tensor<[1, 12, 192, 192], Float32, CPU>, %1957:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1963:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x000000000007ed prog.kernel_launch(%1963:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1964:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x000000000007ee prog.kernel_launch(%1964:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1964:tensor<[1, 192, 1536], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,1536]}]
        addr:0x000000000007ef prog.kernel_launch(%1964:tensor<[1, 192, 1536], Float32, CPU>) -> (%1965:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.25.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000007f0 prog.ret
        addr:0x000000000007f1 prog.label[symbol:model.layers.25.mlp.__entry]
        addr:0x000000000007f2 prog.kernel_launch(%1967:tensor<[1, 192, 1536], Float32, CPU>) -> (%1968:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.25.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000007f3 prog.kernel_launch(%1968:tensor<[1, 192, 8960], Float32, CPU>) -> (%1969:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.25.mlp.act, op_type:SiLU, op_options:null]
        addr:0x000000000007f4 prog.kernel_launch(%1967:tensor<[1, 192, 1536], Float32, CPU>) -> (%1970:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.25.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000007f5 prog.kernel_launch(%1969:tensor<[1, 192, 8960], Float32, CPU>, %1970:tensor<[1, 192, 8960], Float32, CPU>) -> (%1971:tensor<[1, 192, 8960], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x000000000007f6 prog.kernel_launch(%1971:tensor<[1, 192, 8960], Float32, CPU>) -> (%1972:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.25.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x000000000007f7 prog.ret
        addr:0x000000000007f8 prog.label[symbol:model.layers.26.__entry]
        addr:0x000000000007f9 prog.kernel_launch(%1973:tensor<[1, 192, 1536], Float32, CPU>) -> (%1974:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.26.input_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x000000000007fa prog.jump model.layers.26.self_attn.__entry[offset:6]
        addr:0x000000000007fb prog.kernel_launch(%1992:tensor<[1, 192, 1536], Float32, CPU>, %1973:tensor<[1, 192, 1536], Float32, CPU>) -> (%1993:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000007fc prog.kernel_launch(%1993:tensor<[1, 192, 1536], Float32, CPU>) -> (%1994:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.26.post_attention_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x000000000007fd prog.jump model.layers.26.mlp.__entry[offset:25]
        addr:0x000000000007fe prog.kernel_launch(%1999:tensor<[1, 192, 1536], Float32, CPU>, %1993:tensor<[1, 192, 1536], Float32, CPU>) -> (%2000:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x000000000007ff prog.ret
        addr:0x00000000000800 prog.label[symbol:model.layers.26.self_attn.__entry]
        addr:0x00000000000801 prog.kernel_launch(%1974:tensor<[1, 192, 1536], Float32, CPU>) -> (%1975:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.26.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000802 prog.kernel_launch(%1974:tensor<[1, 192, 1536], Float32, CPU>) -> (%1976:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.26.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000803 prog.kernel_launch(%1974:tensor<[1, 192, 1536], Float32, CPU>) -> (%1977:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.26.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000804 prog.kernel_launch(%1975:tensor<[1, 192, 1536], Float32, CPU>) -> (%1975:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,12,128]}]
        addr:0x00000000000805 prog.kernel_launch(%1976:tensor<[1, 192, 256], Float32, CPU>) -> (%1976:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x00000000000806 prog.kernel_launch(%1977:tensor<[1, 192, 256], Float32, CPU>) -> (%1977:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x00000000000807 prog.kernel_launch(%1975:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1978:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000808 prog.kernel_launch(%1976:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1979:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000809 prog.kernel_launch(%1977:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1980:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000080a prog.kernel_launch(%1978:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1981:tensor<[1, 12, 192, 128], Float32, CPU>)[symbol_name:model.layers.26.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x0000000000080b prog.kernel_launch(%1979:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1982:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.26.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x0000000000080c prog.kernel_launch(%1982:tensor<[1, 2, 192, 128], Float32, CPU>, %1980:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1983:tensor<[1, 2, 192, 128], Float32, CPU>, %1984:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.26.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x0000000000080d prog.kernel_launch(%1981:tensor<[1, 12, 192, 128], Float32, CPU>, %1983:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1985:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x0000000000080e prog.kernel_launch(%1985:tensor<[1, 12, 192, 192], Float32, CPU>, %1986:tensor<[1], Float32, CPU>) -> (%1987:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x0000000000080f prog.kernel_launch(%1987:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1988:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.26.self_attn.mask, op_type:CausalMask, op_options:null]
        addr:0x00000000000810 prog.kernel_launch(%1988:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1989:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.26.self_attn.softmax, op_type:Softmax, op_options:null]
        addr:0x00000000000811 prog.kernel_launch(%1989:tensor<[1, 12, 192, 192], Float32, CPU>, %1984:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1990:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x00000000000812 prog.kernel_launch(%1990:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1991:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000813 prog.kernel_launch(%1991:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1991:tensor<[1, 192, 1536], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,1536]}]
        addr:0x00000000000814 prog.kernel_launch(%1991:tensor<[1, 192, 1536], Float32, CPU>) -> (%1992:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.26.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000815 prog.ret
        addr:0x00000000000816 prog.label[symbol:model.layers.26.mlp.__entry]
        addr:0x00000000000817 prog.kernel_launch(%1994:tensor<[1, 192, 1536], Float32, CPU>) -> (%1995:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.26.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000818 prog.kernel_launch(%1995:tensor<[1, 192, 8960], Float32, CPU>) -> (%1996:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.26.mlp.act, op_type:SiLU, op_options:null]
        addr:0x00000000000819 prog.kernel_launch(%1994:tensor<[1, 192, 1536], Float32, CPU>) -> (%1997:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.26.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x0000000000081a prog.kernel_launch(%1996:tensor<[1, 192, 8960], Float32, CPU>, %1997:tensor<[1, 192, 8960], Float32, CPU>) -> (%1998:tensor<[1, 192, 8960], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x0000000000081b prog.kernel_launch(%1998:tensor<[1, 192, 8960], Float32, CPU>) -> (%1999:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.26.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x0000000000081c prog.ret
        addr:0x0000000000081d prog.label[symbol:model.layers.27.__entry]
        addr:0x0000000000081e prog.kernel_launch(%2000:tensor<[1, 192, 1536], Float32, CPU>) -> (%2001:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.27.input_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x0000000000081f prog.jump model.layers.27.self_attn.__entry[offset:6]
        addr:0x00000000000820 prog.kernel_launch(%2019:tensor<[1, 192, 1536], Float32, CPU>, %2000:tensor<[1, 192, 1536], Float32, CPU>) -> (%2020:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x00000000000821 prog.kernel_launch(%2020:tensor<[1, 192, 1536], Float32, CPU>) -> (%2021:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.27.post_attention_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x00000000000822 prog.jump model.layers.27.mlp.__entry[offset:25]
        addr:0x00000000000823 prog.kernel_launch(%2026:tensor<[1, 192, 1536], Float32, CPU>, %2020:tensor<[1, 192, 1536], Float32, CPU>) -> (%2027:tensor<[1, 192, 1536], Float32, CPU>)[op_type:Add, op_options:null]
        addr:0x00000000000824 prog.ret
        addr:0x00000000000825 prog.label[symbol:model.layers.27.self_attn.__entry]
        addr:0x00000000000826 prog.kernel_launch(%2001:tensor<[1, 192, 1536], Float32, CPU>) -> (%2002:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.27.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000827 prog.kernel_launch(%2001:tensor<[1, 192, 1536], Float32, CPU>) -> (%2003:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.27.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000828 prog.kernel_launch(%2001:tensor<[1, 192, 1536], Float32, CPU>) -> (%2004:tensor<[1, 192, 256], Float32, CPU>)[symbol_name:model.layers.27.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000829 prog.kernel_launch(%2002:tensor<[1, 192, 1536], Float32, CPU>) -> (%2002:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,12,128]}]
        addr:0x0000000000082a prog.kernel_launch(%2003:tensor<[1, 192, 256], Float32, CPU>) -> (%2003:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x0000000000082b prog.kernel_launch(%2004:tensor<[1, 192, 256], Float32, CPU>) -> (%2004:tensor<[1, 192, 2, 128], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,2,128]}]
        addr:0x0000000000082c prog.kernel_launch(%2002:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%2005:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000082d prog.kernel_launch(%2003:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%2006:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000082e prog.kernel_launch(%2004:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%2007:tensor<[1, 2, 192, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x0000000000082f prog.kernel_launch(%2005:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%2008:tensor<[1, 12, 192, 128], Float32, CPU>)[symbol_name:model.layers.27.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000830 prog.kernel_launch(%2006:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%2009:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.27.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000831 prog.kernel_launch(%2009:tensor<[1, 2, 192, 128], Float32, CPU>, %2007:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%2010:tensor<[1, 2, 192, 128], Float32, CPU>, %2011:tensor<[1, 2, 192, 128], Float32, CPU>)[symbol_name:model.layers.27.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000832 prog.kernel_launch(%2008:tensor<[1, 12, 192, 128], Float32, CPU>, %2010:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%2012:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":true}]
        addr:0x00000000000833 prog.kernel_launch(%2012:tensor<[1, 12, 192, 192], Float32, CPU>, %2013:tensor<[1], Float32, CPU>) -> (%2014:tensor<[1, 12, 192, 192], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x00000000000834 prog.kernel_launch(%2014:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%2015:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.27.self_attn.mask, op_type:CausalMask, op_options:null]
        addr:0x00000000000835 prog.kernel_launch(%2015:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%2016:tensor<[1, 12, 192, 192], Float32, CPU>)[symbol_name:model.layers.27.self_attn.softmax, op_type:Softmax, op_options:null]
        addr:0x00000000000836 prog.kernel_launch(%2016:tensor<[1, 12, 192, 192], Float32, CPU>, %2011:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%2017:tensor<[1, 12, 192, 128], Float32, CPU>)[op_type:MatMul, op_options:{"matmul_type":"Default","transpose_a":false,"transpose_b":false}]
        addr:0x00000000000837 prog.kernel_launch(%2017:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%2018:tensor<[1, 192, 12, 128], Float32, CPU>)[op_type:Transpose, op_options:{"dim0":1,"dim1":2}]
        addr:0x00000000000838 prog.kernel_launch(%2018:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%2018:tensor<[1, 192, 1536], Float32, CPU>)[op_type:View, op_options:{"to_shape":[1,-1,1536]}]
        addr:0x00000000000839 prog.kernel_launch(%2018:tensor<[1, 192, 1536], Float32, CPU>) -> (%2019:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.27.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000083a prog.ret
        addr:0x0000000000083b prog.label[symbol:model.layers.27.mlp.__entry]
        addr:0x0000000000083c prog.kernel_launch(%2021:tensor<[1, 192, 1536], Float32, CPU>) -> (%2022:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.27.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x0000000000083d prog.kernel_launch(%2022:tensor<[1, 192, 8960], Float32, CPU>) -> (%2023:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.27.mlp.act, op_type:SiLU, op_options:null]
        addr:0x0000000000083e prog.kernel_launch(%2021:tensor<[1, 192, 1536], Float32, CPU>) -> (%2024:tensor<[1, 192, 8960], Float32, CPU>)[symbol_name:model.layers.27.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x0000000000083f prog.kernel_launch(%2023:tensor<[1, 192, 8960], Float32, CPU>, %2024:tensor<[1, 192, 8960], Float32, CPU>) -> (%2025:tensor<[1, 192, 8960], Float32, CPU>)[op_type:Mul, op_options:null]
        addr:0x00000000000840 prog.kernel_launch(%2025:tensor<[1, 192, 8960], Float32, CPU>) -> (%2026:tensor<[1, 192, 1536], Float32, CPU>)[symbol_name:model.layers.27.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x00000000000841 prog.ret
    }
    prog.fragment @__MLLM_JIT_PACKAGE_SYMBOL_TABLE_SEGMENT <CPU> <table> {
        addr:0x00000000000000 prog.value_symbol() -> (%517:tensor<[233373696], Float32, CPU>[@model.embed_tokens.weight])[symbol:model.embed_tokens.weight]
        addr:0x00000000000001 prog.value_symbol() -> (%499:tensor<[1505280], Float32, CPU>[@visual.patch_embed.proj.weight])[symbol:visual.patch_embed.proj.weight]
        addr:0x00000000000002 prog.value_symbol() -> (%338:tensor<[1280], Float32, CPU>[@visual.blocks.0.norm1.weight])[symbol:visual.blocks.0.norm1.weight]
        addr:0x00000000000003 prog.value_symbol() -> (%540:tensor<[1280], Float32, CPU>[@visual.blocks.0.norm1.bias])[symbol:visual.blocks.0.norm1.bias]
        addr:0x00000000000004 prog.value_symbol() -> (%315:tensor<[2795520], UInt8, CPU>[@visual.blocks.0.attn.qkv.weight])[symbol:visual.blocks.0.attn.qkv.weight]
        addr:0x00000000000005 prog.value_symbol() -> (%112:tensor<[931840], UInt8, CPU>[@visual.blocks.0.attn.proj.weight])[symbol:visual.blocks.0.attn.proj.weight]
        addr:0x00000000000006 prog.value_symbol() -> (%313:tensor<[1280], Float32, CPU>[@visual.blocks.0.norm2.weight])[symbol:visual.blocks.0.norm2.weight]
        addr:0x00000000000007 prog.value_symbol() -> (%527:tensor<[1280], Float32, CPU>[@visual.blocks.0.norm2.bias])[symbol:visual.blocks.0.norm2.bias]
        addr:0x00000000000008 prog.value_symbol() -> (%89:tensor<[3727360], UInt8, CPU>[@visual.blocks.0.mlp.fc1.weight])[symbol:visual.blocks.0.mlp.fc1.weight]
        addr:0x00000000000009 prog.value_symbol() -> (%59:tensor<[3696640], UInt8, CPU>[@visual.blocks.0.mlp.fc2.weight])[symbol:visual.blocks.0.mlp.fc2.weight]
        addr:0x0000000000000a prog.value_symbol() -> (%509:tensor<[1280], Float32, CPU>[@visual.blocks.1.norm1.weight])[symbol:visual.blocks.1.norm1.weight]
        addr:0x0000000000000b prog.value_symbol() -> (%311:tensor<[1280], Float32, CPU>[@visual.blocks.1.norm1.bias])[symbol:visual.blocks.1.norm1.bias]
        addr:0x0000000000000c prog.value_symbol() -> (%108:tensor<[2795520], UInt8, CPU>[@visual.blocks.1.attn.qkv.weight])[symbol:visual.blocks.1.attn.qkv.weight]
        addr:0x0000000000000d prog.value_symbol() -> (%117:tensor<[931840], UInt8, CPU>[@visual.blocks.1.attn.proj.weight])[symbol:visual.blocks.1.attn.proj.weight]
        addr:0x0000000000000e prog.value_symbol() -> (%310:tensor<[1280], Float32, CPU>[@visual.blocks.1.norm2.weight])[symbol:visual.blocks.1.norm2.weight]
        addr:0x0000000000000f prog.value_symbol() -> (%351:tensor<[1280], Float32, CPU>[@visual.blocks.1.norm2.bias])[symbol:visual.blocks.1.norm2.bias]
        addr:0x00000000000010 prog.value_symbol() -> (%77:tensor<[3727360], UInt8, CPU>[@visual.blocks.1.mlp.fc1.weight])[symbol:visual.blocks.1.mlp.fc1.weight]
        addr:0x00000000000011 prog.value_symbol() -> (%74:tensor<[3696640], UInt8, CPU>[@visual.blocks.1.mlp.fc2.weight])[symbol:visual.blocks.1.mlp.fc2.weight]
        addr:0x00000000000012 prog.value_symbol() -> (%364:tensor<[1280], Float32, CPU>[@visual.blocks.2.norm1.weight])[symbol:visual.blocks.2.norm1.weight]
        addr:0x00000000000013 prog.value_symbol() -> (%505:tensor<[1280], Float32, CPU>[@visual.blocks.2.norm1.bias])[symbol:visual.blocks.2.norm1.bias]
        addr:0x00000000000014 prog.value_symbol() -> (%101:tensor<[2795520], UInt8, CPU>[@visual.blocks.2.attn.qkv.weight])[symbol:visual.blocks.2.attn.qkv.weight]
        addr:0x00000000000015 prog.value_symbol() -> (%361:tensor<[931840], UInt8, CPU>[@visual.blocks.2.attn.proj.weight])[symbol:visual.blocks.2.attn.proj.weight]
        addr:0x00000000000016 prog.value_symbol() -> (%257:tensor<[1280], Float32, CPU>[@visual.blocks.2.norm2.weight])[symbol:visual.blocks.2.norm2.weight]
        addr:0x00000000000017 prog.value_symbol() -> (%538:tensor<[1280], Float32, CPU>[@visual.blocks.2.norm2.bias])[symbol:visual.blocks.2.norm2.bias]
        addr:0x00000000000018 prog.value_symbol() -> (%453:tensor<[3727360], UInt8, CPU>[@visual.blocks.2.mlp.fc1.weight])[symbol:visual.blocks.2.mlp.fc1.weight]
        addr:0x00000000000019 prog.value_symbol() -> (%62:tensor<[3696640], UInt8, CPU>[@visual.blocks.2.mlp.fc2.weight])[symbol:visual.blocks.2.mlp.fc2.weight]
        addr:0x0000000000001a prog.value_symbol() -> (%221:tensor<[1280], Float32, CPU>[@visual.blocks.3.norm1.weight])[symbol:visual.blocks.3.norm1.weight]
        addr:0x0000000000001b prog.value_symbol() -> (%488:tensor<[1280], Float32, CPU>[@visual.blocks.3.norm1.bias])[symbol:visual.blocks.3.norm1.bias]
        addr:0x0000000000001c prog.value_symbol() -> (%222:tensor<[2795520], UInt8, CPU>[@visual.blocks.3.attn.qkv.weight])[symbol:visual.blocks.3.attn.qkv.weight]
        addr:0x0000000000001d prog.value_symbol() -> (%269:tensor<[931840], UInt8, CPU>[@visual.blocks.3.attn.proj.weight])[symbol:visual.blocks.3.attn.proj.weight]
        addr:0x0000000000001e prog.value_symbol() -> (%542:tensor<[1280], Float32, CPU>[@visual.blocks.3.norm2.weight])[symbol:visual.blocks.3.norm2.weight]
        addr:0x0000000000001f prog.value_symbol() -> (%424:tensor<[1280], Float32, CPU>[@visual.blocks.3.norm2.bias])[symbol:visual.blocks.3.norm2.bias]
        addr:0x00000000000020 prog.value_symbol() -> (%82:tensor<[3727360], UInt8, CPU>[@visual.blocks.3.mlp.fc1.weight])[symbol:visual.blocks.3.mlp.fc1.weight]
        addr:0x00000000000021 prog.value_symbol() -> (%562:tensor<[3696640], UInt8, CPU>[@visual.blocks.3.mlp.fc2.weight])[symbol:visual.blocks.3.mlp.fc2.weight]
        addr:0x00000000000022 prog.value_symbol() -> (%482:tensor<[1280], Float32, CPU>[@visual.blocks.4.norm1.weight])[symbol:visual.blocks.4.norm1.weight]
        addr:0x00000000000023 prog.value_symbol() -> (%215:tensor<[1280], Float32, CPU>[@visual.blocks.4.norm1.bias])[symbol:visual.blocks.4.norm1.bias]
        addr:0x00000000000024 prog.value_symbol() -> (%103:tensor<[2795520], UInt8, CPU>[@visual.blocks.4.attn.qkv.weight])[symbol:visual.blocks.4.attn.qkv.weight]
        addr:0x00000000000025 prog.value_symbol() -> (%320:tensor<[931840], UInt8, CPU>[@visual.blocks.4.attn.proj.weight])[symbol:visual.blocks.4.attn.proj.weight]
        addr:0x00000000000026 prog.value_symbol() -> (%213:tensor<[1280], Float32, CPU>[@visual.blocks.4.norm2.weight])[symbol:visual.blocks.4.norm2.weight]
        addr:0x00000000000027 prog.value_symbol() -> (%214:tensor<[1280], Float32, CPU>[@visual.blocks.4.norm2.bias])[symbol:visual.blocks.4.norm2.bias]
        addr:0x00000000000028 prog.value_symbol() -> (%476:tensor<[3727360], UInt8, CPU>[@visual.blocks.4.mlp.fc1.weight])[symbol:visual.blocks.4.mlp.fc1.weight]
        addr:0x00000000000029 prog.value_symbol() -> (%559:tensor<[3696640], UInt8, CPU>[@visual.blocks.4.mlp.fc2.weight])[symbol:visual.blocks.4.mlp.fc2.weight]
        addr:0x0000000000002a prog.value_symbol() -> (%212:tensor<[1280], Float32, CPU>[@visual.blocks.5.norm1.weight])[symbol:visual.blocks.5.norm1.weight]
        addr:0x0000000000002b prog.value_symbol() -> (%237:tensor<[1280], Float32, CPU>[@visual.blocks.5.norm1.bias])[symbol:visual.blocks.5.norm1.bias]
        addr:0x0000000000002c prog.value_symbol() -> (%111:tensor<[2795520], UInt8, CPU>[@visual.blocks.5.attn.qkv.weight])[symbol:visual.blocks.5.attn.qkv.weight]
        addr:0x0000000000002d prog.value_symbol() -> (%557:tensor<[931840], UInt8, CPU>[@visual.blocks.5.attn.proj.weight])[symbol:visual.blocks.5.attn.proj.weight]
        addr:0x0000000000002e prog.value_symbol() -> (%514:tensor<[1280], Float32, CPU>[@visual.blocks.5.norm2.weight])[symbol:visual.blocks.5.norm2.weight]
        addr:0x0000000000002f prog.value_symbol() -> (%372:tensor<[1280], Float32, CPU>[@visual.blocks.5.norm2.bias])[symbol:visual.blocks.5.norm2.bias]
        addr:0x00000000000030 prog.value_symbol() -> (%92:tensor<[3727360], UInt8, CPU>[@visual.blocks.5.mlp.fc1.weight])[symbol:visual.blocks.5.mlp.fc1.weight]
        addr:0x00000000000031 prog.value_symbol() -> (%522:tensor<[3696640], UInt8, CPU>[@visual.blocks.5.mlp.fc2.weight])[symbol:visual.blocks.5.mlp.fc2.weight]
        addr:0x00000000000032 prog.value_symbol() -> (%369:tensor<[1280], Float32, CPU>[@visual.blocks.6.norm1.weight])[symbol:visual.blocks.6.norm1.weight]
        addr:0x00000000000033 prog.value_symbol() -> (%211:tensor<[1280], Float32, CPU>[@visual.blocks.6.norm1.bias])[symbol:visual.blocks.6.norm1.bias]
        addr:0x00000000000034 prog.value_symbol() -> (%105:tensor<[2795520], UInt8, CPU>[@visual.blocks.6.attn.qkv.weight])[symbol:visual.blocks.6.attn.qkv.weight]
        addr:0x00000000000035 prog.value_symbol() -> (%323:tensor<[931840], UInt8, CPU>[@visual.blocks.6.attn.proj.weight])[symbol:visual.blocks.6.attn.proj.weight]
        addr:0x00000000000036 prog.value_symbol() -> (%529:tensor<[1280], Float32, CPU>[@visual.blocks.6.norm2.weight])[symbol:visual.blocks.6.norm2.weight]
        addr:0x00000000000037 prog.value_symbol() -> (%425:tensor<[1280], Float32, CPU>[@visual.blocks.6.norm2.bias])[symbol:visual.blocks.6.norm2.bias]
        addr:0x00000000000038 prog.value_symbol() -> (%90:tensor<[3727360], UInt8, CPU>[@visual.blocks.6.mlp.fc1.weight])[symbol:visual.blocks.6.mlp.fc1.weight]
        addr:0x00000000000039 prog.value_symbol() -> (%483:tensor<[3696640], UInt8, CPU>[@visual.blocks.6.mlp.fc2.weight])[symbol:visual.blocks.6.mlp.fc2.weight]
        addr:0x0000000000003a prog.value_symbol() -> (%210:tensor<[1280], Float32, CPU>[@visual.blocks.7.norm1.weight])[symbol:visual.blocks.7.norm1.weight]
        addr:0x0000000000003b prog.value_symbol() -> (%258:tensor<[1280], Float32, CPU>[@visual.blocks.7.norm1.bias])[symbol:visual.blocks.7.norm1.bias]
        addr:0x0000000000003c prog.value_symbol() -> (%227:tensor<[2795520], UInt8, CPU>[@visual.blocks.7.attn.qkv.weight])[symbol:visual.blocks.7.attn.qkv.weight]
        addr:0x0000000000003d prog.value_symbol() -> (%123:tensor<[931840], UInt8, CPU>[@visual.blocks.7.attn.proj.weight])[symbol:visual.blocks.7.attn.proj.weight]
        addr:0x0000000000003e prog.value_symbol() -> (%208:tensor<[1280], Float32, CPU>[@visual.blocks.7.norm2.weight])[symbol:visual.blocks.7.norm2.weight]
        addr:0x0000000000003f prog.value_symbol() -> (%209:tensor<[1280], Float32, CPU>[@visual.blocks.7.norm2.bias])[symbol:visual.blocks.7.norm2.bias]
        addr:0x00000000000040 prog.value_symbol() -> (%450:tensor<[3727360], UInt8, CPU>[@visual.blocks.7.mlp.fc1.weight])[symbol:visual.blocks.7.mlp.fc1.weight]
        addr:0x00000000000041 prog.value_symbol() -> (%573:tensor<[3696640], UInt8, CPU>[@visual.blocks.7.mlp.fc2.weight])[symbol:visual.blocks.7.mlp.fc2.weight]
        addr:0x00000000000042 prog.value_symbol() -> (%501:tensor<[1280], Float32, CPU>[@visual.blocks.8.norm1.weight])[symbol:visual.blocks.8.norm1.weight]
        addr:0x00000000000043 prog.value_symbol() -> (%207:tensor<[1280], Float32, CPU>[@visual.blocks.8.norm1.bias])[symbol:visual.blocks.8.norm1.bias]
        addr:0x00000000000044 prog.value_symbol() -> (%431:tensor<[2795520], UInt8, CPU>[@visual.blocks.8.attn.qkv.weight])[symbol:visual.blocks.8.attn.qkv.weight]
        addr:0x00000000000045 prog.value_symbol() -> (%118:tensor<[931840], UInt8, CPU>[@visual.blocks.8.attn.proj.weight])[symbol:visual.blocks.8.attn.proj.weight]
        addr:0x00000000000046 prog.value_symbol() -> (%205:tensor<[1280], Float32, CPU>[@visual.blocks.8.norm2.weight])[symbol:visual.blocks.8.norm2.weight]
        addr:0x00000000000047 prog.value_symbol() -> (%206:tensor<[1280], Float32, CPU>[@visual.blocks.8.norm2.bias])[symbol:visual.blocks.8.norm2.bias]
        addr:0x00000000000048 prog.value_symbol() -> (%94:tensor<[3727360], UInt8, CPU>[@visual.blocks.8.mlp.fc1.weight])[symbol:visual.blocks.8.mlp.fc1.weight]
        addr:0x00000000000049 prog.value_symbol() -> (%72:tensor<[3696640], UInt8, CPU>[@visual.blocks.8.mlp.fc2.weight])[symbol:visual.blocks.8.mlp.fc2.weight]
        addr:0x0000000000004a prog.value_symbol() -> (%486:tensor<[1280], Float32, CPU>[@visual.blocks.9.norm1.weight])[symbol:visual.blocks.9.norm1.weight]
        addr:0x0000000000004b prog.value_symbol() -> (%204:tensor<[1280], Float32, CPU>[@visual.blocks.9.norm1.bias])[symbol:visual.blocks.9.norm1.bias]
        addr:0x0000000000004c prog.value_symbol() -> (%544:tensor<[2795520], UInt8, CPU>[@visual.blocks.9.attn.qkv.weight])[symbol:visual.blocks.9.attn.qkv.weight]
        addr:0x0000000000004d prog.value_symbol() -> (%316:tensor<[931840], UInt8, CPU>[@visual.blocks.9.attn.proj.weight])[symbol:visual.blocks.9.attn.proj.weight]
        addr:0x0000000000004e prog.value_symbol() -> (%513:tensor<[1280], Float32, CPU>[@visual.blocks.9.norm2.weight])[symbol:visual.blocks.9.norm2.weight]
        addr:0x0000000000004f prog.value_symbol() -> (%203:tensor<[1280], Float32, CPU>[@visual.blocks.9.norm2.bias])[symbol:visual.blocks.9.norm2.bias]
        addr:0x00000000000050 prog.value_symbol() -> (%96:tensor<[3727360], UInt8, CPU>[@visual.blocks.9.mlp.fc1.weight])[symbol:visual.blocks.9.mlp.fc1.weight]
        addr:0x00000000000051 prog.value_symbol() -> (%75:tensor<[3696640], UInt8, CPU>[@visual.blocks.9.mlp.fc2.weight])[symbol:visual.blocks.9.mlp.fc2.weight]
        addr:0x00000000000052 prog.value_symbol() -> (%560:tensor<[1280], Float32, CPU>[@visual.blocks.10.norm1.weight])[symbol:visual.blocks.10.norm1.weight]
        addr:0x00000000000053 prog.value_symbol() -> (%308:tensor<[1280], Float32, CPU>[@visual.blocks.10.norm1.bias])[symbol:visual.blocks.10.norm1.bias]
        addr:0x00000000000054 prog.value_symbol() -> (%459:tensor<[2795520], UInt8, CPU>[@visual.blocks.10.attn.qkv.weight])[symbol:visual.blocks.10.attn.qkv.weight]
        addr:0x00000000000055 prog.value_symbol() -> (%346:tensor<[931840], UInt8, CPU>[@visual.blocks.10.attn.proj.weight])[symbol:visual.blocks.10.attn.proj.weight]
        addr:0x00000000000056 prog.value_symbol() -> (%305:tensor<[1280], Float32, CPU>[@visual.blocks.10.norm2.weight])[symbol:visual.blocks.10.norm2.weight]
        addr:0x00000000000057 prog.value_symbol() -> (%427:tensor<[1280], Float32, CPU>[@visual.blocks.10.norm2.bias])[symbol:visual.blocks.10.norm2.bias]
        addr:0x00000000000058 prog.value_symbol() -> (%76:tensor<[3727360], UInt8, CPU>[@visual.blocks.10.mlp.fc1.weight])[symbol:visual.blocks.10.mlp.fc1.weight]
        addr:0x00000000000059 prog.value_symbol() -> (%61:tensor<[3696640], UInt8, CPU>[@visual.blocks.10.mlp.fc2.weight])[symbol:visual.blocks.10.mlp.fc2.weight]
        addr:0x0000000000005a prog.value_symbol() -> (%302:tensor<[1280], Float32, CPU>[@visual.blocks.11.norm1.weight])[symbol:visual.blocks.11.norm1.weight]
        addr:0x0000000000005b prog.value_symbol() -> (%303:tensor<[1280], Float32, CPU>[@visual.blocks.11.norm1.bias])[symbol:visual.blocks.11.norm1.bias]
        addr:0x0000000000005c prog.value_symbol() -> (%523:tensor<[2795520], UInt8, CPU>[@visual.blocks.11.attn.qkv.weight])[symbol:visual.blocks.11.attn.qkv.weight]
        addr:0x0000000000005d prog.value_symbol() -> (%304:tensor<[931840], UInt8, CPU>[@visual.blocks.11.attn.proj.weight])[symbol:visual.blocks.11.attn.proj.weight]
        addr:0x0000000000005e prog.value_symbol() -> (%298:tensor<[1280], Float32, CPU>[@visual.blocks.11.norm2.weight])[symbol:visual.blocks.11.norm2.weight]
        addr:0x0000000000005f prog.value_symbol() -> (%300:tensor<[1280], Float32, CPU>[@visual.blocks.11.norm2.bias])[symbol:visual.blocks.11.norm2.bias]
        addr:0x00000000000060 prog.value_symbol() -> (%79:tensor<[3727360], UInt8, CPU>[@visual.blocks.11.mlp.fc1.weight])[symbol:visual.blocks.11.mlp.fc1.weight]
        addr:0x00000000000061 prog.value_symbol() -> (%71:tensor<[3696640], UInt8, CPU>[@visual.blocks.11.mlp.fc2.weight])[symbol:visual.blocks.11.mlp.fc2.weight]
        addr:0x00000000000062 prog.value_symbol() -> (%312:tensor<[1280], Float32, CPU>[@visual.blocks.12.norm1.weight])[symbol:visual.blocks.12.norm1.weight]
        addr:0x00000000000063 prog.value_symbol() -> (%294:tensor<[1280], Float32, CPU>[@visual.blocks.12.norm1.bias])[symbol:visual.blocks.12.norm1.bias]
        addr:0x00000000000064 prog.value_symbol() -> (%295:tensor<[2795520], UInt8, CPU>[@visual.blocks.12.attn.qkv.weight])[symbol:visual.blocks.12.attn.qkv.weight]
        addr:0x00000000000065 prog.value_symbol() -> (%296:tensor<[931840], UInt8, CPU>[@visual.blocks.12.attn.proj.weight])[symbol:visual.blocks.12.attn.proj.weight]
        addr:0x00000000000066 prog.value_symbol() -> (%293:tensor<[1280], Float32, CPU>[@visual.blocks.12.norm2.weight])[symbol:visual.blocks.12.norm2.weight]
        addr:0x00000000000067 prog.value_symbol() -> (%518:tensor<[1280], Float32, CPU>[@visual.blocks.12.norm2.bias])[symbol:visual.blocks.12.norm2.bias]
        addr:0x00000000000068 prog.value_symbol() -> (%78:tensor<[3727360], UInt8, CPU>[@visual.blocks.12.mlp.fc1.weight])[symbol:visual.blocks.12.mlp.fc1.weight]
        addr:0x00000000000069 prog.value_symbol() -> (%68:tensor<[3696640], UInt8, CPU>[@visual.blocks.12.mlp.fc2.weight])[symbol:visual.blocks.12.mlp.fc2.weight]
        addr:0x0000000000006a prog.value_symbol() -> (%287:tensor<[1280], Float32, CPU>[@visual.blocks.13.norm1.weight])[symbol:visual.blocks.13.norm1.weight]
        addr:0x0000000000006b prog.value_symbol() -> (%289:tensor<[1280], Float32, CPU>[@visual.blocks.13.norm1.bias])[symbol:visual.blocks.13.norm1.bias]
        addr:0x0000000000006c prog.value_symbol() -> (%516:tensor<[2795520], UInt8, CPU>[@visual.blocks.13.attn.qkv.weight])[symbol:visual.blocks.13.attn.qkv.weight]
        addr:0x0000000000006d prog.value_symbol() -> (%115:tensor<[931840], UInt8, CPU>[@visual.blocks.13.attn.proj.weight])[symbol:visual.blocks.13.attn.proj.weight]
        addr:0x0000000000006e prog.value_symbol() -> (%285:tensor<[1280], Float32, CPU>[@visual.blocks.13.norm2.weight])[symbol:visual.blocks.13.norm2.weight]
        addr:0x0000000000006f prog.value_symbol() -> (%286:tensor<[1280], Float32, CPU>[@visual.blocks.13.norm2.bias])[symbol:visual.blocks.13.norm2.bias]
        addr:0x00000000000070 prog.value_symbol() -> (%493:tensor<[3727360], UInt8, CPU>[@visual.blocks.13.mlp.fc1.weight])[symbol:visual.blocks.13.mlp.fc1.weight]
        addr:0x00000000000071 prog.value_symbol() -> (%290:tensor<[3696640], UInt8, CPU>[@visual.blocks.13.mlp.fc2.weight])[symbol:visual.blocks.13.mlp.fc2.weight]
        addr:0x00000000000072 prog.value_symbol() -> (%497:tensor<[1280], Float32, CPU>[@visual.blocks.14.norm1.weight])[symbol:visual.blocks.14.norm1.weight]
        addr:0x00000000000073 prog.value_symbol() -> (%288:tensor<[1280], Float32, CPU>[@visual.blocks.14.norm1.bias])[symbol:visual.blocks.14.norm1.bias]
        addr:0x00000000000074 prog.value_symbol() -> (%110:tensor<[2795520], UInt8, CPU>[@visual.blocks.14.attn.qkv.weight])[symbol:visual.blocks.14.attn.qkv.weight]
        addr:0x00000000000075 prog.value_symbol() -> (%282:tensor<[931840], UInt8, CPU>[@visual.blocks.14.attn.proj.weight])[symbol:visual.blocks.14.attn.proj.weight]
        addr:0x00000000000076 prog.value_symbol() -> (%280:tensor<[1280], Float32, CPU>[@visual.blocks.14.norm2.weight])[symbol:visual.blocks.14.norm2.weight]
        addr:0x00000000000077 prog.value_symbol() -> (%281:tensor<[1280], Float32, CPU>[@visual.blocks.14.norm2.bias])[symbol:visual.blocks.14.norm2.bias]
        addr:0x00000000000078 prog.value_symbol() -> (%80:tensor<[3727360], UInt8, CPU>[@visual.blocks.14.mlp.fc1.weight])[symbol:visual.blocks.14.mlp.fc1.weight]
        addr:0x00000000000079 prog.value_symbol() -> (%73:tensor<[3696640], UInt8, CPU>[@visual.blocks.14.mlp.fc2.weight])[symbol:visual.blocks.14.mlp.fc2.weight]
        addr:0x0000000000007a prog.value_symbol() -> (%274:tensor<[1280], Float32, CPU>[@visual.blocks.15.norm1.weight])[symbol:visual.blocks.15.norm1.weight]
        addr:0x0000000000007b prog.value_symbol() -> (%275:tensor<[1280], Float32, CPU>[@visual.blocks.15.norm1.bias])[symbol:visual.blocks.15.norm1.bias]
        addr:0x0000000000007c prog.value_symbol() -> (%276:tensor<[2795520], UInt8, CPU>[@visual.blocks.15.attn.qkv.weight])[symbol:visual.blocks.15.attn.qkv.weight]
        addr:0x0000000000007d prog.value_symbol() -> (%547:tensor<[931840], UInt8, CPU>[@visual.blocks.15.attn.proj.weight])[symbol:visual.blocks.15.attn.proj.weight]
        addr:0x0000000000007e prog.value_symbol() -> (%496:tensor<[1280], Float32, CPU>[@visual.blocks.15.norm2.weight])[symbol:visual.blocks.15.norm2.weight]
        addr:0x0000000000007f prog.value_symbol() -> (%348:tensor<[1280], Float32, CPU>[@visual.blocks.15.norm2.bias])[symbol:visual.blocks.15.norm2.bias]
        addr:0x00000000000080 prog.value_symbol() -> (%86:tensor<[3727360], UInt8, CPU>[@visual.blocks.15.mlp.fc1.weight])[symbol:visual.blocks.15.mlp.fc1.weight]
        addr:0x00000000000081 prog.value_symbol() -> (%314:tensor<[3696640], UInt8, CPU>[@visual.blocks.15.mlp.fc2.weight])[symbol:visual.blocks.15.mlp.fc2.weight]
        addr:0x00000000000082 prog.value_symbol() -> (%271:tensor<[1280], Float32, CPU>[@visual.blocks.16.norm1.weight])[symbol:visual.blocks.16.norm1.weight]
        addr:0x00000000000083 prog.value_symbol() -> (%272:tensor<[1280], Float32, CPU>[@visual.blocks.16.norm1.bias])[symbol:visual.blocks.16.norm1.bias]
        addr:0x00000000000084 prog.value_symbol() -> (%568:tensor<[2795520], UInt8, CPU>[@visual.blocks.16.attn.qkv.weight])[symbol:visual.blocks.16.attn.qkv.weight]
        addr:0x00000000000085 prog.value_symbol() -> (%273:tensor<[931840], UInt8, CPU>[@visual.blocks.16.attn.proj.weight])[symbol:visual.blocks.16.attn.proj.weight]
        addr:0x00000000000086 prog.value_symbol() -> (%270:tensor<[1280], Float32, CPU>[@visual.blocks.16.norm2.weight])[symbol:visual.blocks.16.norm2.weight]
        addr:0x00000000000087 prog.value_symbol() -> (%284:tensor<[1280], Float32, CPU>[@visual.blocks.16.norm2.bias])[symbol:visual.blocks.16.norm2.bias]
        addr:0x00000000000088 prog.value_symbol() -> (%93:tensor<[3727360], UInt8, CPU>[@visual.blocks.16.mlp.fc1.weight])[symbol:visual.blocks.16.mlp.fc1.weight]
        addr:0x00000000000089 prog.value_symbol() -> (%500:tensor<[3696640], UInt8, CPU>[@visual.blocks.16.mlp.fc2.weight])[symbol:visual.blocks.16.mlp.fc2.weight]
        addr:0x0000000000008a prog.value_symbol() -> (%266:tensor<[1280], Float32, CPU>[@visual.blocks.17.norm1.weight])[symbol:visual.blocks.17.norm1.weight]
        addr:0x0000000000008b prog.value_symbol() -> (%267:tensor<[1280], Float32, CPU>[@visual.blocks.17.norm1.bias])[symbol:visual.blocks.17.norm1.bias]
        addr:0x0000000000008c prog.value_symbol() -> (%100:tensor<[2795520], UInt8, CPU>[@visual.blocks.17.attn.qkv.weight])[symbol:visual.blocks.17.attn.qkv.weight]
        addr:0x0000000000008d prog.value_symbol() -> (%422:tensor<[931840], UInt8, CPU>[@visual.blocks.17.attn.proj.weight])[symbol:visual.blocks.17.attn.proj.weight]
        addr:0x0000000000008e prog.value_symbol() -> (%556:tensor<[1280], Float32, CPU>[@visual.blocks.17.norm2.weight])[symbol:visual.blocks.17.norm2.weight]
        addr:0x0000000000008f prog.value_symbol() -> (%503:tensor<[1280], Float32, CPU>[@visual.blocks.17.norm2.bias])[symbol:visual.blocks.17.norm2.bias]
        addr:0x00000000000090 prog.value_symbol() -> (%268:tensor<[3727360], UInt8, CPU>[@visual.blocks.17.mlp.fc1.weight])[symbol:visual.blocks.17.mlp.fc1.weight]
        addr:0x00000000000091 prog.value_symbol() -> (%69:tensor<[3696640], UInt8, CPU>[@visual.blocks.17.mlp.fc2.weight])[symbol:visual.blocks.17.mlp.fc2.weight]
        addr:0x00000000000092 prog.value_symbol() -> (%265:tensor<[1280], Float32, CPU>[@visual.blocks.18.norm1.weight])[symbol:visual.blocks.18.norm1.weight]
        addr:0x00000000000093 prog.value_symbol() -> (%479:tensor<[1280], Float32, CPU>[@visual.blocks.18.norm1.bias])[symbol:visual.blocks.18.norm1.bias]
        addr:0x00000000000094 prog.value_symbol() -> (%352:tensor<[2795520], UInt8, CPU>[@visual.blocks.18.attn.qkv.weight])[symbol:visual.blocks.18.attn.qkv.weight]
        addr:0x00000000000095 prog.value_symbol() -> (%384:tensor<[931840], UInt8, CPU>[@visual.blocks.18.attn.proj.weight])[symbol:visual.blocks.18.attn.proj.weight]
        addr:0x00000000000096 prog.value_symbol() -> (%264:tensor<[1280], Float32, CPU>[@visual.blocks.18.norm2.weight])[symbol:visual.blocks.18.norm2.weight]
        addr:0x00000000000097 prog.value_symbol() -> (%473:tensor<[1280], Float32, CPU>[@visual.blocks.18.norm2.bias])[symbol:visual.blocks.18.norm2.bias]
        addr:0x00000000000098 prog.value_symbol() -> (%87:tensor<[3727360], UInt8, CPU>[@visual.blocks.18.mlp.fc1.weight])[symbol:visual.blocks.18.mlp.fc1.weight]
        addr:0x00000000000099 prog.value_symbol() -> (%401:tensor<[3696640], UInt8, CPU>[@visual.blocks.18.mlp.fc2.weight])[symbol:visual.blocks.18.mlp.fc2.weight]
        addr:0x0000000000009a prog.value_symbol() -> (%262:tensor<[1280], Float32, CPU>[@visual.blocks.19.norm1.weight])[symbol:visual.blocks.19.norm1.weight]
        addr:0x0000000000009b prog.value_symbol() -> (%407:tensor<[1280], Float32, CPU>[@visual.blocks.19.norm1.bias])[symbol:visual.blocks.19.norm1.bias]
        addr:0x0000000000009c prog.value_symbol() -> (%98:tensor<[2795520], UInt8, CPU>[@visual.blocks.19.attn.qkv.weight])[symbol:visual.blocks.19.attn.qkv.weight]
        addr:0x0000000000009d prog.value_symbol() -> (%113:tensor<[931840], UInt8, CPU>[@visual.blocks.19.attn.proj.weight])[symbol:visual.blocks.19.attn.proj.weight]
        addr:0x0000000000009e prog.value_symbol() -> (%347:tensor<[1280], Float32, CPU>[@visual.blocks.19.norm2.weight])[symbol:visual.blocks.19.norm2.weight]
        addr:0x0000000000009f prog.value_symbol() -> (%259:tensor<[1280], Float32, CPU>[@visual.blocks.19.norm2.bias])[symbol:visual.blocks.19.norm2.bias]
        addr:0x000000000000a0 prog.value_symbol() -> (%261:tensor<[3727360], UInt8, CPU>[@visual.blocks.19.mlp.fc1.weight])[symbol:visual.blocks.19.mlp.fc1.weight]
        addr:0x000000000000a1 prog.value_symbol() -> (%260:tensor<[3696640], UInt8, CPU>[@visual.blocks.19.mlp.fc2.weight])[symbol:visual.blocks.19.mlp.fc2.weight]
        addr:0x000000000000a2 prog.value_symbol() -> (%462:tensor<[1280], Float32, CPU>[@visual.blocks.20.norm1.weight])[symbol:visual.blocks.20.norm1.weight]
        addr:0x000000000000a3 prog.value_symbol() -> (%255:tensor<[1280], Float32, CPU>[@visual.blocks.20.norm1.bias])[symbol:visual.blocks.20.norm1.bias]
        addr:0x000000000000a4 prog.value_symbol() -> (%102:tensor<[2795520], UInt8, CPU>[@visual.blocks.20.attn.qkv.weight])[symbol:visual.blocks.20.attn.qkv.weight]
        addr:0x000000000000a5 prog.value_symbol() -> (%468:tensor<[931840], UInt8, CPU>[@visual.blocks.20.attn.proj.weight])[symbol:visual.blocks.20.attn.proj.weight]
        addr:0x000000000000a6 prog.value_symbol() -> (%543:tensor<[1280], Float32, CPU>[@visual.blocks.20.norm2.weight])[symbol:visual.blocks.20.norm2.weight]
        addr:0x000000000000a7 prog.value_symbol() -> (%508:tensor<[1280], Float32, CPU>[@visual.blocks.20.norm2.bias])[symbol:visual.blocks.20.norm2.bias]
        addr:0x000000000000a8 prog.value_symbol() -> (%340:tensor<[3727360], UInt8, CPU>[@visual.blocks.20.mlp.fc1.weight])[symbol:visual.blocks.20.mlp.fc1.weight]
        addr:0x000000000000a9 prog.value_symbol() -> (%397:tensor<[3696640], UInt8, CPU>[@visual.blocks.20.mlp.fc2.weight])[symbol:visual.blocks.20.mlp.fc2.weight]
        addr:0x000000000000aa prog.value_symbol() -> (%250:tensor<[1280], Float32, CPU>[@visual.blocks.21.norm1.weight])[symbol:visual.blocks.21.norm1.weight]
        addr:0x000000000000ab prog.value_symbol() -> (%515:tensor<[1280], Float32, CPU>[@visual.blocks.21.norm1.bias])[symbol:visual.blocks.21.norm1.bias]
        addr:0x000000000000ac prog.value_symbol() -> (%254:tensor<[2795520], UInt8, CPU>[@visual.blocks.21.attn.qkv.weight])[symbol:visual.blocks.21.attn.qkv.weight]
        addr:0x000000000000ad prog.value_symbol() -> (%114:tensor<[931840], UInt8, CPU>[@visual.blocks.21.attn.proj.weight])[symbol:visual.blocks.21.attn.proj.weight]
        addr:0x000000000000ae prog.value_symbol() -> (%247:tensor<[1280], Float32, CPU>[@visual.blocks.21.norm2.weight])[symbol:visual.blocks.21.norm2.weight]
        addr:0x000000000000af prog.value_symbol() -> (%248:tensor<[1280], Float32, CPU>[@visual.blocks.21.norm2.bias])[symbol:visual.blocks.21.norm2.bias]
        addr:0x000000000000b0 prog.value_symbol() -> (%95:tensor<[3727360], UInt8, CPU>[@visual.blocks.21.mlp.fc1.weight])[symbol:visual.blocks.21.mlp.fc1.weight]
        addr:0x000000000000b1 prog.value_symbol() -> (%251:tensor<[3696640], UInt8, CPU>[@visual.blocks.21.mlp.fc2.weight])[symbol:visual.blocks.21.mlp.fc2.weight]
        addr:0x000000000000b2 prog.value_symbol() -> (%277:tensor<[1280], Float32, CPU>[@visual.blocks.22.norm1.weight])[symbol:visual.blocks.22.norm1.weight]
        addr:0x000000000000b3 prog.value_symbol() -> (%246:tensor<[1280], Float32, CPU>[@visual.blocks.22.norm1.bias])[symbol:visual.blocks.22.norm1.bias]
        addr:0x000000000000b4 prog.value_symbol() -> (%252:tensor<[2795520], UInt8, CPU>[@visual.blocks.22.attn.qkv.weight])[symbol:visual.blocks.22.attn.qkv.weight]
        addr:0x000000000000b5 prog.value_symbol() -> (%376:tensor<[931840], UInt8, CPU>[@visual.blocks.22.attn.proj.weight])[symbol:visual.blocks.22.attn.proj.weight]
        addr:0x000000000000b6 prog.value_symbol() -> (%244:tensor<[1280], Float32, CPU>[@visual.blocks.22.norm2.weight])[symbol:visual.blocks.22.norm2.weight]
        addr:0x000000000000b7 prog.value_symbol() -> (%245:tensor<[1280], Float32, CPU>[@visual.blocks.22.norm2.bias])[symbol:visual.blocks.22.norm2.bias]
        addr:0x000000000000b8 prog.value_symbol() -> (%83:tensor<[3727360], UInt8, CPU>[@visual.blocks.22.mlp.fc1.weight])[symbol:visual.blocks.22.mlp.fc1.weight]
        addr:0x000000000000b9 prog.value_symbol() -> (%64:tensor<[3696640], UInt8, CPU>[@visual.blocks.22.mlp.fc2.weight])[symbol:visual.blocks.22.mlp.fc2.weight]
        addr:0x000000000000ba prog.value_symbol() -> (%511:tensor<[1280], Float32, CPU>[@visual.blocks.23.norm1.weight])[symbol:visual.blocks.23.norm1.weight]
        addr:0x000000000000bb prog.value_symbol() -> (%383:tensor<[1280], Float32, CPU>[@visual.blocks.23.norm1.bias])[symbol:visual.blocks.23.norm1.bias]
        addr:0x000000000000bc prog.value_symbol() -> (%242:tensor<[2795520], UInt8, CPU>[@visual.blocks.23.attn.qkv.weight])[symbol:visual.blocks.23.attn.qkv.weight]
        addr:0x000000000000bd prog.value_symbol() -> (%121:tensor<[931840], UInt8, CPU>[@visual.blocks.23.attn.proj.weight])[symbol:visual.blocks.23.attn.proj.weight]
        addr:0x000000000000be prog.value_symbol() -> (%392:tensor<[1280], Float32, CPU>[@visual.blocks.23.norm2.weight])[symbol:visual.blocks.23.norm2.weight]
        addr:0x000000000000bf prog.value_symbol() -> (%394:tensor<[1280], Float32, CPU>[@visual.blocks.23.norm2.bias])[symbol:visual.blocks.23.norm2.bias]
        addr:0x000000000000c0 prog.value_symbol() -> (%84:tensor<[3727360], UInt8, CPU>[@visual.blocks.23.mlp.fc1.weight])[symbol:visual.blocks.23.mlp.fc1.weight]
        addr:0x000000000000c1 prog.value_symbol() -> (%66:tensor<[3696640], UInt8, CPU>[@visual.blocks.23.mlp.fc2.weight])[symbol:visual.blocks.23.mlp.fc2.weight]
        addr:0x000000000000c2 prog.value_symbol() -> (%240:tensor<[1280], Float32, CPU>[@visual.blocks.24.norm1.weight])[symbol:visual.blocks.24.norm1.weight]
        addr:0x000000000000c3 prog.value_symbol() -> (%241:tensor<[1280], Float32, CPU>[@visual.blocks.24.norm1.bias])[symbol:visual.blocks.24.norm1.bias]
        addr:0x000000000000c4 prog.value_symbol() -> (%104:tensor<[2795520], UInt8, CPU>[@visual.blocks.24.attn.qkv.weight])[symbol:visual.blocks.24.attn.qkv.weight]
        addr:0x000000000000c5 prog.value_symbol() -> (%119:tensor<[931840], UInt8, CPU>[@visual.blocks.24.attn.proj.weight])[symbol:visual.blocks.24.attn.proj.weight]
        addr:0x000000000000c6 prog.value_symbol() -> (%238:tensor<[1280], Float32, CPU>[@visual.blocks.24.norm2.weight])[symbol:visual.blocks.24.norm2.weight]
        addr:0x000000000000c7 prog.value_symbol() -> (%239:tensor<[1280], Float32, CPU>[@visual.blocks.24.norm2.bias])[symbol:visual.blocks.24.norm2.bias]
        addr:0x000000000000c8 prog.value_symbol() -> (%563:tensor<[3727360], UInt8, CPU>[@visual.blocks.24.mlp.fc1.weight])[symbol:visual.blocks.24.mlp.fc1.weight]
        addr:0x000000000000c9 prog.value_symbol() -> (%70:tensor<[3696640], UInt8, CPU>[@visual.blocks.24.mlp.fc2.weight])[symbol:visual.blocks.24.mlp.fc2.weight]
        addr:0x000000000000ca prog.value_symbol() -> (%234:tensor<[1280], Float32, CPU>[@visual.blocks.25.norm1.weight])[symbol:visual.blocks.25.norm1.weight]
        addr:0x000000000000cb prog.value_symbol() -> (%235:tensor<[1280], Float32, CPU>[@visual.blocks.25.norm1.bias])[symbol:visual.blocks.25.norm1.bias]
        addr:0x000000000000cc prog.value_symbol() -> (%97:tensor<[2795520], UInt8, CPU>[@visual.blocks.25.attn.qkv.weight])[symbol:visual.blocks.25.attn.qkv.weight]
        addr:0x000000000000cd prog.value_symbol() -> (%353:tensor<[931840], UInt8, CPU>[@visual.blocks.25.attn.proj.weight])[symbol:visual.blocks.25.attn.proj.weight]
        addr:0x000000000000ce prog.value_symbol() -> (%232:tensor<[1280], Float32, CPU>[@visual.blocks.25.norm2.weight])[symbol:visual.blocks.25.norm2.weight]
        addr:0x000000000000cf prog.value_symbol() -> (%233:tensor<[1280], Float32, CPU>[@visual.blocks.25.norm2.bias])[symbol:visual.blocks.25.norm2.bias]
        addr:0x000000000000d0 prog.value_symbol() -> (%236:tensor<[3727360], UInt8, CPU>[@visual.blocks.25.mlp.fc1.weight])[symbol:visual.blocks.25.mlp.fc1.weight]
        addr:0x000000000000d1 prog.value_symbol() -> (%67:tensor<[3696640], UInt8, CPU>[@visual.blocks.25.mlp.fc2.weight])[symbol:visual.blocks.25.mlp.fc2.weight]
        addr:0x000000000000d2 prog.value_symbol() -> (%231:tensor<[1280], Float32, CPU>[@visual.blocks.26.norm1.weight])[symbol:visual.blocks.26.norm1.weight]
        addr:0x000000000000d3 prog.value_symbol() -> (%256:tensor<[1280], Float32, CPU>[@visual.blocks.26.norm1.bias])[symbol:visual.blocks.26.norm1.bias]
        addr:0x000000000000d4 prog.value_symbol() -> (%106:tensor<[2795520], UInt8, CPU>[@visual.blocks.26.attn.qkv.weight])[symbol:visual.blocks.26.attn.qkv.weight]
        addr:0x000000000000d5 prog.value_symbol() -> (%116:tensor<[931840], UInt8, CPU>[@visual.blocks.26.attn.proj.weight])[symbol:visual.blocks.26.attn.proj.weight]
        addr:0x000000000000d6 prog.value_symbol() -> (%230:tensor<[1280], Float32, CPU>[@visual.blocks.26.norm2.weight])[symbol:visual.blocks.26.norm2.weight]
        addr:0x000000000000d7 prog.value_symbol() -> (%283:tensor<[1280], Float32, CPU>[@visual.blocks.26.norm2.bias])[symbol:visual.blocks.26.norm2.bias]
        addr:0x000000000000d8 prog.value_symbol() -> (%81:tensor<[3727360], UInt8, CPU>[@visual.blocks.26.mlp.fc1.weight])[symbol:visual.blocks.26.mlp.fc1.weight]
        addr:0x000000000000d9 prog.value_symbol() -> (%63:tensor<[3696640], UInt8, CPU>[@visual.blocks.26.mlp.fc2.weight])[symbol:visual.blocks.26.mlp.fc2.weight]
        addr:0x000000000000da prog.value_symbol() -> (%291:tensor<[1280], Float32, CPU>[@visual.blocks.27.norm1.weight])[symbol:visual.blocks.27.norm1.weight]
        addr:0x000000000000db prog.value_symbol() -> (%292:tensor<[1280], Float32, CPU>[@visual.blocks.27.norm1.bias])[symbol:visual.blocks.27.norm1.bias]
        addr:0x000000000000dc prog.value_symbol() -> (%107:tensor<[2795520], UInt8, CPU>[@visual.blocks.27.attn.qkv.weight])[symbol:visual.blocks.27.attn.qkv.weight]
        addr:0x000000000000dd prog.value_symbol() -> (%448:tensor<[931840], UInt8, CPU>[@visual.blocks.27.attn.proj.weight])[symbol:visual.blocks.27.attn.proj.weight]
        addr:0x000000000000de prog.value_symbol() -> (%228:tensor<[1280], Float32, CPU>[@visual.blocks.27.norm2.weight])[symbol:visual.blocks.27.norm2.weight]
        addr:0x000000000000df prog.value_symbol() -> (%229:tensor<[1280], Float32, CPU>[@visual.blocks.27.norm2.bias])[symbol:visual.blocks.27.norm2.bias]
        addr:0x000000000000e0 prog.value_symbol() -> (%85:tensor<[3727360], UInt8, CPU>[@visual.blocks.27.mlp.fc1.weight])[symbol:visual.blocks.27.mlp.fc1.weight]
        addr:0x000000000000e1 prog.value_symbol() -> (%60:tensor<[3696640], UInt8, CPU>[@visual.blocks.27.mlp.fc2.weight])[symbol:visual.blocks.27.mlp.fc2.weight]
        addr:0x000000000000e2 prog.value_symbol() -> (%225:tensor<[1280], Float32, CPU>[@visual.blocks.28.norm1.weight])[symbol:visual.blocks.28.norm1.weight]
        addr:0x000000000000e3 prog.value_symbol() -> (%226:tensor<[1280], Float32, CPU>[@visual.blocks.28.norm1.bias])[symbol:visual.blocks.28.norm1.bias]
        addr:0x000000000000e4 prog.value_symbol() -> (%552:tensor<[2795520], UInt8, CPU>[@visual.blocks.28.attn.qkv.weight])[symbol:visual.blocks.28.attn.qkv.weight]
        addr:0x000000000000e5 prog.value_symbol() -> (%120:tensor<[931840], UInt8, CPU>[@visual.blocks.28.attn.proj.weight])[symbol:visual.blocks.28.attn.proj.weight]
        addr:0x000000000000e6 prog.value_symbol() -> (%224:tensor<[1280], Float32, CPU>[@visual.blocks.28.norm2.weight])[symbol:visual.blocks.28.norm2.weight]
        addr:0x000000000000e7 prog.value_symbol() -> (%472:tensor<[1280], Float32, CPU>[@visual.blocks.28.norm2.bias])[symbol:visual.blocks.28.norm2.bias]
        addr:0x000000000000e8 prog.value_symbol() -> (%88:tensor<[3727360], UInt8, CPU>[@visual.blocks.28.mlp.fc1.weight])[symbol:visual.blocks.28.mlp.fc1.weight]
        addr:0x000000000000e9 prog.value_symbol() -> (%365:tensor<[3696640], UInt8, CPU>[@visual.blocks.28.mlp.fc2.weight])[symbol:visual.blocks.28.mlp.fc2.weight]
        addr:0x000000000000ea prog.value_symbol() -> (%243:tensor<[1280], Float32, CPU>[@visual.blocks.29.norm1.weight])[symbol:visual.blocks.29.norm1.weight]
        addr:0x000000000000eb prog.value_symbol() -> (%535:tensor<[1280], Float32, CPU>[@visual.blocks.29.norm1.bias])[symbol:visual.blocks.29.norm1.bias]
        addr:0x000000000000ec prog.value_symbol() -> (%441:tensor<[2795520], UInt8, CPU>[@visual.blocks.29.attn.qkv.weight])[symbol:visual.blocks.29.attn.qkv.weight]
        addr:0x000000000000ed prog.value_symbol() -> (%403:tensor<[931840], UInt8, CPU>[@visual.blocks.29.attn.proj.weight])[symbol:visual.blocks.29.attn.proj.weight]
        addr:0x000000000000ee prog.value_symbol() -> (%223:tensor<[1280], Float32, CPU>[@visual.blocks.29.norm2.weight])[symbol:visual.blocks.29.norm2.weight]
        addr:0x000000000000ef prog.value_symbol() -> (%279:tensor<[1280], Float32, CPU>[@visual.blocks.29.norm2.bias])[symbol:visual.blocks.29.norm2.bias]
        addr:0x000000000000f0 prog.value_symbol() -> (%91:tensor<[3727360], UInt8, CPU>[@visual.blocks.29.mlp.fc1.weight])[symbol:visual.blocks.29.mlp.fc1.weight]
        addr:0x000000000000f1 prog.value_symbol() -> (%249:tensor<[3696640], UInt8, CPU>[@visual.blocks.29.mlp.fc2.weight])[symbol:visual.blocks.29.mlp.fc2.weight]
        addr:0x000000000000f2 prog.value_symbol() -> (%220:tensor<[1280], Float32, CPU>[@visual.blocks.30.norm1.weight])[symbol:visual.blocks.30.norm1.weight]
        addr:0x000000000000f3 prog.value_symbol() -> (%507:tensor<[1280], Float32, CPU>[@visual.blocks.30.norm1.bias])[symbol:visual.blocks.30.norm1.bias]
        addr:0x000000000000f4 prog.value_symbol() -> (%109:tensor<[2795520], UInt8, CPU>[@visual.blocks.30.attn.qkv.weight])[symbol:visual.blocks.30.attn.qkv.weight]
        addr:0x000000000000f5 prog.value_symbol() -> (%278:tensor<[931840], UInt8, CPU>[@visual.blocks.30.attn.proj.weight])[symbol:visual.blocks.30.attn.proj.weight]
        addr:0x000000000000f6 prog.value_symbol() -> (%218:tensor<[1280], Float32, CPU>[@visual.blocks.30.norm2.weight])[symbol:visual.blocks.30.norm2.weight]
        addr:0x000000000000f7 prog.value_symbol() -> (%219:tensor<[1280], Float32, CPU>[@visual.blocks.30.norm2.bias])[symbol:visual.blocks.30.norm2.bias]
        addr:0x000000000000f8 prog.value_symbol() -> (%443:tensor<[3727360], UInt8, CPU>[@visual.blocks.30.mlp.fc1.weight])[symbol:visual.blocks.30.mlp.fc1.weight]
        addr:0x000000000000f9 prog.value_symbol() -> (%65:tensor<[3696640], UInt8, CPU>[@visual.blocks.30.mlp.fc2.weight])[symbol:visual.blocks.30.mlp.fc2.weight]
        addr:0x000000000000fa prog.value_symbol() -> (%551:tensor<[1280], Float32, CPU>[@visual.blocks.31.norm1.weight])[symbol:visual.blocks.31.norm1.weight]
        addr:0x000000000000fb prog.value_symbol() -> (%217:tensor<[1280], Float32, CPU>[@visual.blocks.31.norm1.bias])[symbol:visual.blocks.31.norm1.bias]
        addr:0x000000000000fc prog.value_symbol() -> (%99:tensor<[2795520], UInt8, CPU>[@visual.blocks.31.attn.qkv.weight])[symbol:visual.blocks.31.attn.qkv.weight]
        addr:0x000000000000fd prog.value_symbol() -> (%122:tensor<[931840], UInt8, CPU>[@visual.blocks.31.attn.proj.weight])[symbol:visual.blocks.31.attn.proj.weight]
        addr:0x000000000000fe prog.value_symbol() -> (%216:tensor<[1280], Float32, CPU>[@visual.blocks.31.norm2.weight])[symbol:visual.blocks.31.norm2.weight]
        addr:0x000000000000ff prog.value_symbol() -> (%567:tensor<[1280], Float32, CPU>[@visual.blocks.31.norm2.bias])[symbol:visual.blocks.31.norm2.bias]
        addr:0x00000000000100 prog.value_symbol() -> (%263:tensor<[3727360], UInt8, CPU>[@visual.blocks.31.mlp.fc1.weight])[symbol:visual.blocks.31.mlp.fc1.weight]
        addr:0x00000000000101 prog.value_symbol() -> (%306:tensor<[3696640], UInt8, CPU>[@visual.blocks.31.mlp.fc2.weight])[symbol:visual.blocks.31.mlp.fc2.weight]
        addr:0x00000000000102 prog.value_symbol() -> (%202:tensor<[1280], Float32, CPU>[@visual.merger.ln_q.weight])[symbol:visual.merger.ln_q.weight]
        addr:0x00000000000103 prog.value_symbol() -> (%465:tensor<[1280], Float32, CPU>[@visual.merger.ln_q.bias])[symbol:visual.merger.ln_q.bias]
        addr:0x00000000000104 prog.value_symbol() -> (%490:tensor<[14786560], UInt8, CPU>[@visual.merger.mlp.0.weight])[symbol:visual.merger.mlp.0.weight]
        addr:0x00000000000105 prog.value_symbol() -> (%253:tensor<[4435968], UInt8, CPU>[@visual.merger.mlp.2.weight])[symbol:visual.merger.mlp.2.weight]
        addr:0x00000000000106 prog.value_symbol() -> (%138:tensor<[1339392], UInt8, CPU>[@model.layers.0.self_attn.q_proj.weight])[symbol:model.layers.0.self_attn.q_proj.weight]
        addr:0x00000000000107 prog.value_symbol() -> (%574:tensor<[223232], UInt8, CPU>[@model.layers.0.self_attn.k_proj.weight])[symbol:model.layers.0.self_attn.k_proj.weight]
        addr:0x00000000000108 prog.value_symbol() -> (%570:tensor<[223232], UInt8, CPU>[@model.layers.0.self_attn.v_proj.weight])[symbol:model.layers.0.self_attn.v_proj.weight]
        addr:0x00000000000109 prog.value_symbol() -> (%536:tensor<[1339392], UInt8, CPU>[@model.layers.0.self_attn.o_proj.weight])[symbol:model.layers.0.self_attn.o_proj.weight]
        addr:0x0000000000010a prog.value_symbol() -> (%566:tensor<[7813120], UInt8, CPU>[@model.layers.0.mlp.gate_proj.weight])[symbol:model.layers.0.mlp.gate_proj.weight]
        addr:0x0000000000010b prog.value_symbol() -> (%170:tensor<[7813120], UInt8, CPU>[@model.layers.0.mlp.up_proj.weight])[symbol:model.layers.0.mlp.up_proj.weight]
        addr:0x0000000000010c prog.value_symbol() -> (%187:tensor<[7753728], UInt8, CPU>[@model.layers.0.mlp.down_proj.weight])[symbol:model.layers.0.mlp.down_proj.weight]
        addr:0x0000000000010d prog.value_symbol() -> (%137:tensor<[1339392], UInt8, CPU>[@model.layers.1.self_attn.q_proj.weight])[symbol:model.layers.1.self_attn.q_proj.weight]
        addr:0x0000000000010e prog.value_symbol() -> (%164:tensor<[223232], UInt8, CPU>[@model.layers.1.self_attn.k_proj.weight])[symbol:model.layers.1.self_attn.k_proj.weight]
        addr:0x0000000000010f prog.value_symbol() -> (%299:tensor<[223232], UInt8, CPU>[@model.layers.1.self_attn.v_proj.weight])[symbol:model.layers.1.self_attn.v_proj.weight]
        addr:0x00000000000110 prog.value_symbol() -> (%144:tensor<[1339392], UInt8, CPU>[@model.layers.1.self_attn.o_proj.weight])[symbol:model.layers.1.self_attn.o_proj.weight]
        addr:0x00000000000111 prog.value_symbol() -> (%506:tensor<[7813120], UInt8, CPU>[@model.layers.1.mlp.gate_proj.weight])[symbol:model.layers.1.mlp.gate_proj.weight]
        addr:0x00000000000112 prog.value_symbol() -> (%399:tensor<[7813120], UInt8, CPU>[@model.layers.1.mlp.up_proj.weight])[symbol:model.layers.1.mlp.up_proj.weight]
        addr:0x00000000000113 prog.value_symbol() -> (%534:tensor<[7753728], UInt8, CPU>[@model.layers.1.mlp.down_proj.weight])[symbol:model.layers.1.mlp.down_proj.weight]
        addr:0x00000000000114 prog.value_symbol() -> (%449:tensor<[1339392], UInt8, CPU>[@model.layers.2.self_attn.q_proj.weight])[symbol:model.layers.2.self_attn.q_proj.weight]
        addr:0x00000000000115 prog.value_symbol() -> (%161:tensor<[223232], UInt8, CPU>[@model.layers.2.self_attn.k_proj.weight])[symbol:model.layers.2.self_attn.k_proj.weight]
        addr:0x00000000000116 prog.value_symbol() -> (%366:tensor<[223232], UInt8, CPU>[@model.layers.2.self_attn.v_proj.weight])[symbol:model.layers.2.self_attn.v_proj.weight]
        addr:0x00000000000117 prog.value_symbol() -> (%147:tensor<[1339392], UInt8, CPU>[@model.layers.2.self_attn.o_proj.weight])[symbol:model.layers.2.self_attn.o_proj.weight]
        addr:0x00000000000118 prog.value_symbol() -> (%440:tensor<[7813120], UInt8, CPU>[@model.layers.2.mlp.gate_proj.weight])[symbol:model.layers.2.mlp.gate_proj.weight]
        addr:0x00000000000119 prog.value_symbol() -> (%436:tensor<[7813120], UInt8, CPU>[@model.layers.2.mlp.up_proj.weight])[symbol:model.layers.2.mlp.up_proj.weight]
        addr:0x0000000000011a prog.value_symbol() -> (%194:tensor<[7753728], UInt8, CPU>[@model.layers.2.mlp.down_proj.weight])[symbol:model.layers.2.mlp.down_proj.weight]
        addr:0x0000000000011b prog.value_symbol() -> (%142:tensor<[1339392], UInt8, CPU>[@model.layers.3.self_attn.q_proj.weight])[symbol:model.layers.3.self_attn.q_proj.weight]
        addr:0x0000000000011c prog.value_symbol() -> (%356:tensor<[223232], UInt8, CPU>[@model.layers.3.self_attn.k_proj.weight])[symbol:model.layers.3.self_attn.k_proj.weight]
        addr:0x0000000000011d prog.value_symbol() -> (%355:tensor<[223232], UInt8, CPU>[@model.layers.3.self_attn.v_proj.weight])[symbol:model.layers.3.self_attn.v_proj.weight]
        addr:0x0000000000011e prog.value_symbol() -> (%148:tensor<[1339392], UInt8, CPU>[@model.layers.3.self_attn.o_proj.weight])[symbol:model.layers.3.self_attn.o_proj.weight]
        addr:0x0000000000011f prog.value_symbol() -> (%378:tensor<[7813120], UInt8, CPU>[@model.layers.3.mlp.gate_proj.weight])[symbol:model.layers.3.mlp.gate_proj.weight]
        addr:0x00000000000120 prog.value_symbol() -> (%173:tensor<[7813120], UInt8, CPU>[@model.layers.3.mlp.up_proj.weight])[symbol:model.layers.3.mlp.up_proj.weight]
        addr:0x00000000000121 prog.value_symbol() -> (%200:tensor<[7753728], UInt8, CPU>[@model.layers.3.mlp.down_proj.weight])[symbol:model.layers.3.mlp.down_proj.weight]
        addr:0x00000000000122 prog.value_symbol() -> (%530:tensor<[1339392], UInt8, CPU>[@model.layers.4.self_attn.q_proj.weight])[symbol:model.layers.4.self_attn.q_proj.weight]
        addr:0x00000000000123 prog.value_symbol() -> (%159:tensor<[223232], UInt8, CPU>[@model.layers.4.self_attn.k_proj.weight])[symbol:model.layers.4.self_attn.k_proj.weight]
        addr:0x00000000000124 prog.value_symbol() -> (%558:tensor<[223232], UInt8, CPU>[@model.layers.4.self_attn.v_proj.weight])[symbol:model.layers.4.self_attn.v_proj.weight]
        addr:0x00000000000125 prog.value_symbol() -> (%428:tensor<[1339392], UInt8, CPU>[@model.layers.4.self_attn.o_proj.weight])[symbol:model.layers.4.self_attn.o_proj.weight]
        addr:0x00000000000126 prog.value_symbol() -> (%354:tensor<[7813120], UInt8, CPU>[@model.layers.4.mlp.gate_proj.weight])[symbol:model.layers.4.mlp.gate_proj.weight]
        addr:0x00000000000127 prog.value_symbol() -> (%174:tensor<[7813120], UInt8, CPU>[@model.layers.4.mlp.up_proj.weight])[symbol:model.layers.4.mlp.up_proj.weight]
        addr:0x00000000000128 prog.value_symbol() -> (%201:tensor<[7753728], UInt8, CPU>[@model.layers.4.mlp.down_proj.weight])[symbol:model.layers.4.mlp.down_proj.weight]
        addr:0x00000000000129 prog.value_symbol() -> (%455:tensor<[1339392], UInt8, CPU>[@model.layers.5.self_attn.q_proj.weight])[symbol:model.layers.5.self_attn.q_proj.weight]
        addr:0x0000000000012a prog.value_symbol() -> (%160:tensor<[223232], UInt8, CPU>[@model.layers.5.self_attn.k_proj.weight])[symbol:model.layers.5.self_attn.k_proj.weight]
        addr:0x0000000000012b prog.value_symbol() -> (%131:tensor<[223232], UInt8, CPU>[@model.layers.5.self_attn.v_proj.weight])[symbol:model.layers.5.self_attn.v_proj.weight]
        addr:0x0000000000012c prog.value_symbol() -> (%398:tensor<[1339392], UInt8, CPU>[@model.layers.5.self_attn.o_proj.weight])[symbol:model.layers.5.self_attn.o_proj.weight]
        addr:0x0000000000012d prog.value_symbol() -> (%502:tensor<[7813120], UInt8, CPU>[@model.layers.5.mlp.gate_proj.weight])[symbol:model.layers.5.mlp.gate_proj.weight]
        addr:0x0000000000012e prog.value_symbol() -> (%168:tensor<[7813120], UInt8, CPU>[@model.layers.5.mlp.up_proj.weight])[symbol:model.layers.5.mlp.up_proj.weight]
        addr:0x0000000000012f prog.value_symbol() -> (%528:tensor<[7753728], UInt8, CPU>[@model.layers.5.mlp.down_proj.weight])[symbol:model.layers.5.mlp.down_proj.weight]
        addr:0x00000000000130 prog.value_symbol() -> (%344:tensor<[1339392], UInt8, CPU>[@model.layers.6.self_attn.q_proj.weight])[symbol:model.layers.6.self_attn.q_proj.weight]
        addr:0x00000000000131 prog.value_symbol() -> (%564:tensor<[223232], UInt8, CPU>[@model.layers.6.self_attn.k_proj.weight])[symbol:model.layers.6.self_attn.k_proj.weight]
        addr:0x00000000000132 prog.value_symbol() -> (%342:tensor<[223232], UInt8, CPU>[@model.layers.6.self_attn.v_proj.weight])[symbol:model.layers.6.self_attn.v_proj.weight]
        addr:0x00000000000133 prog.value_symbol() -> (%153:tensor<[1339392], UInt8, CPU>[@model.layers.6.self_attn.o_proj.weight])[symbol:model.layers.6.self_attn.o_proj.weight]
        addr:0x00000000000134 prog.value_symbol() -> (%185:tensor<[7813120], UInt8, CPU>[@model.layers.6.mlp.gate_proj.weight])[symbol:model.layers.6.mlp.gate_proj.weight]
        addr:0x00000000000135 prog.value_symbol() -> (%332:tensor<[7813120], UInt8, CPU>[@model.layers.6.mlp.up_proj.weight])[symbol:model.layers.6.mlp.up_proj.weight]
        addr:0x00000000000136 prog.value_symbol() -> (%309:tensor<[7753728], UInt8, CPU>[@model.layers.6.mlp.down_proj.weight])[symbol:model.layers.6.mlp.down_proj.weight]
        addr:0x00000000000137 prog.value_symbol() -> (%438:tensor<[1339392], UInt8, CPU>[@model.layers.7.self_attn.q_proj.weight])[symbol:model.layers.7.self_attn.q_proj.weight]
        addr:0x00000000000138 prog.value_symbol() -> (%363:tensor<[223232], UInt8, CPU>[@model.layers.7.self_attn.k_proj.weight])[symbol:model.layers.7.self_attn.k_proj.weight]
        addr:0x00000000000139 prog.value_symbol() -> (%127:tensor<[223232], UInt8, CPU>[@model.layers.7.self_attn.v_proj.weight])[symbol:model.layers.7.self_attn.v_proj.weight]
        addr:0x0000000000013a prog.value_symbol() -> (%145:tensor<[1339392], UInt8, CPU>[@model.layers.7.self_attn.o_proj.weight])[symbol:model.layers.7.self_attn.o_proj.weight]
        addr:0x0000000000013b prog.value_symbol() -> (%184:tensor<[7813120], UInt8, CPU>[@model.layers.7.mlp.gate_proj.weight])[symbol:model.layers.7.mlp.gate_proj.weight]
        addr:0x0000000000013c prog.value_symbol() -> (%165:tensor<[7813120], UInt8, CPU>[@model.layers.7.mlp.up_proj.weight])[symbol:model.layers.7.mlp.up_proj.weight]
        addr:0x0000000000013d prog.value_symbol() -> (%379:tensor<[7753728], UInt8, CPU>[@model.layers.7.mlp.down_proj.weight])[symbol:model.layers.7.mlp.down_proj.weight]
        addr:0x0000000000013e prog.value_symbol() -> (%334:tensor<[1339392], UInt8, CPU>[@model.layers.8.self_attn.q_proj.weight])[symbol:model.layers.8.self_attn.q_proj.weight]
        addr:0x0000000000013f prog.value_symbol() -> (%333:tensor<[223232], UInt8, CPU>[@model.layers.8.self_attn.k_proj.weight])[symbol:model.layers.8.self_attn.k_proj.weight]
        addr:0x00000000000140 prog.value_symbol() -> (%330:tensor<[223232], UInt8, CPU>[@model.layers.8.self_attn.v_proj.weight])[symbol:model.layers.8.self_attn.v_proj.weight]
        addr:0x00000000000141 prog.value_symbol() -> (%154:tensor<[1339392], UInt8, CPU>[@model.layers.8.self_attn.o_proj.weight])[symbol:model.layers.8.self_attn.o_proj.weight]
        addr:0x00000000000142 prog.value_symbol() -> (%335:tensor<[7813120], UInt8, CPU>[@model.layers.8.mlp.gate_proj.weight])[symbol:model.layers.8.mlp.gate_proj.weight]
        addr:0x00000000000143 prog.value_symbol() -> (%167:tensor<[7813120], UInt8, CPU>[@model.layers.8.mlp.up_proj.weight])[symbol:model.layers.8.mlp.up_proj.weight]
        addr:0x00000000000144 prog.value_symbol() -> (%199:tensor<[7753728], UInt8, CPU>[@model.layers.8.mlp.down_proj.weight])[symbol:model.layers.8.mlp.down_proj.weight]
        addr:0x00000000000145 prog.value_symbol() -> (%139:tensor<[1339392], UInt8, CPU>[@model.layers.9.self_attn.q_proj.weight])[symbol:model.layers.9.self_attn.q_proj.weight]
        addr:0x00000000000146 prog.value_symbol() -> (%324:tensor<[223232], UInt8, CPU>[@model.layers.9.self_attn.k_proj.weight])[symbol:model.layers.9.self_attn.k_proj.weight]
        addr:0x00000000000147 prog.value_symbol() -> (%129:tensor<[223232], UInt8, CPU>[@model.layers.9.self_attn.v_proj.weight])[symbol:model.layers.9.self_attn.v_proj.weight]
        addr:0x00000000000148 prog.value_symbol() -> (%149:tensor<[1339392], UInt8, CPU>[@model.layers.9.self_attn.o_proj.weight])[symbol:model.layers.9.self_attn.o_proj.weight]
        addr:0x00000000000149 prog.value_symbol() -> (%183:tensor<[7813120], UInt8, CPU>[@model.layers.9.mlp.gate_proj.weight])[symbol:model.layers.9.mlp.gate_proj.weight]
        addr:0x0000000000014a prog.value_symbol() -> (%406:tensor<[7813120], UInt8, CPU>[@model.layers.9.mlp.up_proj.weight])[symbol:model.layers.9.mlp.up_proj.weight]
        addr:0x0000000000014b prog.value_symbol() -> (%328:tensor<[7753728], UInt8, CPU>[@model.layers.9.mlp.down_proj.weight])[symbol:model.layers.9.mlp.down_proj.weight]
        addr:0x0000000000014c prog.value_symbol() -> (%545:tensor<[1339392], UInt8, CPU>[@model.layers.10.self_attn.q_proj.weight])[symbol:model.layers.10.self_attn.q_proj.weight]
        addr:0x0000000000014d prog.value_symbol() -> (%158:tensor<[223232], UInt8, CPU>[@model.layers.10.self_attn.k_proj.weight])[symbol:model.layers.10.self_attn.k_proj.weight]
        addr:0x0000000000014e prog.value_symbol() -> (%128:tensor<[223232], UInt8, CPU>[@model.layers.10.self_attn.v_proj.weight])[symbol:model.layers.10.self_attn.v_proj.weight]
        addr:0x0000000000014f prog.value_symbol() -> (%546:tensor<[1339392], UInt8, CPU>[@model.layers.10.self_attn.o_proj.weight])[symbol:model.layers.10.self_attn.o_proj.weight]
        addr:0x00000000000150 prog.value_symbol() -> (%175:tensor<[7813120], UInt8, CPU>[@model.layers.10.mlp.gate_proj.weight])[symbol:model.layers.10.mlp.gate_proj.weight]
        addr:0x00000000000151 prog.value_symbol() -> (%555:tensor<[7813120], UInt8, CPU>[@model.layers.10.mlp.up_proj.weight])[symbol:model.layers.10.mlp.up_proj.weight]
        addr:0x00000000000152 prog.value_symbol() -> (%188:tensor<[7753728], UInt8, CPU>[@model.layers.10.mlp.down_proj.weight])[symbol:model.layers.10.mlp.down_proj.weight]
        addr:0x00000000000153 prog.value_symbol() -> (%133:tensor<[1339392], UInt8, CPU>[@model.layers.11.self_attn.q_proj.weight])[symbol:model.layers.11.self_attn.q_proj.weight]
        addr:0x00000000000154 prog.value_symbol() -> (%498:tensor<[223232], UInt8, CPU>[@model.layers.11.self_attn.k_proj.weight])[symbol:model.layers.11.self_attn.k_proj.weight]
        addr:0x00000000000155 prog.value_symbol() -> (%550:tensor<[223232], UInt8, CPU>[@model.layers.11.self_attn.v_proj.weight])[symbol:model.layers.11.self_attn.v_proj.weight]
        addr:0x00000000000156 prog.value_symbol() -> (%143:tensor<[1339392], UInt8, CPU>[@model.layers.11.self_attn.o_proj.weight])[symbol:model.layers.11.self_attn.o_proj.weight]
        addr:0x00000000000157 prog.value_symbol() -> (%531:tensor<[7813120], UInt8, CPU>[@model.layers.11.mlp.gate_proj.weight])[symbol:model.layers.11.mlp.gate_proj.weight]
        addr:0x00000000000158 prog.value_symbol() -> (%169:tensor<[7813120], UInt8, CPU>[@model.layers.11.mlp.up_proj.weight])[symbol:model.layers.11.mlp.up_proj.weight]
        addr:0x00000000000159 prog.value_symbol() -> (%561:tensor<[7753728], UInt8, CPU>[@model.layers.11.mlp.down_proj.weight])[symbol:model.layers.11.mlp.down_proj.weight]
        addr:0x0000000000015a prog.value_symbol() -> (%132:tensor<[1339392], UInt8, CPU>[@model.layers.12.self_attn.q_proj.weight])[symbol:model.layers.12.self_attn.q_proj.weight]
        addr:0x0000000000015b prog.value_symbol() -> (%524:tensor<[223232], UInt8, CPU>[@model.layers.12.self_attn.k_proj.weight])[symbol:model.layers.12.self_attn.k_proj.weight]
        addr:0x0000000000015c prog.value_symbol() -> (%130:tensor<[223232], UInt8, CPU>[@model.layers.12.self_attn.v_proj.weight])[symbol:model.layers.12.self_attn.v_proj.weight]
        addr:0x0000000000015d prog.value_symbol() -> (%156:tensor<[1339392], UInt8, CPU>[@model.layers.12.self_attn.o_proj.weight])[symbol:model.layers.12.self_attn.o_proj.weight]
        addr:0x0000000000015e prog.value_symbol() -> (%182:tensor<[7813120], UInt8, CPU>[@model.layers.12.mlp.gate_proj.weight])[symbol:model.layers.12.mlp.gate_proj.weight]
        addr:0x0000000000015f prog.value_symbol() -> (%510:tensor<[7813120], UInt8, CPU>[@model.layers.12.mlp.up_proj.weight])[symbol:model.layers.12.mlp.up_proj.weight]
        addr:0x00000000000160 prog.value_symbol() -> (%198:tensor<[7753728], UInt8, CPU>[@model.layers.12.mlp.down_proj.weight])[symbol:model.layers.12.mlp.down_proj.weight]
        addr:0x00000000000161 prog.value_symbol() -> (%297:tensor<[1339392], UInt8, CPU>[@model.layers.13.self_attn.q_proj.weight])[symbol:model.layers.13.self_attn.q_proj.weight]
        addr:0x00000000000162 prog.value_symbol() -> (%157:tensor<[223232], UInt8, CPU>[@model.layers.13.self_attn.k_proj.weight])[symbol:model.layers.13.self_attn.k_proj.weight]
        addr:0x00000000000163 prog.value_symbol() -> (%124:tensor<[223232], UInt8, CPU>[@model.layers.13.self_attn.v_proj.weight])[symbol:model.layers.13.self_attn.v_proj.weight]
        addr:0x00000000000164 prog.value_symbol() -> (%487:tensor<[1339392], UInt8, CPU>[@model.layers.13.self_attn.o_proj.weight])[symbol:model.layers.13.self_attn.o_proj.weight]
        addr:0x00000000000165 prog.value_symbol() -> (%179:tensor<[7813120], UInt8, CPU>[@model.layers.13.mlp.gate_proj.weight])[symbol:model.layers.13.mlp.gate_proj.weight]
        addr:0x00000000000166 prog.value_symbol() -> (%423:tensor<[7813120], UInt8, CPU>[@model.layers.13.mlp.up_proj.weight])[symbol:model.layers.13.mlp.up_proj.weight]
        addr:0x00000000000167 prog.value_symbol() -> (%197:tensor<[7753728], UInt8, CPU>[@model.layers.13.mlp.down_proj.weight])[symbol:model.layers.13.mlp.down_proj.weight]
        addr:0x00000000000168 prog.value_symbol() -> (%512:tensor<[1339392], UInt8, CPU>[@model.layers.14.self_attn.q_proj.weight])[symbol:model.layers.14.self_attn.q_proj.weight]
        addr:0x00000000000169 prog.value_symbol() -> (%495:tensor<[223232], UInt8, CPU>[@model.layers.14.self_attn.k_proj.weight])[symbol:model.layers.14.self_attn.k_proj.weight]
        addr:0x0000000000016a prog.value_symbol() -> (%434:tensor<[223232], UInt8, CPU>[@model.layers.14.self_attn.v_proj.weight])[symbol:model.layers.14.self_attn.v_proj.weight]
        addr:0x0000000000016b prog.value_symbol() -> (%152:tensor<[1339392], UInt8, CPU>[@model.layers.14.self_attn.o_proj.weight])[symbol:model.layers.14.self_attn.o_proj.weight]
        addr:0x0000000000016c prog.value_symbol() -> (%437:tensor<[7813120], UInt8, CPU>[@model.layers.14.mlp.gate_proj.weight])[symbol:model.layers.14.mlp.gate_proj.weight]
        addr:0x0000000000016d prog.value_symbol() -> (%359:tensor<[7813120], UInt8, CPU>[@model.layers.14.mlp.up_proj.weight])[symbol:model.layers.14.mlp.up_proj.weight]
        addr:0x0000000000016e prog.value_symbol() -> (%192:tensor<[7753728], UInt8, CPU>[@model.layers.14.mlp.down_proj.weight])[symbol:model.layers.14.mlp.down_proj.weight]
        addr:0x0000000000016f prog.value_symbol() -> (%484:tensor<[1339392], UInt8, CPU>[@model.layers.15.self_attn.q_proj.weight])[symbol:model.layers.15.self_attn.q_proj.weight]
        addr:0x00000000000170 prog.value_symbol() -> (%389:tensor<[223232], UInt8, CPU>[@model.layers.15.self_attn.k_proj.weight])[symbol:model.layers.15.self_attn.k_proj.weight]
        addr:0x00000000000171 prog.value_symbol() -> (%481:tensor<[223232], UInt8, CPU>[@model.layers.15.self_attn.v_proj.weight])[symbol:model.layers.15.self_attn.v_proj.weight]
        addr:0x00000000000172 prog.value_symbol() -> (%146:tensor<[1339392], UInt8, CPU>[@model.layers.15.self_attn.o_proj.weight])[symbol:model.layers.15.self_attn.o_proj.weight]
        addr:0x00000000000173 prog.value_symbol() -> (%343:tensor<[7813120], UInt8, CPU>[@model.layers.15.mlp.gate_proj.weight])[symbol:model.layers.15.mlp.gate_proj.weight]
        addr:0x00000000000174 prog.value_symbol() -> (%414:tensor<[7813120], UInt8, CPU>[@model.layers.15.mlp.up_proj.weight])[symbol:model.layers.15.mlp.up_proj.weight]
        addr:0x00000000000175 prog.value_symbol() -> (%417:tensor<[7753728], UInt8, CPU>[@model.layers.15.mlp.down_proj.weight])[symbol:model.layers.15.mlp.down_proj.weight]
        addr:0x00000000000176 prog.value_symbol() -> (%134:tensor<[1339392], UInt8, CPU>[@model.layers.16.self_attn.q_proj.weight])[symbol:model.layers.16.self_attn.q_proj.weight]
        addr:0x00000000000177 prog.value_symbol() -> (%554:tensor<[223232], UInt8, CPU>[@model.layers.16.self_attn.k_proj.weight])[symbol:model.layers.16.self_attn.k_proj.weight]
        addr:0x00000000000178 prog.value_symbol() -> (%474:tensor<[223232], UInt8, CPU>[@model.layers.16.self_attn.v_proj.weight])[symbol:model.layers.16.self_attn.v_proj.weight]
        addr:0x00000000000179 prog.value_symbol() -> (%405:tensor<[1339392], UInt8, CPU>[@model.layers.16.self_attn.o_proj.weight])[symbol:model.layers.16.self_attn.o_proj.weight]
        addr:0x0000000000017a prog.value_symbol() -> (%463:tensor<[7813120], UInt8, CPU>[@model.layers.16.mlp.gate_proj.weight])[symbol:model.layers.16.mlp.gate_proj.weight]
        addr:0x0000000000017b prog.value_symbol() -> (%171:tensor<[7813120], UInt8, CPU>[@model.layers.16.mlp.up_proj.weight])[symbol:model.layers.16.mlp.up_proj.weight]
        addr:0x0000000000017c prog.value_symbol() -> (%432:tensor<[7753728], UInt8, CPU>[@model.layers.16.mlp.down_proj.weight])[symbol:model.layers.16.mlp.down_proj.weight]
        addr:0x0000000000017d prog.value_symbol() -> (%135:tensor<[1339392], UInt8, CPU>[@model.layers.17.self_attn.q_proj.weight])[symbol:model.layers.17.self_attn.q_proj.weight]
        addr:0x0000000000017e prog.value_symbol() -> (%345:tensor<[223232], UInt8, CPU>[@model.layers.17.self_attn.k_proj.weight])[symbol:model.layers.17.self_attn.k_proj.weight]
        addr:0x0000000000017f prog.value_symbol() -> (%412:tensor<[223232], UInt8, CPU>[@model.layers.17.self_attn.v_proj.weight])[symbol:model.layers.17.self_attn.v_proj.weight]
        addr:0x00000000000180 prog.value_symbol() -> (%446:tensor<[1339392], UInt8, CPU>[@model.layers.17.self_attn.o_proj.weight])[symbol:model.layers.17.self_attn.o_proj.weight]
        addr:0x00000000000181 prog.value_symbol() -> (%439:tensor<[7813120], UInt8, CPU>[@model.layers.17.mlp.gate_proj.weight])[symbol:model.layers.17.mlp.gate_proj.weight]
        addr:0x00000000000182 prog.value_symbol() -> (%469:tensor<[7813120], UInt8, CPU>[@model.layers.17.mlp.up_proj.weight])[symbol:model.layers.17.mlp.up_proj.weight]
        addr:0x00000000000183 prog.value_symbol() -> (%471:tensor<[7753728], UInt8, CPU>[@model.layers.17.mlp.down_proj.weight])[symbol:model.layers.17.mlp.down_proj.weight]
        addr:0x00000000000184 prog.value_symbol() -> (%454:tensor<[1339392], UInt8, CPU>[@model.layers.18.self_attn.q_proj.weight])[symbol:model.layers.18.self_attn.q_proj.weight]
        addr:0x00000000000185 prog.value_symbol() -> (%339:tensor<[223232], UInt8, CPU>[@model.layers.18.self_attn.k_proj.weight])[symbol:model.layers.18.self_attn.k_proj.weight]
        addr:0x00000000000186 prog.value_symbol() -> (%461:tensor<[223232], UInt8, CPU>[@model.layers.18.self_attn.v_proj.weight])[symbol:model.layers.18.self_attn.v_proj.weight]
        addr:0x00000000000187 prog.value_symbol() -> (%319:tensor<[1339392], UInt8, CPU>[@model.layers.18.self_attn.o_proj.weight])[symbol:model.layers.18.self_attn.o_proj.weight]
        addr:0x00000000000188 prog.value_symbol() -> (%457:tensor<[7813120], UInt8, CPU>[@model.layers.18.mlp.gate_proj.weight])[symbol:model.layers.18.mlp.gate_proj.weight]
        addr:0x00000000000189 prog.value_symbol() -> (%456:tensor<[7813120], UInt8, CPU>[@model.layers.18.mlp.up_proj.weight])[symbol:model.layers.18.mlp.up_proj.weight]
        addr:0x0000000000018a prog.value_symbol() -> (%191:tensor<[7753728], UInt8, CPU>[@model.layers.18.mlp.down_proj.weight])[symbol:model.layers.18.mlp.down_proj.weight]
        addr:0x0000000000018b prog.value_symbol() -> (%136:tensor<[1339392], UInt8, CPU>[@model.layers.19.self_attn.q_proj.weight])[symbol:model.layers.19.self_attn.q_proj.weight]
        addr:0x0000000000018c prog.value_symbol() -> (%445:tensor<[223232], UInt8, CPU>[@model.layers.19.self_attn.k_proj.weight])[symbol:model.layers.19.self_attn.k_proj.weight]
        addr:0x0000000000018d prog.value_symbol() -> (%126:tensor<[223232], UInt8, CPU>[@model.layers.19.self_attn.v_proj.weight])[symbol:model.layers.19.self_attn.v_proj.weight]
        addr:0x0000000000018e prog.value_symbol() -> (%444:tensor<[1339392], UInt8, CPU>[@model.layers.19.self_attn.o_proj.weight])[symbol:model.layers.19.self_attn.o_proj.weight]
        addr:0x0000000000018f prog.value_symbol() -> (%176:tensor<[7813120], UInt8, CPU>[@model.layers.19.mlp.gate_proj.weight])[symbol:model.layers.19.mlp.gate_proj.weight]
        addr:0x00000000000190 prog.value_symbol() -> (%451:tensor<[7813120], UInt8, CPU>[@model.layers.19.mlp.up_proj.weight])[symbol:model.layers.19.mlp.up_proj.weight]
        addr:0x00000000000191 prog.value_symbol() -> (%189:tensor<[7753728], UInt8, CPU>[@model.layers.19.mlp.down_proj.weight])[symbol:model.layers.19.mlp.down_proj.weight]
        addr:0x00000000000192 prog.value_symbol() -> (%140:tensor<[1339392], UInt8, CPU>[@model.layers.20.self_attn.q_proj.weight])[symbol:model.layers.20.self_attn.q_proj.weight]
        addr:0x00000000000193 prog.value_symbol() -> (%491:tensor<[223232], UInt8, CPU>[@model.layers.20.self_attn.k_proj.weight])[symbol:model.layers.20.self_attn.k_proj.weight]
        addr:0x00000000000194 prog.value_symbol() -> (%553:tensor<[223232], UInt8, CPU>[@model.layers.20.self_attn.v_proj.weight])[symbol:model.layers.20.self_attn.v_proj.weight]
        addr:0x00000000000195 prog.value_symbol() -> (%569:tensor<[1339392], UInt8, CPU>[@model.layers.20.self_attn.o_proj.weight])[symbol:model.layers.20.self_attn.o_proj.weight]
        addr:0x00000000000196 prog.value_symbol() -> (%181:tensor<[7813120], UInt8, CPU>[@model.layers.20.mlp.gate_proj.weight])[symbol:model.layers.20.mlp.gate_proj.weight]
        addr:0x00000000000197 prog.value_symbol() -> (%548:tensor<[7813120], UInt8, CPU>[@model.layers.20.mlp.up_proj.weight])[symbol:model.layers.20.mlp.up_proj.weight]
        addr:0x00000000000198 prog.value_symbol() -> (%413:tensor<[7753728], UInt8, CPU>[@model.layers.20.mlp.down_proj.weight])[symbol:model.layers.20.mlp.down_proj.weight]
        addr:0x00000000000199 prog.value_symbol() -> (%541:tensor<[1339392], UInt8, CPU>[@model.layers.21.self_attn.q_proj.weight])[symbol:model.layers.21.self_attn.q_proj.weight]
        addr:0x0000000000019a prog.value_symbol() -> (%162:tensor<[223232], UInt8, CPU>[@model.layers.21.self_attn.k_proj.weight])[symbol:model.layers.21.self_attn.k_proj.weight]
        addr:0x0000000000019b prog.value_symbol() -> (%125:tensor<[223232], UInt8, CPU>[@model.layers.21.self_attn.v_proj.weight])[symbol:model.layers.21.self_attn.v_proj.weight]
        addr:0x0000000000019c prog.value_symbol() -> (%419:tensor<[1339392], UInt8, CPU>[@model.layers.21.self_attn.o_proj.weight])[symbol:model.layers.21.self_attn.o_proj.weight]
        addr:0x0000000000019d prog.value_symbol() -> (%186:tensor<[7813120], UInt8, CPU>[@model.layers.21.mlp.gate_proj.weight])[symbol:model.layers.21.mlp.gate_proj.weight]
        addr:0x0000000000019e prog.value_symbol() -> (%368:tensor<[7813120], UInt8, CPU>[@model.layers.21.mlp.up_proj.weight])[symbol:model.layers.21.mlp.up_proj.weight]
        addr:0x0000000000019f prog.value_symbol() -> (%478:tensor<[7753728], UInt8, CPU>[@model.layers.21.mlp.down_proj.weight])[symbol:model.layers.21.mlp.down_proj.weight]
        addr:0x000000000001a0 prog.value_symbol() -> (%494:tensor<[1339392], UInt8, CPU>[@model.layers.22.self_attn.q_proj.weight])[symbol:model.layers.22.self_attn.q_proj.weight]
        addr:0x000000000001a1 prog.value_symbol() -> (%420:tensor<[223232], UInt8, CPU>[@model.layers.22.self_attn.k_proj.weight])[symbol:model.layers.22.self_attn.k_proj.weight]
        addr:0x000000000001a2 prog.value_symbol() -> (%404:tensor<[223232], UInt8, CPU>[@model.layers.22.self_attn.v_proj.weight])[symbol:model.layers.22.self_attn.v_proj.weight]
        addr:0x000000000001a3 prog.value_symbol() -> (%475:tensor<[1339392], UInt8, CPU>[@model.layers.22.self_attn.o_proj.weight])[symbol:model.layers.22.self_attn.o_proj.weight]
        addr:0x000000000001a4 prog.value_symbol() -> (%178:tensor<[7813120], UInt8, CPU>[@model.layers.22.mlp.gate_proj.weight])[symbol:model.layers.22.mlp.gate_proj.weight]
        addr:0x000000000001a5 prog.value_symbol() -> (%410:tensor<[7813120], UInt8, CPU>[@model.layers.22.mlp.up_proj.weight])[symbol:model.layers.22.mlp.up_proj.weight]
        addr:0x000000000001a6 prog.value_symbol() -> (%415:tensor<[7753728], UInt8, CPU>[@model.layers.22.mlp.down_proj.weight])[symbol:model.layers.22.mlp.down_proj.weight]
        addr:0x000000000001a7 prog.value_symbol() -> (%393:tensor<[1339392], UInt8, CPU>[@model.layers.23.self_attn.q_proj.weight])[symbol:model.layers.23.self_attn.q_proj.weight]
        addr:0x000000000001a8 prog.value_symbol() -> (%163:tensor<[223232], UInt8, CPU>[@model.layers.23.self_attn.k_proj.weight])[symbol:model.layers.23.self_attn.k_proj.weight]
        addr:0x000000000001a9 prog.value_symbol() -> (%362:tensor<[223232], UInt8, CPU>[@model.layers.23.self_attn.v_proj.weight])[symbol:model.layers.23.self_attn.v_proj.weight]
        addr:0x000000000001aa prog.value_symbol() -> (%460:tensor<[1339392], UInt8, CPU>[@model.layers.23.self_attn.o_proj.weight])[symbol:model.layers.23.self_attn.o_proj.weight]
        addr:0x000000000001ab prog.value_symbol() -> (%357:tensor<[7813120], UInt8, CPU>[@model.layers.23.mlp.gate_proj.weight])[symbol:model.layers.23.mlp.gate_proj.weight]
        addr:0x000000000001ac prog.value_symbol() -> (%166:tensor<[7813120], UInt8, CPU>[@model.layers.23.mlp.up_proj.weight])[symbol:model.layers.23.mlp.up_proj.weight]
        addr:0x000000000001ad prog.value_symbol() -> (%193:tensor<[7753728], UInt8, CPU>[@model.layers.23.mlp.down_proj.weight])[symbol:model.layers.23.mlp.down_proj.weight]
        addr:0x000000000001ae prog.value_symbol() -> (%141:tensor<[1339392], UInt8, CPU>[@model.layers.24.self_attn.q_proj.weight])[symbol:model.layers.24.self_attn.q_proj.weight]
        addr:0x000000000001af prog.value_symbol() -> (%452:tensor<[223232], UInt8, CPU>[@model.layers.24.self_attn.k_proj.weight])[symbol:model.layers.24.self_attn.k_proj.weight]
        addr:0x000000000001b0 prog.value_symbol() -> (%477:tensor<[223232], UInt8, CPU>[@model.layers.24.self_attn.v_proj.weight])[symbol:model.layers.24.self_attn.v_proj.weight]
        addr:0x000000000001b1 prog.value_symbol() -> (%485:tensor<[1339392], UInt8, CPU>[@model.layers.24.self_attn.o_proj.weight])[symbol:model.layers.24.self_attn.o_proj.weight]
        addr:0x000000000001b2 prog.value_symbol() -> (%180:tensor<[7813120], UInt8, CPU>[@model.layers.24.mlp.gate_proj.weight])[symbol:model.layers.24.mlp.gate_proj.weight]
        addr:0x000000000001b3 prog.value_symbol() -> (%411:tensor<[7813120], UInt8, CPU>[@model.layers.24.mlp.up_proj.weight])[symbol:model.layers.24.mlp.up_proj.weight]
        addr:0x000000000001b4 prog.value_symbol() -> (%196:tensor<[7753728], UInt8, CPU>[@model.layers.24.mlp.down_proj.weight])[symbol:model.layers.24.mlp.down_proj.weight]
        addr:0x000000000001b5 prog.value_symbol() -> (%325:tensor<[1339392], UInt8, CPU>[@model.layers.25.self_attn.q_proj.weight])[symbol:model.layers.25.self_attn.q_proj.weight]
        addr:0x000000000001b6 prog.value_symbol() -> (%385:tensor<[223232], UInt8, CPU>[@model.layers.25.self_attn.k_proj.weight])[symbol:model.layers.25.self_attn.k_proj.weight]
        addr:0x000000000001b7 prog.value_symbol() -> (%381:tensor<[223232], UInt8, CPU>[@model.layers.25.self_attn.v_proj.weight])[symbol:model.layers.25.self_attn.v_proj.weight]
        addr:0x000000000001b8 prog.value_symbol() -> (%150:tensor<[1339392], UInt8, CPU>[@model.layers.25.self_attn.o_proj.weight])[symbol:model.layers.25.self_attn.o_proj.weight]
        addr:0x000000000001b9 prog.value_symbol() -> (%382:tensor<[7813120], UInt8, CPU>[@model.layers.25.mlp.gate_proj.weight])[symbol:model.layers.25.mlp.gate_proj.weight]
        addr:0x000000000001ba prog.value_symbol() -> (%337:tensor<[7813120], UInt8, CPU>[@model.layers.25.mlp.up_proj.weight])[symbol:model.layers.25.mlp.up_proj.weight]
        addr:0x000000000001bb prog.value_symbol() -> (%387:tensor<[7753728], UInt8, CPU>[@model.layers.25.mlp.down_proj.weight])[symbol:model.layers.25.mlp.down_proj.weight]
        addr:0x000000000001bc prog.value_symbol() -> (%327:tensor<[1339392], UInt8, CPU>[@model.layers.26.self_attn.q_proj.weight])[symbol:model.layers.26.self_attn.q_proj.weight]
        addr:0x000000000001bd prog.value_symbol() -> (%321:tensor<[223232], UInt8, CPU>[@model.layers.26.self_attn.k_proj.weight])[symbol:model.layers.26.self_attn.k_proj.weight]
        addr:0x000000000001be prog.value_symbol() -> (%391:tensor<[223232], UInt8, CPU>[@model.layers.26.self_attn.v_proj.weight])[symbol:model.layers.26.self_attn.v_proj.weight]
        addr:0x000000000001bf prog.value_symbol() -> (%155:tensor<[1339392], UInt8, CPU>[@model.layers.26.self_attn.o_proj.weight])[symbol:model.layers.26.self_attn.o_proj.weight]
        addr:0x000000000001c0 prog.value_symbol() -> (%301:tensor<[7813120], UInt8, CPU>[@model.layers.26.mlp.gate_proj.weight])[symbol:model.layers.26.mlp.gate_proj.weight]
        addr:0x000000000001c1 prog.value_symbol() -> (%377:tensor<[7813120], UInt8, CPU>[@model.layers.26.mlp.up_proj.weight])[symbol:model.layers.26.mlp.up_proj.weight]
        addr:0x000000000001c2 prog.value_symbol() -> (%190:tensor<[7753728], UInt8, CPU>[@model.layers.26.mlp.down_proj.weight])[symbol:model.layers.26.mlp.down_proj.weight]
        addr:0x000000000001c3 prog.value_symbol() -> (%367:tensor<[1339392], UInt8, CPU>[@model.layers.27.self_attn.q_proj.weight])[symbol:model.layers.27.self_attn.q_proj.weight]
        addr:0x000000000001c4 prog.value_symbol() -> (%358:tensor<[223232], UInt8, CPU>[@model.layers.27.self_attn.k_proj.weight])[symbol:model.layers.27.self_attn.k_proj.weight]
        addr:0x000000000001c5 prog.value_symbol() -> (%526:tensor<[223232], UInt8, CPU>[@model.layers.27.self_attn.v_proj.weight])[symbol:model.layers.27.self_attn.v_proj.weight]
        addr:0x000000000001c6 prog.value_symbol() -> (%151:tensor<[1339392], UInt8, CPU>[@model.layers.27.self_attn.o_proj.weight])[symbol:model.layers.27.self_attn.o_proj.weight]
        addr:0x000000000001c7 prog.value_symbol() -> (%177:tensor<[7813120], UInt8, CPU>[@model.layers.27.mlp.gate_proj.weight])[symbol:model.layers.27.mlp.gate_proj.weight]
        addr:0x000000000001c8 prog.value_symbol() -> (%172:tensor<[7813120], UInt8, CPU>[@model.layers.27.mlp.up_proj.weight])[symbol:model.layers.27.mlp.up_proj.weight]
        addr:0x000000000001c9 prog.value_symbol() -> (%195:tensor<[7753728], UInt8, CPU>[@model.layers.27.mlp.down_proj.weight])[symbol:model.layers.27.mlp.down_proj.weight]
        addr:0x000000000001ca prog.value_symbol() -> (%464:tensor<[132488192], UInt8, CPU>[@model.lm_head.weight])[symbol:model.lm_head.weight]
        addr:0x000000000001cb prog.kernel_symbol[symbol:visual.patch_embed.proj, op_type:Conv3D, op_options:null]
        addr:0x000000000001cc prog.kernel_symbol[symbol:visual.blocks.0.norm1, op_type:LayerNorm, op_options:null]
        addr:0x000000000001cd prog.kernel_symbol[symbol:visual.blocks.0.norm2, op_type:LayerNorm, op_options:null]
        addr:0x000000000001ce prog.kernel_symbol[symbol:visual.blocks.0.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x000000000001cf prog.kernel_symbol[symbol:visual.blocks.0.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x000000000001d0 prog.kernel_symbol[symbol:visual.blocks.0.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x000000000001d1 prog.kernel_symbol[symbol:visual.blocks.0.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x000000000001d2 prog.kernel_symbol[symbol:visual.blocks.0.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x000000000001d3 prog.kernel_symbol[symbol:visual.blocks.0.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x000000000001d4 prog.kernel_symbol[symbol:visual.blocks.0.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x000000000001d5 prog.kernel_symbol[symbol:visual.blocks.0.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x000000000001d6 prog.kernel_symbol[symbol:visual.blocks.1.norm1, op_type:LayerNorm, op_options:null]
        addr:0x000000000001d7 prog.kernel_symbol[symbol:visual.blocks.1.norm2, op_type:LayerNorm, op_options:null]
        addr:0x000000000001d8 prog.kernel_symbol[symbol:visual.blocks.1.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x000000000001d9 prog.kernel_symbol[symbol:visual.blocks.1.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x000000000001da prog.kernel_symbol[symbol:visual.blocks.1.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x000000000001db prog.kernel_symbol[symbol:visual.blocks.1.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x000000000001dc prog.kernel_symbol[symbol:visual.blocks.1.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x000000000001dd prog.kernel_symbol[symbol:visual.blocks.1.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x000000000001de prog.kernel_symbol[symbol:visual.blocks.1.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x000000000001df prog.kernel_symbol[symbol:visual.blocks.1.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x000000000001e0 prog.kernel_symbol[symbol:visual.blocks.2.norm1, op_type:LayerNorm, op_options:null]
        addr:0x000000000001e1 prog.kernel_symbol[symbol:visual.blocks.2.norm2, op_type:LayerNorm, op_options:null]
        addr:0x000000000001e2 prog.kernel_symbol[symbol:visual.blocks.2.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x000000000001e3 prog.kernel_symbol[symbol:visual.blocks.2.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x000000000001e4 prog.kernel_symbol[symbol:visual.blocks.2.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x000000000001e5 prog.kernel_symbol[symbol:visual.blocks.2.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x000000000001e6 prog.kernel_symbol[symbol:visual.blocks.2.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x000000000001e7 prog.kernel_symbol[symbol:visual.blocks.2.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x000000000001e8 prog.kernel_symbol[symbol:visual.blocks.2.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x000000000001e9 prog.kernel_symbol[symbol:visual.blocks.2.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x000000000001ea prog.kernel_symbol[symbol:visual.blocks.3.norm1, op_type:LayerNorm, op_options:null]
        addr:0x000000000001eb prog.kernel_symbol[symbol:visual.blocks.3.norm2, op_type:LayerNorm, op_options:null]
        addr:0x000000000001ec prog.kernel_symbol[symbol:visual.blocks.3.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x000000000001ed prog.kernel_symbol[symbol:visual.blocks.3.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x000000000001ee prog.kernel_symbol[symbol:visual.blocks.3.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x000000000001ef prog.kernel_symbol[symbol:visual.blocks.3.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x000000000001f0 prog.kernel_symbol[symbol:visual.blocks.3.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x000000000001f1 prog.kernel_symbol[symbol:visual.blocks.3.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x000000000001f2 prog.kernel_symbol[symbol:visual.blocks.3.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x000000000001f3 prog.kernel_symbol[symbol:visual.blocks.3.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x000000000001f4 prog.kernel_symbol[symbol:visual.blocks.4.norm1, op_type:LayerNorm, op_options:null]
        addr:0x000000000001f5 prog.kernel_symbol[symbol:visual.blocks.4.norm2, op_type:LayerNorm, op_options:null]
        addr:0x000000000001f6 prog.kernel_symbol[symbol:visual.blocks.4.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x000000000001f7 prog.kernel_symbol[symbol:visual.blocks.4.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x000000000001f8 prog.kernel_symbol[symbol:visual.blocks.4.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x000000000001f9 prog.kernel_symbol[symbol:visual.blocks.4.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x000000000001fa prog.kernel_symbol[symbol:visual.blocks.4.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x000000000001fb prog.kernel_symbol[symbol:visual.blocks.4.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x000000000001fc prog.kernel_symbol[symbol:visual.blocks.4.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x000000000001fd prog.kernel_symbol[symbol:visual.blocks.4.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x000000000001fe prog.kernel_symbol[symbol:visual.blocks.5.norm1, op_type:LayerNorm, op_options:null]
        addr:0x000000000001ff prog.kernel_symbol[symbol:visual.blocks.5.norm2, op_type:LayerNorm, op_options:null]
        addr:0x00000000000200 prog.kernel_symbol[symbol:visual.blocks.5.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x00000000000201 prog.kernel_symbol[symbol:visual.blocks.5.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x00000000000202 prog.kernel_symbol[symbol:visual.blocks.5.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x00000000000203 prog.kernel_symbol[symbol:visual.blocks.5.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x00000000000204 prog.kernel_symbol[symbol:visual.blocks.5.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x00000000000205 prog.kernel_symbol[symbol:visual.blocks.5.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x00000000000206 prog.kernel_symbol[symbol:visual.blocks.5.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x00000000000207 prog.kernel_symbol[symbol:visual.blocks.5.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x00000000000208 prog.kernel_symbol[symbol:visual.blocks.6.norm1, op_type:LayerNorm, op_options:null]
        addr:0x00000000000209 prog.kernel_symbol[symbol:visual.blocks.6.norm2, op_type:LayerNorm, op_options:null]
        addr:0x0000000000020a prog.kernel_symbol[symbol:visual.blocks.6.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x0000000000020b prog.kernel_symbol[symbol:visual.blocks.6.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x0000000000020c prog.kernel_symbol[symbol:visual.blocks.6.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x0000000000020d prog.kernel_symbol[symbol:visual.blocks.6.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x0000000000020e prog.kernel_symbol[symbol:visual.blocks.6.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x0000000000020f prog.kernel_symbol[symbol:visual.blocks.6.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x00000000000210 prog.kernel_symbol[symbol:visual.blocks.6.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x00000000000211 prog.kernel_symbol[symbol:visual.blocks.6.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x00000000000212 prog.kernel_symbol[symbol:visual.blocks.7.norm1, op_type:LayerNorm, op_options:null]
        addr:0x00000000000213 prog.kernel_symbol[symbol:visual.blocks.7.norm2, op_type:LayerNorm, op_options:null]
        addr:0x00000000000214 prog.kernel_symbol[symbol:visual.blocks.7.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x00000000000215 prog.kernel_symbol[symbol:visual.blocks.7.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x00000000000216 prog.kernel_symbol[symbol:visual.blocks.7.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x00000000000217 prog.kernel_symbol[symbol:visual.blocks.7.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x00000000000218 prog.kernel_symbol[symbol:visual.blocks.7.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x00000000000219 prog.kernel_symbol[symbol:visual.blocks.7.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x0000000000021a prog.kernel_symbol[symbol:visual.blocks.7.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x0000000000021b prog.kernel_symbol[symbol:visual.blocks.7.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x0000000000021c prog.kernel_symbol[symbol:visual.blocks.8.norm1, op_type:LayerNorm, op_options:null]
        addr:0x0000000000021d prog.kernel_symbol[symbol:visual.blocks.8.norm2, op_type:LayerNorm, op_options:null]
        addr:0x0000000000021e prog.kernel_symbol[symbol:visual.blocks.8.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x0000000000021f prog.kernel_symbol[symbol:visual.blocks.8.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x00000000000220 prog.kernel_symbol[symbol:visual.blocks.8.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x00000000000221 prog.kernel_symbol[symbol:visual.blocks.8.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x00000000000222 prog.kernel_symbol[symbol:visual.blocks.8.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x00000000000223 prog.kernel_symbol[symbol:visual.blocks.8.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x00000000000224 prog.kernel_symbol[symbol:visual.blocks.8.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x00000000000225 prog.kernel_symbol[symbol:visual.blocks.8.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x00000000000226 prog.kernel_symbol[symbol:visual.blocks.9.norm1, op_type:LayerNorm, op_options:null]
        addr:0x00000000000227 prog.kernel_symbol[symbol:visual.blocks.9.norm2, op_type:LayerNorm, op_options:null]
        addr:0x00000000000228 prog.kernel_symbol[symbol:visual.blocks.9.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x00000000000229 prog.kernel_symbol[symbol:visual.blocks.9.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x0000000000022a prog.kernel_symbol[symbol:visual.blocks.9.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x0000000000022b prog.kernel_symbol[symbol:visual.blocks.9.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x0000000000022c prog.kernel_symbol[symbol:visual.blocks.9.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x0000000000022d prog.kernel_symbol[symbol:visual.blocks.9.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x0000000000022e prog.kernel_symbol[symbol:visual.blocks.9.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x0000000000022f prog.kernel_symbol[symbol:visual.blocks.9.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x00000000000230 prog.kernel_symbol[symbol:visual.blocks.10.norm1, op_type:LayerNorm, op_options:null]
        addr:0x00000000000231 prog.kernel_symbol[symbol:visual.blocks.10.norm2, op_type:LayerNorm, op_options:null]
        addr:0x00000000000232 prog.kernel_symbol[symbol:visual.blocks.10.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x00000000000233 prog.kernel_symbol[symbol:visual.blocks.10.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x00000000000234 prog.kernel_symbol[symbol:visual.blocks.10.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x00000000000235 prog.kernel_symbol[symbol:visual.blocks.10.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x00000000000236 prog.kernel_symbol[symbol:visual.blocks.10.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x00000000000237 prog.kernel_symbol[symbol:visual.blocks.10.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x00000000000238 prog.kernel_symbol[symbol:visual.blocks.10.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x00000000000239 prog.kernel_symbol[symbol:visual.blocks.10.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x0000000000023a prog.kernel_symbol[symbol:visual.blocks.11.norm1, op_type:LayerNorm, op_options:null]
        addr:0x0000000000023b prog.kernel_symbol[symbol:visual.blocks.11.norm2, op_type:LayerNorm, op_options:null]
        addr:0x0000000000023c prog.kernel_symbol[symbol:visual.blocks.11.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x0000000000023d prog.kernel_symbol[symbol:visual.blocks.11.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x0000000000023e prog.kernel_symbol[symbol:visual.blocks.11.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x0000000000023f prog.kernel_symbol[symbol:visual.blocks.11.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x00000000000240 prog.kernel_symbol[symbol:visual.blocks.11.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x00000000000241 prog.kernel_symbol[symbol:visual.blocks.11.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x00000000000242 prog.kernel_symbol[symbol:visual.blocks.11.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x00000000000243 prog.kernel_symbol[symbol:visual.blocks.11.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x00000000000244 prog.kernel_symbol[symbol:visual.blocks.12.norm1, op_type:LayerNorm, op_options:null]
        addr:0x00000000000245 prog.kernel_symbol[symbol:visual.blocks.12.norm2, op_type:LayerNorm, op_options:null]
        addr:0x00000000000246 prog.kernel_symbol[symbol:visual.blocks.12.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x00000000000247 prog.kernel_symbol[symbol:visual.blocks.12.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x00000000000248 prog.kernel_symbol[symbol:visual.blocks.12.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x00000000000249 prog.kernel_symbol[symbol:visual.blocks.12.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x0000000000024a prog.kernel_symbol[symbol:visual.blocks.12.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x0000000000024b prog.kernel_symbol[symbol:visual.blocks.12.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x0000000000024c prog.kernel_symbol[symbol:visual.blocks.12.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x0000000000024d prog.kernel_symbol[symbol:visual.blocks.12.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x0000000000024e prog.kernel_symbol[symbol:visual.blocks.13.norm1, op_type:LayerNorm, op_options:null]
        addr:0x0000000000024f prog.kernel_symbol[symbol:visual.blocks.13.norm2, op_type:LayerNorm, op_options:null]
        addr:0x00000000000250 prog.kernel_symbol[symbol:visual.blocks.13.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x00000000000251 prog.kernel_symbol[symbol:visual.blocks.13.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x00000000000252 prog.kernel_symbol[symbol:visual.blocks.13.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x00000000000253 prog.kernel_symbol[symbol:visual.blocks.13.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x00000000000254 prog.kernel_symbol[symbol:visual.blocks.13.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x00000000000255 prog.kernel_symbol[symbol:visual.blocks.13.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x00000000000256 prog.kernel_symbol[symbol:visual.blocks.13.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x00000000000257 prog.kernel_symbol[symbol:visual.blocks.13.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x00000000000258 prog.kernel_symbol[symbol:visual.blocks.14.norm1, op_type:LayerNorm, op_options:null]
        addr:0x00000000000259 prog.kernel_symbol[symbol:visual.blocks.14.norm2, op_type:LayerNorm, op_options:null]
        addr:0x0000000000025a prog.kernel_symbol[symbol:visual.blocks.14.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x0000000000025b prog.kernel_symbol[symbol:visual.blocks.14.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x0000000000025c prog.kernel_symbol[symbol:visual.blocks.14.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x0000000000025d prog.kernel_symbol[symbol:visual.blocks.14.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x0000000000025e prog.kernel_symbol[symbol:visual.blocks.14.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x0000000000025f prog.kernel_symbol[symbol:visual.blocks.14.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x00000000000260 prog.kernel_symbol[symbol:visual.blocks.14.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x00000000000261 prog.kernel_symbol[symbol:visual.blocks.14.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x00000000000262 prog.kernel_symbol[symbol:visual.blocks.15.norm1, op_type:LayerNorm, op_options:null]
        addr:0x00000000000263 prog.kernel_symbol[symbol:visual.blocks.15.norm2, op_type:LayerNorm, op_options:null]
        addr:0x00000000000264 prog.kernel_symbol[symbol:visual.blocks.15.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x00000000000265 prog.kernel_symbol[symbol:visual.blocks.15.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x00000000000266 prog.kernel_symbol[symbol:visual.blocks.15.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x00000000000267 prog.kernel_symbol[symbol:visual.blocks.15.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x00000000000268 prog.kernel_symbol[symbol:visual.blocks.15.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x00000000000269 prog.kernel_symbol[symbol:visual.blocks.15.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x0000000000026a prog.kernel_symbol[symbol:visual.blocks.15.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x0000000000026b prog.kernel_symbol[symbol:visual.blocks.15.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x0000000000026c prog.kernel_symbol[symbol:visual.blocks.16.norm1, op_type:LayerNorm, op_options:null]
        addr:0x0000000000026d prog.kernel_symbol[symbol:visual.blocks.16.norm2, op_type:LayerNorm, op_options:null]
        addr:0x0000000000026e prog.kernel_symbol[symbol:visual.blocks.16.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x0000000000026f prog.kernel_symbol[symbol:visual.blocks.16.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x00000000000270 prog.kernel_symbol[symbol:visual.blocks.16.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x00000000000271 prog.kernel_symbol[symbol:visual.blocks.16.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x00000000000272 prog.kernel_symbol[symbol:visual.blocks.16.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x00000000000273 prog.kernel_symbol[symbol:visual.blocks.16.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x00000000000274 prog.kernel_symbol[symbol:visual.blocks.16.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x00000000000275 prog.kernel_symbol[symbol:visual.blocks.16.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x00000000000276 prog.kernel_symbol[symbol:visual.blocks.17.norm1, op_type:LayerNorm, op_options:null]
        addr:0x00000000000277 prog.kernel_symbol[symbol:visual.blocks.17.norm2, op_type:LayerNorm, op_options:null]
        addr:0x00000000000278 prog.kernel_symbol[symbol:visual.blocks.17.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x00000000000279 prog.kernel_symbol[symbol:visual.blocks.17.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x0000000000027a prog.kernel_symbol[symbol:visual.blocks.17.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x0000000000027b prog.kernel_symbol[symbol:visual.blocks.17.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x0000000000027c prog.kernel_symbol[symbol:visual.blocks.17.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x0000000000027d prog.kernel_symbol[symbol:visual.blocks.17.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x0000000000027e prog.kernel_symbol[symbol:visual.blocks.17.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x0000000000027f prog.kernel_symbol[symbol:visual.blocks.17.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x00000000000280 prog.kernel_symbol[symbol:visual.blocks.18.norm1, op_type:LayerNorm, op_options:null]
        addr:0x00000000000281 prog.kernel_symbol[symbol:visual.blocks.18.norm2, op_type:LayerNorm, op_options:null]
        addr:0x00000000000282 prog.kernel_symbol[symbol:visual.blocks.18.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x00000000000283 prog.kernel_symbol[symbol:visual.blocks.18.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x00000000000284 prog.kernel_symbol[symbol:visual.blocks.18.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x00000000000285 prog.kernel_symbol[symbol:visual.blocks.18.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x00000000000286 prog.kernel_symbol[symbol:visual.blocks.18.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x00000000000287 prog.kernel_symbol[symbol:visual.blocks.18.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x00000000000288 prog.kernel_symbol[symbol:visual.blocks.18.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x00000000000289 prog.kernel_symbol[symbol:visual.blocks.18.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x0000000000028a prog.kernel_symbol[symbol:visual.blocks.19.norm1, op_type:LayerNorm, op_options:null]
        addr:0x0000000000028b prog.kernel_symbol[symbol:visual.blocks.19.norm2, op_type:LayerNorm, op_options:null]
        addr:0x0000000000028c prog.kernel_symbol[symbol:visual.blocks.19.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x0000000000028d prog.kernel_symbol[symbol:visual.blocks.19.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x0000000000028e prog.kernel_symbol[symbol:visual.blocks.19.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x0000000000028f prog.kernel_symbol[symbol:visual.blocks.19.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x00000000000290 prog.kernel_symbol[symbol:visual.blocks.19.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x00000000000291 prog.kernel_symbol[symbol:visual.blocks.19.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x00000000000292 prog.kernel_symbol[symbol:visual.blocks.19.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x00000000000293 prog.kernel_symbol[symbol:visual.blocks.19.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x00000000000294 prog.kernel_symbol[symbol:visual.blocks.20.norm1, op_type:LayerNorm, op_options:null]
        addr:0x00000000000295 prog.kernel_symbol[symbol:visual.blocks.20.norm2, op_type:LayerNorm, op_options:null]
        addr:0x00000000000296 prog.kernel_symbol[symbol:visual.blocks.20.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x00000000000297 prog.kernel_symbol[symbol:visual.blocks.20.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x00000000000298 prog.kernel_symbol[symbol:visual.blocks.20.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x00000000000299 prog.kernel_symbol[symbol:visual.blocks.20.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x0000000000029a prog.kernel_symbol[symbol:visual.blocks.20.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x0000000000029b prog.kernel_symbol[symbol:visual.blocks.20.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x0000000000029c prog.kernel_symbol[symbol:visual.blocks.20.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x0000000000029d prog.kernel_symbol[symbol:visual.blocks.20.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x0000000000029e prog.kernel_symbol[symbol:visual.blocks.21.norm1, op_type:LayerNorm, op_options:null]
        addr:0x0000000000029f prog.kernel_symbol[symbol:visual.blocks.21.norm2, op_type:LayerNorm, op_options:null]
        addr:0x000000000002a0 prog.kernel_symbol[symbol:visual.blocks.21.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x000000000002a1 prog.kernel_symbol[symbol:visual.blocks.21.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x000000000002a2 prog.kernel_symbol[symbol:visual.blocks.21.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x000000000002a3 prog.kernel_symbol[symbol:visual.blocks.21.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x000000000002a4 prog.kernel_symbol[symbol:visual.blocks.21.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x000000000002a5 prog.kernel_symbol[symbol:visual.blocks.21.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x000000000002a6 prog.kernel_symbol[symbol:visual.blocks.21.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x000000000002a7 prog.kernel_symbol[symbol:visual.blocks.21.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x000000000002a8 prog.kernel_symbol[symbol:visual.blocks.22.norm1, op_type:LayerNorm, op_options:null]
        addr:0x000000000002a9 prog.kernel_symbol[symbol:visual.blocks.22.norm2, op_type:LayerNorm, op_options:null]
        addr:0x000000000002aa prog.kernel_symbol[symbol:visual.blocks.22.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x000000000002ab prog.kernel_symbol[symbol:visual.blocks.22.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x000000000002ac prog.kernel_symbol[symbol:visual.blocks.22.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x000000000002ad prog.kernel_symbol[symbol:visual.blocks.22.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x000000000002ae prog.kernel_symbol[symbol:visual.blocks.22.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x000000000002af prog.kernel_symbol[symbol:visual.blocks.22.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x000000000002b0 prog.kernel_symbol[symbol:visual.blocks.22.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x000000000002b1 prog.kernel_symbol[symbol:visual.blocks.22.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x000000000002b2 prog.kernel_symbol[symbol:visual.blocks.23.norm1, op_type:LayerNorm, op_options:null]
        addr:0x000000000002b3 prog.kernel_symbol[symbol:visual.blocks.23.norm2, op_type:LayerNorm, op_options:null]
        addr:0x000000000002b4 prog.kernel_symbol[symbol:visual.blocks.23.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x000000000002b5 prog.kernel_symbol[symbol:visual.blocks.23.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x000000000002b6 prog.kernel_symbol[symbol:visual.blocks.23.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x000000000002b7 prog.kernel_symbol[symbol:visual.blocks.23.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x000000000002b8 prog.kernel_symbol[symbol:visual.blocks.23.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x000000000002b9 prog.kernel_symbol[symbol:visual.blocks.23.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x000000000002ba prog.kernel_symbol[symbol:visual.blocks.23.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x000000000002bb prog.kernel_symbol[symbol:visual.blocks.23.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x000000000002bc prog.kernel_symbol[symbol:visual.blocks.24.norm1, op_type:LayerNorm, op_options:null]
        addr:0x000000000002bd prog.kernel_symbol[symbol:visual.blocks.24.norm2, op_type:LayerNorm, op_options:null]
        addr:0x000000000002be prog.kernel_symbol[symbol:visual.blocks.24.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x000000000002bf prog.kernel_symbol[symbol:visual.blocks.24.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x000000000002c0 prog.kernel_symbol[symbol:visual.blocks.24.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x000000000002c1 prog.kernel_symbol[symbol:visual.blocks.24.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x000000000002c2 prog.kernel_symbol[symbol:visual.blocks.24.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x000000000002c3 prog.kernel_symbol[symbol:visual.blocks.24.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x000000000002c4 prog.kernel_symbol[symbol:visual.blocks.24.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x000000000002c5 prog.kernel_symbol[symbol:visual.blocks.24.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x000000000002c6 prog.kernel_symbol[symbol:visual.blocks.25.norm1, op_type:LayerNorm, op_options:null]
        addr:0x000000000002c7 prog.kernel_symbol[symbol:visual.blocks.25.norm2, op_type:LayerNorm, op_options:null]
        addr:0x000000000002c8 prog.kernel_symbol[symbol:visual.blocks.25.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x000000000002c9 prog.kernel_symbol[symbol:visual.blocks.25.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x000000000002ca prog.kernel_symbol[symbol:visual.blocks.25.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x000000000002cb prog.kernel_symbol[symbol:visual.blocks.25.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x000000000002cc prog.kernel_symbol[symbol:visual.blocks.25.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x000000000002cd prog.kernel_symbol[symbol:visual.blocks.25.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x000000000002ce prog.kernel_symbol[symbol:visual.blocks.25.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x000000000002cf prog.kernel_symbol[symbol:visual.blocks.25.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x000000000002d0 prog.kernel_symbol[symbol:visual.blocks.26.norm1, op_type:LayerNorm, op_options:null]
        addr:0x000000000002d1 prog.kernel_symbol[symbol:visual.blocks.26.norm2, op_type:LayerNorm, op_options:null]
        addr:0x000000000002d2 prog.kernel_symbol[symbol:visual.blocks.26.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x000000000002d3 prog.kernel_symbol[symbol:visual.blocks.26.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x000000000002d4 prog.kernel_symbol[symbol:visual.blocks.26.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x000000000002d5 prog.kernel_symbol[symbol:visual.blocks.26.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x000000000002d6 prog.kernel_symbol[symbol:visual.blocks.26.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x000000000002d7 prog.kernel_symbol[symbol:visual.blocks.26.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x000000000002d8 prog.kernel_symbol[symbol:visual.blocks.26.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x000000000002d9 prog.kernel_symbol[symbol:visual.blocks.26.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x000000000002da prog.kernel_symbol[symbol:visual.blocks.27.norm1, op_type:LayerNorm, op_options:null]
        addr:0x000000000002db prog.kernel_symbol[symbol:visual.blocks.27.norm2, op_type:LayerNorm, op_options:null]
        addr:0x000000000002dc prog.kernel_symbol[symbol:visual.blocks.27.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x000000000002dd prog.kernel_symbol[symbol:visual.blocks.27.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x000000000002de prog.kernel_symbol[symbol:visual.blocks.27.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x000000000002df prog.kernel_symbol[symbol:visual.blocks.27.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x000000000002e0 prog.kernel_symbol[symbol:visual.blocks.27.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x000000000002e1 prog.kernel_symbol[symbol:visual.blocks.27.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x000000000002e2 prog.kernel_symbol[symbol:visual.blocks.27.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x000000000002e3 prog.kernel_symbol[symbol:visual.blocks.27.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x000000000002e4 prog.kernel_symbol[symbol:visual.blocks.28.norm1, op_type:LayerNorm, op_options:null]
        addr:0x000000000002e5 prog.kernel_symbol[symbol:visual.blocks.28.norm2, op_type:LayerNorm, op_options:null]
        addr:0x000000000002e6 prog.kernel_symbol[symbol:visual.blocks.28.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x000000000002e7 prog.kernel_symbol[symbol:visual.blocks.28.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x000000000002e8 prog.kernel_symbol[symbol:visual.blocks.28.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x000000000002e9 prog.kernel_symbol[symbol:visual.blocks.28.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x000000000002ea prog.kernel_symbol[symbol:visual.blocks.28.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x000000000002eb prog.kernel_symbol[symbol:visual.blocks.28.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x000000000002ec prog.kernel_symbol[symbol:visual.blocks.28.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x000000000002ed prog.kernel_symbol[symbol:visual.blocks.28.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x000000000002ee prog.kernel_symbol[symbol:visual.blocks.29.norm1, op_type:LayerNorm, op_options:null]
        addr:0x000000000002ef prog.kernel_symbol[symbol:visual.blocks.29.norm2, op_type:LayerNorm, op_options:null]
        addr:0x000000000002f0 prog.kernel_symbol[symbol:visual.blocks.29.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x000000000002f1 prog.kernel_symbol[symbol:visual.blocks.29.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x000000000002f2 prog.kernel_symbol[symbol:visual.blocks.29.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x000000000002f3 prog.kernel_symbol[symbol:visual.blocks.29.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x000000000002f4 prog.kernel_symbol[symbol:visual.blocks.29.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x000000000002f5 prog.kernel_symbol[symbol:visual.blocks.29.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x000000000002f6 prog.kernel_symbol[symbol:visual.blocks.29.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x000000000002f7 prog.kernel_symbol[symbol:visual.blocks.29.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x000000000002f8 prog.kernel_symbol[symbol:visual.blocks.30.norm1, op_type:LayerNorm, op_options:null]
        addr:0x000000000002f9 prog.kernel_symbol[symbol:visual.blocks.30.norm2, op_type:LayerNorm, op_options:null]
        addr:0x000000000002fa prog.kernel_symbol[symbol:visual.blocks.30.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x000000000002fb prog.kernel_symbol[symbol:visual.blocks.30.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x000000000002fc prog.kernel_symbol[symbol:visual.blocks.30.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x000000000002fd prog.kernel_symbol[symbol:visual.blocks.30.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x000000000002fe prog.kernel_symbol[symbol:visual.blocks.30.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x000000000002ff prog.kernel_symbol[symbol:visual.blocks.30.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x00000000000300 prog.kernel_symbol[symbol:visual.blocks.30.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x00000000000301 prog.kernel_symbol[symbol:visual.blocks.30.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x00000000000302 prog.kernel_symbol[symbol:visual.blocks.31.norm1, op_type:LayerNorm, op_options:null]
        addr:0x00000000000303 prog.kernel_symbol[symbol:visual.blocks.31.norm2, op_type:LayerNorm, op_options:null]
        addr:0x00000000000304 prog.kernel_symbol[symbol:visual.blocks.31.attn.qkv, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":3840}]
        addr:0x00000000000305 prog.kernel_symbol[symbol:visual.blocks.31.attn.vision_rope_q, op_type:SiLU, op_options:null]
        addr:0x00000000000306 prog.kernel_symbol[symbol:visual.blocks.31.attn.vision_rope_k, op_type:SiLU, op_options:null]
        addr:0x00000000000307 prog.kernel_symbol[symbol:visual.blocks.31.attn.softmax, op_type:Softmax, op_options:null]
        addr:0x00000000000308 prog.kernel_symbol[symbol:visual.blocks.31.attn.proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":1280}]
        addr:0x00000000000309 prog.kernel_symbol[symbol:visual.blocks.31.mlp.fc1, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1280,"out_channels":5120}]
        addr:0x0000000000030a prog.kernel_symbol[symbol:visual.blocks.31.mlp.act, op_type:QuickGELU, op_options:null]
        addr:0x0000000000030b prog.kernel_symbol[symbol:visual.blocks.31.mlp.fc2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1280}]
        addr:0x0000000000030c prog.kernel_symbol[symbol:visual.merger.ln_q, op_type:LayerNorm, op_options:null]
        addr:0x0000000000030d prog.kernel_symbol[symbol:visual.merger.mlp.0, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":5120}]
        addr:0x0000000000030e prog.kernel_symbol[symbol:visual.merger.mlp.gelu, op_type:GELU, op_options:null]
        addr:0x0000000000030f prog.kernel_symbol[symbol:visual.merger.mlp.2, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":5120,"out_channels":1536}]
        addr:0x00000000000310 prog.kernel_symbol[symbol:model.norm, op_type:RMSNorm, op_options:null]
        addr:0x00000000000311 prog.kernel_symbol[symbol:model.lm_head, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":151936}]
        addr:0x00000000000312 prog.kernel_symbol[symbol:model.layers.0.input_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x00000000000313 prog.kernel_symbol[symbol:model.layers.0.post_attention_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x00000000000314 prog.kernel_symbol[symbol:model.layers.0.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000315 prog.kernel_symbol[symbol:model.layers.0.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000316 prog.kernel_symbol[symbol:model.layers.0.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000317 prog.kernel_symbol[symbol:model.layers.0.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000318 prog.kernel_symbol[symbol:model.layers.0.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000319 prog.kernel_symbol[symbol:model.layers.0.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x0000000000031a prog.kernel_symbol[symbol:model.layers.0.self_attn.mask, op_type:CausalMask, op_options:null]
        addr:0x0000000000031b prog.kernel_symbol[symbol:model.layers.0.self_attn.softmax, op_type:Softmax, op_options:null]
        addr:0x0000000000031c prog.kernel_symbol[symbol:model.layers.0.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000031d prog.kernel_symbol[symbol:model.layers.0.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x0000000000031e prog.kernel_symbol[symbol:model.layers.0.mlp.act, op_type:SiLU, op_options:null]
        addr:0x0000000000031f prog.kernel_symbol[symbol:model.layers.0.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000320 prog.kernel_symbol[symbol:model.layers.0.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x00000000000321 prog.kernel_symbol[symbol:model.layers.1.input_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x00000000000322 prog.kernel_symbol[symbol:model.layers.1.post_attention_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x00000000000323 prog.kernel_symbol[symbol:model.layers.1.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000324 prog.kernel_symbol[symbol:model.layers.1.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000325 prog.kernel_symbol[symbol:model.layers.1.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000326 prog.kernel_symbol[symbol:model.layers.1.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000327 prog.kernel_symbol[symbol:model.layers.1.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000328 prog.kernel_symbol[symbol:model.layers.1.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000329 prog.kernel_symbol[symbol:model.layers.1.self_attn.mask, op_type:CausalMask, op_options:null]
        addr:0x0000000000032a prog.kernel_symbol[symbol:model.layers.1.self_attn.softmax, op_type:Softmax, op_options:null]
        addr:0x0000000000032b prog.kernel_symbol[symbol:model.layers.1.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000032c prog.kernel_symbol[symbol:model.layers.1.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x0000000000032d prog.kernel_symbol[symbol:model.layers.1.mlp.act, op_type:SiLU, op_options:null]
        addr:0x0000000000032e prog.kernel_symbol[symbol:model.layers.1.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x0000000000032f prog.kernel_symbol[symbol:model.layers.1.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x00000000000330 prog.kernel_symbol[symbol:model.layers.2.input_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x00000000000331 prog.kernel_symbol[symbol:model.layers.2.post_attention_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x00000000000332 prog.kernel_symbol[symbol:model.layers.2.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000333 prog.kernel_symbol[symbol:model.layers.2.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000334 prog.kernel_symbol[symbol:model.layers.2.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000335 prog.kernel_symbol[symbol:model.layers.2.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000336 prog.kernel_symbol[symbol:model.layers.2.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000337 prog.kernel_symbol[symbol:model.layers.2.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000338 prog.kernel_symbol[symbol:model.layers.2.self_attn.mask, op_type:CausalMask, op_options:null]
        addr:0x00000000000339 prog.kernel_symbol[symbol:model.layers.2.self_attn.softmax, op_type:Softmax, op_options:null]
        addr:0x0000000000033a prog.kernel_symbol[symbol:model.layers.2.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000033b prog.kernel_symbol[symbol:model.layers.2.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x0000000000033c prog.kernel_symbol[symbol:model.layers.2.mlp.act, op_type:SiLU, op_options:null]
        addr:0x0000000000033d prog.kernel_symbol[symbol:model.layers.2.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x0000000000033e prog.kernel_symbol[symbol:model.layers.2.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x0000000000033f prog.kernel_symbol[symbol:model.layers.3.input_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x00000000000340 prog.kernel_symbol[symbol:model.layers.3.post_attention_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x00000000000341 prog.kernel_symbol[symbol:model.layers.3.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000342 prog.kernel_symbol[symbol:model.layers.3.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000343 prog.kernel_symbol[symbol:model.layers.3.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000344 prog.kernel_symbol[symbol:model.layers.3.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000345 prog.kernel_symbol[symbol:model.layers.3.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000346 prog.kernel_symbol[symbol:model.layers.3.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000347 prog.kernel_symbol[symbol:model.layers.3.self_attn.mask, op_type:CausalMask, op_options:null]
        addr:0x00000000000348 prog.kernel_symbol[symbol:model.layers.3.self_attn.softmax, op_type:Softmax, op_options:null]
        addr:0x00000000000349 prog.kernel_symbol[symbol:model.layers.3.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000034a prog.kernel_symbol[symbol:model.layers.3.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x0000000000034b prog.kernel_symbol[symbol:model.layers.3.mlp.act, op_type:SiLU, op_options:null]
        addr:0x0000000000034c prog.kernel_symbol[symbol:model.layers.3.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x0000000000034d prog.kernel_symbol[symbol:model.layers.3.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x0000000000034e prog.kernel_symbol[symbol:model.layers.4.input_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x0000000000034f prog.kernel_symbol[symbol:model.layers.4.post_attention_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x00000000000350 prog.kernel_symbol[symbol:model.layers.4.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000351 prog.kernel_symbol[symbol:model.layers.4.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000352 prog.kernel_symbol[symbol:model.layers.4.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000353 prog.kernel_symbol[symbol:model.layers.4.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000354 prog.kernel_symbol[symbol:model.layers.4.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000355 prog.kernel_symbol[symbol:model.layers.4.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000356 prog.kernel_symbol[symbol:model.layers.4.self_attn.mask, op_type:CausalMask, op_options:null]
        addr:0x00000000000357 prog.kernel_symbol[symbol:model.layers.4.self_attn.softmax, op_type:Softmax, op_options:null]
        addr:0x00000000000358 prog.kernel_symbol[symbol:model.layers.4.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000359 prog.kernel_symbol[symbol:model.layers.4.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x0000000000035a prog.kernel_symbol[symbol:model.layers.4.mlp.act, op_type:SiLU, op_options:null]
        addr:0x0000000000035b prog.kernel_symbol[symbol:model.layers.4.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x0000000000035c prog.kernel_symbol[symbol:model.layers.4.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x0000000000035d prog.kernel_symbol[symbol:model.layers.5.input_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x0000000000035e prog.kernel_symbol[symbol:model.layers.5.post_attention_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x0000000000035f prog.kernel_symbol[symbol:model.layers.5.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000360 prog.kernel_symbol[symbol:model.layers.5.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000361 prog.kernel_symbol[symbol:model.layers.5.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000362 prog.kernel_symbol[symbol:model.layers.5.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000363 prog.kernel_symbol[symbol:model.layers.5.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000364 prog.kernel_symbol[symbol:model.layers.5.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000365 prog.kernel_symbol[symbol:model.layers.5.self_attn.mask, op_type:CausalMask, op_options:null]
        addr:0x00000000000366 prog.kernel_symbol[symbol:model.layers.5.self_attn.softmax, op_type:Softmax, op_options:null]
        addr:0x00000000000367 prog.kernel_symbol[symbol:model.layers.5.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000368 prog.kernel_symbol[symbol:model.layers.5.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000369 prog.kernel_symbol[symbol:model.layers.5.mlp.act, op_type:SiLU, op_options:null]
        addr:0x0000000000036a prog.kernel_symbol[symbol:model.layers.5.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x0000000000036b prog.kernel_symbol[symbol:model.layers.5.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x0000000000036c prog.kernel_symbol[symbol:model.layers.6.input_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x0000000000036d prog.kernel_symbol[symbol:model.layers.6.post_attention_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x0000000000036e prog.kernel_symbol[symbol:model.layers.6.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000036f prog.kernel_symbol[symbol:model.layers.6.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000370 prog.kernel_symbol[symbol:model.layers.6.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000371 prog.kernel_symbol[symbol:model.layers.6.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000372 prog.kernel_symbol[symbol:model.layers.6.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000373 prog.kernel_symbol[symbol:model.layers.6.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000374 prog.kernel_symbol[symbol:model.layers.6.self_attn.mask, op_type:CausalMask, op_options:null]
        addr:0x00000000000375 prog.kernel_symbol[symbol:model.layers.6.self_attn.softmax, op_type:Softmax, op_options:null]
        addr:0x00000000000376 prog.kernel_symbol[symbol:model.layers.6.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000377 prog.kernel_symbol[symbol:model.layers.6.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000378 prog.kernel_symbol[symbol:model.layers.6.mlp.act, op_type:SiLU, op_options:null]
        addr:0x00000000000379 prog.kernel_symbol[symbol:model.layers.6.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x0000000000037a prog.kernel_symbol[symbol:model.layers.6.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x0000000000037b prog.kernel_symbol[symbol:model.layers.7.input_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x0000000000037c prog.kernel_symbol[symbol:model.layers.7.post_attention_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x0000000000037d prog.kernel_symbol[symbol:model.layers.7.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000037e prog.kernel_symbol[symbol:model.layers.7.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x0000000000037f prog.kernel_symbol[symbol:model.layers.7.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000380 prog.kernel_symbol[symbol:model.layers.7.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000381 prog.kernel_symbol[symbol:model.layers.7.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000382 prog.kernel_symbol[symbol:model.layers.7.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000383 prog.kernel_symbol[symbol:model.layers.7.self_attn.mask, op_type:CausalMask, op_options:null]
        addr:0x00000000000384 prog.kernel_symbol[symbol:model.layers.7.self_attn.softmax, op_type:Softmax, op_options:null]
        addr:0x00000000000385 prog.kernel_symbol[symbol:model.layers.7.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000386 prog.kernel_symbol[symbol:model.layers.7.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000387 prog.kernel_symbol[symbol:model.layers.7.mlp.act, op_type:SiLU, op_options:null]
        addr:0x00000000000388 prog.kernel_symbol[symbol:model.layers.7.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000389 prog.kernel_symbol[symbol:model.layers.7.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x0000000000038a prog.kernel_symbol[symbol:model.layers.8.input_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x0000000000038b prog.kernel_symbol[symbol:model.layers.8.post_attention_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x0000000000038c prog.kernel_symbol[symbol:model.layers.8.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000038d prog.kernel_symbol[symbol:model.layers.8.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x0000000000038e prog.kernel_symbol[symbol:model.layers.8.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x0000000000038f prog.kernel_symbol[symbol:model.layers.8.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000390 prog.kernel_symbol[symbol:model.layers.8.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000391 prog.kernel_symbol[symbol:model.layers.8.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000392 prog.kernel_symbol[symbol:model.layers.8.self_attn.mask, op_type:CausalMask, op_options:null]
        addr:0x00000000000393 prog.kernel_symbol[symbol:model.layers.8.self_attn.softmax, op_type:Softmax, op_options:null]
        addr:0x00000000000394 prog.kernel_symbol[symbol:model.layers.8.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000395 prog.kernel_symbol[symbol:model.layers.8.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000396 prog.kernel_symbol[symbol:model.layers.8.mlp.act, op_type:SiLU, op_options:null]
        addr:0x00000000000397 prog.kernel_symbol[symbol:model.layers.8.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000398 prog.kernel_symbol[symbol:model.layers.8.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x00000000000399 prog.kernel_symbol[symbol:model.layers.9.input_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x0000000000039a prog.kernel_symbol[symbol:model.layers.9.post_attention_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x0000000000039b prog.kernel_symbol[symbol:model.layers.9.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000039c prog.kernel_symbol[symbol:model.layers.9.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x0000000000039d prog.kernel_symbol[symbol:model.layers.9.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x0000000000039e prog.kernel_symbol[symbol:model.layers.9.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x0000000000039f prog.kernel_symbol[symbol:model.layers.9.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000003a0 prog.kernel_symbol[symbol:model.layers.9.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x000000000003a1 prog.kernel_symbol[symbol:model.layers.9.self_attn.mask, op_type:CausalMask, op_options:null]
        addr:0x000000000003a2 prog.kernel_symbol[symbol:model.layers.9.self_attn.softmax, op_type:Softmax, op_options:null]
        addr:0x000000000003a3 prog.kernel_symbol[symbol:model.layers.9.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000003a4 prog.kernel_symbol[symbol:model.layers.9.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000003a5 prog.kernel_symbol[symbol:model.layers.9.mlp.act, op_type:SiLU, op_options:null]
        addr:0x000000000003a6 prog.kernel_symbol[symbol:model.layers.9.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000003a7 prog.kernel_symbol[symbol:model.layers.9.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x000000000003a8 prog.kernel_symbol[symbol:model.layers.10.input_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x000000000003a9 prog.kernel_symbol[symbol:model.layers.10.post_attention_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x000000000003aa prog.kernel_symbol[symbol:model.layers.10.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000003ab prog.kernel_symbol[symbol:model.layers.10.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000003ac prog.kernel_symbol[symbol:model.layers.10.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000003ad prog.kernel_symbol[symbol:model.layers.10.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000003ae prog.kernel_symbol[symbol:model.layers.10.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000003af prog.kernel_symbol[symbol:model.layers.10.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x000000000003b0 prog.kernel_symbol[symbol:model.layers.10.self_attn.mask, op_type:CausalMask, op_options:null]
        addr:0x000000000003b1 prog.kernel_symbol[symbol:model.layers.10.self_attn.softmax, op_type:Softmax, op_options:null]
        addr:0x000000000003b2 prog.kernel_symbol[symbol:model.layers.10.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000003b3 prog.kernel_symbol[symbol:model.layers.10.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000003b4 prog.kernel_symbol[symbol:model.layers.10.mlp.act, op_type:SiLU, op_options:null]
        addr:0x000000000003b5 prog.kernel_symbol[symbol:model.layers.10.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000003b6 prog.kernel_symbol[symbol:model.layers.10.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x000000000003b7 prog.kernel_symbol[symbol:model.layers.11.input_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x000000000003b8 prog.kernel_symbol[symbol:model.layers.11.post_attention_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x000000000003b9 prog.kernel_symbol[symbol:model.layers.11.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000003ba prog.kernel_symbol[symbol:model.layers.11.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000003bb prog.kernel_symbol[symbol:model.layers.11.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000003bc prog.kernel_symbol[symbol:model.layers.11.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000003bd prog.kernel_symbol[symbol:model.layers.11.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000003be prog.kernel_symbol[symbol:model.layers.11.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x000000000003bf prog.kernel_symbol[symbol:model.layers.11.self_attn.mask, op_type:CausalMask, op_options:null]
        addr:0x000000000003c0 prog.kernel_symbol[symbol:model.layers.11.self_attn.softmax, op_type:Softmax, op_options:null]
        addr:0x000000000003c1 prog.kernel_symbol[symbol:model.layers.11.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000003c2 prog.kernel_symbol[symbol:model.layers.11.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000003c3 prog.kernel_symbol[symbol:model.layers.11.mlp.act, op_type:SiLU, op_options:null]
        addr:0x000000000003c4 prog.kernel_symbol[symbol:model.layers.11.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000003c5 prog.kernel_symbol[symbol:model.layers.11.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x000000000003c6 prog.kernel_symbol[symbol:model.layers.12.input_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x000000000003c7 prog.kernel_symbol[symbol:model.layers.12.post_attention_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x000000000003c8 prog.kernel_symbol[symbol:model.layers.12.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000003c9 prog.kernel_symbol[symbol:model.layers.12.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000003ca prog.kernel_symbol[symbol:model.layers.12.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000003cb prog.kernel_symbol[symbol:model.layers.12.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000003cc prog.kernel_symbol[symbol:model.layers.12.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000003cd prog.kernel_symbol[symbol:model.layers.12.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x000000000003ce prog.kernel_symbol[symbol:model.layers.12.self_attn.mask, op_type:CausalMask, op_options:null]
        addr:0x000000000003cf prog.kernel_symbol[symbol:model.layers.12.self_attn.softmax, op_type:Softmax, op_options:null]
        addr:0x000000000003d0 prog.kernel_symbol[symbol:model.layers.12.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000003d1 prog.kernel_symbol[symbol:model.layers.12.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000003d2 prog.kernel_symbol[symbol:model.layers.12.mlp.act, op_type:SiLU, op_options:null]
        addr:0x000000000003d3 prog.kernel_symbol[symbol:model.layers.12.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000003d4 prog.kernel_symbol[symbol:model.layers.12.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x000000000003d5 prog.kernel_symbol[symbol:model.layers.13.input_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x000000000003d6 prog.kernel_symbol[symbol:model.layers.13.post_attention_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x000000000003d7 prog.kernel_symbol[symbol:model.layers.13.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000003d8 prog.kernel_symbol[symbol:model.layers.13.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000003d9 prog.kernel_symbol[symbol:model.layers.13.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000003da prog.kernel_symbol[symbol:model.layers.13.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000003db prog.kernel_symbol[symbol:model.layers.13.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000003dc prog.kernel_symbol[symbol:model.layers.13.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x000000000003dd prog.kernel_symbol[symbol:model.layers.13.self_attn.mask, op_type:CausalMask, op_options:null]
        addr:0x000000000003de prog.kernel_symbol[symbol:model.layers.13.self_attn.softmax, op_type:Softmax, op_options:null]
        addr:0x000000000003df prog.kernel_symbol[symbol:model.layers.13.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000003e0 prog.kernel_symbol[symbol:model.layers.13.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000003e1 prog.kernel_symbol[symbol:model.layers.13.mlp.act, op_type:SiLU, op_options:null]
        addr:0x000000000003e2 prog.kernel_symbol[symbol:model.layers.13.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000003e3 prog.kernel_symbol[symbol:model.layers.13.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x000000000003e4 prog.kernel_symbol[symbol:model.layers.14.input_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x000000000003e5 prog.kernel_symbol[symbol:model.layers.14.post_attention_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x000000000003e6 prog.kernel_symbol[symbol:model.layers.14.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000003e7 prog.kernel_symbol[symbol:model.layers.14.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000003e8 prog.kernel_symbol[symbol:model.layers.14.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000003e9 prog.kernel_symbol[symbol:model.layers.14.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000003ea prog.kernel_symbol[symbol:model.layers.14.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000003eb prog.kernel_symbol[symbol:model.layers.14.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x000000000003ec prog.kernel_symbol[symbol:model.layers.14.self_attn.mask, op_type:CausalMask, op_options:null]
        addr:0x000000000003ed prog.kernel_symbol[symbol:model.layers.14.self_attn.softmax, op_type:Softmax, op_options:null]
        addr:0x000000000003ee prog.kernel_symbol[symbol:model.layers.14.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000003ef prog.kernel_symbol[symbol:model.layers.14.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000003f0 prog.kernel_symbol[symbol:model.layers.14.mlp.act, op_type:SiLU, op_options:null]
        addr:0x000000000003f1 prog.kernel_symbol[symbol:model.layers.14.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000003f2 prog.kernel_symbol[symbol:model.layers.14.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x000000000003f3 prog.kernel_symbol[symbol:model.layers.15.input_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x000000000003f4 prog.kernel_symbol[symbol:model.layers.15.post_attention_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x000000000003f5 prog.kernel_symbol[symbol:model.layers.15.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000003f6 prog.kernel_symbol[symbol:model.layers.15.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000003f7 prog.kernel_symbol[symbol:model.layers.15.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000003f8 prog.kernel_symbol[symbol:model.layers.15.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000003f9 prog.kernel_symbol[symbol:model.layers.15.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000003fa prog.kernel_symbol[symbol:model.layers.15.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x000000000003fb prog.kernel_symbol[symbol:model.layers.15.self_attn.mask, op_type:CausalMask, op_options:null]
        addr:0x000000000003fc prog.kernel_symbol[symbol:model.layers.15.self_attn.softmax, op_type:Softmax, op_options:null]
        addr:0x000000000003fd prog.kernel_symbol[symbol:model.layers.15.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000003fe prog.kernel_symbol[symbol:model.layers.15.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000003ff prog.kernel_symbol[symbol:model.layers.15.mlp.act, op_type:SiLU, op_options:null]
        addr:0x00000000000400 prog.kernel_symbol[symbol:model.layers.15.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000401 prog.kernel_symbol[symbol:model.layers.15.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x00000000000402 prog.kernel_symbol[symbol:model.layers.16.input_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x00000000000403 prog.kernel_symbol[symbol:model.layers.16.post_attention_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x00000000000404 prog.kernel_symbol[symbol:model.layers.16.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000405 prog.kernel_symbol[symbol:model.layers.16.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000406 prog.kernel_symbol[symbol:model.layers.16.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000407 prog.kernel_symbol[symbol:model.layers.16.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000408 prog.kernel_symbol[symbol:model.layers.16.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000409 prog.kernel_symbol[symbol:model.layers.16.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x0000000000040a prog.kernel_symbol[symbol:model.layers.16.self_attn.mask, op_type:CausalMask, op_options:null]
        addr:0x0000000000040b prog.kernel_symbol[symbol:model.layers.16.self_attn.softmax, op_type:Softmax, op_options:null]
        addr:0x0000000000040c prog.kernel_symbol[symbol:model.layers.16.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000040d prog.kernel_symbol[symbol:model.layers.16.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x0000000000040e prog.kernel_symbol[symbol:model.layers.16.mlp.act, op_type:SiLU, op_options:null]
        addr:0x0000000000040f prog.kernel_symbol[symbol:model.layers.16.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000410 prog.kernel_symbol[symbol:model.layers.16.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x00000000000411 prog.kernel_symbol[symbol:model.layers.17.input_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x00000000000412 prog.kernel_symbol[symbol:model.layers.17.post_attention_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x00000000000413 prog.kernel_symbol[symbol:model.layers.17.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000414 prog.kernel_symbol[symbol:model.layers.17.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000415 prog.kernel_symbol[symbol:model.layers.17.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000416 prog.kernel_symbol[symbol:model.layers.17.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000417 prog.kernel_symbol[symbol:model.layers.17.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000418 prog.kernel_symbol[symbol:model.layers.17.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000419 prog.kernel_symbol[symbol:model.layers.17.self_attn.mask, op_type:CausalMask, op_options:null]
        addr:0x0000000000041a prog.kernel_symbol[symbol:model.layers.17.self_attn.softmax, op_type:Softmax, op_options:null]
        addr:0x0000000000041b prog.kernel_symbol[symbol:model.layers.17.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000041c prog.kernel_symbol[symbol:model.layers.17.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x0000000000041d prog.kernel_symbol[symbol:model.layers.17.mlp.act, op_type:SiLU, op_options:null]
        addr:0x0000000000041e prog.kernel_symbol[symbol:model.layers.17.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x0000000000041f prog.kernel_symbol[symbol:model.layers.17.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x00000000000420 prog.kernel_symbol[symbol:model.layers.18.input_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x00000000000421 prog.kernel_symbol[symbol:model.layers.18.post_attention_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x00000000000422 prog.kernel_symbol[symbol:model.layers.18.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000423 prog.kernel_symbol[symbol:model.layers.18.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000424 prog.kernel_symbol[symbol:model.layers.18.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000425 prog.kernel_symbol[symbol:model.layers.18.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000426 prog.kernel_symbol[symbol:model.layers.18.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000427 prog.kernel_symbol[symbol:model.layers.18.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000428 prog.kernel_symbol[symbol:model.layers.18.self_attn.mask, op_type:CausalMask, op_options:null]
        addr:0x00000000000429 prog.kernel_symbol[symbol:model.layers.18.self_attn.softmax, op_type:Softmax, op_options:null]
        addr:0x0000000000042a prog.kernel_symbol[symbol:model.layers.18.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000042b prog.kernel_symbol[symbol:model.layers.18.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x0000000000042c prog.kernel_symbol[symbol:model.layers.18.mlp.act, op_type:SiLU, op_options:null]
        addr:0x0000000000042d prog.kernel_symbol[symbol:model.layers.18.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x0000000000042e prog.kernel_symbol[symbol:model.layers.18.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x0000000000042f prog.kernel_symbol[symbol:model.layers.19.input_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x00000000000430 prog.kernel_symbol[symbol:model.layers.19.post_attention_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x00000000000431 prog.kernel_symbol[symbol:model.layers.19.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000432 prog.kernel_symbol[symbol:model.layers.19.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000433 prog.kernel_symbol[symbol:model.layers.19.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000434 prog.kernel_symbol[symbol:model.layers.19.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000435 prog.kernel_symbol[symbol:model.layers.19.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000436 prog.kernel_symbol[symbol:model.layers.19.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000437 prog.kernel_symbol[symbol:model.layers.19.self_attn.mask, op_type:CausalMask, op_options:null]
        addr:0x00000000000438 prog.kernel_symbol[symbol:model.layers.19.self_attn.softmax, op_type:Softmax, op_options:null]
        addr:0x00000000000439 prog.kernel_symbol[symbol:model.layers.19.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000043a prog.kernel_symbol[symbol:model.layers.19.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x0000000000043b prog.kernel_symbol[symbol:model.layers.19.mlp.act, op_type:SiLU, op_options:null]
        addr:0x0000000000043c prog.kernel_symbol[symbol:model.layers.19.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x0000000000043d prog.kernel_symbol[symbol:model.layers.19.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x0000000000043e prog.kernel_symbol[symbol:model.layers.20.input_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x0000000000043f prog.kernel_symbol[symbol:model.layers.20.post_attention_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x00000000000440 prog.kernel_symbol[symbol:model.layers.20.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000441 prog.kernel_symbol[symbol:model.layers.20.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000442 prog.kernel_symbol[symbol:model.layers.20.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000443 prog.kernel_symbol[symbol:model.layers.20.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000444 prog.kernel_symbol[symbol:model.layers.20.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000445 prog.kernel_symbol[symbol:model.layers.20.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000446 prog.kernel_symbol[symbol:model.layers.20.self_attn.mask, op_type:CausalMask, op_options:null]
        addr:0x00000000000447 prog.kernel_symbol[symbol:model.layers.20.self_attn.softmax, op_type:Softmax, op_options:null]
        addr:0x00000000000448 prog.kernel_symbol[symbol:model.layers.20.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000449 prog.kernel_symbol[symbol:model.layers.20.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x0000000000044a prog.kernel_symbol[symbol:model.layers.20.mlp.act, op_type:SiLU, op_options:null]
        addr:0x0000000000044b prog.kernel_symbol[symbol:model.layers.20.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x0000000000044c prog.kernel_symbol[symbol:model.layers.20.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x0000000000044d prog.kernel_symbol[symbol:model.layers.21.input_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x0000000000044e prog.kernel_symbol[symbol:model.layers.21.post_attention_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x0000000000044f prog.kernel_symbol[symbol:model.layers.21.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000450 prog.kernel_symbol[symbol:model.layers.21.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000451 prog.kernel_symbol[symbol:model.layers.21.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000452 prog.kernel_symbol[symbol:model.layers.21.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000453 prog.kernel_symbol[symbol:model.layers.21.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000454 prog.kernel_symbol[symbol:model.layers.21.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000455 prog.kernel_symbol[symbol:model.layers.21.self_attn.mask, op_type:CausalMask, op_options:null]
        addr:0x00000000000456 prog.kernel_symbol[symbol:model.layers.21.self_attn.softmax, op_type:Softmax, op_options:null]
        addr:0x00000000000457 prog.kernel_symbol[symbol:model.layers.21.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000458 prog.kernel_symbol[symbol:model.layers.21.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000459 prog.kernel_symbol[symbol:model.layers.21.mlp.act, op_type:SiLU, op_options:null]
        addr:0x0000000000045a prog.kernel_symbol[symbol:model.layers.21.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x0000000000045b prog.kernel_symbol[symbol:model.layers.21.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x0000000000045c prog.kernel_symbol[symbol:model.layers.22.input_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x0000000000045d prog.kernel_symbol[symbol:model.layers.22.post_attention_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x0000000000045e prog.kernel_symbol[symbol:model.layers.22.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000045f prog.kernel_symbol[symbol:model.layers.22.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000460 prog.kernel_symbol[symbol:model.layers.22.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000461 prog.kernel_symbol[symbol:model.layers.22.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000462 prog.kernel_symbol[symbol:model.layers.22.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000463 prog.kernel_symbol[symbol:model.layers.22.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000464 prog.kernel_symbol[symbol:model.layers.22.self_attn.mask, op_type:CausalMask, op_options:null]
        addr:0x00000000000465 prog.kernel_symbol[symbol:model.layers.22.self_attn.softmax, op_type:Softmax, op_options:null]
        addr:0x00000000000466 prog.kernel_symbol[symbol:model.layers.22.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000467 prog.kernel_symbol[symbol:model.layers.22.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000468 prog.kernel_symbol[symbol:model.layers.22.mlp.act, op_type:SiLU, op_options:null]
        addr:0x00000000000469 prog.kernel_symbol[symbol:model.layers.22.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x0000000000046a prog.kernel_symbol[symbol:model.layers.22.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x0000000000046b prog.kernel_symbol[symbol:model.layers.23.input_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x0000000000046c prog.kernel_symbol[symbol:model.layers.23.post_attention_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x0000000000046d prog.kernel_symbol[symbol:model.layers.23.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000046e prog.kernel_symbol[symbol:model.layers.23.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x0000000000046f prog.kernel_symbol[symbol:model.layers.23.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x00000000000470 prog.kernel_symbol[symbol:model.layers.23.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000471 prog.kernel_symbol[symbol:model.layers.23.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000472 prog.kernel_symbol[symbol:model.layers.23.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000473 prog.kernel_symbol[symbol:model.layers.23.self_attn.mask, op_type:CausalMask, op_options:null]
        addr:0x00000000000474 prog.kernel_symbol[symbol:model.layers.23.self_attn.softmax, op_type:Softmax, op_options:null]
        addr:0x00000000000475 prog.kernel_symbol[symbol:model.layers.23.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000476 prog.kernel_symbol[symbol:model.layers.23.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000477 prog.kernel_symbol[symbol:model.layers.23.mlp.act, op_type:SiLU, op_options:null]
        addr:0x00000000000478 prog.kernel_symbol[symbol:model.layers.23.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000479 prog.kernel_symbol[symbol:model.layers.23.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x0000000000047a prog.kernel_symbol[symbol:model.layers.24.input_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x0000000000047b prog.kernel_symbol[symbol:model.layers.24.post_attention_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x0000000000047c prog.kernel_symbol[symbol:model.layers.24.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000047d prog.kernel_symbol[symbol:model.layers.24.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x0000000000047e prog.kernel_symbol[symbol:model.layers.24.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x0000000000047f prog.kernel_symbol[symbol:model.layers.24.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000480 prog.kernel_symbol[symbol:model.layers.24.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000481 prog.kernel_symbol[symbol:model.layers.24.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000482 prog.kernel_symbol[symbol:model.layers.24.self_attn.mask, op_type:CausalMask, op_options:null]
        addr:0x00000000000483 prog.kernel_symbol[symbol:model.layers.24.self_attn.softmax, op_type:Softmax, op_options:null]
        addr:0x00000000000484 prog.kernel_symbol[symbol:model.layers.24.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000485 prog.kernel_symbol[symbol:model.layers.24.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000486 prog.kernel_symbol[symbol:model.layers.24.mlp.act, op_type:SiLU, op_options:null]
        addr:0x00000000000487 prog.kernel_symbol[symbol:model.layers.24.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000488 prog.kernel_symbol[symbol:model.layers.24.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x00000000000489 prog.kernel_symbol[symbol:model.layers.25.input_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x0000000000048a prog.kernel_symbol[symbol:model.layers.25.post_attention_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x0000000000048b prog.kernel_symbol[symbol:model.layers.25.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000048c prog.kernel_symbol[symbol:model.layers.25.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x0000000000048d prog.kernel_symbol[symbol:model.layers.25.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x0000000000048e prog.kernel_symbol[symbol:model.layers.25.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x0000000000048f prog.kernel_symbol[symbol:model.layers.25.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x00000000000490 prog.kernel_symbol[symbol:model.layers.25.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x00000000000491 prog.kernel_symbol[symbol:model.layers.25.self_attn.mask, op_type:CausalMask, op_options:null]
        addr:0x00000000000492 prog.kernel_symbol[symbol:model.layers.25.self_attn.softmax, op_type:Softmax, op_options:null]
        addr:0x00000000000493 prog.kernel_symbol[symbol:model.layers.25.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x00000000000494 prog.kernel_symbol[symbol:model.layers.25.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000495 prog.kernel_symbol[symbol:model.layers.25.mlp.act, op_type:SiLU, op_options:null]
        addr:0x00000000000496 prog.kernel_symbol[symbol:model.layers.25.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x00000000000497 prog.kernel_symbol[symbol:model.layers.25.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x00000000000498 prog.kernel_symbol[symbol:model.layers.26.input_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x00000000000499 prog.kernel_symbol[symbol:model.layers.26.post_attention_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x0000000000049a prog.kernel_symbol[symbol:model.layers.26.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x0000000000049b prog.kernel_symbol[symbol:model.layers.26.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x0000000000049c prog.kernel_symbol[symbol:model.layers.26.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x0000000000049d prog.kernel_symbol[symbol:model.layers.26.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x0000000000049e prog.kernel_symbol[symbol:model.layers.26.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x0000000000049f prog.kernel_symbol[symbol:model.layers.26.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x000000000004a0 prog.kernel_symbol[symbol:model.layers.26.self_attn.mask, op_type:CausalMask, op_options:null]
        addr:0x000000000004a1 prog.kernel_symbol[symbol:model.layers.26.self_attn.softmax, op_type:Softmax, op_options:null]
        addr:0x000000000004a2 prog.kernel_symbol[symbol:model.layers.26.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000004a3 prog.kernel_symbol[symbol:model.layers.26.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000004a4 prog.kernel_symbol[symbol:model.layers.26.mlp.act, op_type:SiLU, op_options:null]
        addr:0x000000000004a5 prog.kernel_symbol[symbol:model.layers.26.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000004a6 prog.kernel_symbol[symbol:model.layers.26.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x000000000004a7 prog.kernel_symbol[symbol:model.layers.27.input_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x000000000004a8 prog.kernel_symbol[symbol:model.layers.27.post_attention_layernorm, op_type:RMSNorm, op_options:null]
        addr:0x000000000004a9 prog.kernel_symbol[symbol:model.layers.27.self_attn.q_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000004aa prog.kernel_symbol[symbol:model.layers.27.self_attn.k_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000004ab prog.kernel_symbol[symbol:model.layers.27.self_attn.v_proj, op_type:Linear, op_options:{"bias":true,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":256}]
        addr:0x000000000004ac prog.kernel_symbol[symbol:model.layers.27.self_attn.q_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000004ad prog.kernel_symbol[symbol:model.layers.27.self_attn.k_rope, op_type:MultimodalRoPE, op_options:{"qwen2vl_options":{"max_position_embeddings":32786,"mrope_section":[16,24,24],"rope_theta":1000000.0},"type":2}]
        addr:0x000000000004ae prog.kernel_symbol[symbol:model.layers.27.self_attn.kv_cache, op_type:KVCache, op_options:{"head_dim":128,"kv_head":2,"layer_idx":0,"q_head":12,"use_fa2":false}]
        addr:0x000000000004af prog.kernel_symbol[symbol:model.layers.27.self_attn.mask, op_type:CausalMask, op_options:null]
        addr:0x000000000004b0 prog.kernel_symbol[symbol:model.layers.27.self_attn.softmax, op_type:Softmax, op_options:null]
        addr:0x000000000004b1 prog.kernel_symbol[symbol:model.layers.27.self_attn.o_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":1536}]
        addr:0x000000000004b2 prog.kernel_symbol[symbol:model.layers.27.mlp.gate_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000004b3 prog.kernel_symbol[symbol:model.layers.27.mlp.act, op_type:SiLU, op_options:null]
        addr:0x000000000004b4 prog.kernel_symbol[symbol:model.layers.27.mlp.up_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":1536,"out_channels":8960}]
        addr:0x000000000004b5 prog.kernel_symbol[symbol:model.layers.27.mlp.down_proj, op_type:Linear, op_options:{"bias":false,"impl_type":"KaiLinear_f32_qai8dxp_qsi4c32p_mxk_nxk_qai8dxp1x8_qsi4c32p8x8_1x8x32","in_channels":8960,"out_channels":1536}]
        addr:0x000000000004b6 prog.kernel_symbol[symbol:model.embed_tokens, op_type:Embedding, op_options:{"hidden_size":1536,"vocab_size":151936}]
    }
}
