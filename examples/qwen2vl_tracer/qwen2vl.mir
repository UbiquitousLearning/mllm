@main () -> () {
    graph.SubGraphOp @init <notype> {
        () -> () {
            tensor.CPU.register () -> (%517:tensor<[233373696], Float32, CPU>[@model.embed_tokens.weight])[symbol:model.embed_tokens.weight]
            tensor.CPU.register () -> (%499:tensor<[1505280], Float32, CPU>[@visual.patch_embed.proj.weight])[symbol:visual.patch_embed.proj.weight]
            tensor.CPU.register () -> (%338:tensor<[1280], Float32, CPU>[@visual.blocks.0.norm1.weight])[symbol:visual.blocks.0.norm1.weight]
            tensor.CPU.register () -> (%540:tensor<[1280], Float32, CPU>[@visual.blocks.0.norm1.bias])[symbol:visual.blocks.0.norm1.bias]
            tensor.CPU.register () -> (%315:tensor<[2795520], UInt8, CPU>[@visual.blocks.0.attn.qkv.weight])[symbol:visual.blocks.0.attn.qkv.weight]
            tensor.CPU.register () -> (%112:tensor<[931840], UInt8, CPU>[@visual.blocks.0.attn.proj.weight])[symbol:visual.blocks.0.attn.proj.weight]
            tensor.CPU.register () -> (%313:tensor<[1280], Float32, CPU>[@visual.blocks.0.norm2.weight])[symbol:visual.blocks.0.norm2.weight]
            tensor.CPU.register () -> (%527:tensor<[1280], Float32, CPU>[@visual.blocks.0.norm2.bias])[symbol:visual.blocks.0.norm2.bias]
            tensor.CPU.register () -> (%89:tensor<[3727360], UInt8, CPU>[@visual.blocks.0.mlp.fc1.weight])[symbol:visual.blocks.0.mlp.fc1.weight]
            tensor.CPU.register () -> (%59:tensor<[3696640], UInt8, CPU>[@visual.blocks.0.mlp.fc2.weight])[symbol:visual.blocks.0.mlp.fc2.weight]
            tensor.CPU.register () -> (%509:tensor<[1280], Float32, CPU>[@visual.blocks.1.norm1.weight])[symbol:visual.blocks.1.norm1.weight]
            tensor.CPU.register () -> (%311:tensor<[1280], Float32, CPU>[@visual.blocks.1.norm1.bias])[symbol:visual.blocks.1.norm1.bias]
            tensor.CPU.register () -> (%108:tensor<[2795520], UInt8, CPU>[@visual.blocks.1.attn.qkv.weight])[symbol:visual.blocks.1.attn.qkv.weight]
            tensor.CPU.register () -> (%117:tensor<[931840], UInt8, CPU>[@visual.blocks.1.attn.proj.weight])[symbol:visual.blocks.1.attn.proj.weight]
            tensor.CPU.register () -> (%310:tensor<[1280], Float32, CPU>[@visual.blocks.1.norm2.weight])[symbol:visual.blocks.1.norm2.weight]
            tensor.CPU.register () -> (%351:tensor<[1280], Float32, CPU>[@visual.blocks.1.norm2.bias])[symbol:visual.blocks.1.norm2.bias]
            tensor.CPU.register () -> (%77:tensor<[3727360], UInt8, CPU>[@visual.blocks.1.mlp.fc1.weight])[symbol:visual.blocks.1.mlp.fc1.weight]
            tensor.CPU.register () -> (%74:tensor<[3696640], UInt8, CPU>[@visual.blocks.1.mlp.fc2.weight])[symbol:visual.blocks.1.mlp.fc2.weight]
            tensor.CPU.register () -> (%364:tensor<[1280], Float32, CPU>[@visual.blocks.2.norm1.weight])[symbol:visual.blocks.2.norm1.weight]
            tensor.CPU.register () -> (%505:tensor<[1280], Float32, CPU>[@visual.blocks.2.norm1.bias])[symbol:visual.blocks.2.norm1.bias]
            tensor.CPU.register () -> (%101:tensor<[2795520], UInt8, CPU>[@visual.blocks.2.attn.qkv.weight])[symbol:visual.blocks.2.attn.qkv.weight]
            tensor.CPU.register () -> (%361:tensor<[931840], UInt8, CPU>[@visual.blocks.2.attn.proj.weight])[symbol:visual.blocks.2.attn.proj.weight]
            tensor.CPU.register () -> (%257:tensor<[1280], Float32, CPU>[@visual.blocks.2.norm2.weight])[symbol:visual.blocks.2.norm2.weight]
            tensor.CPU.register () -> (%538:tensor<[1280], Float32, CPU>[@visual.blocks.2.norm2.bias])[symbol:visual.blocks.2.norm2.bias]
            tensor.CPU.register () -> (%453:tensor<[3727360], UInt8, CPU>[@visual.blocks.2.mlp.fc1.weight])[symbol:visual.blocks.2.mlp.fc1.weight]
            tensor.CPU.register () -> (%62:tensor<[3696640], UInt8, CPU>[@visual.blocks.2.mlp.fc2.weight])[symbol:visual.blocks.2.mlp.fc2.weight]
            tensor.CPU.register () -> (%221:tensor<[1280], Float32, CPU>[@visual.blocks.3.norm1.weight])[symbol:visual.blocks.3.norm1.weight]
            tensor.CPU.register () -> (%488:tensor<[1280], Float32, CPU>[@visual.blocks.3.norm1.bias])[symbol:visual.blocks.3.norm1.bias]
            tensor.CPU.register () -> (%222:tensor<[2795520], UInt8, CPU>[@visual.blocks.3.attn.qkv.weight])[symbol:visual.blocks.3.attn.qkv.weight]
            tensor.CPU.register () -> (%269:tensor<[931840], UInt8, CPU>[@visual.blocks.3.attn.proj.weight])[symbol:visual.blocks.3.attn.proj.weight]
            tensor.CPU.register () -> (%542:tensor<[1280], Float32, CPU>[@visual.blocks.3.norm2.weight])[symbol:visual.blocks.3.norm2.weight]
            tensor.CPU.register () -> (%424:tensor<[1280], Float32, CPU>[@visual.blocks.3.norm2.bias])[symbol:visual.blocks.3.norm2.bias]
            tensor.CPU.register () -> (%82:tensor<[3727360], UInt8, CPU>[@visual.blocks.3.mlp.fc1.weight])[symbol:visual.blocks.3.mlp.fc1.weight]
            tensor.CPU.register () -> (%562:tensor<[3696640], UInt8, CPU>[@visual.blocks.3.mlp.fc2.weight])[symbol:visual.blocks.3.mlp.fc2.weight]
            tensor.CPU.register () -> (%482:tensor<[1280], Float32, CPU>[@visual.blocks.4.norm1.weight])[symbol:visual.blocks.4.norm1.weight]
            tensor.CPU.register () -> (%215:tensor<[1280], Float32, CPU>[@visual.blocks.4.norm1.bias])[symbol:visual.blocks.4.norm1.bias]
            tensor.CPU.register () -> (%103:tensor<[2795520], UInt8, CPU>[@visual.blocks.4.attn.qkv.weight])[symbol:visual.blocks.4.attn.qkv.weight]
            tensor.CPU.register () -> (%320:tensor<[931840], UInt8, CPU>[@visual.blocks.4.attn.proj.weight])[symbol:visual.blocks.4.attn.proj.weight]
            tensor.CPU.register () -> (%213:tensor<[1280], Float32, CPU>[@visual.blocks.4.norm2.weight])[symbol:visual.blocks.4.norm2.weight]
            tensor.CPU.register () -> (%214:tensor<[1280], Float32, CPU>[@visual.blocks.4.norm2.bias])[symbol:visual.blocks.4.norm2.bias]
            tensor.CPU.register () -> (%476:tensor<[3727360], UInt8, CPU>[@visual.blocks.4.mlp.fc1.weight])[symbol:visual.blocks.4.mlp.fc1.weight]
            tensor.CPU.register () -> (%559:tensor<[3696640], UInt8, CPU>[@visual.blocks.4.mlp.fc2.weight])[symbol:visual.blocks.4.mlp.fc2.weight]
            tensor.CPU.register () -> (%212:tensor<[1280], Float32, CPU>[@visual.blocks.5.norm1.weight])[symbol:visual.blocks.5.norm1.weight]
            tensor.CPU.register () -> (%237:tensor<[1280], Float32, CPU>[@visual.blocks.5.norm1.bias])[symbol:visual.blocks.5.norm1.bias]
            tensor.CPU.register () -> (%111:tensor<[2795520], UInt8, CPU>[@visual.blocks.5.attn.qkv.weight])[symbol:visual.blocks.5.attn.qkv.weight]
            tensor.CPU.register () -> (%557:tensor<[931840], UInt8, CPU>[@visual.blocks.5.attn.proj.weight])[symbol:visual.blocks.5.attn.proj.weight]
            tensor.CPU.register () -> (%514:tensor<[1280], Float32, CPU>[@visual.blocks.5.norm2.weight])[symbol:visual.blocks.5.norm2.weight]
            tensor.CPU.register () -> (%372:tensor<[1280], Float32, CPU>[@visual.blocks.5.norm2.bias])[symbol:visual.blocks.5.norm2.bias]
            tensor.CPU.register () -> (%92:tensor<[3727360], UInt8, CPU>[@visual.blocks.5.mlp.fc1.weight])[symbol:visual.blocks.5.mlp.fc1.weight]
            tensor.CPU.register () -> (%522:tensor<[3696640], UInt8, CPU>[@visual.blocks.5.mlp.fc2.weight])[symbol:visual.blocks.5.mlp.fc2.weight]
            tensor.CPU.register () -> (%369:tensor<[1280], Float32, CPU>[@visual.blocks.6.norm1.weight])[symbol:visual.blocks.6.norm1.weight]
            tensor.CPU.register () -> (%211:tensor<[1280], Float32, CPU>[@visual.blocks.6.norm1.bias])[symbol:visual.blocks.6.norm1.bias]
            tensor.CPU.register () -> (%105:tensor<[2795520], UInt8, CPU>[@visual.blocks.6.attn.qkv.weight])[symbol:visual.blocks.6.attn.qkv.weight]
            tensor.CPU.register () -> (%323:tensor<[931840], UInt8, CPU>[@visual.blocks.6.attn.proj.weight])[symbol:visual.blocks.6.attn.proj.weight]
            tensor.CPU.register () -> (%529:tensor<[1280], Float32, CPU>[@visual.blocks.6.norm2.weight])[symbol:visual.blocks.6.norm2.weight]
            tensor.CPU.register () -> (%425:tensor<[1280], Float32, CPU>[@visual.blocks.6.norm2.bias])[symbol:visual.blocks.6.norm2.bias]
            tensor.CPU.register () -> (%90:tensor<[3727360], UInt8, CPU>[@visual.blocks.6.mlp.fc1.weight])[symbol:visual.blocks.6.mlp.fc1.weight]
            tensor.CPU.register () -> (%483:tensor<[3696640], UInt8, CPU>[@visual.blocks.6.mlp.fc2.weight])[symbol:visual.blocks.6.mlp.fc2.weight]
            tensor.CPU.register () -> (%210:tensor<[1280], Float32, CPU>[@visual.blocks.7.norm1.weight])[symbol:visual.blocks.7.norm1.weight]
            tensor.CPU.register () -> (%258:tensor<[1280], Float32, CPU>[@visual.blocks.7.norm1.bias])[symbol:visual.blocks.7.norm1.bias]
            tensor.CPU.register () -> (%227:tensor<[2795520], UInt8, CPU>[@visual.blocks.7.attn.qkv.weight])[symbol:visual.blocks.7.attn.qkv.weight]
            tensor.CPU.register () -> (%123:tensor<[931840], UInt8, CPU>[@visual.blocks.7.attn.proj.weight])[symbol:visual.blocks.7.attn.proj.weight]
            tensor.CPU.register () -> (%208:tensor<[1280], Float32, CPU>[@visual.blocks.7.norm2.weight])[symbol:visual.blocks.7.norm2.weight]
            tensor.CPU.register () -> (%209:tensor<[1280], Float32, CPU>[@visual.blocks.7.norm2.bias])[symbol:visual.blocks.7.norm2.bias]
            tensor.CPU.register () -> (%450:tensor<[3727360], UInt8, CPU>[@visual.blocks.7.mlp.fc1.weight])[symbol:visual.blocks.7.mlp.fc1.weight]
            tensor.CPU.register () -> (%573:tensor<[3696640], UInt8, CPU>[@visual.blocks.7.mlp.fc2.weight])[symbol:visual.blocks.7.mlp.fc2.weight]
            tensor.CPU.register () -> (%501:tensor<[1280], Float32, CPU>[@visual.blocks.8.norm1.weight])[symbol:visual.blocks.8.norm1.weight]
            tensor.CPU.register () -> (%207:tensor<[1280], Float32, CPU>[@visual.blocks.8.norm1.bias])[symbol:visual.blocks.8.norm1.bias]
            tensor.CPU.register () -> (%431:tensor<[2795520], UInt8, CPU>[@visual.blocks.8.attn.qkv.weight])[symbol:visual.blocks.8.attn.qkv.weight]
            tensor.CPU.register () -> (%118:tensor<[931840], UInt8, CPU>[@visual.blocks.8.attn.proj.weight])[symbol:visual.blocks.8.attn.proj.weight]
            tensor.CPU.register () -> (%205:tensor<[1280], Float32, CPU>[@visual.blocks.8.norm2.weight])[symbol:visual.blocks.8.norm2.weight]
            tensor.CPU.register () -> (%206:tensor<[1280], Float32, CPU>[@visual.blocks.8.norm2.bias])[symbol:visual.blocks.8.norm2.bias]
            tensor.CPU.register () -> (%94:tensor<[3727360], UInt8, CPU>[@visual.blocks.8.mlp.fc1.weight])[symbol:visual.blocks.8.mlp.fc1.weight]
            tensor.CPU.register () -> (%72:tensor<[3696640], UInt8, CPU>[@visual.blocks.8.mlp.fc2.weight])[symbol:visual.blocks.8.mlp.fc2.weight]
            tensor.CPU.register () -> (%486:tensor<[1280], Float32, CPU>[@visual.blocks.9.norm1.weight])[symbol:visual.blocks.9.norm1.weight]
            tensor.CPU.register () -> (%204:tensor<[1280], Float32, CPU>[@visual.blocks.9.norm1.bias])[symbol:visual.blocks.9.norm1.bias]
            tensor.CPU.register () -> (%544:tensor<[2795520], UInt8, CPU>[@visual.blocks.9.attn.qkv.weight])[symbol:visual.blocks.9.attn.qkv.weight]
            tensor.CPU.register () -> (%316:tensor<[931840], UInt8, CPU>[@visual.blocks.9.attn.proj.weight])[symbol:visual.blocks.9.attn.proj.weight]
            tensor.CPU.register () -> (%513:tensor<[1280], Float32, CPU>[@visual.blocks.9.norm2.weight])[symbol:visual.blocks.9.norm2.weight]
            tensor.CPU.register () -> (%203:tensor<[1280], Float32, CPU>[@visual.blocks.9.norm2.bias])[symbol:visual.blocks.9.norm2.bias]
            tensor.CPU.register () -> (%96:tensor<[3727360], UInt8, CPU>[@visual.blocks.9.mlp.fc1.weight])[symbol:visual.blocks.9.mlp.fc1.weight]
            tensor.CPU.register () -> (%75:tensor<[3696640], UInt8, CPU>[@visual.blocks.9.mlp.fc2.weight])[symbol:visual.blocks.9.mlp.fc2.weight]
            tensor.CPU.register () -> (%560:tensor<[1280], Float32, CPU>[@visual.blocks.10.norm1.weight])[symbol:visual.blocks.10.norm1.weight]
            tensor.CPU.register () -> (%308:tensor<[1280], Float32, CPU>[@visual.blocks.10.norm1.bias])[symbol:visual.blocks.10.norm1.bias]
            tensor.CPU.register () -> (%459:tensor<[2795520], UInt8, CPU>[@visual.blocks.10.attn.qkv.weight])[symbol:visual.blocks.10.attn.qkv.weight]
            tensor.CPU.register () -> (%346:tensor<[931840], UInt8, CPU>[@visual.blocks.10.attn.proj.weight])[symbol:visual.blocks.10.attn.proj.weight]
            tensor.CPU.register () -> (%305:tensor<[1280], Float32, CPU>[@visual.blocks.10.norm2.weight])[symbol:visual.blocks.10.norm2.weight]
            tensor.CPU.register () -> (%427:tensor<[1280], Float32, CPU>[@visual.blocks.10.norm2.bias])[symbol:visual.blocks.10.norm2.bias]
            tensor.CPU.register () -> (%76:tensor<[3727360], UInt8, CPU>[@visual.blocks.10.mlp.fc1.weight])[symbol:visual.blocks.10.mlp.fc1.weight]
            tensor.CPU.register () -> (%61:tensor<[3696640], UInt8, CPU>[@visual.blocks.10.mlp.fc2.weight])[symbol:visual.blocks.10.mlp.fc2.weight]
            tensor.CPU.register () -> (%302:tensor<[1280], Float32, CPU>[@visual.blocks.11.norm1.weight])[symbol:visual.blocks.11.norm1.weight]
            tensor.CPU.register () -> (%303:tensor<[1280], Float32, CPU>[@visual.blocks.11.norm1.bias])[symbol:visual.blocks.11.norm1.bias]
            tensor.CPU.register () -> (%523:tensor<[2795520], UInt8, CPU>[@visual.blocks.11.attn.qkv.weight])[symbol:visual.blocks.11.attn.qkv.weight]
            tensor.CPU.register () -> (%304:tensor<[931840], UInt8, CPU>[@visual.blocks.11.attn.proj.weight])[symbol:visual.blocks.11.attn.proj.weight]
            tensor.CPU.register () -> (%298:tensor<[1280], Float32, CPU>[@visual.blocks.11.norm2.weight])[symbol:visual.blocks.11.norm2.weight]
            tensor.CPU.register () -> (%300:tensor<[1280], Float32, CPU>[@visual.blocks.11.norm2.bias])[symbol:visual.blocks.11.norm2.bias]
            tensor.CPU.register () -> (%79:tensor<[3727360], UInt8, CPU>[@visual.blocks.11.mlp.fc1.weight])[symbol:visual.blocks.11.mlp.fc1.weight]
            tensor.CPU.register () -> (%71:tensor<[3696640], UInt8, CPU>[@visual.blocks.11.mlp.fc2.weight])[symbol:visual.blocks.11.mlp.fc2.weight]
            tensor.CPU.register () -> (%312:tensor<[1280], Float32, CPU>[@visual.blocks.12.norm1.weight])[symbol:visual.blocks.12.norm1.weight]
            tensor.CPU.register () -> (%294:tensor<[1280], Float32, CPU>[@visual.blocks.12.norm1.bias])[symbol:visual.blocks.12.norm1.bias]
            tensor.CPU.register () -> (%295:tensor<[2795520], UInt8, CPU>[@visual.blocks.12.attn.qkv.weight])[symbol:visual.blocks.12.attn.qkv.weight]
            tensor.CPU.register () -> (%296:tensor<[931840], UInt8, CPU>[@visual.blocks.12.attn.proj.weight])[symbol:visual.blocks.12.attn.proj.weight]
            tensor.CPU.register () -> (%293:tensor<[1280], Float32, CPU>[@visual.blocks.12.norm2.weight])[symbol:visual.blocks.12.norm2.weight]
            tensor.CPU.register () -> (%518:tensor<[1280], Float32, CPU>[@visual.blocks.12.norm2.bias])[symbol:visual.blocks.12.norm2.bias]
            tensor.CPU.register () -> (%78:tensor<[3727360], UInt8, CPU>[@visual.blocks.12.mlp.fc1.weight])[symbol:visual.blocks.12.mlp.fc1.weight]
            tensor.CPU.register () -> (%68:tensor<[3696640], UInt8, CPU>[@visual.blocks.12.mlp.fc2.weight])[symbol:visual.blocks.12.mlp.fc2.weight]
            tensor.CPU.register () -> (%287:tensor<[1280], Float32, CPU>[@visual.blocks.13.norm1.weight])[symbol:visual.blocks.13.norm1.weight]
            tensor.CPU.register () -> (%289:tensor<[1280], Float32, CPU>[@visual.blocks.13.norm1.bias])[symbol:visual.blocks.13.norm1.bias]
            tensor.CPU.register () -> (%516:tensor<[2795520], UInt8, CPU>[@visual.blocks.13.attn.qkv.weight])[symbol:visual.blocks.13.attn.qkv.weight]
            tensor.CPU.register () -> (%115:tensor<[931840], UInt8, CPU>[@visual.blocks.13.attn.proj.weight])[symbol:visual.blocks.13.attn.proj.weight]
            tensor.CPU.register () -> (%285:tensor<[1280], Float32, CPU>[@visual.blocks.13.norm2.weight])[symbol:visual.blocks.13.norm2.weight]
            tensor.CPU.register () -> (%286:tensor<[1280], Float32, CPU>[@visual.blocks.13.norm2.bias])[symbol:visual.blocks.13.norm2.bias]
            tensor.CPU.register () -> (%493:tensor<[3727360], UInt8, CPU>[@visual.blocks.13.mlp.fc1.weight])[symbol:visual.blocks.13.mlp.fc1.weight]
            tensor.CPU.register () -> (%290:tensor<[3696640], UInt8, CPU>[@visual.blocks.13.mlp.fc2.weight])[symbol:visual.blocks.13.mlp.fc2.weight]
            tensor.CPU.register () -> (%497:tensor<[1280], Float32, CPU>[@visual.blocks.14.norm1.weight])[symbol:visual.blocks.14.norm1.weight]
            tensor.CPU.register () -> (%288:tensor<[1280], Float32, CPU>[@visual.blocks.14.norm1.bias])[symbol:visual.blocks.14.norm1.bias]
            tensor.CPU.register () -> (%110:tensor<[2795520], UInt8, CPU>[@visual.blocks.14.attn.qkv.weight])[symbol:visual.blocks.14.attn.qkv.weight]
            tensor.CPU.register () -> (%282:tensor<[931840], UInt8, CPU>[@visual.blocks.14.attn.proj.weight])[symbol:visual.blocks.14.attn.proj.weight]
            tensor.CPU.register () -> (%280:tensor<[1280], Float32, CPU>[@visual.blocks.14.norm2.weight])[symbol:visual.blocks.14.norm2.weight]
            tensor.CPU.register () -> (%281:tensor<[1280], Float32, CPU>[@visual.blocks.14.norm2.bias])[symbol:visual.blocks.14.norm2.bias]
            tensor.CPU.register () -> (%80:tensor<[3727360], UInt8, CPU>[@visual.blocks.14.mlp.fc1.weight])[symbol:visual.blocks.14.mlp.fc1.weight]
            tensor.CPU.register () -> (%73:tensor<[3696640], UInt8, CPU>[@visual.blocks.14.mlp.fc2.weight])[symbol:visual.blocks.14.mlp.fc2.weight]
            tensor.CPU.register () -> (%274:tensor<[1280], Float32, CPU>[@visual.blocks.15.norm1.weight])[symbol:visual.blocks.15.norm1.weight]
            tensor.CPU.register () -> (%275:tensor<[1280], Float32, CPU>[@visual.blocks.15.norm1.bias])[symbol:visual.blocks.15.norm1.bias]
            tensor.CPU.register () -> (%276:tensor<[2795520], UInt8, CPU>[@visual.blocks.15.attn.qkv.weight])[symbol:visual.blocks.15.attn.qkv.weight]
            tensor.CPU.register () -> (%547:tensor<[931840], UInt8, CPU>[@visual.blocks.15.attn.proj.weight])[symbol:visual.blocks.15.attn.proj.weight]
            tensor.CPU.register () -> (%496:tensor<[1280], Float32, CPU>[@visual.blocks.15.norm2.weight])[symbol:visual.blocks.15.norm2.weight]
            tensor.CPU.register () -> (%348:tensor<[1280], Float32, CPU>[@visual.blocks.15.norm2.bias])[symbol:visual.blocks.15.norm2.bias]
            tensor.CPU.register () -> (%86:tensor<[3727360], UInt8, CPU>[@visual.blocks.15.mlp.fc1.weight])[symbol:visual.blocks.15.mlp.fc1.weight]
            tensor.CPU.register () -> (%314:tensor<[3696640], UInt8, CPU>[@visual.blocks.15.mlp.fc2.weight])[symbol:visual.blocks.15.mlp.fc2.weight]
            tensor.CPU.register () -> (%271:tensor<[1280], Float32, CPU>[@visual.blocks.16.norm1.weight])[symbol:visual.blocks.16.norm1.weight]
            tensor.CPU.register () -> (%272:tensor<[1280], Float32, CPU>[@visual.blocks.16.norm1.bias])[symbol:visual.blocks.16.norm1.bias]
            tensor.CPU.register () -> (%568:tensor<[2795520], UInt8, CPU>[@visual.blocks.16.attn.qkv.weight])[symbol:visual.blocks.16.attn.qkv.weight]
            tensor.CPU.register () -> (%273:tensor<[931840], UInt8, CPU>[@visual.blocks.16.attn.proj.weight])[symbol:visual.blocks.16.attn.proj.weight]
            tensor.CPU.register () -> (%270:tensor<[1280], Float32, CPU>[@visual.blocks.16.norm2.weight])[symbol:visual.blocks.16.norm2.weight]
            tensor.CPU.register () -> (%284:tensor<[1280], Float32, CPU>[@visual.blocks.16.norm2.bias])[symbol:visual.blocks.16.norm2.bias]
            tensor.CPU.register () -> (%93:tensor<[3727360], UInt8, CPU>[@visual.blocks.16.mlp.fc1.weight])[symbol:visual.blocks.16.mlp.fc1.weight]
            tensor.CPU.register () -> (%500:tensor<[3696640], UInt8, CPU>[@visual.blocks.16.mlp.fc2.weight])[symbol:visual.blocks.16.mlp.fc2.weight]
            tensor.CPU.register () -> (%266:tensor<[1280], Float32, CPU>[@visual.blocks.17.norm1.weight])[symbol:visual.blocks.17.norm1.weight]
            tensor.CPU.register () -> (%267:tensor<[1280], Float32, CPU>[@visual.blocks.17.norm1.bias])[symbol:visual.blocks.17.norm1.bias]
            tensor.CPU.register () -> (%100:tensor<[2795520], UInt8, CPU>[@visual.blocks.17.attn.qkv.weight])[symbol:visual.blocks.17.attn.qkv.weight]
            tensor.CPU.register () -> (%422:tensor<[931840], UInt8, CPU>[@visual.blocks.17.attn.proj.weight])[symbol:visual.blocks.17.attn.proj.weight]
            tensor.CPU.register () -> (%556:tensor<[1280], Float32, CPU>[@visual.blocks.17.norm2.weight])[symbol:visual.blocks.17.norm2.weight]
            tensor.CPU.register () -> (%503:tensor<[1280], Float32, CPU>[@visual.blocks.17.norm2.bias])[symbol:visual.blocks.17.norm2.bias]
            tensor.CPU.register () -> (%268:tensor<[3727360], UInt8, CPU>[@visual.blocks.17.mlp.fc1.weight])[symbol:visual.blocks.17.mlp.fc1.weight]
            tensor.CPU.register () -> (%69:tensor<[3696640], UInt8, CPU>[@visual.blocks.17.mlp.fc2.weight])[symbol:visual.blocks.17.mlp.fc2.weight]
            tensor.CPU.register () -> (%265:tensor<[1280], Float32, CPU>[@visual.blocks.18.norm1.weight])[symbol:visual.blocks.18.norm1.weight]
            tensor.CPU.register () -> (%479:tensor<[1280], Float32, CPU>[@visual.blocks.18.norm1.bias])[symbol:visual.blocks.18.norm1.bias]
            tensor.CPU.register () -> (%352:tensor<[2795520], UInt8, CPU>[@visual.blocks.18.attn.qkv.weight])[symbol:visual.blocks.18.attn.qkv.weight]
            tensor.CPU.register () -> (%384:tensor<[931840], UInt8, CPU>[@visual.blocks.18.attn.proj.weight])[symbol:visual.blocks.18.attn.proj.weight]
            tensor.CPU.register () -> (%264:tensor<[1280], Float32, CPU>[@visual.blocks.18.norm2.weight])[symbol:visual.blocks.18.norm2.weight]
            tensor.CPU.register () -> (%473:tensor<[1280], Float32, CPU>[@visual.blocks.18.norm2.bias])[symbol:visual.blocks.18.norm2.bias]
            tensor.CPU.register () -> (%87:tensor<[3727360], UInt8, CPU>[@visual.blocks.18.mlp.fc1.weight])[symbol:visual.blocks.18.mlp.fc1.weight]
            tensor.CPU.register () -> (%401:tensor<[3696640], UInt8, CPU>[@visual.blocks.18.mlp.fc2.weight])[symbol:visual.blocks.18.mlp.fc2.weight]
            tensor.CPU.register () -> (%262:tensor<[1280], Float32, CPU>[@visual.blocks.19.norm1.weight])[symbol:visual.blocks.19.norm1.weight]
            tensor.CPU.register () -> (%407:tensor<[1280], Float32, CPU>[@visual.blocks.19.norm1.bias])[symbol:visual.blocks.19.norm1.bias]
            tensor.CPU.register () -> (%98:tensor<[2795520], UInt8, CPU>[@visual.blocks.19.attn.qkv.weight])[symbol:visual.blocks.19.attn.qkv.weight]
            tensor.CPU.register () -> (%113:tensor<[931840], UInt8, CPU>[@visual.blocks.19.attn.proj.weight])[symbol:visual.blocks.19.attn.proj.weight]
            tensor.CPU.register () -> (%347:tensor<[1280], Float32, CPU>[@visual.blocks.19.norm2.weight])[symbol:visual.blocks.19.norm2.weight]
            tensor.CPU.register () -> (%259:tensor<[1280], Float32, CPU>[@visual.blocks.19.norm2.bias])[symbol:visual.blocks.19.norm2.bias]
            tensor.CPU.register () -> (%261:tensor<[3727360], UInt8, CPU>[@visual.blocks.19.mlp.fc1.weight])[symbol:visual.blocks.19.mlp.fc1.weight]
            tensor.CPU.register () -> (%260:tensor<[3696640], UInt8, CPU>[@visual.blocks.19.mlp.fc2.weight])[symbol:visual.blocks.19.mlp.fc2.weight]
            tensor.CPU.register () -> (%462:tensor<[1280], Float32, CPU>[@visual.blocks.20.norm1.weight])[symbol:visual.blocks.20.norm1.weight]
            tensor.CPU.register () -> (%255:tensor<[1280], Float32, CPU>[@visual.blocks.20.norm1.bias])[symbol:visual.blocks.20.norm1.bias]
            tensor.CPU.register () -> (%102:tensor<[2795520], UInt8, CPU>[@visual.blocks.20.attn.qkv.weight])[symbol:visual.blocks.20.attn.qkv.weight]
            tensor.CPU.register () -> (%468:tensor<[931840], UInt8, CPU>[@visual.blocks.20.attn.proj.weight])[symbol:visual.blocks.20.attn.proj.weight]
            tensor.CPU.register () -> (%543:tensor<[1280], Float32, CPU>[@visual.blocks.20.norm2.weight])[symbol:visual.blocks.20.norm2.weight]
            tensor.CPU.register () -> (%508:tensor<[1280], Float32, CPU>[@visual.blocks.20.norm2.bias])[symbol:visual.blocks.20.norm2.bias]
            tensor.CPU.register () -> (%340:tensor<[3727360], UInt8, CPU>[@visual.blocks.20.mlp.fc1.weight])[symbol:visual.blocks.20.mlp.fc1.weight]
            tensor.CPU.register () -> (%397:tensor<[3696640], UInt8, CPU>[@visual.blocks.20.mlp.fc2.weight])[symbol:visual.blocks.20.mlp.fc2.weight]
            tensor.CPU.register () -> (%250:tensor<[1280], Float32, CPU>[@visual.blocks.21.norm1.weight])[symbol:visual.blocks.21.norm1.weight]
            tensor.CPU.register () -> (%515:tensor<[1280], Float32, CPU>[@visual.blocks.21.norm1.bias])[symbol:visual.blocks.21.norm1.bias]
            tensor.CPU.register () -> (%254:tensor<[2795520], UInt8, CPU>[@visual.blocks.21.attn.qkv.weight])[symbol:visual.blocks.21.attn.qkv.weight]
            tensor.CPU.register () -> (%114:tensor<[931840], UInt8, CPU>[@visual.blocks.21.attn.proj.weight])[symbol:visual.blocks.21.attn.proj.weight]
            tensor.CPU.register () -> (%247:tensor<[1280], Float32, CPU>[@visual.blocks.21.norm2.weight])[symbol:visual.blocks.21.norm2.weight]
            tensor.CPU.register () -> (%248:tensor<[1280], Float32, CPU>[@visual.blocks.21.norm2.bias])[symbol:visual.blocks.21.norm2.bias]
            tensor.CPU.register () -> (%95:tensor<[3727360], UInt8, CPU>[@visual.blocks.21.mlp.fc1.weight])[symbol:visual.blocks.21.mlp.fc1.weight]
            tensor.CPU.register () -> (%251:tensor<[3696640], UInt8, CPU>[@visual.blocks.21.mlp.fc2.weight])[symbol:visual.blocks.21.mlp.fc2.weight]
            tensor.CPU.register () -> (%277:tensor<[1280], Float32, CPU>[@visual.blocks.22.norm1.weight])[symbol:visual.blocks.22.norm1.weight]
            tensor.CPU.register () -> (%246:tensor<[1280], Float32, CPU>[@visual.blocks.22.norm1.bias])[symbol:visual.blocks.22.norm1.bias]
            tensor.CPU.register () -> (%252:tensor<[2795520], UInt8, CPU>[@visual.blocks.22.attn.qkv.weight])[symbol:visual.blocks.22.attn.qkv.weight]
            tensor.CPU.register () -> (%376:tensor<[931840], UInt8, CPU>[@visual.blocks.22.attn.proj.weight])[symbol:visual.blocks.22.attn.proj.weight]
            tensor.CPU.register () -> (%244:tensor<[1280], Float32, CPU>[@visual.blocks.22.norm2.weight])[symbol:visual.blocks.22.norm2.weight]
            tensor.CPU.register () -> (%245:tensor<[1280], Float32, CPU>[@visual.blocks.22.norm2.bias])[symbol:visual.blocks.22.norm2.bias]
            tensor.CPU.register () -> (%83:tensor<[3727360], UInt8, CPU>[@visual.blocks.22.mlp.fc1.weight])[symbol:visual.blocks.22.mlp.fc1.weight]
            tensor.CPU.register () -> (%64:tensor<[3696640], UInt8, CPU>[@visual.blocks.22.mlp.fc2.weight])[symbol:visual.blocks.22.mlp.fc2.weight]
            tensor.CPU.register () -> (%511:tensor<[1280], Float32, CPU>[@visual.blocks.23.norm1.weight])[symbol:visual.blocks.23.norm1.weight]
            tensor.CPU.register () -> (%383:tensor<[1280], Float32, CPU>[@visual.blocks.23.norm1.bias])[symbol:visual.blocks.23.norm1.bias]
            tensor.CPU.register () -> (%242:tensor<[2795520], UInt8, CPU>[@visual.blocks.23.attn.qkv.weight])[symbol:visual.blocks.23.attn.qkv.weight]
            tensor.CPU.register () -> (%121:tensor<[931840], UInt8, CPU>[@visual.blocks.23.attn.proj.weight])[symbol:visual.blocks.23.attn.proj.weight]
            tensor.CPU.register () -> (%392:tensor<[1280], Float32, CPU>[@visual.blocks.23.norm2.weight])[symbol:visual.blocks.23.norm2.weight]
            tensor.CPU.register () -> (%394:tensor<[1280], Float32, CPU>[@visual.blocks.23.norm2.bias])[symbol:visual.blocks.23.norm2.bias]
            tensor.CPU.register () -> (%84:tensor<[3727360], UInt8, CPU>[@visual.blocks.23.mlp.fc1.weight])[symbol:visual.blocks.23.mlp.fc1.weight]
            tensor.CPU.register () -> (%66:tensor<[3696640], UInt8, CPU>[@visual.blocks.23.mlp.fc2.weight])[symbol:visual.blocks.23.mlp.fc2.weight]
            tensor.CPU.register () -> (%240:tensor<[1280], Float32, CPU>[@visual.blocks.24.norm1.weight])[symbol:visual.blocks.24.norm1.weight]
            tensor.CPU.register () -> (%241:tensor<[1280], Float32, CPU>[@visual.blocks.24.norm1.bias])[symbol:visual.blocks.24.norm1.bias]
            tensor.CPU.register () -> (%104:tensor<[2795520], UInt8, CPU>[@visual.blocks.24.attn.qkv.weight])[symbol:visual.blocks.24.attn.qkv.weight]
            tensor.CPU.register () -> (%119:tensor<[931840], UInt8, CPU>[@visual.blocks.24.attn.proj.weight])[symbol:visual.blocks.24.attn.proj.weight]
            tensor.CPU.register () -> (%238:tensor<[1280], Float32, CPU>[@visual.blocks.24.norm2.weight])[symbol:visual.blocks.24.norm2.weight]
            tensor.CPU.register () -> (%239:tensor<[1280], Float32, CPU>[@visual.blocks.24.norm2.bias])[symbol:visual.blocks.24.norm2.bias]
            tensor.CPU.register () -> (%563:tensor<[3727360], UInt8, CPU>[@visual.blocks.24.mlp.fc1.weight])[symbol:visual.blocks.24.mlp.fc1.weight]
            tensor.CPU.register () -> (%70:tensor<[3696640], UInt8, CPU>[@visual.blocks.24.mlp.fc2.weight])[symbol:visual.blocks.24.mlp.fc2.weight]
            tensor.CPU.register () -> (%234:tensor<[1280], Float32, CPU>[@visual.blocks.25.norm1.weight])[symbol:visual.blocks.25.norm1.weight]
            tensor.CPU.register () -> (%235:tensor<[1280], Float32, CPU>[@visual.blocks.25.norm1.bias])[symbol:visual.blocks.25.norm1.bias]
            tensor.CPU.register () -> (%97:tensor<[2795520], UInt8, CPU>[@visual.blocks.25.attn.qkv.weight])[symbol:visual.blocks.25.attn.qkv.weight]
            tensor.CPU.register () -> (%353:tensor<[931840], UInt8, CPU>[@visual.blocks.25.attn.proj.weight])[symbol:visual.blocks.25.attn.proj.weight]
            tensor.CPU.register () -> (%232:tensor<[1280], Float32, CPU>[@visual.blocks.25.norm2.weight])[symbol:visual.blocks.25.norm2.weight]
            tensor.CPU.register () -> (%233:tensor<[1280], Float32, CPU>[@visual.blocks.25.norm2.bias])[symbol:visual.blocks.25.norm2.bias]
            tensor.CPU.register () -> (%236:tensor<[3727360], UInt8, CPU>[@visual.blocks.25.mlp.fc1.weight])[symbol:visual.blocks.25.mlp.fc1.weight]
            tensor.CPU.register () -> (%67:tensor<[3696640], UInt8, CPU>[@visual.blocks.25.mlp.fc2.weight])[symbol:visual.blocks.25.mlp.fc2.weight]
            tensor.CPU.register () -> (%231:tensor<[1280], Float32, CPU>[@visual.blocks.26.norm1.weight])[symbol:visual.blocks.26.norm1.weight]
            tensor.CPU.register () -> (%256:tensor<[1280], Float32, CPU>[@visual.blocks.26.norm1.bias])[symbol:visual.blocks.26.norm1.bias]
            tensor.CPU.register () -> (%106:tensor<[2795520], UInt8, CPU>[@visual.blocks.26.attn.qkv.weight])[symbol:visual.blocks.26.attn.qkv.weight]
            tensor.CPU.register () -> (%116:tensor<[931840], UInt8, CPU>[@visual.blocks.26.attn.proj.weight])[symbol:visual.blocks.26.attn.proj.weight]
            tensor.CPU.register () -> (%230:tensor<[1280], Float32, CPU>[@visual.blocks.26.norm2.weight])[symbol:visual.blocks.26.norm2.weight]
            tensor.CPU.register () -> (%283:tensor<[1280], Float32, CPU>[@visual.blocks.26.norm2.bias])[symbol:visual.blocks.26.norm2.bias]
            tensor.CPU.register () -> (%81:tensor<[3727360], UInt8, CPU>[@visual.blocks.26.mlp.fc1.weight])[symbol:visual.blocks.26.mlp.fc1.weight]
            tensor.CPU.register () -> (%63:tensor<[3696640], UInt8, CPU>[@visual.blocks.26.mlp.fc2.weight])[symbol:visual.blocks.26.mlp.fc2.weight]
            tensor.CPU.register () -> (%291:tensor<[1280], Float32, CPU>[@visual.blocks.27.norm1.weight])[symbol:visual.blocks.27.norm1.weight]
            tensor.CPU.register () -> (%292:tensor<[1280], Float32, CPU>[@visual.blocks.27.norm1.bias])[symbol:visual.blocks.27.norm1.bias]
            tensor.CPU.register () -> (%107:tensor<[2795520], UInt8, CPU>[@visual.blocks.27.attn.qkv.weight])[symbol:visual.blocks.27.attn.qkv.weight]
            tensor.CPU.register () -> (%448:tensor<[931840], UInt8, CPU>[@visual.blocks.27.attn.proj.weight])[symbol:visual.blocks.27.attn.proj.weight]
            tensor.CPU.register () -> (%228:tensor<[1280], Float32, CPU>[@visual.blocks.27.norm2.weight])[symbol:visual.blocks.27.norm2.weight]
            tensor.CPU.register () -> (%229:tensor<[1280], Float32, CPU>[@visual.blocks.27.norm2.bias])[symbol:visual.blocks.27.norm2.bias]
            tensor.CPU.register () -> (%85:tensor<[3727360], UInt8, CPU>[@visual.blocks.27.mlp.fc1.weight])[symbol:visual.blocks.27.mlp.fc1.weight]
            tensor.CPU.register () -> (%60:tensor<[3696640], UInt8, CPU>[@visual.blocks.27.mlp.fc2.weight])[symbol:visual.blocks.27.mlp.fc2.weight]
            tensor.CPU.register () -> (%225:tensor<[1280], Float32, CPU>[@visual.blocks.28.norm1.weight])[symbol:visual.blocks.28.norm1.weight]
            tensor.CPU.register () -> (%226:tensor<[1280], Float32, CPU>[@visual.blocks.28.norm1.bias])[symbol:visual.blocks.28.norm1.bias]
            tensor.CPU.register () -> (%552:tensor<[2795520], UInt8, CPU>[@visual.blocks.28.attn.qkv.weight])[symbol:visual.blocks.28.attn.qkv.weight]
            tensor.CPU.register () -> (%120:tensor<[931840], UInt8, CPU>[@visual.blocks.28.attn.proj.weight])[symbol:visual.blocks.28.attn.proj.weight]
            tensor.CPU.register () -> (%224:tensor<[1280], Float32, CPU>[@visual.blocks.28.norm2.weight])[symbol:visual.blocks.28.norm2.weight]
            tensor.CPU.register () -> (%472:tensor<[1280], Float32, CPU>[@visual.blocks.28.norm2.bias])[symbol:visual.blocks.28.norm2.bias]
            tensor.CPU.register () -> (%88:tensor<[3727360], UInt8, CPU>[@visual.blocks.28.mlp.fc1.weight])[symbol:visual.blocks.28.mlp.fc1.weight]
            tensor.CPU.register () -> (%365:tensor<[3696640], UInt8, CPU>[@visual.blocks.28.mlp.fc2.weight])[symbol:visual.blocks.28.mlp.fc2.weight]
            tensor.CPU.register () -> (%243:tensor<[1280], Float32, CPU>[@visual.blocks.29.norm1.weight])[symbol:visual.blocks.29.norm1.weight]
            tensor.CPU.register () -> (%535:tensor<[1280], Float32, CPU>[@visual.blocks.29.norm1.bias])[symbol:visual.blocks.29.norm1.bias]
            tensor.CPU.register () -> (%441:tensor<[2795520], UInt8, CPU>[@visual.blocks.29.attn.qkv.weight])[symbol:visual.blocks.29.attn.qkv.weight]
            tensor.CPU.register () -> (%403:tensor<[931840], UInt8, CPU>[@visual.blocks.29.attn.proj.weight])[symbol:visual.blocks.29.attn.proj.weight]
            tensor.CPU.register () -> (%223:tensor<[1280], Float32, CPU>[@visual.blocks.29.norm2.weight])[symbol:visual.blocks.29.norm2.weight]
            tensor.CPU.register () -> (%279:tensor<[1280], Float32, CPU>[@visual.blocks.29.norm2.bias])[symbol:visual.blocks.29.norm2.bias]
            tensor.CPU.register () -> (%91:tensor<[3727360], UInt8, CPU>[@visual.blocks.29.mlp.fc1.weight])[symbol:visual.blocks.29.mlp.fc1.weight]
            tensor.CPU.register () -> (%249:tensor<[3696640], UInt8, CPU>[@visual.blocks.29.mlp.fc2.weight])[symbol:visual.blocks.29.mlp.fc2.weight]
            tensor.CPU.register () -> (%220:tensor<[1280], Float32, CPU>[@visual.blocks.30.norm1.weight])[symbol:visual.blocks.30.norm1.weight]
            tensor.CPU.register () -> (%507:tensor<[1280], Float32, CPU>[@visual.blocks.30.norm1.bias])[symbol:visual.blocks.30.norm1.bias]
            tensor.CPU.register () -> (%109:tensor<[2795520], UInt8, CPU>[@visual.blocks.30.attn.qkv.weight])[symbol:visual.blocks.30.attn.qkv.weight]
            tensor.CPU.register () -> (%278:tensor<[931840], UInt8, CPU>[@visual.blocks.30.attn.proj.weight])[symbol:visual.blocks.30.attn.proj.weight]
            tensor.CPU.register () -> (%218:tensor<[1280], Float32, CPU>[@visual.blocks.30.norm2.weight])[symbol:visual.blocks.30.norm2.weight]
            tensor.CPU.register () -> (%219:tensor<[1280], Float32, CPU>[@visual.blocks.30.norm2.bias])[symbol:visual.blocks.30.norm2.bias]
            tensor.CPU.register () -> (%443:tensor<[3727360], UInt8, CPU>[@visual.blocks.30.mlp.fc1.weight])[symbol:visual.blocks.30.mlp.fc1.weight]
            tensor.CPU.register () -> (%65:tensor<[3696640], UInt8, CPU>[@visual.blocks.30.mlp.fc2.weight])[symbol:visual.blocks.30.mlp.fc2.weight]
            tensor.CPU.register () -> (%551:tensor<[1280], Float32, CPU>[@visual.blocks.31.norm1.weight])[symbol:visual.blocks.31.norm1.weight]
            tensor.CPU.register () -> (%217:tensor<[1280], Float32, CPU>[@visual.blocks.31.norm1.bias])[symbol:visual.blocks.31.norm1.bias]
            tensor.CPU.register () -> (%99:tensor<[2795520], UInt8, CPU>[@visual.blocks.31.attn.qkv.weight])[symbol:visual.blocks.31.attn.qkv.weight]
            tensor.CPU.register () -> (%122:tensor<[931840], UInt8, CPU>[@visual.blocks.31.attn.proj.weight])[symbol:visual.blocks.31.attn.proj.weight]
            tensor.CPU.register () -> (%216:tensor<[1280], Float32, CPU>[@visual.blocks.31.norm2.weight])[symbol:visual.blocks.31.norm2.weight]
            tensor.CPU.register () -> (%567:tensor<[1280], Float32, CPU>[@visual.blocks.31.norm2.bias])[symbol:visual.blocks.31.norm2.bias]
            tensor.CPU.register () -> (%263:tensor<[3727360], UInt8, CPU>[@visual.blocks.31.mlp.fc1.weight])[symbol:visual.blocks.31.mlp.fc1.weight]
            tensor.CPU.register () -> (%306:tensor<[3696640], UInt8, CPU>[@visual.blocks.31.mlp.fc2.weight])[symbol:visual.blocks.31.mlp.fc2.weight]
            tensor.CPU.register () -> (%202:tensor<[1280], Float32, CPU>[@visual.merger.ln_q.weight])[symbol:visual.merger.ln_q.weight]
            tensor.CPU.register () -> (%465:tensor<[1280], Float32, CPU>[@visual.merger.ln_q.bias])[symbol:visual.merger.ln_q.bias]
            tensor.CPU.register () -> (%490:tensor<[14786560], UInt8, CPU>[@visual.merger.mlp.0.weight])[symbol:visual.merger.mlp.0.weight]
            tensor.CPU.register () -> (%253:tensor<[4435968], UInt8, CPU>[@visual.merger.mlp.2.weight])[symbol:visual.merger.mlp.2.weight]
            tensor.CPU.register () -> (%138:tensor<[1339392], UInt8, CPU>[@model.layers.0.self_attn.q_proj.weight])[symbol:model.layers.0.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%574:tensor<[223232], UInt8, CPU>[@model.layers.0.self_attn.k_proj.weight])[symbol:model.layers.0.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%570:tensor<[223232], UInt8, CPU>[@model.layers.0.self_attn.v_proj.weight])[symbol:model.layers.0.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%536:tensor<[1339392], UInt8, CPU>[@model.layers.0.self_attn.o_proj.weight])[symbol:model.layers.0.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%566:tensor<[7813120], UInt8, CPU>[@model.layers.0.mlp.gate_proj.weight])[symbol:model.layers.0.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%170:tensor<[7813120], UInt8, CPU>[@model.layers.0.mlp.up_proj.weight])[symbol:model.layers.0.mlp.up_proj.weight]
            tensor.CPU.register () -> (%187:tensor<[7753728], UInt8, CPU>[@model.layers.0.mlp.down_proj.weight])[symbol:model.layers.0.mlp.down_proj.weight]
            tensor.CPU.register () -> (%137:tensor<[1339392], UInt8, CPU>[@model.layers.1.self_attn.q_proj.weight])[symbol:model.layers.1.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%164:tensor<[223232], UInt8, CPU>[@model.layers.1.self_attn.k_proj.weight])[symbol:model.layers.1.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%299:tensor<[223232], UInt8, CPU>[@model.layers.1.self_attn.v_proj.weight])[symbol:model.layers.1.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%144:tensor<[1339392], UInt8, CPU>[@model.layers.1.self_attn.o_proj.weight])[symbol:model.layers.1.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%506:tensor<[7813120], UInt8, CPU>[@model.layers.1.mlp.gate_proj.weight])[symbol:model.layers.1.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%399:tensor<[7813120], UInt8, CPU>[@model.layers.1.mlp.up_proj.weight])[symbol:model.layers.1.mlp.up_proj.weight]
            tensor.CPU.register () -> (%534:tensor<[7753728], UInt8, CPU>[@model.layers.1.mlp.down_proj.weight])[symbol:model.layers.1.mlp.down_proj.weight]
            tensor.CPU.register () -> (%449:tensor<[1339392], UInt8, CPU>[@model.layers.2.self_attn.q_proj.weight])[symbol:model.layers.2.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%161:tensor<[223232], UInt8, CPU>[@model.layers.2.self_attn.k_proj.weight])[symbol:model.layers.2.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%366:tensor<[223232], UInt8, CPU>[@model.layers.2.self_attn.v_proj.weight])[symbol:model.layers.2.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%147:tensor<[1339392], UInt8, CPU>[@model.layers.2.self_attn.o_proj.weight])[symbol:model.layers.2.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%440:tensor<[7813120], UInt8, CPU>[@model.layers.2.mlp.gate_proj.weight])[symbol:model.layers.2.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%436:tensor<[7813120], UInt8, CPU>[@model.layers.2.mlp.up_proj.weight])[symbol:model.layers.2.mlp.up_proj.weight]
            tensor.CPU.register () -> (%194:tensor<[7753728], UInt8, CPU>[@model.layers.2.mlp.down_proj.weight])[symbol:model.layers.2.mlp.down_proj.weight]
            tensor.CPU.register () -> (%142:tensor<[1339392], UInt8, CPU>[@model.layers.3.self_attn.q_proj.weight])[symbol:model.layers.3.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%356:tensor<[223232], UInt8, CPU>[@model.layers.3.self_attn.k_proj.weight])[symbol:model.layers.3.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%355:tensor<[223232], UInt8, CPU>[@model.layers.3.self_attn.v_proj.weight])[symbol:model.layers.3.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%148:tensor<[1339392], UInt8, CPU>[@model.layers.3.self_attn.o_proj.weight])[symbol:model.layers.3.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%378:tensor<[7813120], UInt8, CPU>[@model.layers.3.mlp.gate_proj.weight])[symbol:model.layers.3.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%173:tensor<[7813120], UInt8, CPU>[@model.layers.3.mlp.up_proj.weight])[symbol:model.layers.3.mlp.up_proj.weight]
            tensor.CPU.register () -> (%200:tensor<[7753728], UInt8, CPU>[@model.layers.3.mlp.down_proj.weight])[symbol:model.layers.3.mlp.down_proj.weight]
            tensor.CPU.register () -> (%530:tensor<[1339392], UInt8, CPU>[@model.layers.4.self_attn.q_proj.weight])[symbol:model.layers.4.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%159:tensor<[223232], UInt8, CPU>[@model.layers.4.self_attn.k_proj.weight])[symbol:model.layers.4.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%558:tensor<[223232], UInt8, CPU>[@model.layers.4.self_attn.v_proj.weight])[symbol:model.layers.4.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%428:tensor<[1339392], UInt8, CPU>[@model.layers.4.self_attn.o_proj.weight])[symbol:model.layers.4.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%354:tensor<[7813120], UInt8, CPU>[@model.layers.4.mlp.gate_proj.weight])[symbol:model.layers.4.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%174:tensor<[7813120], UInt8, CPU>[@model.layers.4.mlp.up_proj.weight])[symbol:model.layers.4.mlp.up_proj.weight]
            tensor.CPU.register () -> (%201:tensor<[7753728], UInt8, CPU>[@model.layers.4.mlp.down_proj.weight])[symbol:model.layers.4.mlp.down_proj.weight]
            tensor.CPU.register () -> (%455:tensor<[1339392], UInt8, CPU>[@model.layers.5.self_attn.q_proj.weight])[symbol:model.layers.5.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%160:tensor<[223232], UInt8, CPU>[@model.layers.5.self_attn.k_proj.weight])[symbol:model.layers.5.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%131:tensor<[223232], UInt8, CPU>[@model.layers.5.self_attn.v_proj.weight])[symbol:model.layers.5.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%398:tensor<[1339392], UInt8, CPU>[@model.layers.5.self_attn.o_proj.weight])[symbol:model.layers.5.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%502:tensor<[7813120], UInt8, CPU>[@model.layers.5.mlp.gate_proj.weight])[symbol:model.layers.5.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%168:tensor<[7813120], UInt8, CPU>[@model.layers.5.mlp.up_proj.weight])[symbol:model.layers.5.mlp.up_proj.weight]
            tensor.CPU.register () -> (%528:tensor<[7753728], UInt8, CPU>[@model.layers.5.mlp.down_proj.weight])[symbol:model.layers.5.mlp.down_proj.weight]
            tensor.CPU.register () -> (%344:tensor<[1339392], UInt8, CPU>[@model.layers.6.self_attn.q_proj.weight])[symbol:model.layers.6.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%564:tensor<[223232], UInt8, CPU>[@model.layers.6.self_attn.k_proj.weight])[symbol:model.layers.6.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%342:tensor<[223232], UInt8, CPU>[@model.layers.6.self_attn.v_proj.weight])[symbol:model.layers.6.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%153:tensor<[1339392], UInt8, CPU>[@model.layers.6.self_attn.o_proj.weight])[symbol:model.layers.6.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%185:tensor<[7813120], UInt8, CPU>[@model.layers.6.mlp.gate_proj.weight])[symbol:model.layers.6.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%332:tensor<[7813120], UInt8, CPU>[@model.layers.6.mlp.up_proj.weight])[symbol:model.layers.6.mlp.up_proj.weight]
            tensor.CPU.register () -> (%309:tensor<[7753728], UInt8, CPU>[@model.layers.6.mlp.down_proj.weight])[symbol:model.layers.6.mlp.down_proj.weight]
            tensor.CPU.register () -> (%438:tensor<[1339392], UInt8, CPU>[@model.layers.7.self_attn.q_proj.weight])[symbol:model.layers.7.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%363:tensor<[223232], UInt8, CPU>[@model.layers.7.self_attn.k_proj.weight])[symbol:model.layers.7.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%127:tensor<[223232], UInt8, CPU>[@model.layers.7.self_attn.v_proj.weight])[symbol:model.layers.7.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%145:tensor<[1339392], UInt8, CPU>[@model.layers.7.self_attn.o_proj.weight])[symbol:model.layers.7.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%184:tensor<[7813120], UInt8, CPU>[@model.layers.7.mlp.gate_proj.weight])[symbol:model.layers.7.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%165:tensor<[7813120], UInt8, CPU>[@model.layers.7.mlp.up_proj.weight])[symbol:model.layers.7.mlp.up_proj.weight]
            tensor.CPU.register () -> (%379:tensor<[7753728], UInt8, CPU>[@model.layers.7.mlp.down_proj.weight])[symbol:model.layers.7.mlp.down_proj.weight]
            tensor.CPU.register () -> (%334:tensor<[1339392], UInt8, CPU>[@model.layers.8.self_attn.q_proj.weight])[symbol:model.layers.8.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%333:tensor<[223232], UInt8, CPU>[@model.layers.8.self_attn.k_proj.weight])[symbol:model.layers.8.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%330:tensor<[223232], UInt8, CPU>[@model.layers.8.self_attn.v_proj.weight])[symbol:model.layers.8.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%154:tensor<[1339392], UInt8, CPU>[@model.layers.8.self_attn.o_proj.weight])[symbol:model.layers.8.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%335:tensor<[7813120], UInt8, CPU>[@model.layers.8.mlp.gate_proj.weight])[symbol:model.layers.8.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%167:tensor<[7813120], UInt8, CPU>[@model.layers.8.mlp.up_proj.weight])[symbol:model.layers.8.mlp.up_proj.weight]
            tensor.CPU.register () -> (%199:tensor<[7753728], UInt8, CPU>[@model.layers.8.mlp.down_proj.weight])[symbol:model.layers.8.mlp.down_proj.weight]
            tensor.CPU.register () -> (%139:tensor<[1339392], UInt8, CPU>[@model.layers.9.self_attn.q_proj.weight])[symbol:model.layers.9.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%324:tensor<[223232], UInt8, CPU>[@model.layers.9.self_attn.k_proj.weight])[symbol:model.layers.9.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%129:tensor<[223232], UInt8, CPU>[@model.layers.9.self_attn.v_proj.weight])[symbol:model.layers.9.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%149:tensor<[1339392], UInt8, CPU>[@model.layers.9.self_attn.o_proj.weight])[symbol:model.layers.9.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%183:tensor<[7813120], UInt8, CPU>[@model.layers.9.mlp.gate_proj.weight])[symbol:model.layers.9.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%406:tensor<[7813120], UInt8, CPU>[@model.layers.9.mlp.up_proj.weight])[symbol:model.layers.9.mlp.up_proj.weight]
            tensor.CPU.register () -> (%328:tensor<[7753728], UInt8, CPU>[@model.layers.9.mlp.down_proj.weight])[symbol:model.layers.9.mlp.down_proj.weight]
            tensor.CPU.register () -> (%545:tensor<[1339392], UInt8, CPU>[@model.layers.10.self_attn.q_proj.weight])[symbol:model.layers.10.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%158:tensor<[223232], UInt8, CPU>[@model.layers.10.self_attn.k_proj.weight])[symbol:model.layers.10.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%128:tensor<[223232], UInt8, CPU>[@model.layers.10.self_attn.v_proj.weight])[symbol:model.layers.10.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%546:tensor<[1339392], UInt8, CPU>[@model.layers.10.self_attn.o_proj.weight])[symbol:model.layers.10.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%175:tensor<[7813120], UInt8, CPU>[@model.layers.10.mlp.gate_proj.weight])[symbol:model.layers.10.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%555:tensor<[7813120], UInt8, CPU>[@model.layers.10.mlp.up_proj.weight])[symbol:model.layers.10.mlp.up_proj.weight]
            tensor.CPU.register () -> (%188:tensor<[7753728], UInt8, CPU>[@model.layers.10.mlp.down_proj.weight])[symbol:model.layers.10.mlp.down_proj.weight]
            tensor.CPU.register () -> (%133:tensor<[1339392], UInt8, CPU>[@model.layers.11.self_attn.q_proj.weight])[symbol:model.layers.11.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%498:tensor<[223232], UInt8, CPU>[@model.layers.11.self_attn.k_proj.weight])[symbol:model.layers.11.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%550:tensor<[223232], UInt8, CPU>[@model.layers.11.self_attn.v_proj.weight])[symbol:model.layers.11.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%143:tensor<[1339392], UInt8, CPU>[@model.layers.11.self_attn.o_proj.weight])[symbol:model.layers.11.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%531:tensor<[7813120], UInt8, CPU>[@model.layers.11.mlp.gate_proj.weight])[symbol:model.layers.11.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%169:tensor<[7813120], UInt8, CPU>[@model.layers.11.mlp.up_proj.weight])[symbol:model.layers.11.mlp.up_proj.weight]
            tensor.CPU.register () -> (%561:tensor<[7753728], UInt8, CPU>[@model.layers.11.mlp.down_proj.weight])[symbol:model.layers.11.mlp.down_proj.weight]
            tensor.CPU.register () -> (%132:tensor<[1339392], UInt8, CPU>[@model.layers.12.self_attn.q_proj.weight])[symbol:model.layers.12.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%524:tensor<[223232], UInt8, CPU>[@model.layers.12.self_attn.k_proj.weight])[symbol:model.layers.12.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%130:tensor<[223232], UInt8, CPU>[@model.layers.12.self_attn.v_proj.weight])[symbol:model.layers.12.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%156:tensor<[1339392], UInt8, CPU>[@model.layers.12.self_attn.o_proj.weight])[symbol:model.layers.12.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%182:tensor<[7813120], UInt8, CPU>[@model.layers.12.mlp.gate_proj.weight])[symbol:model.layers.12.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%510:tensor<[7813120], UInt8, CPU>[@model.layers.12.mlp.up_proj.weight])[symbol:model.layers.12.mlp.up_proj.weight]
            tensor.CPU.register () -> (%198:tensor<[7753728], UInt8, CPU>[@model.layers.12.mlp.down_proj.weight])[symbol:model.layers.12.mlp.down_proj.weight]
            tensor.CPU.register () -> (%297:tensor<[1339392], UInt8, CPU>[@model.layers.13.self_attn.q_proj.weight])[symbol:model.layers.13.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%157:tensor<[223232], UInt8, CPU>[@model.layers.13.self_attn.k_proj.weight])[symbol:model.layers.13.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%124:tensor<[223232], UInt8, CPU>[@model.layers.13.self_attn.v_proj.weight])[symbol:model.layers.13.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%487:tensor<[1339392], UInt8, CPU>[@model.layers.13.self_attn.o_proj.weight])[symbol:model.layers.13.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%179:tensor<[7813120], UInt8, CPU>[@model.layers.13.mlp.gate_proj.weight])[symbol:model.layers.13.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%423:tensor<[7813120], UInt8, CPU>[@model.layers.13.mlp.up_proj.weight])[symbol:model.layers.13.mlp.up_proj.weight]
            tensor.CPU.register () -> (%197:tensor<[7753728], UInt8, CPU>[@model.layers.13.mlp.down_proj.weight])[symbol:model.layers.13.mlp.down_proj.weight]
            tensor.CPU.register () -> (%512:tensor<[1339392], UInt8, CPU>[@model.layers.14.self_attn.q_proj.weight])[symbol:model.layers.14.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%495:tensor<[223232], UInt8, CPU>[@model.layers.14.self_attn.k_proj.weight])[symbol:model.layers.14.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%434:tensor<[223232], UInt8, CPU>[@model.layers.14.self_attn.v_proj.weight])[symbol:model.layers.14.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%152:tensor<[1339392], UInt8, CPU>[@model.layers.14.self_attn.o_proj.weight])[symbol:model.layers.14.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%437:tensor<[7813120], UInt8, CPU>[@model.layers.14.mlp.gate_proj.weight])[symbol:model.layers.14.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%359:tensor<[7813120], UInt8, CPU>[@model.layers.14.mlp.up_proj.weight])[symbol:model.layers.14.mlp.up_proj.weight]
            tensor.CPU.register () -> (%192:tensor<[7753728], UInt8, CPU>[@model.layers.14.mlp.down_proj.weight])[symbol:model.layers.14.mlp.down_proj.weight]
            tensor.CPU.register () -> (%484:tensor<[1339392], UInt8, CPU>[@model.layers.15.self_attn.q_proj.weight])[symbol:model.layers.15.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%389:tensor<[223232], UInt8, CPU>[@model.layers.15.self_attn.k_proj.weight])[symbol:model.layers.15.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%481:tensor<[223232], UInt8, CPU>[@model.layers.15.self_attn.v_proj.weight])[symbol:model.layers.15.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%146:tensor<[1339392], UInt8, CPU>[@model.layers.15.self_attn.o_proj.weight])[symbol:model.layers.15.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%343:tensor<[7813120], UInt8, CPU>[@model.layers.15.mlp.gate_proj.weight])[symbol:model.layers.15.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%414:tensor<[7813120], UInt8, CPU>[@model.layers.15.mlp.up_proj.weight])[symbol:model.layers.15.mlp.up_proj.weight]
            tensor.CPU.register () -> (%417:tensor<[7753728], UInt8, CPU>[@model.layers.15.mlp.down_proj.weight])[symbol:model.layers.15.mlp.down_proj.weight]
            tensor.CPU.register () -> (%134:tensor<[1339392], UInt8, CPU>[@model.layers.16.self_attn.q_proj.weight])[symbol:model.layers.16.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%554:tensor<[223232], UInt8, CPU>[@model.layers.16.self_attn.k_proj.weight])[symbol:model.layers.16.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%474:tensor<[223232], UInt8, CPU>[@model.layers.16.self_attn.v_proj.weight])[symbol:model.layers.16.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%405:tensor<[1339392], UInt8, CPU>[@model.layers.16.self_attn.o_proj.weight])[symbol:model.layers.16.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%463:tensor<[7813120], UInt8, CPU>[@model.layers.16.mlp.gate_proj.weight])[symbol:model.layers.16.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%171:tensor<[7813120], UInt8, CPU>[@model.layers.16.mlp.up_proj.weight])[symbol:model.layers.16.mlp.up_proj.weight]
            tensor.CPU.register () -> (%432:tensor<[7753728], UInt8, CPU>[@model.layers.16.mlp.down_proj.weight])[symbol:model.layers.16.mlp.down_proj.weight]
            tensor.CPU.register () -> (%135:tensor<[1339392], UInt8, CPU>[@model.layers.17.self_attn.q_proj.weight])[symbol:model.layers.17.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%345:tensor<[223232], UInt8, CPU>[@model.layers.17.self_attn.k_proj.weight])[symbol:model.layers.17.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%412:tensor<[223232], UInt8, CPU>[@model.layers.17.self_attn.v_proj.weight])[symbol:model.layers.17.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%446:tensor<[1339392], UInt8, CPU>[@model.layers.17.self_attn.o_proj.weight])[symbol:model.layers.17.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%439:tensor<[7813120], UInt8, CPU>[@model.layers.17.mlp.gate_proj.weight])[symbol:model.layers.17.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%469:tensor<[7813120], UInt8, CPU>[@model.layers.17.mlp.up_proj.weight])[symbol:model.layers.17.mlp.up_proj.weight]
            tensor.CPU.register () -> (%471:tensor<[7753728], UInt8, CPU>[@model.layers.17.mlp.down_proj.weight])[symbol:model.layers.17.mlp.down_proj.weight]
            tensor.CPU.register () -> (%454:tensor<[1339392], UInt8, CPU>[@model.layers.18.self_attn.q_proj.weight])[symbol:model.layers.18.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%339:tensor<[223232], UInt8, CPU>[@model.layers.18.self_attn.k_proj.weight])[symbol:model.layers.18.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%461:tensor<[223232], UInt8, CPU>[@model.layers.18.self_attn.v_proj.weight])[symbol:model.layers.18.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%319:tensor<[1339392], UInt8, CPU>[@model.layers.18.self_attn.o_proj.weight])[symbol:model.layers.18.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%457:tensor<[7813120], UInt8, CPU>[@model.layers.18.mlp.gate_proj.weight])[symbol:model.layers.18.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%456:tensor<[7813120], UInt8, CPU>[@model.layers.18.mlp.up_proj.weight])[symbol:model.layers.18.mlp.up_proj.weight]
            tensor.CPU.register () -> (%191:tensor<[7753728], UInt8, CPU>[@model.layers.18.mlp.down_proj.weight])[symbol:model.layers.18.mlp.down_proj.weight]
            tensor.CPU.register () -> (%136:tensor<[1339392], UInt8, CPU>[@model.layers.19.self_attn.q_proj.weight])[symbol:model.layers.19.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%445:tensor<[223232], UInt8, CPU>[@model.layers.19.self_attn.k_proj.weight])[symbol:model.layers.19.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%126:tensor<[223232], UInt8, CPU>[@model.layers.19.self_attn.v_proj.weight])[symbol:model.layers.19.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%444:tensor<[1339392], UInt8, CPU>[@model.layers.19.self_attn.o_proj.weight])[symbol:model.layers.19.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%176:tensor<[7813120], UInt8, CPU>[@model.layers.19.mlp.gate_proj.weight])[symbol:model.layers.19.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%451:tensor<[7813120], UInt8, CPU>[@model.layers.19.mlp.up_proj.weight])[symbol:model.layers.19.mlp.up_proj.weight]
            tensor.CPU.register () -> (%189:tensor<[7753728], UInt8, CPU>[@model.layers.19.mlp.down_proj.weight])[symbol:model.layers.19.mlp.down_proj.weight]
            tensor.CPU.register () -> (%140:tensor<[1339392], UInt8, CPU>[@model.layers.20.self_attn.q_proj.weight])[symbol:model.layers.20.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%491:tensor<[223232], UInt8, CPU>[@model.layers.20.self_attn.k_proj.weight])[symbol:model.layers.20.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%553:tensor<[223232], UInt8, CPU>[@model.layers.20.self_attn.v_proj.weight])[symbol:model.layers.20.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%569:tensor<[1339392], UInt8, CPU>[@model.layers.20.self_attn.o_proj.weight])[symbol:model.layers.20.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%181:tensor<[7813120], UInt8, CPU>[@model.layers.20.mlp.gate_proj.weight])[symbol:model.layers.20.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%548:tensor<[7813120], UInt8, CPU>[@model.layers.20.mlp.up_proj.weight])[symbol:model.layers.20.mlp.up_proj.weight]
            tensor.CPU.register () -> (%413:tensor<[7753728], UInt8, CPU>[@model.layers.20.mlp.down_proj.weight])[symbol:model.layers.20.mlp.down_proj.weight]
            tensor.CPU.register () -> (%541:tensor<[1339392], UInt8, CPU>[@model.layers.21.self_attn.q_proj.weight])[symbol:model.layers.21.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%162:tensor<[223232], UInt8, CPU>[@model.layers.21.self_attn.k_proj.weight])[symbol:model.layers.21.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%125:tensor<[223232], UInt8, CPU>[@model.layers.21.self_attn.v_proj.weight])[symbol:model.layers.21.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%419:tensor<[1339392], UInt8, CPU>[@model.layers.21.self_attn.o_proj.weight])[symbol:model.layers.21.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%186:tensor<[7813120], UInt8, CPU>[@model.layers.21.mlp.gate_proj.weight])[symbol:model.layers.21.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%368:tensor<[7813120], UInt8, CPU>[@model.layers.21.mlp.up_proj.weight])[symbol:model.layers.21.mlp.up_proj.weight]
            tensor.CPU.register () -> (%478:tensor<[7753728], UInt8, CPU>[@model.layers.21.mlp.down_proj.weight])[symbol:model.layers.21.mlp.down_proj.weight]
            tensor.CPU.register () -> (%494:tensor<[1339392], UInt8, CPU>[@model.layers.22.self_attn.q_proj.weight])[symbol:model.layers.22.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%420:tensor<[223232], UInt8, CPU>[@model.layers.22.self_attn.k_proj.weight])[symbol:model.layers.22.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%404:tensor<[223232], UInt8, CPU>[@model.layers.22.self_attn.v_proj.weight])[symbol:model.layers.22.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%475:tensor<[1339392], UInt8, CPU>[@model.layers.22.self_attn.o_proj.weight])[symbol:model.layers.22.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%178:tensor<[7813120], UInt8, CPU>[@model.layers.22.mlp.gate_proj.weight])[symbol:model.layers.22.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%410:tensor<[7813120], UInt8, CPU>[@model.layers.22.mlp.up_proj.weight])[symbol:model.layers.22.mlp.up_proj.weight]
            tensor.CPU.register () -> (%415:tensor<[7753728], UInt8, CPU>[@model.layers.22.mlp.down_proj.weight])[symbol:model.layers.22.mlp.down_proj.weight]
            tensor.CPU.register () -> (%393:tensor<[1339392], UInt8, CPU>[@model.layers.23.self_attn.q_proj.weight])[symbol:model.layers.23.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%163:tensor<[223232], UInt8, CPU>[@model.layers.23.self_attn.k_proj.weight])[symbol:model.layers.23.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%362:tensor<[223232], UInt8, CPU>[@model.layers.23.self_attn.v_proj.weight])[symbol:model.layers.23.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%460:tensor<[1339392], UInt8, CPU>[@model.layers.23.self_attn.o_proj.weight])[symbol:model.layers.23.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%357:tensor<[7813120], UInt8, CPU>[@model.layers.23.mlp.gate_proj.weight])[symbol:model.layers.23.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%166:tensor<[7813120], UInt8, CPU>[@model.layers.23.mlp.up_proj.weight])[symbol:model.layers.23.mlp.up_proj.weight]
            tensor.CPU.register () -> (%193:tensor<[7753728], UInt8, CPU>[@model.layers.23.mlp.down_proj.weight])[symbol:model.layers.23.mlp.down_proj.weight]
            tensor.CPU.register () -> (%141:tensor<[1339392], UInt8, CPU>[@model.layers.24.self_attn.q_proj.weight])[symbol:model.layers.24.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%452:tensor<[223232], UInt8, CPU>[@model.layers.24.self_attn.k_proj.weight])[symbol:model.layers.24.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%477:tensor<[223232], UInt8, CPU>[@model.layers.24.self_attn.v_proj.weight])[symbol:model.layers.24.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%485:tensor<[1339392], UInt8, CPU>[@model.layers.24.self_attn.o_proj.weight])[symbol:model.layers.24.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%180:tensor<[7813120], UInt8, CPU>[@model.layers.24.mlp.gate_proj.weight])[symbol:model.layers.24.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%411:tensor<[7813120], UInt8, CPU>[@model.layers.24.mlp.up_proj.weight])[symbol:model.layers.24.mlp.up_proj.weight]
            tensor.CPU.register () -> (%196:tensor<[7753728], UInt8, CPU>[@model.layers.24.mlp.down_proj.weight])[symbol:model.layers.24.mlp.down_proj.weight]
            tensor.CPU.register () -> (%325:tensor<[1339392], UInt8, CPU>[@model.layers.25.self_attn.q_proj.weight])[symbol:model.layers.25.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%385:tensor<[223232], UInt8, CPU>[@model.layers.25.self_attn.k_proj.weight])[symbol:model.layers.25.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%381:tensor<[223232], UInt8, CPU>[@model.layers.25.self_attn.v_proj.weight])[symbol:model.layers.25.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%150:tensor<[1339392], UInt8, CPU>[@model.layers.25.self_attn.o_proj.weight])[symbol:model.layers.25.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%382:tensor<[7813120], UInt8, CPU>[@model.layers.25.mlp.gate_proj.weight])[symbol:model.layers.25.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%337:tensor<[7813120], UInt8, CPU>[@model.layers.25.mlp.up_proj.weight])[symbol:model.layers.25.mlp.up_proj.weight]
            tensor.CPU.register () -> (%387:tensor<[7753728], UInt8, CPU>[@model.layers.25.mlp.down_proj.weight])[symbol:model.layers.25.mlp.down_proj.weight]
            tensor.CPU.register () -> (%327:tensor<[1339392], UInt8, CPU>[@model.layers.26.self_attn.q_proj.weight])[symbol:model.layers.26.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%321:tensor<[223232], UInt8, CPU>[@model.layers.26.self_attn.k_proj.weight])[symbol:model.layers.26.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%391:tensor<[223232], UInt8, CPU>[@model.layers.26.self_attn.v_proj.weight])[symbol:model.layers.26.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%155:tensor<[1339392], UInt8, CPU>[@model.layers.26.self_attn.o_proj.weight])[symbol:model.layers.26.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%301:tensor<[7813120], UInt8, CPU>[@model.layers.26.mlp.gate_proj.weight])[symbol:model.layers.26.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%377:tensor<[7813120], UInt8, CPU>[@model.layers.26.mlp.up_proj.weight])[symbol:model.layers.26.mlp.up_proj.weight]
            tensor.CPU.register () -> (%190:tensor<[7753728], UInt8, CPU>[@model.layers.26.mlp.down_proj.weight])[symbol:model.layers.26.mlp.down_proj.weight]
            tensor.CPU.register () -> (%367:tensor<[1339392], UInt8, CPU>[@model.layers.27.self_attn.q_proj.weight])[symbol:model.layers.27.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%358:tensor<[223232], UInt8, CPU>[@model.layers.27.self_attn.k_proj.weight])[symbol:model.layers.27.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%526:tensor<[223232], UInt8, CPU>[@model.layers.27.self_attn.v_proj.weight])[symbol:model.layers.27.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%151:tensor<[1339392], UInt8, CPU>[@model.layers.27.self_attn.o_proj.weight])[symbol:model.layers.27.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%177:tensor<[7813120], UInt8, CPU>[@model.layers.27.mlp.gate_proj.weight])[symbol:model.layers.27.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%172:tensor<[7813120], UInt8, CPU>[@model.layers.27.mlp.up_proj.weight])[symbol:model.layers.27.mlp.up_proj.weight]
            tensor.CPU.register () -> (%195:tensor<[7753728], UInt8, CPU>[@model.layers.27.mlp.down_proj.weight])[symbol:model.layers.27.mlp.down_proj.weight]
            tensor.CPU.register () -> (%464:tensor<[132488192], UInt8, CPU>[@model.lm_head.weight])[symbol:model.lm_head.weight]
        }
    }
    graph.SubGraphOp @deinit <notype> {
        () -> () {
            
        }
    }
    // Calculate the text embeddings
    linalg.CPU.EmbeddingOp(%582:tensor<[1, 192], Int64, CPU>) -> (%583:tensor<[1, 192, 1536], Float32, CPU>)
    // visual inputs is: [img, visual_embedding_sin, visual_embedding_cos]
    graph.CallGraphOp @visual (%580:tensor<[680, 1176], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1266:tensor<[170, 1536], Float32, CPU>)
    graph.SubGraphOp @visual <CPU> {
        (%580:tensor<[680, 1176], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1266:tensor<[170, 1536], Float32, CPU>) {
            graph.CallGraphOp @visual.patch_embed (%580:tensor<[680, 1176], Float32, CPU>) -> (%590:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.0 (%590:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%611:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.1 (%611:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%632:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.2 (%632:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%653:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.3 (%653:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%674:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.4 (%674:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%695:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.5 (%695:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%716:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.6 (%716:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%737:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.7 (%737:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%758:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.8 (%758:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%779:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.9 (%779:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%800:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.10 (%800:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%821:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.11 (%821:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%842:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.12 (%842:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%863:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.13 (%863:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%884:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.14 (%884:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%905:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.15 (%905:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%926:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.16 (%926:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%947:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.17 (%947:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%968:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.18 (%968:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%989:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.19 (%989:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1010:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.20 (%1010:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1031:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.21 (%1031:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1052:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.22 (%1052:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1073:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.23 (%1073:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1094:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.24 (%1094:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1115:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.25 (%1115:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1136:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.26 (%1136:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1157:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.27 (%1157:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1178:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.28 (%1178:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1199:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.29 (%1199:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1220:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.30 (%1220:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1241:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.31 (%1241:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1262:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.merger (%1262:tensor<[680, 1280], Float32, CPU>) -> (%1266:tensor<[170, 1536], Float32, CPU>)
            cf.ReturnOp (%1266:tensor<[170, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.patch_embed <CPU> {
        (%580:tensor<[680, 1176], Float32, CPU>) -> (%590:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.ViewOp(%580:tensor<[680, 1176], Float32, CPU>) -> (%580:tensor<[680, 3, 2, 14, 14], Float32, CPU>)
            linalg.CPU.Conv3DOp(%580:tensor<[680, 3, 2, 14, 14], Float32, CPU>) -> (%590:tensor<[680, 1280, 1, 1, 1], Float32, CPU>)
            linalg.CPU.ViewOp(%590:tensor<[680, 1280, 1, 1, 1], Float32, CPU>) -> (%590:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%590:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.0 <CPU> {
        (%590:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%611:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LayerNormOp(%590:tensor<[680, 1280], Float32, CPU>) -> (%591:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.0.attn (%591:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%605:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%590:tensor<[680, 1280], Float32, CPU>, %605:tensor<[680, 1280], Float32, CPU>) -> (%606:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LayerNormOp(%606:tensor<[680, 1280], Float32, CPU>) -> (%607:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.0.mlp (%607:tensor<[680, 1280], Float32, CPU>) -> (%610:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%606:tensor<[680, 1280], Float32, CPU>, %610:tensor<[680, 1280], Float32, CPU>) -> (%611:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%611:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.0.attn <CPU> {
        (%591:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%605:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%591:tensor<[680, 1280], Float32, CPU>) -> (%592:tensor<[680, 3840], Float32, CPU>)
            linalg.CPU.ViewOp(%592:tensor<[680, 3840], Float32, CPU>) -> (%592:tensor<[680, 3, 16, 80], Float32, CPU>)
            linalg.CPU.PermuteOp(%592:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%593:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.SplitOp(%593:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%593:tensor<[3, 680, 16, 80], Float32, CPU>, %593:tensor<[3, 680, 16, 80], Float32, CPU>, %593:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%593:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%594:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%593:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%595:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%594:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%596:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%595:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%597:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%593:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%598:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.MatMulOp(%596:tensor<[1, 16, 680, 80], Float32, CPU>, %597:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%599:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MulOp(%599:tensor<[1, 16, 680, 680], Float32, CPU>, %600:tensor<[1], Float32, CPU>) -> (%601:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%601:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%602:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MatMulOp(%602:tensor<[1, 16, 680, 680], Float32, CPU>, %598:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%603:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%603:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%604:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.ViewOp(%604:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%604:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LinearOp(%604:tensor<[680, 1280], Float32, CPU>) -> (%605:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%605:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.0.mlp <CPU> {
        (%607:tensor<[680, 1280], Float32, CPU>) -> (%610:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%607:tensor<[680, 1280], Float32, CPU>) -> (%608:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.QuickGELUOp(%608:tensor<[680, 5120], Float32, CPU>) -> (%609:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.LinearOp(%609:tensor<[680, 5120], Float32, CPU>) -> (%610:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%610:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.1 <CPU> {
        (%611:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%632:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LayerNormOp(%611:tensor<[680, 1280], Float32, CPU>) -> (%612:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.1.attn (%612:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%626:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%611:tensor<[680, 1280], Float32, CPU>, %626:tensor<[680, 1280], Float32, CPU>) -> (%627:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LayerNormOp(%627:tensor<[680, 1280], Float32, CPU>) -> (%628:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.1.mlp (%628:tensor<[680, 1280], Float32, CPU>) -> (%631:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%627:tensor<[680, 1280], Float32, CPU>, %631:tensor<[680, 1280], Float32, CPU>) -> (%632:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%632:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.1.attn <CPU> {
        (%612:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%626:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%612:tensor<[680, 1280], Float32, CPU>) -> (%613:tensor<[680, 3840], Float32, CPU>)
            linalg.CPU.ViewOp(%613:tensor<[680, 3840], Float32, CPU>) -> (%613:tensor<[680, 3, 16, 80], Float32, CPU>)
            linalg.CPU.PermuteOp(%613:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%614:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.SplitOp(%614:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%614:tensor<[3, 680, 16, 80], Float32, CPU>, %614:tensor<[3, 680, 16, 80], Float32, CPU>, %614:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%614:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%615:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%614:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%616:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%615:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%617:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%616:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%618:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%614:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%619:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.MatMulOp(%617:tensor<[1, 16, 680, 80], Float32, CPU>, %618:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%620:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MulOp(%620:tensor<[1, 16, 680, 680], Float32, CPU>, %621:tensor<[1], Float32, CPU>) -> (%622:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%622:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%623:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MatMulOp(%623:tensor<[1, 16, 680, 680], Float32, CPU>, %619:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%624:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%624:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%625:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.ViewOp(%625:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%625:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LinearOp(%625:tensor<[680, 1280], Float32, CPU>) -> (%626:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%626:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.1.mlp <CPU> {
        (%628:tensor<[680, 1280], Float32, CPU>) -> (%631:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%628:tensor<[680, 1280], Float32, CPU>) -> (%629:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.QuickGELUOp(%629:tensor<[680, 5120], Float32, CPU>) -> (%630:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.LinearOp(%630:tensor<[680, 5120], Float32, CPU>) -> (%631:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%631:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.2 <CPU> {
        (%632:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%653:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LayerNormOp(%632:tensor<[680, 1280], Float32, CPU>) -> (%633:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.2.attn (%633:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%647:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%632:tensor<[680, 1280], Float32, CPU>, %647:tensor<[680, 1280], Float32, CPU>) -> (%648:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LayerNormOp(%648:tensor<[680, 1280], Float32, CPU>) -> (%649:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.2.mlp (%649:tensor<[680, 1280], Float32, CPU>) -> (%652:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%648:tensor<[680, 1280], Float32, CPU>, %652:tensor<[680, 1280], Float32, CPU>) -> (%653:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%653:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.2.attn <CPU> {
        (%633:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%647:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%633:tensor<[680, 1280], Float32, CPU>) -> (%634:tensor<[680, 3840], Float32, CPU>)
            linalg.CPU.ViewOp(%634:tensor<[680, 3840], Float32, CPU>) -> (%634:tensor<[680, 3, 16, 80], Float32, CPU>)
            linalg.CPU.PermuteOp(%634:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%635:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.SplitOp(%635:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%635:tensor<[3, 680, 16, 80], Float32, CPU>, %635:tensor<[3, 680, 16, 80], Float32, CPU>, %635:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%635:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%636:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%635:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%637:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%636:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%638:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%637:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%639:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%635:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%640:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.MatMulOp(%638:tensor<[1, 16, 680, 80], Float32, CPU>, %639:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%641:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MulOp(%641:tensor<[1, 16, 680, 680], Float32, CPU>, %642:tensor<[1], Float32, CPU>) -> (%643:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%643:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%644:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MatMulOp(%644:tensor<[1, 16, 680, 680], Float32, CPU>, %640:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%645:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%645:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%646:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.ViewOp(%646:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%646:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LinearOp(%646:tensor<[680, 1280], Float32, CPU>) -> (%647:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%647:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.2.mlp <CPU> {
        (%649:tensor<[680, 1280], Float32, CPU>) -> (%652:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%649:tensor<[680, 1280], Float32, CPU>) -> (%650:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.QuickGELUOp(%650:tensor<[680, 5120], Float32, CPU>) -> (%651:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.LinearOp(%651:tensor<[680, 5120], Float32, CPU>) -> (%652:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%652:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.3 <CPU> {
        (%653:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%674:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LayerNormOp(%653:tensor<[680, 1280], Float32, CPU>) -> (%654:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.3.attn (%654:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%668:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%653:tensor<[680, 1280], Float32, CPU>, %668:tensor<[680, 1280], Float32, CPU>) -> (%669:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LayerNormOp(%669:tensor<[680, 1280], Float32, CPU>) -> (%670:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.3.mlp (%670:tensor<[680, 1280], Float32, CPU>) -> (%673:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%669:tensor<[680, 1280], Float32, CPU>, %673:tensor<[680, 1280], Float32, CPU>) -> (%674:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%674:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.3.attn <CPU> {
        (%654:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%668:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%654:tensor<[680, 1280], Float32, CPU>) -> (%655:tensor<[680, 3840], Float32, CPU>)
            linalg.CPU.ViewOp(%655:tensor<[680, 3840], Float32, CPU>) -> (%655:tensor<[680, 3, 16, 80], Float32, CPU>)
            linalg.CPU.PermuteOp(%655:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%656:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.SplitOp(%656:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%656:tensor<[3, 680, 16, 80], Float32, CPU>, %656:tensor<[3, 680, 16, 80], Float32, CPU>, %656:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%656:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%657:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%656:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%658:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%657:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%659:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%658:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%660:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%656:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%661:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.MatMulOp(%659:tensor<[1, 16, 680, 80], Float32, CPU>, %660:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%662:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MulOp(%662:tensor<[1, 16, 680, 680], Float32, CPU>, %663:tensor<[1], Float32, CPU>) -> (%664:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%664:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%665:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MatMulOp(%665:tensor<[1, 16, 680, 680], Float32, CPU>, %661:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%666:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%666:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%667:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.ViewOp(%667:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%667:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LinearOp(%667:tensor<[680, 1280], Float32, CPU>) -> (%668:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%668:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.3.mlp <CPU> {
        (%670:tensor<[680, 1280], Float32, CPU>) -> (%673:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%670:tensor<[680, 1280], Float32, CPU>) -> (%671:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.QuickGELUOp(%671:tensor<[680, 5120], Float32, CPU>) -> (%672:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.LinearOp(%672:tensor<[680, 5120], Float32, CPU>) -> (%673:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%673:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.4 <CPU> {
        (%674:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%695:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LayerNormOp(%674:tensor<[680, 1280], Float32, CPU>) -> (%675:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.4.attn (%675:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%689:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%674:tensor<[680, 1280], Float32, CPU>, %689:tensor<[680, 1280], Float32, CPU>) -> (%690:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LayerNormOp(%690:tensor<[680, 1280], Float32, CPU>) -> (%691:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.4.mlp (%691:tensor<[680, 1280], Float32, CPU>) -> (%694:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%690:tensor<[680, 1280], Float32, CPU>, %694:tensor<[680, 1280], Float32, CPU>) -> (%695:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%695:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.4.attn <CPU> {
        (%675:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%689:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%675:tensor<[680, 1280], Float32, CPU>) -> (%676:tensor<[680, 3840], Float32, CPU>)
            linalg.CPU.ViewOp(%676:tensor<[680, 3840], Float32, CPU>) -> (%676:tensor<[680, 3, 16, 80], Float32, CPU>)
            linalg.CPU.PermuteOp(%676:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%677:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.SplitOp(%677:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%677:tensor<[3, 680, 16, 80], Float32, CPU>, %677:tensor<[3, 680, 16, 80], Float32, CPU>, %677:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%677:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%678:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%677:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%679:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%678:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%680:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%679:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%681:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%677:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%682:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.MatMulOp(%680:tensor<[1, 16, 680, 80], Float32, CPU>, %681:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%683:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MulOp(%683:tensor<[1, 16, 680, 680], Float32, CPU>, %684:tensor<[1], Float32, CPU>) -> (%685:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%685:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%686:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MatMulOp(%686:tensor<[1, 16, 680, 680], Float32, CPU>, %682:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%687:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%687:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%688:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.ViewOp(%688:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%688:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LinearOp(%688:tensor<[680, 1280], Float32, CPU>) -> (%689:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%689:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.4.mlp <CPU> {
        (%691:tensor<[680, 1280], Float32, CPU>) -> (%694:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%691:tensor<[680, 1280], Float32, CPU>) -> (%692:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.QuickGELUOp(%692:tensor<[680, 5120], Float32, CPU>) -> (%693:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.LinearOp(%693:tensor<[680, 5120], Float32, CPU>) -> (%694:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%694:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.5 <CPU> {
        (%695:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%716:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LayerNormOp(%695:tensor<[680, 1280], Float32, CPU>) -> (%696:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.5.attn (%696:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%710:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%695:tensor<[680, 1280], Float32, CPU>, %710:tensor<[680, 1280], Float32, CPU>) -> (%711:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LayerNormOp(%711:tensor<[680, 1280], Float32, CPU>) -> (%712:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.5.mlp (%712:tensor<[680, 1280], Float32, CPU>) -> (%715:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%711:tensor<[680, 1280], Float32, CPU>, %715:tensor<[680, 1280], Float32, CPU>) -> (%716:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%716:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.5.attn <CPU> {
        (%696:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%710:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%696:tensor<[680, 1280], Float32, CPU>) -> (%697:tensor<[680, 3840], Float32, CPU>)
            linalg.CPU.ViewOp(%697:tensor<[680, 3840], Float32, CPU>) -> (%697:tensor<[680, 3, 16, 80], Float32, CPU>)
            linalg.CPU.PermuteOp(%697:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%698:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.SplitOp(%698:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%698:tensor<[3, 680, 16, 80], Float32, CPU>, %698:tensor<[3, 680, 16, 80], Float32, CPU>, %698:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%698:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%699:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%698:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%700:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%699:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%701:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%700:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%702:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%698:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%703:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.MatMulOp(%701:tensor<[1, 16, 680, 80], Float32, CPU>, %702:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%704:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MulOp(%704:tensor<[1, 16, 680, 680], Float32, CPU>, %705:tensor<[1], Float32, CPU>) -> (%706:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%706:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%707:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MatMulOp(%707:tensor<[1, 16, 680, 680], Float32, CPU>, %703:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%708:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%708:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%709:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.ViewOp(%709:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%709:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LinearOp(%709:tensor<[680, 1280], Float32, CPU>) -> (%710:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%710:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.5.mlp <CPU> {
        (%712:tensor<[680, 1280], Float32, CPU>) -> (%715:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%712:tensor<[680, 1280], Float32, CPU>) -> (%713:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.QuickGELUOp(%713:tensor<[680, 5120], Float32, CPU>) -> (%714:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.LinearOp(%714:tensor<[680, 5120], Float32, CPU>) -> (%715:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%715:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.6 <CPU> {
        (%716:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%737:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LayerNormOp(%716:tensor<[680, 1280], Float32, CPU>) -> (%717:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.6.attn (%717:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%731:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%716:tensor<[680, 1280], Float32, CPU>, %731:tensor<[680, 1280], Float32, CPU>) -> (%732:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LayerNormOp(%732:tensor<[680, 1280], Float32, CPU>) -> (%733:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.6.mlp (%733:tensor<[680, 1280], Float32, CPU>) -> (%736:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%732:tensor<[680, 1280], Float32, CPU>, %736:tensor<[680, 1280], Float32, CPU>) -> (%737:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%737:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.6.attn <CPU> {
        (%717:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%731:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%717:tensor<[680, 1280], Float32, CPU>) -> (%718:tensor<[680, 3840], Float32, CPU>)
            linalg.CPU.ViewOp(%718:tensor<[680, 3840], Float32, CPU>) -> (%718:tensor<[680, 3, 16, 80], Float32, CPU>)
            linalg.CPU.PermuteOp(%718:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%719:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.SplitOp(%719:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%719:tensor<[3, 680, 16, 80], Float32, CPU>, %719:tensor<[3, 680, 16, 80], Float32, CPU>, %719:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%719:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%720:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%719:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%721:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%720:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%722:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%721:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%723:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%719:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%724:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.MatMulOp(%722:tensor<[1, 16, 680, 80], Float32, CPU>, %723:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%725:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MulOp(%725:tensor<[1, 16, 680, 680], Float32, CPU>, %726:tensor<[1], Float32, CPU>) -> (%727:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%727:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%728:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MatMulOp(%728:tensor<[1, 16, 680, 680], Float32, CPU>, %724:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%729:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%729:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%730:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.ViewOp(%730:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%730:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LinearOp(%730:tensor<[680, 1280], Float32, CPU>) -> (%731:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%731:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.6.mlp <CPU> {
        (%733:tensor<[680, 1280], Float32, CPU>) -> (%736:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%733:tensor<[680, 1280], Float32, CPU>) -> (%734:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.QuickGELUOp(%734:tensor<[680, 5120], Float32, CPU>) -> (%735:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.LinearOp(%735:tensor<[680, 5120], Float32, CPU>) -> (%736:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%736:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.7 <CPU> {
        (%737:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%758:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LayerNormOp(%737:tensor<[680, 1280], Float32, CPU>) -> (%738:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.7.attn (%738:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%752:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%737:tensor<[680, 1280], Float32, CPU>, %752:tensor<[680, 1280], Float32, CPU>) -> (%753:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LayerNormOp(%753:tensor<[680, 1280], Float32, CPU>) -> (%754:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.7.mlp (%754:tensor<[680, 1280], Float32, CPU>) -> (%757:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%753:tensor<[680, 1280], Float32, CPU>, %757:tensor<[680, 1280], Float32, CPU>) -> (%758:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%758:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.7.attn <CPU> {
        (%738:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%752:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%738:tensor<[680, 1280], Float32, CPU>) -> (%739:tensor<[680, 3840], Float32, CPU>)
            linalg.CPU.ViewOp(%739:tensor<[680, 3840], Float32, CPU>) -> (%739:tensor<[680, 3, 16, 80], Float32, CPU>)
            linalg.CPU.PermuteOp(%739:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%740:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.SplitOp(%740:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%740:tensor<[3, 680, 16, 80], Float32, CPU>, %740:tensor<[3, 680, 16, 80], Float32, CPU>, %740:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%740:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%741:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%740:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%742:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%741:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%743:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%742:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%744:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%740:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%745:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.MatMulOp(%743:tensor<[1, 16, 680, 80], Float32, CPU>, %744:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%746:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MulOp(%746:tensor<[1, 16, 680, 680], Float32, CPU>, %747:tensor<[1], Float32, CPU>) -> (%748:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%748:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%749:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MatMulOp(%749:tensor<[1, 16, 680, 680], Float32, CPU>, %745:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%750:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%750:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%751:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.ViewOp(%751:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%751:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LinearOp(%751:tensor<[680, 1280], Float32, CPU>) -> (%752:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%752:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.7.mlp <CPU> {
        (%754:tensor<[680, 1280], Float32, CPU>) -> (%757:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%754:tensor<[680, 1280], Float32, CPU>) -> (%755:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.QuickGELUOp(%755:tensor<[680, 5120], Float32, CPU>) -> (%756:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.LinearOp(%756:tensor<[680, 5120], Float32, CPU>) -> (%757:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%757:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.8 <CPU> {
        (%758:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%779:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LayerNormOp(%758:tensor<[680, 1280], Float32, CPU>) -> (%759:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.8.attn (%759:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%773:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%758:tensor<[680, 1280], Float32, CPU>, %773:tensor<[680, 1280], Float32, CPU>) -> (%774:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LayerNormOp(%774:tensor<[680, 1280], Float32, CPU>) -> (%775:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.8.mlp (%775:tensor<[680, 1280], Float32, CPU>) -> (%778:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%774:tensor<[680, 1280], Float32, CPU>, %778:tensor<[680, 1280], Float32, CPU>) -> (%779:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%779:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.8.attn <CPU> {
        (%759:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%773:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%759:tensor<[680, 1280], Float32, CPU>) -> (%760:tensor<[680, 3840], Float32, CPU>)
            linalg.CPU.ViewOp(%760:tensor<[680, 3840], Float32, CPU>) -> (%760:tensor<[680, 3, 16, 80], Float32, CPU>)
            linalg.CPU.PermuteOp(%760:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%761:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.SplitOp(%761:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%761:tensor<[3, 680, 16, 80], Float32, CPU>, %761:tensor<[3, 680, 16, 80], Float32, CPU>, %761:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%761:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%762:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%761:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%763:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%762:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%764:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%763:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%765:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%761:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%766:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.MatMulOp(%764:tensor<[1, 16, 680, 80], Float32, CPU>, %765:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%767:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MulOp(%767:tensor<[1, 16, 680, 680], Float32, CPU>, %768:tensor<[1], Float32, CPU>) -> (%769:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%769:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%770:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MatMulOp(%770:tensor<[1, 16, 680, 680], Float32, CPU>, %766:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%771:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%771:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%772:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.ViewOp(%772:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%772:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LinearOp(%772:tensor<[680, 1280], Float32, CPU>) -> (%773:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%773:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.8.mlp <CPU> {
        (%775:tensor<[680, 1280], Float32, CPU>) -> (%778:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%775:tensor<[680, 1280], Float32, CPU>) -> (%776:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.QuickGELUOp(%776:tensor<[680, 5120], Float32, CPU>) -> (%777:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.LinearOp(%777:tensor<[680, 5120], Float32, CPU>) -> (%778:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%778:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.9 <CPU> {
        (%779:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%800:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LayerNormOp(%779:tensor<[680, 1280], Float32, CPU>) -> (%780:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.9.attn (%780:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%794:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%779:tensor<[680, 1280], Float32, CPU>, %794:tensor<[680, 1280], Float32, CPU>) -> (%795:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LayerNormOp(%795:tensor<[680, 1280], Float32, CPU>) -> (%796:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.9.mlp (%796:tensor<[680, 1280], Float32, CPU>) -> (%799:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%795:tensor<[680, 1280], Float32, CPU>, %799:tensor<[680, 1280], Float32, CPU>) -> (%800:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%800:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.9.attn <CPU> {
        (%780:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%794:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%780:tensor<[680, 1280], Float32, CPU>) -> (%781:tensor<[680, 3840], Float32, CPU>)
            linalg.CPU.ViewOp(%781:tensor<[680, 3840], Float32, CPU>) -> (%781:tensor<[680, 3, 16, 80], Float32, CPU>)
            linalg.CPU.PermuteOp(%781:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%782:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.SplitOp(%782:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%782:tensor<[3, 680, 16, 80], Float32, CPU>, %782:tensor<[3, 680, 16, 80], Float32, CPU>, %782:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%782:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%783:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%782:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%784:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%783:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%785:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%784:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%786:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%782:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%787:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.MatMulOp(%785:tensor<[1, 16, 680, 80], Float32, CPU>, %786:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%788:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MulOp(%788:tensor<[1, 16, 680, 680], Float32, CPU>, %789:tensor<[1], Float32, CPU>) -> (%790:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%790:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%791:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MatMulOp(%791:tensor<[1, 16, 680, 680], Float32, CPU>, %787:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%792:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%792:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%793:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.ViewOp(%793:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%793:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LinearOp(%793:tensor<[680, 1280], Float32, CPU>) -> (%794:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%794:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.9.mlp <CPU> {
        (%796:tensor<[680, 1280], Float32, CPU>) -> (%799:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%796:tensor<[680, 1280], Float32, CPU>) -> (%797:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.QuickGELUOp(%797:tensor<[680, 5120], Float32, CPU>) -> (%798:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.LinearOp(%798:tensor<[680, 5120], Float32, CPU>) -> (%799:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%799:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.10 <CPU> {
        (%800:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%821:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LayerNormOp(%800:tensor<[680, 1280], Float32, CPU>) -> (%801:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.10.attn (%801:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%815:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%800:tensor<[680, 1280], Float32, CPU>, %815:tensor<[680, 1280], Float32, CPU>) -> (%816:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LayerNormOp(%816:tensor<[680, 1280], Float32, CPU>) -> (%817:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.10.mlp (%817:tensor<[680, 1280], Float32, CPU>) -> (%820:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%816:tensor<[680, 1280], Float32, CPU>, %820:tensor<[680, 1280], Float32, CPU>) -> (%821:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%821:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.10.attn <CPU> {
        (%801:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%815:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%801:tensor<[680, 1280], Float32, CPU>) -> (%802:tensor<[680, 3840], Float32, CPU>)
            linalg.CPU.ViewOp(%802:tensor<[680, 3840], Float32, CPU>) -> (%802:tensor<[680, 3, 16, 80], Float32, CPU>)
            linalg.CPU.PermuteOp(%802:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%803:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.SplitOp(%803:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%803:tensor<[3, 680, 16, 80], Float32, CPU>, %803:tensor<[3, 680, 16, 80], Float32, CPU>, %803:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%803:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%804:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%803:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%805:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%804:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%806:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%805:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%807:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%803:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%808:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.MatMulOp(%806:tensor<[1, 16, 680, 80], Float32, CPU>, %807:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%809:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MulOp(%809:tensor<[1, 16, 680, 680], Float32, CPU>, %810:tensor<[1], Float32, CPU>) -> (%811:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%811:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%812:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MatMulOp(%812:tensor<[1, 16, 680, 680], Float32, CPU>, %808:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%813:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%813:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%814:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.ViewOp(%814:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%814:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LinearOp(%814:tensor<[680, 1280], Float32, CPU>) -> (%815:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%815:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.10.mlp <CPU> {
        (%817:tensor<[680, 1280], Float32, CPU>) -> (%820:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%817:tensor<[680, 1280], Float32, CPU>) -> (%818:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.QuickGELUOp(%818:tensor<[680, 5120], Float32, CPU>) -> (%819:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.LinearOp(%819:tensor<[680, 5120], Float32, CPU>) -> (%820:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%820:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.11 <CPU> {
        (%821:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%842:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LayerNormOp(%821:tensor<[680, 1280], Float32, CPU>) -> (%822:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.11.attn (%822:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%836:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%821:tensor<[680, 1280], Float32, CPU>, %836:tensor<[680, 1280], Float32, CPU>) -> (%837:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LayerNormOp(%837:tensor<[680, 1280], Float32, CPU>) -> (%838:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.11.mlp (%838:tensor<[680, 1280], Float32, CPU>) -> (%841:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%837:tensor<[680, 1280], Float32, CPU>, %841:tensor<[680, 1280], Float32, CPU>) -> (%842:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%842:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.11.attn <CPU> {
        (%822:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%836:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%822:tensor<[680, 1280], Float32, CPU>) -> (%823:tensor<[680, 3840], Float32, CPU>)
            linalg.CPU.ViewOp(%823:tensor<[680, 3840], Float32, CPU>) -> (%823:tensor<[680, 3, 16, 80], Float32, CPU>)
            linalg.CPU.PermuteOp(%823:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%824:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.SplitOp(%824:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%824:tensor<[3, 680, 16, 80], Float32, CPU>, %824:tensor<[3, 680, 16, 80], Float32, CPU>, %824:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%824:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%825:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%824:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%826:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%825:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%827:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%826:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%828:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%824:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%829:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.MatMulOp(%827:tensor<[1, 16, 680, 80], Float32, CPU>, %828:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%830:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MulOp(%830:tensor<[1, 16, 680, 680], Float32, CPU>, %831:tensor<[1], Float32, CPU>) -> (%832:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%832:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%833:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MatMulOp(%833:tensor<[1, 16, 680, 680], Float32, CPU>, %829:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%834:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%834:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%835:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.ViewOp(%835:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%835:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LinearOp(%835:tensor<[680, 1280], Float32, CPU>) -> (%836:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%836:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.11.mlp <CPU> {
        (%838:tensor<[680, 1280], Float32, CPU>) -> (%841:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%838:tensor<[680, 1280], Float32, CPU>) -> (%839:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.QuickGELUOp(%839:tensor<[680, 5120], Float32, CPU>) -> (%840:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.LinearOp(%840:tensor<[680, 5120], Float32, CPU>) -> (%841:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%841:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.12 <CPU> {
        (%842:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%863:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LayerNormOp(%842:tensor<[680, 1280], Float32, CPU>) -> (%843:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.12.attn (%843:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%857:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%842:tensor<[680, 1280], Float32, CPU>, %857:tensor<[680, 1280], Float32, CPU>) -> (%858:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LayerNormOp(%858:tensor<[680, 1280], Float32, CPU>) -> (%859:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.12.mlp (%859:tensor<[680, 1280], Float32, CPU>) -> (%862:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%858:tensor<[680, 1280], Float32, CPU>, %862:tensor<[680, 1280], Float32, CPU>) -> (%863:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%863:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.12.attn <CPU> {
        (%843:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%857:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%843:tensor<[680, 1280], Float32, CPU>) -> (%844:tensor<[680, 3840], Float32, CPU>)
            linalg.CPU.ViewOp(%844:tensor<[680, 3840], Float32, CPU>) -> (%844:tensor<[680, 3, 16, 80], Float32, CPU>)
            linalg.CPU.PermuteOp(%844:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%845:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.SplitOp(%845:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%845:tensor<[3, 680, 16, 80], Float32, CPU>, %845:tensor<[3, 680, 16, 80], Float32, CPU>, %845:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%845:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%846:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%845:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%847:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%846:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%848:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%847:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%849:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%845:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%850:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.MatMulOp(%848:tensor<[1, 16, 680, 80], Float32, CPU>, %849:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%851:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MulOp(%851:tensor<[1, 16, 680, 680], Float32, CPU>, %852:tensor<[1], Float32, CPU>) -> (%853:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%853:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%854:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MatMulOp(%854:tensor<[1, 16, 680, 680], Float32, CPU>, %850:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%855:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%855:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%856:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.ViewOp(%856:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%856:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LinearOp(%856:tensor<[680, 1280], Float32, CPU>) -> (%857:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%857:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.12.mlp <CPU> {
        (%859:tensor<[680, 1280], Float32, CPU>) -> (%862:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%859:tensor<[680, 1280], Float32, CPU>) -> (%860:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.QuickGELUOp(%860:tensor<[680, 5120], Float32, CPU>) -> (%861:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.LinearOp(%861:tensor<[680, 5120], Float32, CPU>) -> (%862:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%862:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.13 <CPU> {
        (%863:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%884:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LayerNormOp(%863:tensor<[680, 1280], Float32, CPU>) -> (%864:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.13.attn (%864:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%878:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%863:tensor<[680, 1280], Float32, CPU>, %878:tensor<[680, 1280], Float32, CPU>) -> (%879:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LayerNormOp(%879:tensor<[680, 1280], Float32, CPU>) -> (%880:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.13.mlp (%880:tensor<[680, 1280], Float32, CPU>) -> (%883:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%879:tensor<[680, 1280], Float32, CPU>, %883:tensor<[680, 1280], Float32, CPU>) -> (%884:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%884:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.13.attn <CPU> {
        (%864:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%878:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%864:tensor<[680, 1280], Float32, CPU>) -> (%865:tensor<[680, 3840], Float32, CPU>)
            linalg.CPU.ViewOp(%865:tensor<[680, 3840], Float32, CPU>) -> (%865:tensor<[680, 3, 16, 80], Float32, CPU>)
            linalg.CPU.PermuteOp(%865:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%866:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.SplitOp(%866:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%866:tensor<[3, 680, 16, 80], Float32, CPU>, %866:tensor<[3, 680, 16, 80], Float32, CPU>, %866:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%866:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%867:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%866:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%868:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%867:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%869:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%868:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%870:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%866:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%871:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.MatMulOp(%869:tensor<[1, 16, 680, 80], Float32, CPU>, %870:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%872:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MulOp(%872:tensor<[1, 16, 680, 680], Float32, CPU>, %873:tensor<[1], Float32, CPU>) -> (%874:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%874:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%875:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MatMulOp(%875:tensor<[1, 16, 680, 680], Float32, CPU>, %871:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%876:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%876:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%877:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.ViewOp(%877:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%877:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LinearOp(%877:tensor<[680, 1280], Float32, CPU>) -> (%878:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%878:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.13.mlp <CPU> {
        (%880:tensor<[680, 1280], Float32, CPU>) -> (%883:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%880:tensor<[680, 1280], Float32, CPU>) -> (%881:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.QuickGELUOp(%881:tensor<[680, 5120], Float32, CPU>) -> (%882:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.LinearOp(%882:tensor<[680, 5120], Float32, CPU>) -> (%883:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%883:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.14 <CPU> {
        (%884:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%905:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LayerNormOp(%884:tensor<[680, 1280], Float32, CPU>) -> (%885:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.14.attn (%885:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%899:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%884:tensor<[680, 1280], Float32, CPU>, %899:tensor<[680, 1280], Float32, CPU>) -> (%900:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LayerNormOp(%900:tensor<[680, 1280], Float32, CPU>) -> (%901:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.14.mlp (%901:tensor<[680, 1280], Float32, CPU>) -> (%904:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%900:tensor<[680, 1280], Float32, CPU>, %904:tensor<[680, 1280], Float32, CPU>) -> (%905:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%905:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.14.attn <CPU> {
        (%885:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%899:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%885:tensor<[680, 1280], Float32, CPU>) -> (%886:tensor<[680, 3840], Float32, CPU>)
            linalg.CPU.ViewOp(%886:tensor<[680, 3840], Float32, CPU>) -> (%886:tensor<[680, 3, 16, 80], Float32, CPU>)
            linalg.CPU.PermuteOp(%886:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%887:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.SplitOp(%887:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%887:tensor<[3, 680, 16, 80], Float32, CPU>, %887:tensor<[3, 680, 16, 80], Float32, CPU>, %887:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%887:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%888:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%887:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%889:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%888:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%890:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%889:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%891:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%887:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%892:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.MatMulOp(%890:tensor<[1, 16, 680, 80], Float32, CPU>, %891:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%893:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MulOp(%893:tensor<[1, 16, 680, 680], Float32, CPU>, %894:tensor<[1], Float32, CPU>) -> (%895:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%895:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%896:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MatMulOp(%896:tensor<[1, 16, 680, 680], Float32, CPU>, %892:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%897:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%897:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%898:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.ViewOp(%898:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%898:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LinearOp(%898:tensor<[680, 1280], Float32, CPU>) -> (%899:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%899:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.14.mlp <CPU> {
        (%901:tensor<[680, 1280], Float32, CPU>) -> (%904:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%901:tensor<[680, 1280], Float32, CPU>) -> (%902:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.QuickGELUOp(%902:tensor<[680, 5120], Float32, CPU>) -> (%903:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.LinearOp(%903:tensor<[680, 5120], Float32, CPU>) -> (%904:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%904:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.15 <CPU> {
        (%905:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%926:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LayerNormOp(%905:tensor<[680, 1280], Float32, CPU>) -> (%906:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.15.attn (%906:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%920:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%905:tensor<[680, 1280], Float32, CPU>, %920:tensor<[680, 1280], Float32, CPU>) -> (%921:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LayerNormOp(%921:tensor<[680, 1280], Float32, CPU>) -> (%922:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.15.mlp (%922:tensor<[680, 1280], Float32, CPU>) -> (%925:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%921:tensor<[680, 1280], Float32, CPU>, %925:tensor<[680, 1280], Float32, CPU>) -> (%926:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%926:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.15.attn <CPU> {
        (%906:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%920:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%906:tensor<[680, 1280], Float32, CPU>) -> (%907:tensor<[680, 3840], Float32, CPU>)
            linalg.CPU.ViewOp(%907:tensor<[680, 3840], Float32, CPU>) -> (%907:tensor<[680, 3, 16, 80], Float32, CPU>)
            linalg.CPU.PermuteOp(%907:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%908:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.SplitOp(%908:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%908:tensor<[3, 680, 16, 80], Float32, CPU>, %908:tensor<[3, 680, 16, 80], Float32, CPU>, %908:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%908:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%909:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%908:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%910:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%909:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%911:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%910:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%912:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%908:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%913:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.MatMulOp(%911:tensor<[1, 16, 680, 80], Float32, CPU>, %912:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%914:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MulOp(%914:tensor<[1, 16, 680, 680], Float32, CPU>, %915:tensor<[1], Float32, CPU>) -> (%916:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%916:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%917:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MatMulOp(%917:tensor<[1, 16, 680, 680], Float32, CPU>, %913:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%918:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%918:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%919:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.ViewOp(%919:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%919:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LinearOp(%919:tensor<[680, 1280], Float32, CPU>) -> (%920:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%920:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.15.mlp <CPU> {
        (%922:tensor<[680, 1280], Float32, CPU>) -> (%925:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%922:tensor<[680, 1280], Float32, CPU>) -> (%923:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.QuickGELUOp(%923:tensor<[680, 5120], Float32, CPU>) -> (%924:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.LinearOp(%924:tensor<[680, 5120], Float32, CPU>) -> (%925:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%925:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.16 <CPU> {
        (%926:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%947:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LayerNormOp(%926:tensor<[680, 1280], Float32, CPU>) -> (%927:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.16.attn (%927:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%941:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%926:tensor<[680, 1280], Float32, CPU>, %941:tensor<[680, 1280], Float32, CPU>) -> (%942:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LayerNormOp(%942:tensor<[680, 1280], Float32, CPU>) -> (%943:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.16.mlp (%943:tensor<[680, 1280], Float32, CPU>) -> (%946:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%942:tensor<[680, 1280], Float32, CPU>, %946:tensor<[680, 1280], Float32, CPU>) -> (%947:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%947:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.16.attn <CPU> {
        (%927:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%941:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%927:tensor<[680, 1280], Float32, CPU>) -> (%928:tensor<[680, 3840], Float32, CPU>)
            linalg.CPU.ViewOp(%928:tensor<[680, 3840], Float32, CPU>) -> (%928:tensor<[680, 3, 16, 80], Float32, CPU>)
            linalg.CPU.PermuteOp(%928:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%929:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.SplitOp(%929:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%929:tensor<[3, 680, 16, 80], Float32, CPU>, %929:tensor<[3, 680, 16, 80], Float32, CPU>, %929:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%929:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%930:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%929:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%931:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%930:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%932:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%931:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%933:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%929:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%934:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.MatMulOp(%932:tensor<[1, 16, 680, 80], Float32, CPU>, %933:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%935:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MulOp(%935:tensor<[1, 16, 680, 680], Float32, CPU>, %936:tensor<[1], Float32, CPU>) -> (%937:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%937:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%938:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MatMulOp(%938:tensor<[1, 16, 680, 680], Float32, CPU>, %934:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%939:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%939:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%940:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.ViewOp(%940:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%940:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LinearOp(%940:tensor<[680, 1280], Float32, CPU>) -> (%941:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%941:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.16.mlp <CPU> {
        (%943:tensor<[680, 1280], Float32, CPU>) -> (%946:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%943:tensor<[680, 1280], Float32, CPU>) -> (%944:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.QuickGELUOp(%944:tensor<[680, 5120], Float32, CPU>) -> (%945:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.LinearOp(%945:tensor<[680, 5120], Float32, CPU>) -> (%946:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%946:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.17 <CPU> {
        (%947:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%968:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LayerNormOp(%947:tensor<[680, 1280], Float32, CPU>) -> (%948:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.17.attn (%948:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%962:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%947:tensor<[680, 1280], Float32, CPU>, %962:tensor<[680, 1280], Float32, CPU>) -> (%963:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LayerNormOp(%963:tensor<[680, 1280], Float32, CPU>) -> (%964:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.17.mlp (%964:tensor<[680, 1280], Float32, CPU>) -> (%967:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%963:tensor<[680, 1280], Float32, CPU>, %967:tensor<[680, 1280], Float32, CPU>) -> (%968:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%968:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.17.attn <CPU> {
        (%948:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%962:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%948:tensor<[680, 1280], Float32, CPU>) -> (%949:tensor<[680, 3840], Float32, CPU>)
            linalg.CPU.ViewOp(%949:tensor<[680, 3840], Float32, CPU>) -> (%949:tensor<[680, 3, 16, 80], Float32, CPU>)
            linalg.CPU.PermuteOp(%949:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%950:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.SplitOp(%950:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%950:tensor<[3, 680, 16, 80], Float32, CPU>, %950:tensor<[3, 680, 16, 80], Float32, CPU>, %950:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%950:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%951:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%950:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%952:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%951:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%953:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%952:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%954:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%950:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%955:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.MatMulOp(%953:tensor<[1, 16, 680, 80], Float32, CPU>, %954:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%956:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MulOp(%956:tensor<[1, 16, 680, 680], Float32, CPU>, %957:tensor<[1], Float32, CPU>) -> (%958:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%958:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%959:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MatMulOp(%959:tensor<[1, 16, 680, 680], Float32, CPU>, %955:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%960:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%960:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%961:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.ViewOp(%961:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%961:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LinearOp(%961:tensor<[680, 1280], Float32, CPU>) -> (%962:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%962:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.17.mlp <CPU> {
        (%964:tensor<[680, 1280], Float32, CPU>) -> (%967:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%964:tensor<[680, 1280], Float32, CPU>) -> (%965:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.QuickGELUOp(%965:tensor<[680, 5120], Float32, CPU>) -> (%966:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.LinearOp(%966:tensor<[680, 5120], Float32, CPU>) -> (%967:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%967:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.18 <CPU> {
        (%968:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%989:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LayerNormOp(%968:tensor<[680, 1280], Float32, CPU>) -> (%969:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.18.attn (%969:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%983:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%968:tensor<[680, 1280], Float32, CPU>, %983:tensor<[680, 1280], Float32, CPU>) -> (%984:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LayerNormOp(%984:tensor<[680, 1280], Float32, CPU>) -> (%985:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.18.mlp (%985:tensor<[680, 1280], Float32, CPU>) -> (%988:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%984:tensor<[680, 1280], Float32, CPU>, %988:tensor<[680, 1280], Float32, CPU>) -> (%989:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%989:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.18.attn <CPU> {
        (%969:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%983:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%969:tensor<[680, 1280], Float32, CPU>) -> (%970:tensor<[680, 3840], Float32, CPU>)
            linalg.CPU.ViewOp(%970:tensor<[680, 3840], Float32, CPU>) -> (%970:tensor<[680, 3, 16, 80], Float32, CPU>)
            linalg.CPU.PermuteOp(%970:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%971:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.SplitOp(%971:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%971:tensor<[3, 680, 16, 80], Float32, CPU>, %971:tensor<[3, 680, 16, 80], Float32, CPU>, %971:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%971:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%972:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%971:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%973:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%972:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%974:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%973:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%975:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%971:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%976:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.MatMulOp(%974:tensor<[1, 16, 680, 80], Float32, CPU>, %975:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%977:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MulOp(%977:tensor<[1, 16, 680, 680], Float32, CPU>, %978:tensor<[1], Float32, CPU>) -> (%979:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%979:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%980:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MatMulOp(%980:tensor<[1, 16, 680, 680], Float32, CPU>, %976:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%981:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%981:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%982:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.ViewOp(%982:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%982:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LinearOp(%982:tensor<[680, 1280], Float32, CPU>) -> (%983:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%983:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.18.mlp <CPU> {
        (%985:tensor<[680, 1280], Float32, CPU>) -> (%988:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%985:tensor<[680, 1280], Float32, CPU>) -> (%986:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.QuickGELUOp(%986:tensor<[680, 5120], Float32, CPU>) -> (%987:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.LinearOp(%987:tensor<[680, 5120], Float32, CPU>) -> (%988:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%988:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.19 <CPU> {
        (%989:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1010:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LayerNormOp(%989:tensor<[680, 1280], Float32, CPU>) -> (%990:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.19.attn (%990:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1004:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%989:tensor<[680, 1280], Float32, CPU>, %1004:tensor<[680, 1280], Float32, CPU>) -> (%1005:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LayerNormOp(%1005:tensor<[680, 1280], Float32, CPU>) -> (%1006:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.19.mlp (%1006:tensor<[680, 1280], Float32, CPU>) -> (%1009:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%1005:tensor<[680, 1280], Float32, CPU>, %1009:tensor<[680, 1280], Float32, CPU>) -> (%1010:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%1010:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.19.attn <CPU> {
        (%990:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1004:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%990:tensor<[680, 1280], Float32, CPU>) -> (%991:tensor<[680, 3840], Float32, CPU>)
            linalg.CPU.ViewOp(%991:tensor<[680, 3840], Float32, CPU>) -> (%991:tensor<[680, 3, 16, 80], Float32, CPU>)
            linalg.CPU.PermuteOp(%991:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%992:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.SplitOp(%992:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%992:tensor<[3, 680, 16, 80], Float32, CPU>, %992:tensor<[3, 680, 16, 80], Float32, CPU>, %992:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%992:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%993:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%992:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%994:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%993:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%995:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%994:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%996:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%992:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%997:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.MatMulOp(%995:tensor<[1, 16, 680, 80], Float32, CPU>, %996:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%998:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MulOp(%998:tensor<[1, 16, 680, 680], Float32, CPU>, %999:tensor<[1], Float32, CPU>) -> (%1000:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%1000:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%1001:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MatMulOp(%1001:tensor<[1, 16, 680, 680], Float32, CPU>, %997:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1002:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%1002:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1003:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.ViewOp(%1003:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1003:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LinearOp(%1003:tensor<[680, 1280], Float32, CPU>) -> (%1004:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%1004:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.19.mlp <CPU> {
        (%1006:tensor<[680, 1280], Float32, CPU>) -> (%1009:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%1006:tensor<[680, 1280], Float32, CPU>) -> (%1007:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.QuickGELUOp(%1007:tensor<[680, 5120], Float32, CPU>) -> (%1008:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.LinearOp(%1008:tensor<[680, 5120], Float32, CPU>) -> (%1009:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%1009:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.20 <CPU> {
        (%1010:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1031:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LayerNormOp(%1010:tensor<[680, 1280], Float32, CPU>) -> (%1011:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.20.attn (%1011:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1025:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%1010:tensor<[680, 1280], Float32, CPU>, %1025:tensor<[680, 1280], Float32, CPU>) -> (%1026:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LayerNormOp(%1026:tensor<[680, 1280], Float32, CPU>) -> (%1027:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.20.mlp (%1027:tensor<[680, 1280], Float32, CPU>) -> (%1030:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%1026:tensor<[680, 1280], Float32, CPU>, %1030:tensor<[680, 1280], Float32, CPU>) -> (%1031:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%1031:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.20.attn <CPU> {
        (%1011:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1025:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%1011:tensor<[680, 1280], Float32, CPU>) -> (%1012:tensor<[680, 3840], Float32, CPU>)
            linalg.CPU.ViewOp(%1012:tensor<[680, 3840], Float32, CPU>) -> (%1012:tensor<[680, 3, 16, 80], Float32, CPU>)
            linalg.CPU.PermuteOp(%1012:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%1013:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.SplitOp(%1013:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%1013:tensor<[3, 680, 16, 80], Float32, CPU>, %1013:tensor<[3, 680, 16, 80], Float32, CPU>, %1013:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%1013:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1014:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%1013:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1015:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%1014:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1016:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%1015:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1017:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%1013:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%1018:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.MatMulOp(%1016:tensor<[1, 16, 680, 80], Float32, CPU>, %1017:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1019:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MulOp(%1019:tensor<[1, 16, 680, 680], Float32, CPU>, %1020:tensor<[1], Float32, CPU>) -> (%1021:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%1021:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%1022:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MatMulOp(%1022:tensor<[1, 16, 680, 680], Float32, CPU>, %1018:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1023:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%1023:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1024:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.ViewOp(%1024:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1024:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LinearOp(%1024:tensor<[680, 1280], Float32, CPU>) -> (%1025:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%1025:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.20.mlp <CPU> {
        (%1027:tensor<[680, 1280], Float32, CPU>) -> (%1030:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%1027:tensor<[680, 1280], Float32, CPU>) -> (%1028:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.QuickGELUOp(%1028:tensor<[680, 5120], Float32, CPU>) -> (%1029:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.LinearOp(%1029:tensor<[680, 5120], Float32, CPU>) -> (%1030:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%1030:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.21 <CPU> {
        (%1031:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1052:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LayerNormOp(%1031:tensor<[680, 1280], Float32, CPU>) -> (%1032:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.21.attn (%1032:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1046:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%1031:tensor<[680, 1280], Float32, CPU>, %1046:tensor<[680, 1280], Float32, CPU>) -> (%1047:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LayerNormOp(%1047:tensor<[680, 1280], Float32, CPU>) -> (%1048:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.21.mlp (%1048:tensor<[680, 1280], Float32, CPU>) -> (%1051:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%1047:tensor<[680, 1280], Float32, CPU>, %1051:tensor<[680, 1280], Float32, CPU>) -> (%1052:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%1052:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.21.attn <CPU> {
        (%1032:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1046:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%1032:tensor<[680, 1280], Float32, CPU>) -> (%1033:tensor<[680, 3840], Float32, CPU>)
            linalg.CPU.ViewOp(%1033:tensor<[680, 3840], Float32, CPU>) -> (%1033:tensor<[680, 3, 16, 80], Float32, CPU>)
            linalg.CPU.PermuteOp(%1033:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%1034:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.SplitOp(%1034:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%1034:tensor<[3, 680, 16, 80], Float32, CPU>, %1034:tensor<[3, 680, 16, 80], Float32, CPU>, %1034:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%1034:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1035:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%1034:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1036:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%1035:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1037:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%1036:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1038:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%1034:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%1039:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.MatMulOp(%1037:tensor<[1, 16, 680, 80], Float32, CPU>, %1038:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1040:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MulOp(%1040:tensor<[1, 16, 680, 680], Float32, CPU>, %1041:tensor<[1], Float32, CPU>) -> (%1042:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%1042:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%1043:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MatMulOp(%1043:tensor<[1, 16, 680, 680], Float32, CPU>, %1039:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1044:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%1044:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1045:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.ViewOp(%1045:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1045:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LinearOp(%1045:tensor<[680, 1280], Float32, CPU>) -> (%1046:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%1046:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.21.mlp <CPU> {
        (%1048:tensor<[680, 1280], Float32, CPU>) -> (%1051:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%1048:tensor<[680, 1280], Float32, CPU>) -> (%1049:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.QuickGELUOp(%1049:tensor<[680, 5120], Float32, CPU>) -> (%1050:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.LinearOp(%1050:tensor<[680, 5120], Float32, CPU>) -> (%1051:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%1051:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.22 <CPU> {
        (%1052:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1073:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LayerNormOp(%1052:tensor<[680, 1280], Float32, CPU>) -> (%1053:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.22.attn (%1053:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1067:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%1052:tensor<[680, 1280], Float32, CPU>, %1067:tensor<[680, 1280], Float32, CPU>) -> (%1068:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LayerNormOp(%1068:tensor<[680, 1280], Float32, CPU>) -> (%1069:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.22.mlp (%1069:tensor<[680, 1280], Float32, CPU>) -> (%1072:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%1068:tensor<[680, 1280], Float32, CPU>, %1072:tensor<[680, 1280], Float32, CPU>) -> (%1073:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%1073:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.22.attn <CPU> {
        (%1053:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1067:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%1053:tensor<[680, 1280], Float32, CPU>) -> (%1054:tensor<[680, 3840], Float32, CPU>)
            linalg.CPU.ViewOp(%1054:tensor<[680, 3840], Float32, CPU>) -> (%1054:tensor<[680, 3, 16, 80], Float32, CPU>)
            linalg.CPU.PermuteOp(%1054:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%1055:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.SplitOp(%1055:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%1055:tensor<[3, 680, 16, 80], Float32, CPU>, %1055:tensor<[3, 680, 16, 80], Float32, CPU>, %1055:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%1055:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1056:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%1055:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1057:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%1056:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1058:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%1057:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1059:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%1055:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%1060:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.MatMulOp(%1058:tensor<[1, 16, 680, 80], Float32, CPU>, %1059:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1061:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MulOp(%1061:tensor<[1, 16, 680, 680], Float32, CPU>, %1062:tensor<[1], Float32, CPU>) -> (%1063:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%1063:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%1064:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MatMulOp(%1064:tensor<[1, 16, 680, 680], Float32, CPU>, %1060:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1065:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%1065:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1066:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.ViewOp(%1066:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1066:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LinearOp(%1066:tensor<[680, 1280], Float32, CPU>) -> (%1067:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%1067:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.22.mlp <CPU> {
        (%1069:tensor<[680, 1280], Float32, CPU>) -> (%1072:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%1069:tensor<[680, 1280], Float32, CPU>) -> (%1070:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.QuickGELUOp(%1070:tensor<[680, 5120], Float32, CPU>) -> (%1071:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.LinearOp(%1071:tensor<[680, 5120], Float32, CPU>) -> (%1072:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%1072:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.23 <CPU> {
        (%1073:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1094:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LayerNormOp(%1073:tensor<[680, 1280], Float32, CPU>) -> (%1074:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.23.attn (%1074:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1088:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%1073:tensor<[680, 1280], Float32, CPU>, %1088:tensor<[680, 1280], Float32, CPU>) -> (%1089:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LayerNormOp(%1089:tensor<[680, 1280], Float32, CPU>) -> (%1090:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.23.mlp (%1090:tensor<[680, 1280], Float32, CPU>) -> (%1093:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%1089:tensor<[680, 1280], Float32, CPU>, %1093:tensor<[680, 1280], Float32, CPU>) -> (%1094:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%1094:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.23.attn <CPU> {
        (%1074:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1088:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%1074:tensor<[680, 1280], Float32, CPU>) -> (%1075:tensor<[680, 3840], Float32, CPU>)
            linalg.CPU.ViewOp(%1075:tensor<[680, 3840], Float32, CPU>) -> (%1075:tensor<[680, 3, 16, 80], Float32, CPU>)
            linalg.CPU.PermuteOp(%1075:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%1076:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.SplitOp(%1076:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%1076:tensor<[3, 680, 16, 80], Float32, CPU>, %1076:tensor<[3, 680, 16, 80], Float32, CPU>, %1076:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%1076:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1077:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%1076:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1078:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%1077:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1079:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%1078:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1080:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%1076:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%1081:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.MatMulOp(%1079:tensor<[1, 16, 680, 80], Float32, CPU>, %1080:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1082:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MulOp(%1082:tensor<[1, 16, 680, 680], Float32, CPU>, %1083:tensor<[1], Float32, CPU>) -> (%1084:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%1084:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%1085:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MatMulOp(%1085:tensor<[1, 16, 680, 680], Float32, CPU>, %1081:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1086:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%1086:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1087:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.ViewOp(%1087:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1087:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LinearOp(%1087:tensor<[680, 1280], Float32, CPU>) -> (%1088:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%1088:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.23.mlp <CPU> {
        (%1090:tensor<[680, 1280], Float32, CPU>) -> (%1093:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%1090:tensor<[680, 1280], Float32, CPU>) -> (%1091:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.QuickGELUOp(%1091:tensor<[680, 5120], Float32, CPU>) -> (%1092:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.LinearOp(%1092:tensor<[680, 5120], Float32, CPU>) -> (%1093:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%1093:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.24 <CPU> {
        (%1094:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1115:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LayerNormOp(%1094:tensor<[680, 1280], Float32, CPU>) -> (%1095:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.24.attn (%1095:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1109:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%1094:tensor<[680, 1280], Float32, CPU>, %1109:tensor<[680, 1280], Float32, CPU>) -> (%1110:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LayerNormOp(%1110:tensor<[680, 1280], Float32, CPU>) -> (%1111:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.24.mlp (%1111:tensor<[680, 1280], Float32, CPU>) -> (%1114:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%1110:tensor<[680, 1280], Float32, CPU>, %1114:tensor<[680, 1280], Float32, CPU>) -> (%1115:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%1115:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.24.attn <CPU> {
        (%1095:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1109:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%1095:tensor<[680, 1280], Float32, CPU>) -> (%1096:tensor<[680, 3840], Float32, CPU>)
            linalg.CPU.ViewOp(%1096:tensor<[680, 3840], Float32, CPU>) -> (%1096:tensor<[680, 3, 16, 80], Float32, CPU>)
            linalg.CPU.PermuteOp(%1096:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%1097:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.SplitOp(%1097:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%1097:tensor<[3, 680, 16, 80], Float32, CPU>, %1097:tensor<[3, 680, 16, 80], Float32, CPU>, %1097:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%1097:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1098:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%1097:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1099:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%1098:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1100:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%1099:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1101:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%1097:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%1102:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.MatMulOp(%1100:tensor<[1, 16, 680, 80], Float32, CPU>, %1101:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1103:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MulOp(%1103:tensor<[1, 16, 680, 680], Float32, CPU>, %1104:tensor<[1], Float32, CPU>) -> (%1105:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%1105:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%1106:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MatMulOp(%1106:tensor<[1, 16, 680, 680], Float32, CPU>, %1102:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1107:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%1107:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1108:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.ViewOp(%1108:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1108:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LinearOp(%1108:tensor<[680, 1280], Float32, CPU>) -> (%1109:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%1109:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.24.mlp <CPU> {
        (%1111:tensor<[680, 1280], Float32, CPU>) -> (%1114:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%1111:tensor<[680, 1280], Float32, CPU>) -> (%1112:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.QuickGELUOp(%1112:tensor<[680, 5120], Float32, CPU>) -> (%1113:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.LinearOp(%1113:tensor<[680, 5120], Float32, CPU>) -> (%1114:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%1114:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.25 <CPU> {
        (%1115:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1136:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LayerNormOp(%1115:tensor<[680, 1280], Float32, CPU>) -> (%1116:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.25.attn (%1116:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1130:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%1115:tensor<[680, 1280], Float32, CPU>, %1130:tensor<[680, 1280], Float32, CPU>) -> (%1131:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LayerNormOp(%1131:tensor<[680, 1280], Float32, CPU>) -> (%1132:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.25.mlp (%1132:tensor<[680, 1280], Float32, CPU>) -> (%1135:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%1131:tensor<[680, 1280], Float32, CPU>, %1135:tensor<[680, 1280], Float32, CPU>) -> (%1136:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%1136:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.25.attn <CPU> {
        (%1116:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1130:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%1116:tensor<[680, 1280], Float32, CPU>) -> (%1117:tensor<[680, 3840], Float32, CPU>)
            linalg.CPU.ViewOp(%1117:tensor<[680, 3840], Float32, CPU>) -> (%1117:tensor<[680, 3, 16, 80], Float32, CPU>)
            linalg.CPU.PermuteOp(%1117:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%1118:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.SplitOp(%1118:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%1118:tensor<[3, 680, 16, 80], Float32, CPU>, %1118:tensor<[3, 680, 16, 80], Float32, CPU>, %1118:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%1118:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1119:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%1118:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1120:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%1119:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1121:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%1120:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1122:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%1118:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%1123:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.MatMulOp(%1121:tensor<[1, 16, 680, 80], Float32, CPU>, %1122:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1124:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MulOp(%1124:tensor<[1, 16, 680, 680], Float32, CPU>, %1125:tensor<[1], Float32, CPU>) -> (%1126:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%1126:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%1127:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MatMulOp(%1127:tensor<[1, 16, 680, 680], Float32, CPU>, %1123:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1128:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%1128:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1129:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.ViewOp(%1129:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1129:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LinearOp(%1129:tensor<[680, 1280], Float32, CPU>) -> (%1130:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%1130:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.25.mlp <CPU> {
        (%1132:tensor<[680, 1280], Float32, CPU>) -> (%1135:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%1132:tensor<[680, 1280], Float32, CPU>) -> (%1133:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.QuickGELUOp(%1133:tensor<[680, 5120], Float32, CPU>) -> (%1134:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.LinearOp(%1134:tensor<[680, 5120], Float32, CPU>) -> (%1135:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%1135:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.26 <CPU> {
        (%1136:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1157:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LayerNormOp(%1136:tensor<[680, 1280], Float32, CPU>) -> (%1137:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.26.attn (%1137:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1151:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%1136:tensor<[680, 1280], Float32, CPU>, %1151:tensor<[680, 1280], Float32, CPU>) -> (%1152:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LayerNormOp(%1152:tensor<[680, 1280], Float32, CPU>) -> (%1153:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.26.mlp (%1153:tensor<[680, 1280], Float32, CPU>) -> (%1156:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%1152:tensor<[680, 1280], Float32, CPU>, %1156:tensor<[680, 1280], Float32, CPU>) -> (%1157:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%1157:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.26.attn <CPU> {
        (%1137:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1151:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%1137:tensor<[680, 1280], Float32, CPU>) -> (%1138:tensor<[680, 3840], Float32, CPU>)
            linalg.CPU.ViewOp(%1138:tensor<[680, 3840], Float32, CPU>) -> (%1138:tensor<[680, 3, 16, 80], Float32, CPU>)
            linalg.CPU.PermuteOp(%1138:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%1139:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.SplitOp(%1139:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%1139:tensor<[3, 680, 16, 80], Float32, CPU>, %1139:tensor<[3, 680, 16, 80], Float32, CPU>, %1139:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%1139:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1140:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%1139:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1141:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%1140:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1142:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%1141:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1143:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%1139:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%1144:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.MatMulOp(%1142:tensor<[1, 16, 680, 80], Float32, CPU>, %1143:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1145:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MulOp(%1145:tensor<[1, 16, 680, 680], Float32, CPU>, %1146:tensor<[1], Float32, CPU>) -> (%1147:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%1147:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%1148:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MatMulOp(%1148:tensor<[1, 16, 680, 680], Float32, CPU>, %1144:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1149:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%1149:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1150:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.ViewOp(%1150:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1150:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LinearOp(%1150:tensor<[680, 1280], Float32, CPU>) -> (%1151:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%1151:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.26.mlp <CPU> {
        (%1153:tensor<[680, 1280], Float32, CPU>) -> (%1156:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%1153:tensor<[680, 1280], Float32, CPU>) -> (%1154:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.QuickGELUOp(%1154:tensor<[680, 5120], Float32, CPU>) -> (%1155:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.LinearOp(%1155:tensor<[680, 5120], Float32, CPU>) -> (%1156:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%1156:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.27 <CPU> {
        (%1157:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1178:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LayerNormOp(%1157:tensor<[680, 1280], Float32, CPU>) -> (%1158:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.27.attn (%1158:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1172:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%1157:tensor<[680, 1280], Float32, CPU>, %1172:tensor<[680, 1280], Float32, CPU>) -> (%1173:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LayerNormOp(%1173:tensor<[680, 1280], Float32, CPU>) -> (%1174:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.27.mlp (%1174:tensor<[680, 1280], Float32, CPU>) -> (%1177:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%1173:tensor<[680, 1280], Float32, CPU>, %1177:tensor<[680, 1280], Float32, CPU>) -> (%1178:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%1178:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.27.attn <CPU> {
        (%1158:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1172:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%1158:tensor<[680, 1280], Float32, CPU>) -> (%1159:tensor<[680, 3840], Float32, CPU>)
            linalg.CPU.ViewOp(%1159:tensor<[680, 3840], Float32, CPU>) -> (%1159:tensor<[680, 3, 16, 80], Float32, CPU>)
            linalg.CPU.PermuteOp(%1159:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%1160:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.SplitOp(%1160:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%1160:tensor<[3, 680, 16, 80], Float32, CPU>, %1160:tensor<[3, 680, 16, 80], Float32, CPU>, %1160:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%1160:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1161:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%1160:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1162:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%1161:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1163:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%1162:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1164:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%1160:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%1165:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.MatMulOp(%1163:tensor<[1, 16, 680, 80], Float32, CPU>, %1164:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1166:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MulOp(%1166:tensor<[1, 16, 680, 680], Float32, CPU>, %1167:tensor<[1], Float32, CPU>) -> (%1168:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%1168:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%1169:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MatMulOp(%1169:tensor<[1, 16, 680, 680], Float32, CPU>, %1165:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1170:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%1170:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1171:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.ViewOp(%1171:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1171:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LinearOp(%1171:tensor<[680, 1280], Float32, CPU>) -> (%1172:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%1172:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.27.mlp <CPU> {
        (%1174:tensor<[680, 1280], Float32, CPU>) -> (%1177:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%1174:tensor<[680, 1280], Float32, CPU>) -> (%1175:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.QuickGELUOp(%1175:tensor<[680, 5120], Float32, CPU>) -> (%1176:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.LinearOp(%1176:tensor<[680, 5120], Float32, CPU>) -> (%1177:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%1177:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.28 <CPU> {
        (%1178:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1199:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LayerNormOp(%1178:tensor<[680, 1280], Float32, CPU>) -> (%1179:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.28.attn (%1179:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1193:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%1178:tensor<[680, 1280], Float32, CPU>, %1193:tensor<[680, 1280], Float32, CPU>) -> (%1194:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LayerNormOp(%1194:tensor<[680, 1280], Float32, CPU>) -> (%1195:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.28.mlp (%1195:tensor<[680, 1280], Float32, CPU>) -> (%1198:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%1194:tensor<[680, 1280], Float32, CPU>, %1198:tensor<[680, 1280], Float32, CPU>) -> (%1199:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%1199:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.28.attn <CPU> {
        (%1179:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1193:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%1179:tensor<[680, 1280], Float32, CPU>) -> (%1180:tensor<[680, 3840], Float32, CPU>)
            linalg.CPU.ViewOp(%1180:tensor<[680, 3840], Float32, CPU>) -> (%1180:tensor<[680, 3, 16, 80], Float32, CPU>)
            linalg.CPU.PermuteOp(%1180:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%1181:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.SplitOp(%1181:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%1181:tensor<[3, 680, 16, 80], Float32, CPU>, %1181:tensor<[3, 680, 16, 80], Float32, CPU>, %1181:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%1181:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1182:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%1181:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1183:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%1182:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1184:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%1183:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1185:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%1181:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%1186:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.MatMulOp(%1184:tensor<[1, 16, 680, 80], Float32, CPU>, %1185:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1187:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MulOp(%1187:tensor<[1, 16, 680, 680], Float32, CPU>, %1188:tensor<[1], Float32, CPU>) -> (%1189:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%1189:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%1190:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MatMulOp(%1190:tensor<[1, 16, 680, 680], Float32, CPU>, %1186:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1191:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%1191:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1192:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.ViewOp(%1192:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1192:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LinearOp(%1192:tensor<[680, 1280], Float32, CPU>) -> (%1193:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%1193:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.28.mlp <CPU> {
        (%1195:tensor<[680, 1280], Float32, CPU>) -> (%1198:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%1195:tensor<[680, 1280], Float32, CPU>) -> (%1196:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.QuickGELUOp(%1196:tensor<[680, 5120], Float32, CPU>) -> (%1197:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.LinearOp(%1197:tensor<[680, 5120], Float32, CPU>) -> (%1198:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%1198:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.29 <CPU> {
        (%1199:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1220:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LayerNormOp(%1199:tensor<[680, 1280], Float32, CPU>) -> (%1200:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.29.attn (%1200:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1214:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%1199:tensor<[680, 1280], Float32, CPU>, %1214:tensor<[680, 1280], Float32, CPU>) -> (%1215:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LayerNormOp(%1215:tensor<[680, 1280], Float32, CPU>) -> (%1216:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.29.mlp (%1216:tensor<[680, 1280], Float32, CPU>) -> (%1219:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%1215:tensor<[680, 1280], Float32, CPU>, %1219:tensor<[680, 1280], Float32, CPU>) -> (%1220:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%1220:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.29.attn <CPU> {
        (%1200:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1214:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%1200:tensor<[680, 1280], Float32, CPU>) -> (%1201:tensor<[680, 3840], Float32, CPU>)
            linalg.CPU.ViewOp(%1201:tensor<[680, 3840], Float32, CPU>) -> (%1201:tensor<[680, 3, 16, 80], Float32, CPU>)
            linalg.CPU.PermuteOp(%1201:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%1202:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.SplitOp(%1202:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%1202:tensor<[3, 680, 16, 80], Float32, CPU>, %1202:tensor<[3, 680, 16, 80], Float32, CPU>, %1202:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%1202:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1203:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%1202:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1204:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%1203:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1205:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%1204:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1206:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%1202:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%1207:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.MatMulOp(%1205:tensor<[1, 16, 680, 80], Float32, CPU>, %1206:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1208:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MulOp(%1208:tensor<[1, 16, 680, 680], Float32, CPU>, %1209:tensor<[1], Float32, CPU>) -> (%1210:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%1210:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%1211:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MatMulOp(%1211:tensor<[1, 16, 680, 680], Float32, CPU>, %1207:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1212:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%1212:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1213:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.ViewOp(%1213:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1213:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LinearOp(%1213:tensor<[680, 1280], Float32, CPU>) -> (%1214:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%1214:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.29.mlp <CPU> {
        (%1216:tensor<[680, 1280], Float32, CPU>) -> (%1219:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%1216:tensor<[680, 1280], Float32, CPU>) -> (%1217:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.QuickGELUOp(%1217:tensor<[680, 5120], Float32, CPU>) -> (%1218:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.LinearOp(%1218:tensor<[680, 5120], Float32, CPU>) -> (%1219:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%1219:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.30 <CPU> {
        (%1220:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1241:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LayerNormOp(%1220:tensor<[680, 1280], Float32, CPU>) -> (%1221:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.30.attn (%1221:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1235:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%1220:tensor<[680, 1280], Float32, CPU>, %1235:tensor<[680, 1280], Float32, CPU>) -> (%1236:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LayerNormOp(%1236:tensor<[680, 1280], Float32, CPU>) -> (%1237:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.30.mlp (%1237:tensor<[680, 1280], Float32, CPU>) -> (%1240:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%1236:tensor<[680, 1280], Float32, CPU>, %1240:tensor<[680, 1280], Float32, CPU>) -> (%1241:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%1241:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.30.attn <CPU> {
        (%1221:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1235:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%1221:tensor<[680, 1280], Float32, CPU>) -> (%1222:tensor<[680, 3840], Float32, CPU>)
            linalg.CPU.ViewOp(%1222:tensor<[680, 3840], Float32, CPU>) -> (%1222:tensor<[680, 3, 16, 80], Float32, CPU>)
            linalg.CPU.PermuteOp(%1222:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%1223:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.SplitOp(%1223:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%1223:tensor<[3, 680, 16, 80], Float32, CPU>, %1223:tensor<[3, 680, 16, 80], Float32, CPU>, %1223:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%1223:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1224:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%1223:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1225:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%1224:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1226:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%1225:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1227:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%1223:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%1228:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.MatMulOp(%1226:tensor<[1, 16, 680, 80], Float32, CPU>, %1227:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1229:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MulOp(%1229:tensor<[1, 16, 680, 680], Float32, CPU>, %1230:tensor<[1], Float32, CPU>) -> (%1231:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%1231:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%1232:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MatMulOp(%1232:tensor<[1, 16, 680, 680], Float32, CPU>, %1228:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1233:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%1233:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1234:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.ViewOp(%1234:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1234:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LinearOp(%1234:tensor<[680, 1280], Float32, CPU>) -> (%1235:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%1235:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.30.mlp <CPU> {
        (%1237:tensor<[680, 1280], Float32, CPU>) -> (%1240:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%1237:tensor<[680, 1280], Float32, CPU>) -> (%1238:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.QuickGELUOp(%1238:tensor<[680, 5120], Float32, CPU>) -> (%1239:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.LinearOp(%1239:tensor<[680, 5120], Float32, CPU>) -> (%1240:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%1240:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.31 <CPU> {
        (%1241:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1262:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LayerNormOp(%1241:tensor<[680, 1280], Float32, CPU>) -> (%1242:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.31.attn (%1242:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1256:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%1241:tensor<[680, 1280], Float32, CPU>, %1256:tensor<[680, 1280], Float32, CPU>) -> (%1257:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LayerNormOp(%1257:tensor<[680, 1280], Float32, CPU>) -> (%1258:tensor<[680, 1280], Float32, CPU>)
            graph.CallGraphOp @visual.blocks.31.mlp (%1258:tensor<[680, 1280], Float32, CPU>) -> (%1261:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.AddOp(%1257:tensor<[680, 1280], Float32, CPU>, %1261:tensor<[680, 1280], Float32, CPU>) -> (%1262:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%1262:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.31.attn <CPU> {
        (%1242:tensor<[680, 1280], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1256:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%1242:tensor<[680, 1280], Float32, CPU>) -> (%1243:tensor<[680, 3840], Float32, CPU>)
            linalg.CPU.ViewOp(%1243:tensor<[680, 3840], Float32, CPU>) -> (%1243:tensor<[680, 3, 16, 80], Float32, CPU>)
            linalg.CPU.PermuteOp(%1243:tensor<[680, 3, 16, 80], Float32, CPU>) -> (%1244:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.SplitOp(%1244:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%1244:tensor<[3, 680, 16, 80], Float32, CPU>, %1244:tensor<[3, 680, 16, 80], Float32, CPU>, %1244:tensor<[3, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%1244:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1245:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.VisionRoPEOp(%1244:tensor<[3, 680, 16, 80], Float32, CPU>, %588:tensor<[680, 40], Float32, CPU>, %589:tensor<[680, 40], Float32, CPU>) -> (%1246:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%1245:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1247:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%1246:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1248:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%1244:tensor<[3, 680, 16, 80], Float32, CPU>) -> (%1249:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.MatMulOp(%1247:tensor<[1, 16, 680, 80], Float32, CPU>, %1248:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1250:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MulOp(%1250:tensor<[1, 16, 680, 680], Float32, CPU>, %1251:tensor<[1], Float32, CPU>) -> (%1252:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%1252:tensor<[1, 16, 680, 680], Float32, CPU>) -> (%1253:tensor<[1, 16, 680, 680], Float32, CPU>)
            linalg.CPU.MatMulOp(%1253:tensor<[1, 16, 680, 680], Float32, CPU>, %1249:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1254:tensor<[1, 16, 680, 80], Float32, CPU>)
            linalg.CPU.TransposeOp(%1254:tensor<[1, 16, 680, 80], Float32, CPU>) -> (%1255:tensor<[1, 680, 16, 80], Float32, CPU>)
            linalg.CPU.ViewOp(%1255:tensor<[1, 680, 16, 80], Float32, CPU>) -> (%1255:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.LinearOp(%1255:tensor<[680, 1280], Float32, CPU>) -> (%1256:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%1256:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.blocks.31.mlp <CPU> {
        (%1258:tensor<[680, 1280], Float32, CPU>) -> (%1261:tensor<[680, 1280], Float32, CPU>) {
            linalg.CPU.LinearOp(%1258:tensor<[680, 1280], Float32, CPU>) -> (%1259:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.QuickGELUOp(%1259:tensor<[680, 5120], Float32, CPU>) -> (%1260:tensor<[680, 5120], Float32, CPU>)
            linalg.CPU.LinearOp(%1260:tensor<[680, 5120], Float32, CPU>) -> (%1261:tensor<[680, 1280], Float32, CPU>)
            cf.ReturnOp (%1261:tensor<[680, 1280], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @visual.merger <CPU> {
        (%1262:tensor<[680, 1280], Float32, CPU>) -> (%1266:tensor<[170, 1536], Float32, CPU>) {
            linalg.CPU.LayerNormOp(%1262:tensor<[680, 1280], Float32, CPU>) -> (%1263:tensor<[680, 1280], Float32, CPU>)
            linalg.CPU.ViewOp(%1263:tensor<[680, 1280], Float32, CPU>) -> (%1263:tensor<[170, 5120], Float32, CPU>)
            linalg.CPU.LinearOp(%1263:tensor<[170, 5120], Float32, CPU>) -> (%1264:tensor<[170, 5120], Float32, CPU>)
            linalg.CPU.GELUOp(%1264:tensor<[170, 5120], Float32, CPU>) -> (%1265:tensor<[170, 5120], Float32, CPU>)
            linalg.CPU.LinearOp(%1265:tensor<[170, 5120], Float32, CPU>) -> (%1266:tensor<[170, 1536], Float32, CPU>)
            cf.ReturnOp (%1266:tensor<[170, 1536], Float32, CPU>) -> ()
        }
    }
    // goooooooooood!!!
    // Everything works fine when tracing ViT
    // Copy visual_embeddings into input_embedding
    linalg.CPU.SliceOp(%583:tensor<[1, 192, 1536], Float32, CPU>) -> (%583:tensor<[1, 192, 1536], Float32, CPU>)
    linalg.CPU.CopyOp(%1266:tensor<[170, 1536], Float32, CPU>, %583:tensor<[1, 192, 1536], Float32, CPU>) -> ()
    // inputs to llm is: [input_embeddings, llm_embedding_sin, llm_embedding_cos]
    graph.CallGraphOp @model (%583:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%2029:tensor<[1, 1, 151936], Float32, CPU>)
    graph.SubGraphOp @model <CPU> {
        (%583:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%2029:tensor<[1, 1, 151936], Float32, CPU>) {
            graph.CallGraphOp @model.layers.0 (%583:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1298:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.1 (%1298:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1325:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.2 (%1325:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1352:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.3 (%1352:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1379:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.4 (%1379:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1406:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.5 (%1406:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1433:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.6 (%1433:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1460:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.7 (%1460:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1487:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.8 (%1487:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1514:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.9 (%1514:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1541:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.10 (%1541:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1568:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.11 (%1568:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1595:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.12 (%1595:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1622:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.13 (%1622:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1649:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.14 (%1649:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1676:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.15 (%1676:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1703:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.16 (%1703:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1730:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.17 (%1730:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1757:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.18 (%1757:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1784:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.19 (%1784:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1811:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.20 (%1811:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1838:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.21 (%1838:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1865:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.22 (%1865:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1892:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.23 (%1892:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1919:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.24 (%1919:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1946:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.25 (%1946:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1973:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.26 (%1973:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%2000:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.27 (%2000:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%2027:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.RMSNormOp(%2027:tensor<[1, 192, 1536], Float32, CPU>) -> (%2028:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.SliceOp(%2028:tensor<[1, 192, 1536], Float32, CPU>) -> (%2028:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.LinearOp(%2028:tensor<[1, 192, 1536], Float32, CPU>) -> (%2029:tensor<[1, 1, 151936], Float32, CPU>)
            cf.ReturnOp (%2029:tensor<[1, 1, 151936], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.0 <CPU> {
        (%583:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1298:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.RMSNormOp(%583:tensor<[1, 192, 1536], Float32, CPU>) -> (%1272:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.0.self_attn (%1272:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1290:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.AddOp(%1290:tensor<[1, 192, 1536], Float32, CPU>, %583:tensor<[1, 192, 1536], Float32, CPU>) -> (%1291:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.RMSNormOp(%1291:tensor<[1, 192, 1536], Float32, CPU>) -> (%1292:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.0.mlp (%1292:tensor<[1, 192, 1536], Float32, CPU>) -> (%1297:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.AddOp(%1297:tensor<[1, 192, 1536], Float32, CPU>, %1291:tensor<[1, 192, 1536], Float32, CPU>) -> (%1298:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1298:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.0.self_attn <CPU> {
        (%1272:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1290:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.LinearOp(%1272:tensor<[1, 192, 1536], Float32, CPU>) -> (%1273:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.LinearOp(%1272:tensor<[1, 192, 1536], Float32, CPU>) -> (%1274:tensor<[1, 192, 256], Float32, CPU>)
            linalg.CPU.LinearOp(%1272:tensor<[1, 192, 1536], Float32, CPU>) -> (%1275:tensor<[1, 192, 256], Float32, CPU>)
            linalg.CPU.ViewOp(%1273:tensor<[1, 192, 1536], Float32, CPU>) -> (%1273:tensor<[1, 192, 12, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1274:tensor<[1, 192, 256], Float32, CPU>) -> (%1274:tensor<[1, 192, 2, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1275:tensor<[1, 192, 256], Float32, CPU>) -> (%1275:tensor<[1, 192, 2, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1273:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1276:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1274:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1277:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1275:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1278:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.MultimodalRoPEOp(%1276:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1279:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.MultimodalRoPEOp(%1277:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1280:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.KVCacheOp(%1280:tensor<[1, 2, 192, 128], Float32, CPU>, %1278:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1281:tensor<[1, 2, 192, 128], Float32, CPU>, %1282:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.MatMulOp(%1279:tensor<[1, 12, 192, 128], Float32, CPU>, %1281:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1283:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.MulOp(%1283:tensor<[1, 12, 192, 192], Float32, CPU>, %1284:tensor<[1], Float32, CPU>) -> (%1285:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.CausalMaskOp(%1285:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1286:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%1286:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1287:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.MatMulOp(%1287:tensor<[1, 12, 192, 192], Float32, CPU>, %1282:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1288:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1288:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1289:tensor<[1, 192, 12, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1289:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1289:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.LinearOp(%1289:tensor<[1, 192, 1536], Float32, CPU>) -> (%1290:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1290:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.0.mlp <CPU> {
        (%1292:tensor<[1, 192, 1536], Float32, CPU>) -> (%1297:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.LinearOp(%1292:tensor<[1, 192, 1536], Float32, CPU>) -> (%1293:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.SiLUOp(%1293:tensor<[1, 192, 8960], Float32, CPU>) -> (%1294:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.LinearOp(%1292:tensor<[1, 192, 1536], Float32, CPU>) -> (%1295:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.MulOp(%1294:tensor<[1, 192, 8960], Float32, CPU>, %1295:tensor<[1, 192, 8960], Float32, CPU>) -> (%1296:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.LinearOp(%1296:tensor<[1, 192, 8960], Float32, CPU>) -> (%1297:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1297:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.1 <CPU> {
        (%1298:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1325:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.RMSNormOp(%1298:tensor<[1, 192, 1536], Float32, CPU>) -> (%1299:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.1.self_attn (%1299:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1317:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.AddOp(%1317:tensor<[1, 192, 1536], Float32, CPU>, %1298:tensor<[1, 192, 1536], Float32, CPU>) -> (%1318:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.RMSNormOp(%1318:tensor<[1, 192, 1536], Float32, CPU>) -> (%1319:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.1.mlp (%1319:tensor<[1, 192, 1536], Float32, CPU>) -> (%1324:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.AddOp(%1324:tensor<[1, 192, 1536], Float32, CPU>, %1318:tensor<[1, 192, 1536], Float32, CPU>) -> (%1325:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1325:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.1.self_attn <CPU> {
        (%1299:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1317:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.LinearOp(%1299:tensor<[1, 192, 1536], Float32, CPU>) -> (%1300:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.LinearOp(%1299:tensor<[1, 192, 1536], Float32, CPU>) -> (%1301:tensor<[1, 192, 256], Float32, CPU>)
            linalg.CPU.LinearOp(%1299:tensor<[1, 192, 1536], Float32, CPU>) -> (%1302:tensor<[1, 192, 256], Float32, CPU>)
            linalg.CPU.ViewOp(%1300:tensor<[1, 192, 1536], Float32, CPU>) -> (%1300:tensor<[1, 192, 12, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1301:tensor<[1, 192, 256], Float32, CPU>) -> (%1301:tensor<[1, 192, 2, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1302:tensor<[1, 192, 256], Float32, CPU>) -> (%1302:tensor<[1, 192, 2, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1300:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1303:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1301:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1304:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1302:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1305:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.MultimodalRoPEOp(%1303:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1306:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.MultimodalRoPEOp(%1304:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1307:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.KVCacheOp(%1307:tensor<[1, 2, 192, 128], Float32, CPU>, %1305:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1308:tensor<[1, 2, 192, 128], Float32, CPU>, %1309:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.MatMulOp(%1306:tensor<[1, 12, 192, 128], Float32, CPU>, %1308:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1310:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.MulOp(%1310:tensor<[1, 12, 192, 192], Float32, CPU>, %1311:tensor<[1], Float32, CPU>) -> (%1312:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.CausalMaskOp(%1312:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1313:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%1313:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1314:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.MatMulOp(%1314:tensor<[1, 12, 192, 192], Float32, CPU>, %1309:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1315:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1315:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1316:tensor<[1, 192, 12, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1316:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1316:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.LinearOp(%1316:tensor<[1, 192, 1536], Float32, CPU>) -> (%1317:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1317:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.1.mlp <CPU> {
        (%1319:tensor<[1, 192, 1536], Float32, CPU>) -> (%1324:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.LinearOp(%1319:tensor<[1, 192, 1536], Float32, CPU>) -> (%1320:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.SiLUOp(%1320:tensor<[1, 192, 8960], Float32, CPU>) -> (%1321:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.LinearOp(%1319:tensor<[1, 192, 1536], Float32, CPU>) -> (%1322:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.MulOp(%1321:tensor<[1, 192, 8960], Float32, CPU>, %1322:tensor<[1, 192, 8960], Float32, CPU>) -> (%1323:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.LinearOp(%1323:tensor<[1, 192, 8960], Float32, CPU>) -> (%1324:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1324:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.2 <CPU> {
        (%1325:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1352:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.RMSNormOp(%1325:tensor<[1, 192, 1536], Float32, CPU>) -> (%1326:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.2.self_attn (%1326:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1344:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.AddOp(%1344:tensor<[1, 192, 1536], Float32, CPU>, %1325:tensor<[1, 192, 1536], Float32, CPU>) -> (%1345:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.RMSNormOp(%1345:tensor<[1, 192, 1536], Float32, CPU>) -> (%1346:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.2.mlp (%1346:tensor<[1, 192, 1536], Float32, CPU>) -> (%1351:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.AddOp(%1351:tensor<[1, 192, 1536], Float32, CPU>, %1345:tensor<[1, 192, 1536], Float32, CPU>) -> (%1352:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1352:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.2.self_attn <CPU> {
        (%1326:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1344:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.LinearOp(%1326:tensor<[1, 192, 1536], Float32, CPU>) -> (%1327:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.LinearOp(%1326:tensor<[1, 192, 1536], Float32, CPU>) -> (%1328:tensor<[1, 192, 256], Float32, CPU>)
            linalg.CPU.LinearOp(%1326:tensor<[1, 192, 1536], Float32, CPU>) -> (%1329:tensor<[1, 192, 256], Float32, CPU>)
            linalg.CPU.ViewOp(%1327:tensor<[1, 192, 1536], Float32, CPU>) -> (%1327:tensor<[1, 192, 12, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1328:tensor<[1, 192, 256], Float32, CPU>) -> (%1328:tensor<[1, 192, 2, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1329:tensor<[1, 192, 256], Float32, CPU>) -> (%1329:tensor<[1, 192, 2, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1327:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1330:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1328:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1331:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1329:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1332:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.MultimodalRoPEOp(%1330:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1333:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.MultimodalRoPEOp(%1331:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1334:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.KVCacheOp(%1334:tensor<[1, 2, 192, 128], Float32, CPU>, %1332:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1335:tensor<[1, 2, 192, 128], Float32, CPU>, %1336:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.MatMulOp(%1333:tensor<[1, 12, 192, 128], Float32, CPU>, %1335:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1337:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.MulOp(%1337:tensor<[1, 12, 192, 192], Float32, CPU>, %1338:tensor<[1], Float32, CPU>) -> (%1339:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.CausalMaskOp(%1339:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1340:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%1340:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1341:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.MatMulOp(%1341:tensor<[1, 12, 192, 192], Float32, CPU>, %1336:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1342:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1342:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1343:tensor<[1, 192, 12, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1343:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1343:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.LinearOp(%1343:tensor<[1, 192, 1536], Float32, CPU>) -> (%1344:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1344:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.2.mlp <CPU> {
        (%1346:tensor<[1, 192, 1536], Float32, CPU>) -> (%1351:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.LinearOp(%1346:tensor<[1, 192, 1536], Float32, CPU>) -> (%1347:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.SiLUOp(%1347:tensor<[1, 192, 8960], Float32, CPU>) -> (%1348:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.LinearOp(%1346:tensor<[1, 192, 1536], Float32, CPU>) -> (%1349:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.MulOp(%1348:tensor<[1, 192, 8960], Float32, CPU>, %1349:tensor<[1, 192, 8960], Float32, CPU>) -> (%1350:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.LinearOp(%1350:tensor<[1, 192, 8960], Float32, CPU>) -> (%1351:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1351:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.3 <CPU> {
        (%1352:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1379:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.RMSNormOp(%1352:tensor<[1, 192, 1536], Float32, CPU>) -> (%1353:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.3.self_attn (%1353:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1371:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.AddOp(%1371:tensor<[1, 192, 1536], Float32, CPU>, %1352:tensor<[1, 192, 1536], Float32, CPU>) -> (%1372:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.RMSNormOp(%1372:tensor<[1, 192, 1536], Float32, CPU>) -> (%1373:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.3.mlp (%1373:tensor<[1, 192, 1536], Float32, CPU>) -> (%1378:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.AddOp(%1378:tensor<[1, 192, 1536], Float32, CPU>, %1372:tensor<[1, 192, 1536], Float32, CPU>) -> (%1379:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1379:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.3.self_attn <CPU> {
        (%1353:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1371:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.LinearOp(%1353:tensor<[1, 192, 1536], Float32, CPU>) -> (%1354:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.LinearOp(%1353:tensor<[1, 192, 1536], Float32, CPU>) -> (%1355:tensor<[1, 192, 256], Float32, CPU>)
            linalg.CPU.LinearOp(%1353:tensor<[1, 192, 1536], Float32, CPU>) -> (%1356:tensor<[1, 192, 256], Float32, CPU>)
            linalg.CPU.ViewOp(%1354:tensor<[1, 192, 1536], Float32, CPU>) -> (%1354:tensor<[1, 192, 12, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1355:tensor<[1, 192, 256], Float32, CPU>) -> (%1355:tensor<[1, 192, 2, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1356:tensor<[1, 192, 256], Float32, CPU>) -> (%1356:tensor<[1, 192, 2, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1354:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1357:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1355:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1358:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1356:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1359:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.MultimodalRoPEOp(%1357:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1360:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.MultimodalRoPEOp(%1358:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1361:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.KVCacheOp(%1361:tensor<[1, 2, 192, 128], Float32, CPU>, %1359:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1362:tensor<[1, 2, 192, 128], Float32, CPU>, %1363:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.MatMulOp(%1360:tensor<[1, 12, 192, 128], Float32, CPU>, %1362:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1364:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.MulOp(%1364:tensor<[1, 12, 192, 192], Float32, CPU>, %1365:tensor<[1], Float32, CPU>) -> (%1366:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.CausalMaskOp(%1366:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1367:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%1367:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1368:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.MatMulOp(%1368:tensor<[1, 12, 192, 192], Float32, CPU>, %1363:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1369:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1369:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1370:tensor<[1, 192, 12, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1370:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1370:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.LinearOp(%1370:tensor<[1, 192, 1536], Float32, CPU>) -> (%1371:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1371:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.3.mlp <CPU> {
        (%1373:tensor<[1, 192, 1536], Float32, CPU>) -> (%1378:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.LinearOp(%1373:tensor<[1, 192, 1536], Float32, CPU>) -> (%1374:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.SiLUOp(%1374:tensor<[1, 192, 8960], Float32, CPU>) -> (%1375:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.LinearOp(%1373:tensor<[1, 192, 1536], Float32, CPU>) -> (%1376:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.MulOp(%1375:tensor<[1, 192, 8960], Float32, CPU>, %1376:tensor<[1, 192, 8960], Float32, CPU>) -> (%1377:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.LinearOp(%1377:tensor<[1, 192, 8960], Float32, CPU>) -> (%1378:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1378:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.4 <CPU> {
        (%1379:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1406:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.RMSNormOp(%1379:tensor<[1, 192, 1536], Float32, CPU>) -> (%1380:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.4.self_attn (%1380:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1398:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.AddOp(%1398:tensor<[1, 192, 1536], Float32, CPU>, %1379:tensor<[1, 192, 1536], Float32, CPU>) -> (%1399:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.RMSNormOp(%1399:tensor<[1, 192, 1536], Float32, CPU>) -> (%1400:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.4.mlp (%1400:tensor<[1, 192, 1536], Float32, CPU>) -> (%1405:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.AddOp(%1405:tensor<[1, 192, 1536], Float32, CPU>, %1399:tensor<[1, 192, 1536], Float32, CPU>) -> (%1406:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1406:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.4.self_attn <CPU> {
        (%1380:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1398:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.LinearOp(%1380:tensor<[1, 192, 1536], Float32, CPU>) -> (%1381:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.LinearOp(%1380:tensor<[1, 192, 1536], Float32, CPU>) -> (%1382:tensor<[1, 192, 256], Float32, CPU>)
            linalg.CPU.LinearOp(%1380:tensor<[1, 192, 1536], Float32, CPU>) -> (%1383:tensor<[1, 192, 256], Float32, CPU>)
            linalg.CPU.ViewOp(%1381:tensor<[1, 192, 1536], Float32, CPU>) -> (%1381:tensor<[1, 192, 12, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1382:tensor<[1, 192, 256], Float32, CPU>) -> (%1382:tensor<[1, 192, 2, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1383:tensor<[1, 192, 256], Float32, CPU>) -> (%1383:tensor<[1, 192, 2, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1381:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1384:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1382:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1385:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1383:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1386:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.MultimodalRoPEOp(%1384:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1387:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.MultimodalRoPEOp(%1385:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1388:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.KVCacheOp(%1388:tensor<[1, 2, 192, 128], Float32, CPU>, %1386:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1389:tensor<[1, 2, 192, 128], Float32, CPU>, %1390:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.MatMulOp(%1387:tensor<[1, 12, 192, 128], Float32, CPU>, %1389:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1391:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.MulOp(%1391:tensor<[1, 12, 192, 192], Float32, CPU>, %1392:tensor<[1], Float32, CPU>) -> (%1393:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.CausalMaskOp(%1393:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1394:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%1394:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1395:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.MatMulOp(%1395:tensor<[1, 12, 192, 192], Float32, CPU>, %1390:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1396:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1396:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1397:tensor<[1, 192, 12, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1397:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1397:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.LinearOp(%1397:tensor<[1, 192, 1536], Float32, CPU>) -> (%1398:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1398:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.4.mlp <CPU> {
        (%1400:tensor<[1, 192, 1536], Float32, CPU>) -> (%1405:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.LinearOp(%1400:tensor<[1, 192, 1536], Float32, CPU>) -> (%1401:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.SiLUOp(%1401:tensor<[1, 192, 8960], Float32, CPU>) -> (%1402:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.LinearOp(%1400:tensor<[1, 192, 1536], Float32, CPU>) -> (%1403:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.MulOp(%1402:tensor<[1, 192, 8960], Float32, CPU>, %1403:tensor<[1, 192, 8960], Float32, CPU>) -> (%1404:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.LinearOp(%1404:tensor<[1, 192, 8960], Float32, CPU>) -> (%1405:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1405:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.5 <CPU> {
        (%1406:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1433:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.RMSNormOp(%1406:tensor<[1, 192, 1536], Float32, CPU>) -> (%1407:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.5.self_attn (%1407:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1425:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.AddOp(%1425:tensor<[1, 192, 1536], Float32, CPU>, %1406:tensor<[1, 192, 1536], Float32, CPU>) -> (%1426:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.RMSNormOp(%1426:tensor<[1, 192, 1536], Float32, CPU>) -> (%1427:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.5.mlp (%1427:tensor<[1, 192, 1536], Float32, CPU>) -> (%1432:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.AddOp(%1432:tensor<[1, 192, 1536], Float32, CPU>, %1426:tensor<[1, 192, 1536], Float32, CPU>) -> (%1433:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1433:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.5.self_attn <CPU> {
        (%1407:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1425:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.LinearOp(%1407:tensor<[1, 192, 1536], Float32, CPU>) -> (%1408:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.LinearOp(%1407:tensor<[1, 192, 1536], Float32, CPU>) -> (%1409:tensor<[1, 192, 256], Float32, CPU>)
            linalg.CPU.LinearOp(%1407:tensor<[1, 192, 1536], Float32, CPU>) -> (%1410:tensor<[1, 192, 256], Float32, CPU>)
            linalg.CPU.ViewOp(%1408:tensor<[1, 192, 1536], Float32, CPU>) -> (%1408:tensor<[1, 192, 12, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1409:tensor<[1, 192, 256], Float32, CPU>) -> (%1409:tensor<[1, 192, 2, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1410:tensor<[1, 192, 256], Float32, CPU>) -> (%1410:tensor<[1, 192, 2, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1408:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1411:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1409:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1412:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1410:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1413:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.MultimodalRoPEOp(%1411:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1414:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.MultimodalRoPEOp(%1412:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1415:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.KVCacheOp(%1415:tensor<[1, 2, 192, 128], Float32, CPU>, %1413:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1416:tensor<[1, 2, 192, 128], Float32, CPU>, %1417:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.MatMulOp(%1414:tensor<[1, 12, 192, 128], Float32, CPU>, %1416:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1418:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.MulOp(%1418:tensor<[1, 12, 192, 192], Float32, CPU>, %1419:tensor<[1], Float32, CPU>) -> (%1420:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.CausalMaskOp(%1420:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1421:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%1421:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1422:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.MatMulOp(%1422:tensor<[1, 12, 192, 192], Float32, CPU>, %1417:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1423:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1423:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1424:tensor<[1, 192, 12, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1424:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1424:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.LinearOp(%1424:tensor<[1, 192, 1536], Float32, CPU>) -> (%1425:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1425:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.5.mlp <CPU> {
        (%1427:tensor<[1, 192, 1536], Float32, CPU>) -> (%1432:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.LinearOp(%1427:tensor<[1, 192, 1536], Float32, CPU>) -> (%1428:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.SiLUOp(%1428:tensor<[1, 192, 8960], Float32, CPU>) -> (%1429:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.LinearOp(%1427:tensor<[1, 192, 1536], Float32, CPU>) -> (%1430:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.MulOp(%1429:tensor<[1, 192, 8960], Float32, CPU>, %1430:tensor<[1, 192, 8960], Float32, CPU>) -> (%1431:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.LinearOp(%1431:tensor<[1, 192, 8960], Float32, CPU>) -> (%1432:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1432:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.6 <CPU> {
        (%1433:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1460:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.RMSNormOp(%1433:tensor<[1, 192, 1536], Float32, CPU>) -> (%1434:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.6.self_attn (%1434:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1452:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.AddOp(%1452:tensor<[1, 192, 1536], Float32, CPU>, %1433:tensor<[1, 192, 1536], Float32, CPU>) -> (%1453:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.RMSNormOp(%1453:tensor<[1, 192, 1536], Float32, CPU>) -> (%1454:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.6.mlp (%1454:tensor<[1, 192, 1536], Float32, CPU>) -> (%1459:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.AddOp(%1459:tensor<[1, 192, 1536], Float32, CPU>, %1453:tensor<[1, 192, 1536], Float32, CPU>) -> (%1460:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1460:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.6.self_attn <CPU> {
        (%1434:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1452:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.LinearOp(%1434:tensor<[1, 192, 1536], Float32, CPU>) -> (%1435:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.LinearOp(%1434:tensor<[1, 192, 1536], Float32, CPU>) -> (%1436:tensor<[1, 192, 256], Float32, CPU>)
            linalg.CPU.LinearOp(%1434:tensor<[1, 192, 1536], Float32, CPU>) -> (%1437:tensor<[1, 192, 256], Float32, CPU>)
            linalg.CPU.ViewOp(%1435:tensor<[1, 192, 1536], Float32, CPU>) -> (%1435:tensor<[1, 192, 12, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1436:tensor<[1, 192, 256], Float32, CPU>) -> (%1436:tensor<[1, 192, 2, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1437:tensor<[1, 192, 256], Float32, CPU>) -> (%1437:tensor<[1, 192, 2, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1435:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1438:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1436:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1439:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1437:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1440:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.MultimodalRoPEOp(%1438:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1441:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.MultimodalRoPEOp(%1439:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1442:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.KVCacheOp(%1442:tensor<[1, 2, 192, 128], Float32, CPU>, %1440:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1443:tensor<[1, 2, 192, 128], Float32, CPU>, %1444:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.MatMulOp(%1441:tensor<[1, 12, 192, 128], Float32, CPU>, %1443:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1445:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.MulOp(%1445:tensor<[1, 12, 192, 192], Float32, CPU>, %1446:tensor<[1], Float32, CPU>) -> (%1447:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.CausalMaskOp(%1447:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1448:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%1448:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1449:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.MatMulOp(%1449:tensor<[1, 12, 192, 192], Float32, CPU>, %1444:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1450:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1450:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1451:tensor<[1, 192, 12, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1451:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1451:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.LinearOp(%1451:tensor<[1, 192, 1536], Float32, CPU>) -> (%1452:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1452:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.6.mlp <CPU> {
        (%1454:tensor<[1, 192, 1536], Float32, CPU>) -> (%1459:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.LinearOp(%1454:tensor<[1, 192, 1536], Float32, CPU>) -> (%1455:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.SiLUOp(%1455:tensor<[1, 192, 8960], Float32, CPU>) -> (%1456:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.LinearOp(%1454:tensor<[1, 192, 1536], Float32, CPU>) -> (%1457:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.MulOp(%1456:tensor<[1, 192, 8960], Float32, CPU>, %1457:tensor<[1, 192, 8960], Float32, CPU>) -> (%1458:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.LinearOp(%1458:tensor<[1, 192, 8960], Float32, CPU>) -> (%1459:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1459:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.7 <CPU> {
        (%1460:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1487:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.RMSNormOp(%1460:tensor<[1, 192, 1536], Float32, CPU>) -> (%1461:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.7.self_attn (%1461:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1479:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.AddOp(%1479:tensor<[1, 192, 1536], Float32, CPU>, %1460:tensor<[1, 192, 1536], Float32, CPU>) -> (%1480:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.RMSNormOp(%1480:tensor<[1, 192, 1536], Float32, CPU>) -> (%1481:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.7.mlp (%1481:tensor<[1, 192, 1536], Float32, CPU>) -> (%1486:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.AddOp(%1486:tensor<[1, 192, 1536], Float32, CPU>, %1480:tensor<[1, 192, 1536], Float32, CPU>) -> (%1487:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1487:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.7.self_attn <CPU> {
        (%1461:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1479:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.LinearOp(%1461:tensor<[1, 192, 1536], Float32, CPU>) -> (%1462:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.LinearOp(%1461:tensor<[1, 192, 1536], Float32, CPU>) -> (%1463:tensor<[1, 192, 256], Float32, CPU>)
            linalg.CPU.LinearOp(%1461:tensor<[1, 192, 1536], Float32, CPU>) -> (%1464:tensor<[1, 192, 256], Float32, CPU>)
            linalg.CPU.ViewOp(%1462:tensor<[1, 192, 1536], Float32, CPU>) -> (%1462:tensor<[1, 192, 12, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1463:tensor<[1, 192, 256], Float32, CPU>) -> (%1463:tensor<[1, 192, 2, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1464:tensor<[1, 192, 256], Float32, CPU>) -> (%1464:tensor<[1, 192, 2, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1462:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1465:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1463:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1466:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1464:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1467:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.MultimodalRoPEOp(%1465:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1468:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.MultimodalRoPEOp(%1466:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1469:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.KVCacheOp(%1469:tensor<[1, 2, 192, 128], Float32, CPU>, %1467:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1470:tensor<[1, 2, 192, 128], Float32, CPU>, %1471:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.MatMulOp(%1468:tensor<[1, 12, 192, 128], Float32, CPU>, %1470:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1472:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.MulOp(%1472:tensor<[1, 12, 192, 192], Float32, CPU>, %1473:tensor<[1], Float32, CPU>) -> (%1474:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.CausalMaskOp(%1474:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1475:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%1475:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1476:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.MatMulOp(%1476:tensor<[1, 12, 192, 192], Float32, CPU>, %1471:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1477:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1477:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1478:tensor<[1, 192, 12, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1478:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1478:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.LinearOp(%1478:tensor<[1, 192, 1536], Float32, CPU>) -> (%1479:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1479:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.7.mlp <CPU> {
        (%1481:tensor<[1, 192, 1536], Float32, CPU>) -> (%1486:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.LinearOp(%1481:tensor<[1, 192, 1536], Float32, CPU>) -> (%1482:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.SiLUOp(%1482:tensor<[1, 192, 8960], Float32, CPU>) -> (%1483:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.LinearOp(%1481:tensor<[1, 192, 1536], Float32, CPU>) -> (%1484:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.MulOp(%1483:tensor<[1, 192, 8960], Float32, CPU>, %1484:tensor<[1, 192, 8960], Float32, CPU>) -> (%1485:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.LinearOp(%1485:tensor<[1, 192, 8960], Float32, CPU>) -> (%1486:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1486:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.8 <CPU> {
        (%1487:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1514:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.RMSNormOp(%1487:tensor<[1, 192, 1536], Float32, CPU>) -> (%1488:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.8.self_attn (%1488:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1506:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.AddOp(%1506:tensor<[1, 192, 1536], Float32, CPU>, %1487:tensor<[1, 192, 1536], Float32, CPU>) -> (%1507:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.RMSNormOp(%1507:tensor<[1, 192, 1536], Float32, CPU>) -> (%1508:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.8.mlp (%1508:tensor<[1, 192, 1536], Float32, CPU>) -> (%1513:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.AddOp(%1513:tensor<[1, 192, 1536], Float32, CPU>, %1507:tensor<[1, 192, 1536], Float32, CPU>) -> (%1514:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1514:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.8.self_attn <CPU> {
        (%1488:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1506:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.LinearOp(%1488:tensor<[1, 192, 1536], Float32, CPU>) -> (%1489:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.LinearOp(%1488:tensor<[1, 192, 1536], Float32, CPU>) -> (%1490:tensor<[1, 192, 256], Float32, CPU>)
            linalg.CPU.LinearOp(%1488:tensor<[1, 192, 1536], Float32, CPU>) -> (%1491:tensor<[1, 192, 256], Float32, CPU>)
            linalg.CPU.ViewOp(%1489:tensor<[1, 192, 1536], Float32, CPU>) -> (%1489:tensor<[1, 192, 12, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1490:tensor<[1, 192, 256], Float32, CPU>) -> (%1490:tensor<[1, 192, 2, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1491:tensor<[1, 192, 256], Float32, CPU>) -> (%1491:tensor<[1, 192, 2, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1489:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1492:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1490:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1493:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1491:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1494:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.MultimodalRoPEOp(%1492:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1495:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.MultimodalRoPEOp(%1493:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1496:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.KVCacheOp(%1496:tensor<[1, 2, 192, 128], Float32, CPU>, %1494:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1497:tensor<[1, 2, 192, 128], Float32, CPU>, %1498:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.MatMulOp(%1495:tensor<[1, 12, 192, 128], Float32, CPU>, %1497:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1499:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.MulOp(%1499:tensor<[1, 12, 192, 192], Float32, CPU>, %1500:tensor<[1], Float32, CPU>) -> (%1501:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.CausalMaskOp(%1501:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1502:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%1502:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1503:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.MatMulOp(%1503:tensor<[1, 12, 192, 192], Float32, CPU>, %1498:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1504:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1504:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1505:tensor<[1, 192, 12, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1505:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1505:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.LinearOp(%1505:tensor<[1, 192, 1536], Float32, CPU>) -> (%1506:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1506:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.8.mlp <CPU> {
        (%1508:tensor<[1, 192, 1536], Float32, CPU>) -> (%1513:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.LinearOp(%1508:tensor<[1, 192, 1536], Float32, CPU>) -> (%1509:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.SiLUOp(%1509:tensor<[1, 192, 8960], Float32, CPU>) -> (%1510:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.LinearOp(%1508:tensor<[1, 192, 1536], Float32, CPU>) -> (%1511:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.MulOp(%1510:tensor<[1, 192, 8960], Float32, CPU>, %1511:tensor<[1, 192, 8960], Float32, CPU>) -> (%1512:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.LinearOp(%1512:tensor<[1, 192, 8960], Float32, CPU>) -> (%1513:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1513:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.9 <CPU> {
        (%1514:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1541:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.RMSNormOp(%1514:tensor<[1, 192, 1536], Float32, CPU>) -> (%1515:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.9.self_attn (%1515:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1533:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.AddOp(%1533:tensor<[1, 192, 1536], Float32, CPU>, %1514:tensor<[1, 192, 1536], Float32, CPU>) -> (%1534:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.RMSNormOp(%1534:tensor<[1, 192, 1536], Float32, CPU>) -> (%1535:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.9.mlp (%1535:tensor<[1, 192, 1536], Float32, CPU>) -> (%1540:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.AddOp(%1540:tensor<[1, 192, 1536], Float32, CPU>, %1534:tensor<[1, 192, 1536], Float32, CPU>) -> (%1541:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1541:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.9.self_attn <CPU> {
        (%1515:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1533:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.LinearOp(%1515:tensor<[1, 192, 1536], Float32, CPU>) -> (%1516:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.LinearOp(%1515:tensor<[1, 192, 1536], Float32, CPU>) -> (%1517:tensor<[1, 192, 256], Float32, CPU>)
            linalg.CPU.LinearOp(%1515:tensor<[1, 192, 1536], Float32, CPU>) -> (%1518:tensor<[1, 192, 256], Float32, CPU>)
            linalg.CPU.ViewOp(%1516:tensor<[1, 192, 1536], Float32, CPU>) -> (%1516:tensor<[1, 192, 12, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1517:tensor<[1, 192, 256], Float32, CPU>) -> (%1517:tensor<[1, 192, 2, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1518:tensor<[1, 192, 256], Float32, CPU>) -> (%1518:tensor<[1, 192, 2, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1516:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1519:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1517:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1520:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1518:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1521:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.MultimodalRoPEOp(%1519:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1522:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.MultimodalRoPEOp(%1520:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1523:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.KVCacheOp(%1523:tensor<[1, 2, 192, 128], Float32, CPU>, %1521:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1524:tensor<[1, 2, 192, 128], Float32, CPU>, %1525:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.MatMulOp(%1522:tensor<[1, 12, 192, 128], Float32, CPU>, %1524:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1526:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.MulOp(%1526:tensor<[1, 12, 192, 192], Float32, CPU>, %1527:tensor<[1], Float32, CPU>) -> (%1528:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.CausalMaskOp(%1528:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1529:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%1529:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1530:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.MatMulOp(%1530:tensor<[1, 12, 192, 192], Float32, CPU>, %1525:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1531:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1531:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1532:tensor<[1, 192, 12, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1532:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1532:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.LinearOp(%1532:tensor<[1, 192, 1536], Float32, CPU>) -> (%1533:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1533:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.9.mlp <CPU> {
        (%1535:tensor<[1, 192, 1536], Float32, CPU>) -> (%1540:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.LinearOp(%1535:tensor<[1, 192, 1536], Float32, CPU>) -> (%1536:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.SiLUOp(%1536:tensor<[1, 192, 8960], Float32, CPU>) -> (%1537:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.LinearOp(%1535:tensor<[1, 192, 1536], Float32, CPU>) -> (%1538:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.MulOp(%1537:tensor<[1, 192, 8960], Float32, CPU>, %1538:tensor<[1, 192, 8960], Float32, CPU>) -> (%1539:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.LinearOp(%1539:tensor<[1, 192, 8960], Float32, CPU>) -> (%1540:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1540:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.10 <CPU> {
        (%1541:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1568:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.RMSNormOp(%1541:tensor<[1, 192, 1536], Float32, CPU>) -> (%1542:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.10.self_attn (%1542:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1560:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.AddOp(%1560:tensor<[1, 192, 1536], Float32, CPU>, %1541:tensor<[1, 192, 1536], Float32, CPU>) -> (%1561:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.RMSNormOp(%1561:tensor<[1, 192, 1536], Float32, CPU>) -> (%1562:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.10.mlp (%1562:tensor<[1, 192, 1536], Float32, CPU>) -> (%1567:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.AddOp(%1567:tensor<[1, 192, 1536], Float32, CPU>, %1561:tensor<[1, 192, 1536], Float32, CPU>) -> (%1568:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1568:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.10.self_attn <CPU> {
        (%1542:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1560:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.LinearOp(%1542:tensor<[1, 192, 1536], Float32, CPU>) -> (%1543:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.LinearOp(%1542:tensor<[1, 192, 1536], Float32, CPU>) -> (%1544:tensor<[1, 192, 256], Float32, CPU>)
            linalg.CPU.LinearOp(%1542:tensor<[1, 192, 1536], Float32, CPU>) -> (%1545:tensor<[1, 192, 256], Float32, CPU>)
            linalg.CPU.ViewOp(%1543:tensor<[1, 192, 1536], Float32, CPU>) -> (%1543:tensor<[1, 192, 12, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1544:tensor<[1, 192, 256], Float32, CPU>) -> (%1544:tensor<[1, 192, 2, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1545:tensor<[1, 192, 256], Float32, CPU>) -> (%1545:tensor<[1, 192, 2, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1543:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1546:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1544:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1547:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1545:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1548:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.MultimodalRoPEOp(%1546:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1549:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.MultimodalRoPEOp(%1547:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1550:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.KVCacheOp(%1550:tensor<[1, 2, 192, 128], Float32, CPU>, %1548:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1551:tensor<[1, 2, 192, 128], Float32, CPU>, %1552:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.MatMulOp(%1549:tensor<[1, 12, 192, 128], Float32, CPU>, %1551:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1553:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.MulOp(%1553:tensor<[1, 12, 192, 192], Float32, CPU>, %1554:tensor<[1], Float32, CPU>) -> (%1555:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.CausalMaskOp(%1555:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1556:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%1556:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1557:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.MatMulOp(%1557:tensor<[1, 12, 192, 192], Float32, CPU>, %1552:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1558:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1558:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1559:tensor<[1, 192, 12, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1559:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1559:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.LinearOp(%1559:tensor<[1, 192, 1536], Float32, CPU>) -> (%1560:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1560:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.10.mlp <CPU> {
        (%1562:tensor<[1, 192, 1536], Float32, CPU>) -> (%1567:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.LinearOp(%1562:tensor<[1, 192, 1536], Float32, CPU>) -> (%1563:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.SiLUOp(%1563:tensor<[1, 192, 8960], Float32, CPU>) -> (%1564:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.LinearOp(%1562:tensor<[1, 192, 1536], Float32, CPU>) -> (%1565:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.MulOp(%1564:tensor<[1, 192, 8960], Float32, CPU>, %1565:tensor<[1, 192, 8960], Float32, CPU>) -> (%1566:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.LinearOp(%1566:tensor<[1, 192, 8960], Float32, CPU>) -> (%1567:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1567:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.11 <CPU> {
        (%1568:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1595:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.RMSNormOp(%1568:tensor<[1, 192, 1536], Float32, CPU>) -> (%1569:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.11.self_attn (%1569:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1587:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.AddOp(%1587:tensor<[1, 192, 1536], Float32, CPU>, %1568:tensor<[1, 192, 1536], Float32, CPU>) -> (%1588:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.RMSNormOp(%1588:tensor<[1, 192, 1536], Float32, CPU>) -> (%1589:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.11.mlp (%1589:tensor<[1, 192, 1536], Float32, CPU>) -> (%1594:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.AddOp(%1594:tensor<[1, 192, 1536], Float32, CPU>, %1588:tensor<[1, 192, 1536], Float32, CPU>) -> (%1595:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1595:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.11.self_attn <CPU> {
        (%1569:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1587:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.LinearOp(%1569:tensor<[1, 192, 1536], Float32, CPU>) -> (%1570:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.LinearOp(%1569:tensor<[1, 192, 1536], Float32, CPU>) -> (%1571:tensor<[1, 192, 256], Float32, CPU>)
            linalg.CPU.LinearOp(%1569:tensor<[1, 192, 1536], Float32, CPU>) -> (%1572:tensor<[1, 192, 256], Float32, CPU>)
            linalg.CPU.ViewOp(%1570:tensor<[1, 192, 1536], Float32, CPU>) -> (%1570:tensor<[1, 192, 12, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1571:tensor<[1, 192, 256], Float32, CPU>) -> (%1571:tensor<[1, 192, 2, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1572:tensor<[1, 192, 256], Float32, CPU>) -> (%1572:tensor<[1, 192, 2, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1570:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1573:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1571:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1574:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1572:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1575:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.MultimodalRoPEOp(%1573:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1576:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.MultimodalRoPEOp(%1574:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1577:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.KVCacheOp(%1577:tensor<[1, 2, 192, 128], Float32, CPU>, %1575:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1578:tensor<[1, 2, 192, 128], Float32, CPU>, %1579:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.MatMulOp(%1576:tensor<[1, 12, 192, 128], Float32, CPU>, %1578:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1580:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.MulOp(%1580:tensor<[1, 12, 192, 192], Float32, CPU>, %1581:tensor<[1], Float32, CPU>) -> (%1582:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.CausalMaskOp(%1582:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1583:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%1583:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1584:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.MatMulOp(%1584:tensor<[1, 12, 192, 192], Float32, CPU>, %1579:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1585:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1585:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1586:tensor<[1, 192, 12, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1586:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1586:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.LinearOp(%1586:tensor<[1, 192, 1536], Float32, CPU>) -> (%1587:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1587:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.11.mlp <CPU> {
        (%1589:tensor<[1, 192, 1536], Float32, CPU>) -> (%1594:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.LinearOp(%1589:tensor<[1, 192, 1536], Float32, CPU>) -> (%1590:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.SiLUOp(%1590:tensor<[1, 192, 8960], Float32, CPU>) -> (%1591:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.LinearOp(%1589:tensor<[1, 192, 1536], Float32, CPU>) -> (%1592:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.MulOp(%1591:tensor<[1, 192, 8960], Float32, CPU>, %1592:tensor<[1, 192, 8960], Float32, CPU>) -> (%1593:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.LinearOp(%1593:tensor<[1, 192, 8960], Float32, CPU>) -> (%1594:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1594:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.12 <CPU> {
        (%1595:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1622:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.RMSNormOp(%1595:tensor<[1, 192, 1536], Float32, CPU>) -> (%1596:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.12.self_attn (%1596:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1614:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.AddOp(%1614:tensor<[1, 192, 1536], Float32, CPU>, %1595:tensor<[1, 192, 1536], Float32, CPU>) -> (%1615:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.RMSNormOp(%1615:tensor<[1, 192, 1536], Float32, CPU>) -> (%1616:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.12.mlp (%1616:tensor<[1, 192, 1536], Float32, CPU>) -> (%1621:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.AddOp(%1621:tensor<[1, 192, 1536], Float32, CPU>, %1615:tensor<[1, 192, 1536], Float32, CPU>) -> (%1622:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1622:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.12.self_attn <CPU> {
        (%1596:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1614:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.LinearOp(%1596:tensor<[1, 192, 1536], Float32, CPU>) -> (%1597:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.LinearOp(%1596:tensor<[1, 192, 1536], Float32, CPU>) -> (%1598:tensor<[1, 192, 256], Float32, CPU>)
            linalg.CPU.LinearOp(%1596:tensor<[1, 192, 1536], Float32, CPU>) -> (%1599:tensor<[1, 192, 256], Float32, CPU>)
            linalg.CPU.ViewOp(%1597:tensor<[1, 192, 1536], Float32, CPU>) -> (%1597:tensor<[1, 192, 12, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1598:tensor<[1, 192, 256], Float32, CPU>) -> (%1598:tensor<[1, 192, 2, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1599:tensor<[1, 192, 256], Float32, CPU>) -> (%1599:tensor<[1, 192, 2, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1597:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1600:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1598:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1601:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1599:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1602:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.MultimodalRoPEOp(%1600:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1603:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.MultimodalRoPEOp(%1601:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1604:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.KVCacheOp(%1604:tensor<[1, 2, 192, 128], Float32, CPU>, %1602:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1605:tensor<[1, 2, 192, 128], Float32, CPU>, %1606:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.MatMulOp(%1603:tensor<[1, 12, 192, 128], Float32, CPU>, %1605:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1607:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.MulOp(%1607:tensor<[1, 12, 192, 192], Float32, CPU>, %1608:tensor<[1], Float32, CPU>) -> (%1609:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.CausalMaskOp(%1609:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1610:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%1610:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1611:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.MatMulOp(%1611:tensor<[1, 12, 192, 192], Float32, CPU>, %1606:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1612:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1612:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1613:tensor<[1, 192, 12, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1613:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1613:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.LinearOp(%1613:tensor<[1, 192, 1536], Float32, CPU>) -> (%1614:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1614:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.12.mlp <CPU> {
        (%1616:tensor<[1, 192, 1536], Float32, CPU>) -> (%1621:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.LinearOp(%1616:tensor<[1, 192, 1536], Float32, CPU>) -> (%1617:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.SiLUOp(%1617:tensor<[1, 192, 8960], Float32, CPU>) -> (%1618:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.LinearOp(%1616:tensor<[1, 192, 1536], Float32, CPU>) -> (%1619:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.MulOp(%1618:tensor<[1, 192, 8960], Float32, CPU>, %1619:tensor<[1, 192, 8960], Float32, CPU>) -> (%1620:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.LinearOp(%1620:tensor<[1, 192, 8960], Float32, CPU>) -> (%1621:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1621:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.13 <CPU> {
        (%1622:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1649:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.RMSNormOp(%1622:tensor<[1, 192, 1536], Float32, CPU>) -> (%1623:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.13.self_attn (%1623:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1641:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.AddOp(%1641:tensor<[1, 192, 1536], Float32, CPU>, %1622:tensor<[1, 192, 1536], Float32, CPU>) -> (%1642:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.RMSNormOp(%1642:tensor<[1, 192, 1536], Float32, CPU>) -> (%1643:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.13.mlp (%1643:tensor<[1, 192, 1536], Float32, CPU>) -> (%1648:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.AddOp(%1648:tensor<[1, 192, 1536], Float32, CPU>, %1642:tensor<[1, 192, 1536], Float32, CPU>) -> (%1649:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1649:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.13.self_attn <CPU> {
        (%1623:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1641:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.LinearOp(%1623:tensor<[1, 192, 1536], Float32, CPU>) -> (%1624:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.LinearOp(%1623:tensor<[1, 192, 1536], Float32, CPU>) -> (%1625:tensor<[1, 192, 256], Float32, CPU>)
            linalg.CPU.LinearOp(%1623:tensor<[1, 192, 1536], Float32, CPU>) -> (%1626:tensor<[1, 192, 256], Float32, CPU>)
            linalg.CPU.ViewOp(%1624:tensor<[1, 192, 1536], Float32, CPU>) -> (%1624:tensor<[1, 192, 12, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1625:tensor<[1, 192, 256], Float32, CPU>) -> (%1625:tensor<[1, 192, 2, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1626:tensor<[1, 192, 256], Float32, CPU>) -> (%1626:tensor<[1, 192, 2, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1624:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1627:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1625:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1628:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1626:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1629:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.MultimodalRoPEOp(%1627:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1630:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.MultimodalRoPEOp(%1628:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1631:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.KVCacheOp(%1631:tensor<[1, 2, 192, 128], Float32, CPU>, %1629:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1632:tensor<[1, 2, 192, 128], Float32, CPU>, %1633:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.MatMulOp(%1630:tensor<[1, 12, 192, 128], Float32, CPU>, %1632:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1634:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.MulOp(%1634:tensor<[1, 12, 192, 192], Float32, CPU>, %1635:tensor<[1], Float32, CPU>) -> (%1636:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.CausalMaskOp(%1636:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1637:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%1637:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1638:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.MatMulOp(%1638:tensor<[1, 12, 192, 192], Float32, CPU>, %1633:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1639:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1639:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1640:tensor<[1, 192, 12, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1640:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1640:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.LinearOp(%1640:tensor<[1, 192, 1536], Float32, CPU>) -> (%1641:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1641:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.13.mlp <CPU> {
        (%1643:tensor<[1, 192, 1536], Float32, CPU>) -> (%1648:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.LinearOp(%1643:tensor<[1, 192, 1536], Float32, CPU>) -> (%1644:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.SiLUOp(%1644:tensor<[1, 192, 8960], Float32, CPU>) -> (%1645:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.LinearOp(%1643:tensor<[1, 192, 1536], Float32, CPU>) -> (%1646:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.MulOp(%1645:tensor<[1, 192, 8960], Float32, CPU>, %1646:tensor<[1, 192, 8960], Float32, CPU>) -> (%1647:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.LinearOp(%1647:tensor<[1, 192, 8960], Float32, CPU>) -> (%1648:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1648:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.14 <CPU> {
        (%1649:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1676:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.RMSNormOp(%1649:tensor<[1, 192, 1536], Float32, CPU>) -> (%1650:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.14.self_attn (%1650:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1668:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.AddOp(%1668:tensor<[1, 192, 1536], Float32, CPU>, %1649:tensor<[1, 192, 1536], Float32, CPU>) -> (%1669:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.RMSNormOp(%1669:tensor<[1, 192, 1536], Float32, CPU>) -> (%1670:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.14.mlp (%1670:tensor<[1, 192, 1536], Float32, CPU>) -> (%1675:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.AddOp(%1675:tensor<[1, 192, 1536], Float32, CPU>, %1669:tensor<[1, 192, 1536], Float32, CPU>) -> (%1676:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1676:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.14.self_attn <CPU> {
        (%1650:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1668:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.LinearOp(%1650:tensor<[1, 192, 1536], Float32, CPU>) -> (%1651:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.LinearOp(%1650:tensor<[1, 192, 1536], Float32, CPU>) -> (%1652:tensor<[1, 192, 256], Float32, CPU>)
            linalg.CPU.LinearOp(%1650:tensor<[1, 192, 1536], Float32, CPU>) -> (%1653:tensor<[1, 192, 256], Float32, CPU>)
            linalg.CPU.ViewOp(%1651:tensor<[1, 192, 1536], Float32, CPU>) -> (%1651:tensor<[1, 192, 12, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1652:tensor<[1, 192, 256], Float32, CPU>) -> (%1652:tensor<[1, 192, 2, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1653:tensor<[1, 192, 256], Float32, CPU>) -> (%1653:tensor<[1, 192, 2, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1651:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1654:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1652:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1655:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1653:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1656:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.MultimodalRoPEOp(%1654:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1657:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.MultimodalRoPEOp(%1655:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1658:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.KVCacheOp(%1658:tensor<[1, 2, 192, 128], Float32, CPU>, %1656:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1659:tensor<[1, 2, 192, 128], Float32, CPU>, %1660:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.MatMulOp(%1657:tensor<[1, 12, 192, 128], Float32, CPU>, %1659:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1661:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.MulOp(%1661:tensor<[1, 12, 192, 192], Float32, CPU>, %1662:tensor<[1], Float32, CPU>) -> (%1663:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.CausalMaskOp(%1663:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1664:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%1664:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1665:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.MatMulOp(%1665:tensor<[1, 12, 192, 192], Float32, CPU>, %1660:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1666:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1666:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1667:tensor<[1, 192, 12, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1667:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1667:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.LinearOp(%1667:tensor<[1, 192, 1536], Float32, CPU>) -> (%1668:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1668:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.14.mlp <CPU> {
        (%1670:tensor<[1, 192, 1536], Float32, CPU>) -> (%1675:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.LinearOp(%1670:tensor<[1, 192, 1536], Float32, CPU>) -> (%1671:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.SiLUOp(%1671:tensor<[1, 192, 8960], Float32, CPU>) -> (%1672:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.LinearOp(%1670:tensor<[1, 192, 1536], Float32, CPU>) -> (%1673:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.MulOp(%1672:tensor<[1, 192, 8960], Float32, CPU>, %1673:tensor<[1, 192, 8960], Float32, CPU>) -> (%1674:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.LinearOp(%1674:tensor<[1, 192, 8960], Float32, CPU>) -> (%1675:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1675:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.15 <CPU> {
        (%1676:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1703:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.RMSNormOp(%1676:tensor<[1, 192, 1536], Float32, CPU>) -> (%1677:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.15.self_attn (%1677:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1695:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.AddOp(%1695:tensor<[1, 192, 1536], Float32, CPU>, %1676:tensor<[1, 192, 1536], Float32, CPU>) -> (%1696:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.RMSNormOp(%1696:tensor<[1, 192, 1536], Float32, CPU>) -> (%1697:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.15.mlp (%1697:tensor<[1, 192, 1536], Float32, CPU>) -> (%1702:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.AddOp(%1702:tensor<[1, 192, 1536], Float32, CPU>, %1696:tensor<[1, 192, 1536], Float32, CPU>) -> (%1703:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1703:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.15.self_attn <CPU> {
        (%1677:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1695:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.LinearOp(%1677:tensor<[1, 192, 1536], Float32, CPU>) -> (%1678:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.LinearOp(%1677:tensor<[1, 192, 1536], Float32, CPU>) -> (%1679:tensor<[1, 192, 256], Float32, CPU>)
            linalg.CPU.LinearOp(%1677:tensor<[1, 192, 1536], Float32, CPU>) -> (%1680:tensor<[1, 192, 256], Float32, CPU>)
            linalg.CPU.ViewOp(%1678:tensor<[1, 192, 1536], Float32, CPU>) -> (%1678:tensor<[1, 192, 12, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1679:tensor<[1, 192, 256], Float32, CPU>) -> (%1679:tensor<[1, 192, 2, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1680:tensor<[1, 192, 256], Float32, CPU>) -> (%1680:tensor<[1, 192, 2, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1678:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1681:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1679:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1682:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1680:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1683:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.MultimodalRoPEOp(%1681:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1684:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.MultimodalRoPEOp(%1682:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1685:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.KVCacheOp(%1685:tensor<[1, 2, 192, 128], Float32, CPU>, %1683:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1686:tensor<[1, 2, 192, 128], Float32, CPU>, %1687:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.MatMulOp(%1684:tensor<[1, 12, 192, 128], Float32, CPU>, %1686:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1688:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.MulOp(%1688:tensor<[1, 12, 192, 192], Float32, CPU>, %1689:tensor<[1], Float32, CPU>) -> (%1690:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.CausalMaskOp(%1690:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1691:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%1691:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1692:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.MatMulOp(%1692:tensor<[1, 12, 192, 192], Float32, CPU>, %1687:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1693:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1693:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1694:tensor<[1, 192, 12, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1694:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1694:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.LinearOp(%1694:tensor<[1, 192, 1536], Float32, CPU>) -> (%1695:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1695:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.15.mlp <CPU> {
        (%1697:tensor<[1, 192, 1536], Float32, CPU>) -> (%1702:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.LinearOp(%1697:tensor<[1, 192, 1536], Float32, CPU>) -> (%1698:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.SiLUOp(%1698:tensor<[1, 192, 8960], Float32, CPU>) -> (%1699:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.LinearOp(%1697:tensor<[1, 192, 1536], Float32, CPU>) -> (%1700:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.MulOp(%1699:tensor<[1, 192, 8960], Float32, CPU>, %1700:tensor<[1, 192, 8960], Float32, CPU>) -> (%1701:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.LinearOp(%1701:tensor<[1, 192, 8960], Float32, CPU>) -> (%1702:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1702:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.16 <CPU> {
        (%1703:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1730:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.RMSNormOp(%1703:tensor<[1, 192, 1536], Float32, CPU>) -> (%1704:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.16.self_attn (%1704:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1722:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.AddOp(%1722:tensor<[1, 192, 1536], Float32, CPU>, %1703:tensor<[1, 192, 1536], Float32, CPU>) -> (%1723:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.RMSNormOp(%1723:tensor<[1, 192, 1536], Float32, CPU>) -> (%1724:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.16.mlp (%1724:tensor<[1, 192, 1536], Float32, CPU>) -> (%1729:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.AddOp(%1729:tensor<[1, 192, 1536], Float32, CPU>, %1723:tensor<[1, 192, 1536], Float32, CPU>) -> (%1730:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1730:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.16.self_attn <CPU> {
        (%1704:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1722:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.LinearOp(%1704:tensor<[1, 192, 1536], Float32, CPU>) -> (%1705:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.LinearOp(%1704:tensor<[1, 192, 1536], Float32, CPU>) -> (%1706:tensor<[1, 192, 256], Float32, CPU>)
            linalg.CPU.LinearOp(%1704:tensor<[1, 192, 1536], Float32, CPU>) -> (%1707:tensor<[1, 192, 256], Float32, CPU>)
            linalg.CPU.ViewOp(%1705:tensor<[1, 192, 1536], Float32, CPU>) -> (%1705:tensor<[1, 192, 12, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1706:tensor<[1, 192, 256], Float32, CPU>) -> (%1706:tensor<[1, 192, 2, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1707:tensor<[1, 192, 256], Float32, CPU>) -> (%1707:tensor<[1, 192, 2, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1705:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1708:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1706:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1709:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1707:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1710:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.MultimodalRoPEOp(%1708:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1711:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.MultimodalRoPEOp(%1709:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1712:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.KVCacheOp(%1712:tensor<[1, 2, 192, 128], Float32, CPU>, %1710:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1713:tensor<[1, 2, 192, 128], Float32, CPU>, %1714:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.MatMulOp(%1711:tensor<[1, 12, 192, 128], Float32, CPU>, %1713:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1715:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.MulOp(%1715:tensor<[1, 12, 192, 192], Float32, CPU>, %1716:tensor<[1], Float32, CPU>) -> (%1717:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.CausalMaskOp(%1717:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1718:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%1718:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1719:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.MatMulOp(%1719:tensor<[1, 12, 192, 192], Float32, CPU>, %1714:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1720:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1720:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1721:tensor<[1, 192, 12, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1721:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1721:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.LinearOp(%1721:tensor<[1, 192, 1536], Float32, CPU>) -> (%1722:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1722:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.16.mlp <CPU> {
        (%1724:tensor<[1, 192, 1536], Float32, CPU>) -> (%1729:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.LinearOp(%1724:tensor<[1, 192, 1536], Float32, CPU>) -> (%1725:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.SiLUOp(%1725:tensor<[1, 192, 8960], Float32, CPU>) -> (%1726:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.LinearOp(%1724:tensor<[1, 192, 1536], Float32, CPU>) -> (%1727:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.MulOp(%1726:tensor<[1, 192, 8960], Float32, CPU>, %1727:tensor<[1, 192, 8960], Float32, CPU>) -> (%1728:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.LinearOp(%1728:tensor<[1, 192, 8960], Float32, CPU>) -> (%1729:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1729:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.17 <CPU> {
        (%1730:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1757:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.RMSNormOp(%1730:tensor<[1, 192, 1536], Float32, CPU>) -> (%1731:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.17.self_attn (%1731:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1749:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.AddOp(%1749:tensor<[1, 192, 1536], Float32, CPU>, %1730:tensor<[1, 192, 1536], Float32, CPU>) -> (%1750:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.RMSNormOp(%1750:tensor<[1, 192, 1536], Float32, CPU>) -> (%1751:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.17.mlp (%1751:tensor<[1, 192, 1536], Float32, CPU>) -> (%1756:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.AddOp(%1756:tensor<[1, 192, 1536], Float32, CPU>, %1750:tensor<[1, 192, 1536], Float32, CPU>) -> (%1757:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1757:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.17.self_attn <CPU> {
        (%1731:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1749:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.LinearOp(%1731:tensor<[1, 192, 1536], Float32, CPU>) -> (%1732:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.LinearOp(%1731:tensor<[1, 192, 1536], Float32, CPU>) -> (%1733:tensor<[1, 192, 256], Float32, CPU>)
            linalg.CPU.LinearOp(%1731:tensor<[1, 192, 1536], Float32, CPU>) -> (%1734:tensor<[1, 192, 256], Float32, CPU>)
            linalg.CPU.ViewOp(%1732:tensor<[1, 192, 1536], Float32, CPU>) -> (%1732:tensor<[1, 192, 12, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1733:tensor<[1, 192, 256], Float32, CPU>) -> (%1733:tensor<[1, 192, 2, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1734:tensor<[1, 192, 256], Float32, CPU>) -> (%1734:tensor<[1, 192, 2, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1732:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1735:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1733:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1736:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1734:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1737:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.MultimodalRoPEOp(%1735:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1738:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.MultimodalRoPEOp(%1736:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1739:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.KVCacheOp(%1739:tensor<[1, 2, 192, 128], Float32, CPU>, %1737:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1740:tensor<[1, 2, 192, 128], Float32, CPU>, %1741:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.MatMulOp(%1738:tensor<[1, 12, 192, 128], Float32, CPU>, %1740:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1742:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.MulOp(%1742:tensor<[1, 12, 192, 192], Float32, CPU>, %1743:tensor<[1], Float32, CPU>) -> (%1744:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.CausalMaskOp(%1744:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1745:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%1745:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1746:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.MatMulOp(%1746:tensor<[1, 12, 192, 192], Float32, CPU>, %1741:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1747:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1747:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1748:tensor<[1, 192, 12, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1748:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1748:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.LinearOp(%1748:tensor<[1, 192, 1536], Float32, CPU>) -> (%1749:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1749:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.17.mlp <CPU> {
        (%1751:tensor<[1, 192, 1536], Float32, CPU>) -> (%1756:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.LinearOp(%1751:tensor<[1, 192, 1536], Float32, CPU>) -> (%1752:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.SiLUOp(%1752:tensor<[1, 192, 8960], Float32, CPU>) -> (%1753:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.LinearOp(%1751:tensor<[1, 192, 1536], Float32, CPU>) -> (%1754:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.MulOp(%1753:tensor<[1, 192, 8960], Float32, CPU>, %1754:tensor<[1, 192, 8960], Float32, CPU>) -> (%1755:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.LinearOp(%1755:tensor<[1, 192, 8960], Float32, CPU>) -> (%1756:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1756:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.18 <CPU> {
        (%1757:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1784:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.RMSNormOp(%1757:tensor<[1, 192, 1536], Float32, CPU>) -> (%1758:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.18.self_attn (%1758:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1776:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.AddOp(%1776:tensor<[1, 192, 1536], Float32, CPU>, %1757:tensor<[1, 192, 1536], Float32, CPU>) -> (%1777:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.RMSNormOp(%1777:tensor<[1, 192, 1536], Float32, CPU>) -> (%1778:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.18.mlp (%1778:tensor<[1, 192, 1536], Float32, CPU>) -> (%1783:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.AddOp(%1783:tensor<[1, 192, 1536], Float32, CPU>, %1777:tensor<[1, 192, 1536], Float32, CPU>) -> (%1784:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1784:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.18.self_attn <CPU> {
        (%1758:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1776:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.LinearOp(%1758:tensor<[1, 192, 1536], Float32, CPU>) -> (%1759:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.LinearOp(%1758:tensor<[1, 192, 1536], Float32, CPU>) -> (%1760:tensor<[1, 192, 256], Float32, CPU>)
            linalg.CPU.LinearOp(%1758:tensor<[1, 192, 1536], Float32, CPU>) -> (%1761:tensor<[1, 192, 256], Float32, CPU>)
            linalg.CPU.ViewOp(%1759:tensor<[1, 192, 1536], Float32, CPU>) -> (%1759:tensor<[1, 192, 12, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1760:tensor<[1, 192, 256], Float32, CPU>) -> (%1760:tensor<[1, 192, 2, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1761:tensor<[1, 192, 256], Float32, CPU>) -> (%1761:tensor<[1, 192, 2, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1759:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1762:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1760:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1763:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1761:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1764:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.MultimodalRoPEOp(%1762:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1765:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.MultimodalRoPEOp(%1763:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1766:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.KVCacheOp(%1766:tensor<[1, 2, 192, 128], Float32, CPU>, %1764:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1767:tensor<[1, 2, 192, 128], Float32, CPU>, %1768:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.MatMulOp(%1765:tensor<[1, 12, 192, 128], Float32, CPU>, %1767:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1769:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.MulOp(%1769:tensor<[1, 12, 192, 192], Float32, CPU>, %1770:tensor<[1], Float32, CPU>) -> (%1771:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.CausalMaskOp(%1771:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1772:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%1772:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1773:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.MatMulOp(%1773:tensor<[1, 12, 192, 192], Float32, CPU>, %1768:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1774:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1774:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1775:tensor<[1, 192, 12, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1775:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1775:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.LinearOp(%1775:tensor<[1, 192, 1536], Float32, CPU>) -> (%1776:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1776:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.18.mlp <CPU> {
        (%1778:tensor<[1, 192, 1536], Float32, CPU>) -> (%1783:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.LinearOp(%1778:tensor<[1, 192, 1536], Float32, CPU>) -> (%1779:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.SiLUOp(%1779:tensor<[1, 192, 8960], Float32, CPU>) -> (%1780:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.LinearOp(%1778:tensor<[1, 192, 1536], Float32, CPU>) -> (%1781:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.MulOp(%1780:tensor<[1, 192, 8960], Float32, CPU>, %1781:tensor<[1, 192, 8960], Float32, CPU>) -> (%1782:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.LinearOp(%1782:tensor<[1, 192, 8960], Float32, CPU>) -> (%1783:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1783:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.19 <CPU> {
        (%1784:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1811:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.RMSNormOp(%1784:tensor<[1, 192, 1536], Float32, CPU>) -> (%1785:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.19.self_attn (%1785:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1803:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.AddOp(%1803:tensor<[1, 192, 1536], Float32, CPU>, %1784:tensor<[1, 192, 1536], Float32, CPU>) -> (%1804:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.RMSNormOp(%1804:tensor<[1, 192, 1536], Float32, CPU>) -> (%1805:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.19.mlp (%1805:tensor<[1, 192, 1536], Float32, CPU>) -> (%1810:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.AddOp(%1810:tensor<[1, 192, 1536], Float32, CPU>, %1804:tensor<[1, 192, 1536], Float32, CPU>) -> (%1811:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1811:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.19.self_attn <CPU> {
        (%1785:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1803:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.LinearOp(%1785:tensor<[1, 192, 1536], Float32, CPU>) -> (%1786:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.LinearOp(%1785:tensor<[1, 192, 1536], Float32, CPU>) -> (%1787:tensor<[1, 192, 256], Float32, CPU>)
            linalg.CPU.LinearOp(%1785:tensor<[1, 192, 1536], Float32, CPU>) -> (%1788:tensor<[1, 192, 256], Float32, CPU>)
            linalg.CPU.ViewOp(%1786:tensor<[1, 192, 1536], Float32, CPU>) -> (%1786:tensor<[1, 192, 12, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1787:tensor<[1, 192, 256], Float32, CPU>) -> (%1787:tensor<[1, 192, 2, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1788:tensor<[1, 192, 256], Float32, CPU>) -> (%1788:tensor<[1, 192, 2, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1786:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1789:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1787:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1790:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1788:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1791:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.MultimodalRoPEOp(%1789:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1792:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.MultimodalRoPEOp(%1790:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1793:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.KVCacheOp(%1793:tensor<[1, 2, 192, 128], Float32, CPU>, %1791:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1794:tensor<[1, 2, 192, 128], Float32, CPU>, %1795:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.MatMulOp(%1792:tensor<[1, 12, 192, 128], Float32, CPU>, %1794:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1796:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.MulOp(%1796:tensor<[1, 12, 192, 192], Float32, CPU>, %1797:tensor<[1], Float32, CPU>) -> (%1798:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.CausalMaskOp(%1798:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1799:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%1799:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1800:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.MatMulOp(%1800:tensor<[1, 12, 192, 192], Float32, CPU>, %1795:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1801:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1801:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1802:tensor<[1, 192, 12, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1802:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1802:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.LinearOp(%1802:tensor<[1, 192, 1536], Float32, CPU>) -> (%1803:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1803:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.19.mlp <CPU> {
        (%1805:tensor<[1, 192, 1536], Float32, CPU>) -> (%1810:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.LinearOp(%1805:tensor<[1, 192, 1536], Float32, CPU>) -> (%1806:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.SiLUOp(%1806:tensor<[1, 192, 8960], Float32, CPU>) -> (%1807:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.LinearOp(%1805:tensor<[1, 192, 1536], Float32, CPU>) -> (%1808:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.MulOp(%1807:tensor<[1, 192, 8960], Float32, CPU>, %1808:tensor<[1, 192, 8960], Float32, CPU>) -> (%1809:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.LinearOp(%1809:tensor<[1, 192, 8960], Float32, CPU>) -> (%1810:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1810:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.20 <CPU> {
        (%1811:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1838:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.RMSNormOp(%1811:tensor<[1, 192, 1536], Float32, CPU>) -> (%1812:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.20.self_attn (%1812:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1830:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.AddOp(%1830:tensor<[1, 192, 1536], Float32, CPU>, %1811:tensor<[1, 192, 1536], Float32, CPU>) -> (%1831:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.RMSNormOp(%1831:tensor<[1, 192, 1536], Float32, CPU>) -> (%1832:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.20.mlp (%1832:tensor<[1, 192, 1536], Float32, CPU>) -> (%1837:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.AddOp(%1837:tensor<[1, 192, 1536], Float32, CPU>, %1831:tensor<[1, 192, 1536], Float32, CPU>) -> (%1838:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1838:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.20.self_attn <CPU> {
        (%1812:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1830:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.LinearOp(%1812:tensor<[1, 192, 1536], Float32, CPU>) -> (%1813:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.LinearOp(%1812:tensor<[1, 192, 1536], Float32, CPU>) -> (%1814:tensor<[1, 192, 256], Float32, CPU>)
            linalg.CPU.LinearOp(%1812:tensor<[1, 192, 1536], Float32, CPU>) -> (%1815:tensor<[1, 192, 256], Float32, CPU>)
            linalg.CPU.ViewOp(%1813:tensor<[1, 192, 1536], Float32, CPU>) -> (%1813:tensor<[1, 192, 12, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1814:tensor<[1, 192, 256], Float32, CPU>) -> (%1814:tensor<[1, 192, 2, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1815:tensor<[1, 192, 256], Float32, CPU>) -> (%1815:tensor<[1, 192, 2, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1813:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1816:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1814:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1817:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1815:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1818:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.MultimodalRoPEOp(%1816:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1819:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.MultimodalRoPEOp(%1817:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1820:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.KVCacheOp(%1820:tensor<[1, 2, 192, 128], Float32, CPU>, %1818:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1821:tensor<[1, 2, 192, 128], Float32, CPU>, %1822:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.MatMulOp(%1819:tensor<[1, 12, 192, 128], Float32, CPU>, %1821:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1823:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.MulOp(%1823:tensor<[1, 12, 192, 192], Float32, CPU>, %1824:tensor<[1], Float32, CPU>) -> (%1825:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.CausalMaskOp(%1825:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1826:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%1826:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1827:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.MatMulOp(%1827:tensor<[1, 12, 192, 192], Float32, CPU>, %1822:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1828:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1828:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1829:tensor<[1, 192, 12, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1829:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1829:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.LinearOp(%1829:tensor<[1, 192, 1536], Float32, CPU>) -> (%1830:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1830:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.20.mlp <CPU> {
        (%1832:tensor<[1, 192, 1536], Float32, CPU>) -> (%1837:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.LinearOp(%1832:tensor<[1, 192, 1536], Float32, CPU>) -> (%1833:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.SiLUOp(%1833:tensor<[1, 192, 8960], Float32, CPU>) -> (%1834:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.LinearOp(%1832:tensor<[1, 192, 1536], Float32, CPU>) -> (%1835:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.MulOp(%1834:tensor<[1, 192, 8960], Float32, CPU>, %1835:tensor<[1, 192, 8960], Float32, CPU>) -> (%1836:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.LinearOp(%1836:tensor<[1, 192, 8960], Float32, CPU>) -> (%1837:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1837:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.21 <CPU> {
        (%1838:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1865:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.RMSNormOp(%1838:tensor<[1, 192, 1536], Float32, CPU>) -> (%1839:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.21.self_attn (%1839:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1857:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.AddOp(%1857:tensor<[1, 192, 1536], Float32, CPU>, %1838:tensor<[1, 192, 1536], Float32, CPU>) -> (%1858:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.RMSNormOp(%1858:tensor<[1, 192, 1536], Float32, CPU>) -> (%1859:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.21.mlp (%1859:tensor<[1, 192, 1536], Float32, CPU>) -> (%1864:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.AddOp(%1864:tensor<[1, 192, 1536], Float32, CPU>, %1858:tensor<[1, 192, 1536], Float32, CPU>) -> (%1865:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1865:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.21.self_attn <CPU> {
        (%1839:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1857:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.LinearOp(%1839:tensor<[1, 192, 1536], Float32, CPU>) -> (%1840:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.LinearOp(%1839:tensor<[1, 192, 1536], Float32, CPU>) -> (%1841:tensor<[1, 192, 256], Float32, CPU>)
            linalg.CPU.LinearOp(%1839:tensor<[1, 192, 1536], Float32, CPU>) -> (%1842:tensor<[1, 192, 256], Float32, CPU>)
            linalg.CPU.ViewOp(%1840:tensor<[1, 192, 1536], Float32, CPU>) -> (%1840:tensor<[1, 192, 12, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1841:tensor<[1, 192, 256], Float32, CPU>) -> (%1841:tensor<[1, 192, 2, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1842:tensor<[1, 192, 256], Float32, CPU>) -> (%1842:tensor<[1, 192, 2, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1840:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1843:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1841:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1844:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1842:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1845:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.MultimodalRoPEOp(%1843:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1846:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.MultimodalRoPEOp(%1844:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1847:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.KVCacheOp(%1847:tensor<[1, 2, 192, 128], Float32, CPU>, %1845:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1848:tensor<[1, 2, 192, 128], Float32, CPU>, %1849:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.MatMulOp(%1846:tensor<[1, 12, 192, 128], Float32, CPU>, %1848:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1850:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.MulOp(%1850:tensor<[1, 12, 192, 192], Float32, CPU>, %1851:tensor<[1], Float32, CPU>) -> (%1852:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.CausalMaskOp(%1852:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1853:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%1853:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1854:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.MatMulOp(%1854:tensor<[1, 12, 192, 192], Float32, CPU>, %1849:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1855:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1855:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1856:tensor<[1, 192, 12, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1856:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1856:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.LinearOp(%1856:tensor<[1, 192, 1536], Float32, CPU>) -> (%1857:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1857:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.21.mlp <CPU> {
        (%1859:tensor<[1, 192, 1536], Float32, CPU>) -> (%1864:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.LinearOp(%1859:tensor<[1, 192, 1536], Float32, CPU>) -> (%1860:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.SiLUOp(%1860:tensor<[1, 192, 8960], Float32, CPU>) -> (%1861:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.LinearOp(%1859:tensor<[1, 192, 1536], Float32, CPU>) -> (%1862:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.MulOp(%1861:tensor<[1, 192, 8960], Float32, CPU>, %1862:tensor<[1, 192, 8960], Float32, CPU>) -> (%1863:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.LinearOp(%1863:tensor<[1, 192, 8960], Float32, CPU>) -> (%1864:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1864:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.22 <CPU> {
        (%1865:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1892:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.RMSNormOp(%1865:tensor<[1, 192, 1536], Float32, CPU>) -> (%1866:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.22.self_attn (%1866:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1884:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.AddOp(%1884:tensor<[1, 192, 1536], Float32, CPU>, %1865:tensor<[1, 192, 1536], Float32, CPU>) -> (%1885:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.RMSNormOp(%1885:tensor<[1, 192, 1536], Float32, CPU>) -> (%1886:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.22.mlp (%1886:tensor<[1, 192, 1536], Float32, CPU>) -> (%1891:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.AddOp(%1891:tensor<[1, 192, 1536], Float32, CPU>, %1885:tensor<[1, 192, 1536], Float32, CPU>) -> (%1892:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1892:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.22.self_attn <CPU> {
        (%1866:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1884:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.LinearOp(%1866:tensor<[1, 192, 1536], Float32, CPU>) -> (%1867:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.LinearOp(%1866:tensor<[1, 192, 1536], Float32, CPU>) -> (%1868:tensor<[1, 192, 256], Float32, CPU>)
            linalg.CPU.LinearOp(%1866:tensor<[1, 192, 1536], Float32, CPU>) -> (%1869:tensor<[1, 192, 256], Float32, CPU>)
            linalg.CPU.ViewOp(%1867:tensor<[1, 192, 1536], Float32, CPU>) -> (%1867:tensor<[1, 192, 12, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1868:tensor<[1, 192, 256], Float32, CPU>) -> (%1868:tensor<[1, 192, 2, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1869:tensor<[1, 192, 256], Float32, CPU>) -> (%1869:tensor<[1, 192, 2, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1867:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1870:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1868:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1871:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1869:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1872:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.MultimodalRoPEOp(%1870:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1873:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.MultimodalRoPEOp(%1871:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1874:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.KVCacheOp(%1874:tensor<[1, 2, 192, 128], Float32, CPU>, %1872:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1875:tensor<[1, 2, 192, 128], Float32, CPU>, %1876:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.MatMulOp(%1873:tensor<[1, 12, 192, 128], Float32, CPU>, %1875:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1877:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.MulOp(%1877:tensor<[1, 12, 192, 192], Float32, CPU>, %1878:tensor<[1], Float32, CPU>) -> (%1879:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.CausalMaskOp(%1879:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1880:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%1880:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1881:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.MatMulOp(%1881:tensor<[1, 12, 192, 192], Float32, CPU>, %1876:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1882:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1882:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1883:tensor<[1, 192, 12, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1883:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1883:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.LinearOp(%1883:tensor<[1, 192, 1536], Float32, CPU>) -> (%1884:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1884:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.22.mlp <CPU> {
        (%1886:tensor<[1, 192, 1536], Float32, CPU>) -> (%1891:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.LinearOp(%1886:tensor<[1, 192, 1536], Float32, CPU>) -> (%1887:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.SiLUOp(%1887:tensor<[1, 192, 8960], Float32, CPU>) -> (%1888:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.LinearOp(%1886:tensor<[1, 192, 1536], Float32, CPU>) -> (%1889:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.MulOp(%1888:tensor<[1, 192, 8960], Float32, CPU>, %1889:tensor<[1, 192, 8960], Float32, CPU>) -> (%1890:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.LinearOp(%1890:tensor<[1, 192, 8960], Float32, CPU>) -> (%1891:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1891:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.23 <CPU> {
        (%1892:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1919:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.RMSNormOp(%1892:tensor<[1, 192, 1536], Float32, CPU>) -> (%1893:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.23.self_attn (%1893:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1911:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.AddOp(%1911:tensor<[1, 192, 1536], Float32, CPU>, %1892:tensor<[1, 192, 1536], Float32, CPU>) -> (%1912:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.RMSNormOp(%1912:tensor<[1, 192, 1536], Float32, CPU>) -> (%1913:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.23.mlp (%1913:tensor<[1, 192, 1536], Float32, CPU>) -> (%1918:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.AddOp(%1918:tensor<[1, 192, 1536], Float32, CPU>, %1912:tensor<[1, 192, 1536], Float32, CPU>) -> (%1919:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1919:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.23.self_attn <CPU> {
        (%1893:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1911:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.LinearOp(%1893:tensor<[1, 192, 1536], Float32, CPU>) -> (%1894:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.LinearOp(%1893:tensor<[1, 192, 1536], Float32, CPU>) -> (%1895:tensor<[1, 192, 256], Float32, CPU>)
            linalg.CPU.LinearOp(%1893:tensor<[1, 192, 1536], Float32, CPU>) -> (%1896:tensor<[1, 192, 256], Float32, CPU>)
            linalg.CPU.ViewOp(%1894:tensor<[1, 192, 1536], Float32, CPU>) -> (%1894:tensor<[1, 192, 12, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1895:tensor<[1, 192, 256], Float32, CPU>) -> (%1895:tensor<[1, 192, 2, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1896:tensor<[1, 192, 256], Float32, CPU>) -> (%1896:tensor<[1, 192, 2, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1894:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1897:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1895:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1898:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1896:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1899:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.MultimodalRoPEOp(%1897:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1900:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.MultimodalRoPEOp(%1898:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1901:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.KVCacheOp(%1901:tensor<[1, 2, 192, 128], Float32, CPU>, %1899:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1902:tensor<[1, 2, 192, 128], Float32, CPU>, %1903:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.MatMulOp(%1900:tensor<[1, 12, 192, 128], Float32, CPU>, %1902:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1904:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.MulOp(%1904:tensor<[1, 12, 192, 192], Float32, CPU>, %1905:tensor<[1], Float32, CPU>) -> (%1906:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.CausalMaskOp(%1906:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1907:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%1907:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1908:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.MatMulOp(%1908:tensor<[1, 12, 192, 192], Float32, CPU>, %1903:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1909:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1909:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1910:tensor<[1, 192, 12, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1910:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1910:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.LinearOp(%1910:tensor<[1, 192, 1536], Float32, CPU>) -> (%1911:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1911:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.23.mlp <CPU> {
        (%1913:tensor<[1, 192, 1536], Float32, CPU>) -> (%1918:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.LinearOp(%1913:tensor<[1, 192, 1536], Float32, CPU>) -> (%1914:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.SiLUOp(%1914:tensor<[1, 192, 8960], Float32, CPU>) -> (%1915:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.LinearOp(%1913:tensor<[1, 192, 1536], Float32, CPU>) -> (%1916:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.MulOp(%1915:tensor<[1, 192, 8960], Float32, CPU>, %1916:tensor<[1, 192, 8960], Float32, CPU>) -> (%1917:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.LinearOp(%1917:tensor<[1, 192, 8960], Float32, CPU>) -> (%1918:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1918:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.24 <CPU> {
        (%1919:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1946:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.RMSNormOp(%1919:tensor<[1, 192, 1536], Float32, CPU>) -> (%1920:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.24.self_attn (%1920:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1938:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.AddOp(%1938:tensor<[1, 192, 1536], Float32, CPU>, %1919:tensor<[1, 192, 1536], Float32, CPU>) -> (%1939:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.RMSNormOp(%1939:tensor<[1, 192, 1536], Float32, CPU>) -> (%1940:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.24.mlp (%1940:tensor<[1, 192, 1536], Float32, CPU>) -> (%1945:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.AddOp(%1945:tensor<[1, 192, 1536], Float32, CPU>, %1939:tensor<[1, 192, 1536], Float32, CPU>) -> (%1946:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1946:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.24.self_attn <CPU> {
        (%1920:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1938:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.LinearOp(%1920:tensor<[1, 192, 1536], Float32, CPU>) -> (%1921:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.LinearOp(%1920:tensor<[1, 192, 1536], Float32, CPU>) -> (%1922:tensor<[1, 192, 256], Float32, CPU>)
            linalg.CPU.LinearOp(%1920:tensor<[1, 192, 1536], Float32, CPU>) -> (%1923:tensor<[1, 192, 256], Float32, CPU>)
            linalg.CPU.ViewOp(%1921:tensor<[1, 192, 1536], Float32, CPU>) -> (%1921:tensor<[1, 192, 12, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1922:tensor<[1, 192, 256], Float32, CPU>) -> (%1922:tensor<[1, 192, 2, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1923:tensor<[1, 192, 256], Float32, CPU>) -> (%1923:tensor<[1, 192, 2, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1921:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1924:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1922:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1925:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1923:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1926:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.MultimodalRoPEOp(%1924:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1927:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.MultimodalRoPEOp(%1925:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1928:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.KVCacheOp(%1928:tensor<[1, 2, 192, 128], Float32, CPU>, %1926:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1929:tensor<[1, 2, 192, 128], Float32, CPU>, %1930:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.MatMulOp(%1927:tensor<[1, 12, 192, 128], Float32, CPU>, %1929:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1931:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.MulOp(%1931:tensor<[1, 12, 192, 192], Float32, CPU>, %1932:tensor<[1], Float32, CPU>) -> (%1933:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.CausalMaskOp(%1933:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1934:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%1934:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1935:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.MatMulOp(%1935:tensor<[1, 12, 192, 192], Float32, CPU>, %1930:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1936:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1936:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1937:tensor<[1, 192, 12, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1937:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1937:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.LinearOp(%1937:tensor<[1, 192, 1536], Float32, CPU>) -> (%1938:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1938:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.24.mlp <CPU> {
        (%1940:tensor<[1, 192, 1536], Float32, CPU>) -> (%1945:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.LinearOp(%1940:tensor<[1, 192, 1536], Float32, CPU>) -> (%1941:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.SiLUOp(%1941:tensor<[1, 192, 8960], Float32, CPU>) -> (%1942:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.LinearOp(%1940:tensor<[1, 192, 1536], Float32, CPU>) -> (%1943:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.MulOp(%1942:tensor<[1, 192, 8960], Float32, CPU>, %1943:tensor<[1, 192, 8960], Float32, CPU>) -> (%1944:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.LinearOp(%1944:tensor<[1, 192, 8960], Float32, CPU>) -> (%1945:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1945:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.25 <CPU> {
        (%1946:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1973:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.RMSNormOp(%1946:tensor<[1, 192, 1536], Float32, CPU>) -> (%1947:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.25.self_attn (%1947:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1965:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.AddOp(%1965:tensor<[1, 192, 1536], Float32, CPU>, %1946:tensor<[1, 192, 1536], Float32, CPU>) -> (%1966:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.RMSNormOp(%1966:tensor<[1, 192, 1536], Float32, CPU>) -> (%1967:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.25.mlp (%1967:tensor<[1, 192, 1536], Float32, CPU>) -> (%1972:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.AddOp(%1972:tensor<[1, 192, 1536], Float32, CPU>, %1966:tensor<[1, 192, 1536], Float32, CPU>) -> (%1973:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1973:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.25.self_attn <CPU> {
        (%1947:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1965:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.LinearOp(%1947:tensor<[1, 192, 1536], Float32, CPU>) -> (%1948:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.LinearOp(%1947:tensor<[1, 192, 1536], Float32, CPU>) -> (%1949:tensor<[1, 192, 256], Float32, CPU>)
            linalg.CPU.LinearOp(%1947:tensor<[1, 192, 1536], Float32, CPU>) -> (%1950:tensor<[1, 192, 256], Float32, CPU>)
            linalg.CPU.ViewOp(%1948:tensor<[1, 192, 1536], Float32, CPU>) -> (%1948:tensor<[1, 192, 12, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1949:tensor<[1, 192, 256], Float32, CPU>) -> (%1949:tensor<[1, 192, 2, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1950:tensor<[1, 192, 256], Float32, CPU>) -> (%1950:tensor<[1, 192, 2, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1948:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1951:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1949:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1952:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1950:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1953:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.MultimodalRoPEOp(%1951:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1954:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.MultimodalRoPEOp(%1952:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1955:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.KVCacheOp(%1955:tensor<[1, 2, 192, 128], Float32, CPU>, %1953:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1956:tensor<[1, 2, 192, 128], Float32, CPU>, %1957:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.MatMulOp(%1954:tensor<[1, 12, 192, 128], Float32, CPU>, %1956:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1958:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.MulOp(%1958:tensor<[1, 12, 192, 192], Float32, CPU>, %1959:tensor<[1], Float32, CPU>) -> (%1960:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.CausalMaskOp(%1960:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1961:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%1961:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1962:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.MatMulOp(%1962:tensor<[1, 12, 192, 192], Float32, CPU>, %1957:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1963:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1963:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1964:tensor<[1, 192, 12, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1964:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1964:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.LinearOp(%1964:tensor<[1, 192, 1536], Float32, CPU>) -> (%1965:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1965:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.25.mlp <CPU> {
        (%1967:tensor<[1, 192, 1536], Float32, CPU>) -> (%1972:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.LinearOp(%1967:tensor<[1, 192, 1536], Float32, CPU>) -> (%1968:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.SiLUOp(%1968:tensor<[1, 192, 8960], Float32, CPU>) -> (%1969:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.LinearOp(%1967:tensor<[1, 192, 1536], Float32, CPU>) -> (%1970:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.MulOp(%1969:tensor<[1, 192, 8960], Float32, CPU>, %1970:tensor<[1, 192, 8960], Float32, CPU>) -> (%1971:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.LinearOp(%1971:tensor<[1, 192, 8960], Float32, CPU>) -> (%1972:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1972:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.26 <CPU> {
        (%1973:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%2000:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.RMSNormOp(%1973:tensor<[1, 192, 1536], Float32, CPU>) -> (%1974:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.26.self_attn (%1974:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1992:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.AddOp(%1992:tensor<[1, 192, 1536], Float32, CPU>, %1973:tensor<[1, 192, 1536], Float32, CPU>) -> (%1993:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.RMSNormOp(%1993:tensor<[1, 192, 1536], Float32, CPU>) -> (%1994:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.26.mlp (%1994:tensor<[1, 192, 1536], Float32, CPU>) -> (%1999:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.AddOp(%1999:tensor<[1, 192, 1536], Float32, CPU>, %1993:tensor<[1, 192, 1536], Float32, CPU>) -> (%2000:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%2000:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.26.self_attn <CPU> {
        (%1974:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1992:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.LinearOp(%1974:tensor<[1, 192, 1536], Float32, CPU>) -> (%1975:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.LinearOp(%1974:tensor<[1, 192, 1536], Float32, CPU>) -> (%1976:tensor<[1, 192, 256], Float32, CPU>)
            linalg.CPU.LinearOp(%1974:tensor<[1, 192, 1536], Float32, CPU>) -> (%1977:tensor<[1, 192, 256], Float32, CPU>)
            linalg.CPU.ViewOp(%1975:tensor<[1, 192, 1536], Float32, CPU>) -> (%1975:tensor<[1, 192, 12, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1976:tensor<[1, 192, 256], Float32, CPU>) -> (%1976:tensor<[1, 192, 2, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1977:tensor<[1, 192, 256], Float32, CPU>) -> (%1977:tensor<[1, 192, 2, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1975:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1978:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1976:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1979:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1977:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%1980:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.MultimodalRoPEOp(%1978:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1981:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.MultimodalRoPEOp(%1979:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%1982:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.KVCacheOp(%1982:tensor<[1, 2, 192, 128], Float32, CPU>, %1980:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1983:tensor<[1, 2, 192, 128], Float32, CPU>, %1984:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.MatMulOp(%1981:tensor<[1, 12, 192, 128], Float32, CPU>, %1983:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1985:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.MulOp(%1985:tensor<[1, 12, 192, 192], Float32, CPU>, %1986:tensor<[1], Float32, CPU>) -> (%1987:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.CausalMaskOp(%1987:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1988:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%1988:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%1989:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.MatMulOp(%1989:tensor<[1, 12, 192, 192], Float32, CPU>, %1984:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%1990:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1990:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%1991:tensor<[1, 192, 12, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1991:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%1991:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.LinearOp(%1991:tensor<[1, 192, 1536], Float32, CPU>) -> (%1992:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1992:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.26.mlp <CPU> {
        (%1994:tensor<[1, 192, 1536], Float32, CPU>) -> (%1999:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.LinearOp(%1994:tensor<[1, 192, 1536], Float32, CPU>) -> (%1995:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.SiLUOp(%1995:tensor<[1, 192, 8960], Float32, CPU>) -> (%1996:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.LinearOp(%1994:tensor<[1, 192, 1536], Float32, CPU>) -> (%1997:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.MulOp(%1996:tensor<[1, 192, 8960], Float32, CPU>, %1997:tensor<[1, 192, 8960], Float32, CPU>) -> (%1998:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.LinearOp(%1998:tensor<[1, 192, 8960], Float32, CPU>) -> (%1999:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%1999:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.27 <CPU> {
        (%2000:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%2027:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.RMSNormOp(%2000:tensor<[1, 192, 1536], Float32, CPU>) -> (%2001:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.27.self_attn (%2001:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%2019:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.AddOp(%2019:tensor<[1, 192, 1536], Float32, CPU>, %2000:tensor<[1, 192, 1536], Float32, CPU>) -> (%2020:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.RMSNormOp(%2020:tensor<[1, 192, 1536], Float32, CPU>) -> (%2021:tensor<[1, 192, 1536], Float32, CPU>)
            graph.CallGraphOp @model.layers.27.mlp (%2021:tensor<[1, 192, 1536], Float32, CPU>) -> (%2026:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.AddOp(%2026:tensor<[1, 192, 1536], Float32, CPU>, %2020:tensor<[1, 192, 1536], Float32, CPU>) -> (%2027:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%2027:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.27.self_attn <CPU> {
        (%2001:tensor<[1, 192, 1536], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%2019:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.LinearOp(%2001:tensor<[1, 192, 1536], Float32, CPU>) -> (%2002:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.LinearOp(%2001:tensor<[1, 192, 1536], Float32, CPU>) -> (%2003:tensor<[1, 192, 256], Float32, CPU>)
            linalg.CPU.LinearOp(%2001:tensor<[1, 192, 1536], Float32, CPU>) -> (%2004:tensor<[1, 192, 256], Float32, CPU>)
            linalg.CPU.ViewOp(%2002:tensor<[1, 192, 1536], Float32, CPU>) -> (%2002:tensor<[1, 192, 12, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%2003:tensor<[1, 192, 256], Float32, CPU>) -> (%2003:tensor<[1, 192, 2, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%2004:tensor<[1, 192, 256], Float32, CPU>) -> (%2004:tensor<[1, 192, 2, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%2002:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%2005:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%2003:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%2006:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%2004:tensor<[1, 192, 2, 128], Float32, CPU>) -> (%2007:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.MultimodalRoPEOp(%2005:tensor<[1, 12, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%2008:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.MultimodalRoPEOp(%2006:tensor<[1, 2, 192, 128], Float32, CPU>, %1270:tensor<[192, 128], Float32, CPU>, %1271:tensor<[192, 128], Float32, CPU>) -> (%2009:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.KVCacheOp(%2009:tensor<[1, 2, 192, 128], Float32, CPU>, %2007:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%2010:tensor<[1, 2, 192, 128], Float32, CPU>, %2011:tensor<[1, 2, 192, 128], Float32, CPU>)
            linalg.CPU.MatMulOp(%2008:tensor<[1, 12, 192, 128], Float32, CPU>, %2010:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%2012:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.MulOp(%2012:tensor<[1, 12, 192, 192], Float32, CPU>, %2013:tensor<[1], Float32, CPU>) -> (%2014:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.CausalMaskOp(%2014:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%2015:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%2015:tensor<[1, 12, 192, 192], Float32, CPU>) -> (%2016:tensor<[1, 12, 192, 192], Float32, CPU>)
            linalg.CPU.MatMulOp(%2016:tensor<[1, 12, 192, 192], Float32, CPU>, %2011:tensor<[1, 2, 192, 128], Float32, CPU>) -> (%2017:tensor<[1, 12, 192, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%2017:tensor<[1, 12, 192, 128], Float32, CPU>) -> (%2018:tensor<[1, 192, 12, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%2018:tensor<[1, 192, 12, 128], Float32, CPU>) -> (%2018:tensor<[1, 192, 1536], Float32, CPU>)
            linalg.CPU.LinearOp(%2018:tensor<[1, 192, 1536], Float32, CPU>) -> (%2019:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%2019:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.27.mlp <CPU> {
        (%2021:tensor<[1, 192, 1536], Float32, CPU>) -> (%2026:tensor<[1, 192, 1536], Float32, CPU>) {
            linalg.CPU.LinearOp(%2021:tensor<[1, 192, 1536], Float32, CPU>) -> (%2022:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.SiLUOp(%2022:tensor<[1, 192, 8960], Float32, CPU>) -> (%2023:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.LinearOp(%2021:tensor<[1, 192, 1536], Float32, CPU>) -> (%2024:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.MulOp(%2023:tensor<[1, 192, 8960], Float32, CPU>, %2024:tensor<[1, 192, 8960], Float32, CPU>) -> (%2025:tensor<[1, 192, 8960], Float32, CPU>)
            linalg.CPU.LinearOp(%2025:tensor<[1, 192, 8960], Float32, CPU>) -> (%2026:tensor<[1, 192, 1536], Float32, CPU>)
            cf.ReturnOp (%2026:tensor<[1, 192, 1536], Float32, CPU>) -> ()
        }
    }
    //        
    //      o o    
    //            
    //       
    //             
    //        
    // Bravo! Tracing finished successfully!
}
