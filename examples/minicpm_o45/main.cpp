// Copyright (c) MLLM Team.
// Licensed under the MIT License.

#include <fstream>
#include <iostream>
#include <chrono>
#include <sstream>
#include <string>
#include <vector>

#include <fmt/core.h>

#include "mllm/mllm.hpp"
#include "mllm/models/minicpm_o45/configuration_minicpm_o45.hpp"
#include "mllm/models/minicpm_o45/modeling_minicpm_o45.hpp"
#include "mllm/models/minicpm_o45/modeling_minicpm_o45_token2wav.hpp"
#include "mllm/models/minicpm_o45/tokenization_minicpm_o45.hpp"
#include "mllm/models/minicpm_o45/token2wav_prompt_cache.hpp"

#include "wenet_audio/wav.h"

using mllm::Argparse;

MLLM_MAIN({
  mllm::Logger::level() = mllm::LogLevel::kError;

  auto& help = Argparse::add<bool>("-h|--help").help("Show help message");
  auto& model_path = Argparse::add<std::string>("-m|--model_path").help("Model path").def("");
  auto& model_version = Argparse::add<std::string>("-mv|--model_version").help("Model version: v1/v2").def("v1");
  auto& tokenizer_path = Argparse::add<std::string>("-t|--tokenizer_path").help("Tokenizer path (tokenizer.json)").def("");
  auto& config_path = Argparse::add<std::string>("-c|--config_path").help("Config path").def("");
  auto& prompt = Argparse::add<std::string>("-p|--prompt").help("Prompt text").def("Describe the input.");
  auto& image_path = Argparse::add<std::string>("-i|--image").help("Optional image path").def("");
  auto& audio_path = Argparse::add<std::string>("-a|--audio").help("Optional audio path (wav)").def("");
  auto& generate_tts_tokens = Argparse::add<bool>("-gt|--generate_tts_tokens")
                                  .help("Generate TTS tokens (text->tts-token stage, no waveform)")
                                  .def(false);
  auto& text_max_new_tokens = Argparse::add<int32_t>("--text_max_new_tokens").help("Max new text tokens").def(512);
  auto& tts_max_new_tokens = Argparse::add<int32_t>("--tts_max_new_tokens").help("Max new TTS tokens").def(1024);
  auto& tts_min_new_tokens = Argparse::add<int32_t>("--tts_min_new_tokens").help("Min new TTS tokens").def(50);
  auto& tts_force_no_stop = Argparse::add<bool>("--tts_force_no_stop").help("Disable TTS EOS stopping").def(false);
  auto& tts_temperature = Argparse::add<float>("--tts_temperature").help("TTS sampling temperature").def(0.8f);
  auto& tts_top_k = Argparse::add<int32_t>("--tts_top_k").help("TTS top-k sampling (<=0 disables)").def(25);
  auto& tts_top_p = Argparse::add<float>("--tts_top_p").help("TTS top-p sampling (<=0 or >=1 disables)").def(0.85f);
  auto& tts_repetition_penalty =
      Argparse::add<float>("--tts_repetition_penalty").help("TTS repetition penalty (1.0 disables)").def(1.05f);
  auto& tts_repetition_window =
      Argparse::add<int32_t>("--tts_repetition_window").help("TTS repetition window size in generated tokens").def(16);
  auto& tts_greedy = Argparse::add<bool>("--tts_greedy").help("Use greedy decoding for TTS tokens").def(false);
  auto& tts_tokens_out = Argparse::add<std::string>("--tts_tokens_out").help("Output path for generated TTS token ids").def("");
  auto& tts_tokens_in =
      Argparse::add<std::string>("--tts_tokens_in").help("Input path for pre-generated TTS token ids (one per line or whitespace).").def("");
  auto& tts_wav_out = Argparse::add<std::string>("--tts_wav_out")
                          .help("Output wav path. If set, run native C++ token2wav.")
                          .def("");
  auto& tts_token2wav_model_path = Argparse::add<std::string>("--tts_token2wav_model_path")
                                       .help("Path to token2wav .mllm (if empty, fallback to --model_path).")
                                       .def("");
  auto& tts_token2wav_model_version = Argparse::add<std::string>("--tts_token2wav_model_version")
                                          .help("token2wav model version: v1/v2")
                                          .def("v1");
  auto& tts_prompt_cache = Argparse::add<std::string>("--tts_prompt_cache")
                               .help("Path to fixed prompt cache generated by export_prompt_cache.py")
                               .def("");
  auto& tts_token2wav_n_timesteps = Argparse::add<int32_t>("--tts_token2wav_n_timesteps")
                                        .help("Flow diffusion steps for native token2wav")
                                        .def(10);
  auto& debug_progress = Argparse::add<bool>("--debug_progress").help("Print step-level debug progress.").def(false);
  auto& debug_interval =
      Argparse::add<int32_t>("--debug_interval").help("Token step interval for debug progress logs.").def(16);

  Argparse::parse(argc, argv);

  if (help.isSet()) {
    Argparse::printHelp();
    mllm::shutdownContext();
    return 0;
  }

  mllm::ModelFileVersion file_version = mllm::ModelFileVersion::kV1;
  if (model_version.get() == "v2") { file_version = mllm::ModelFileVersion::kV2; }

  auto token2wav_model_path = tts_token2wav_model_path.get().empty() ? model_path.get() : tts_token2wav_model_path.get();
  mllm::ModelFileVersion token2wav_file_version = mllm::ModelFileVersion::kV1;
  if (tts_token2wav_model_version.get() == "v2") { token2wav_file_version = mllm::ModelFileVersion::kV2; }

  auto run_native_token2wav = !tts_wav_out.get().empty();
  if (run_native_token2wav && tts_prompt_cache.get().empty()) {
    MLLM_ERROR_EXIT(mllm::ExitCode::kCoreError, "--tts_prompt_cache is required when --tts_wav_out is set.");
  }

  auto debug_t0 = std::chrono::steady_clock::now();
  auto debug_log = [&](const std::string& msg) {
    if (!debug_progress.get()) { return; }
    auto now = std::chrono::steady_clock::now();
    auto sec = std::chrono::duration_cast<std::chrono::milliseconds>(now - debug_t0).count() / 1000.0;
    fmt::print("[debug +{:.3f}s] {}\n", sec, msg);
  };

  if (!tts_tokens_in.get().empty()) {
    if (!run_native_token2wav) {
      MLLM_ERROR_EXIT(mllm::ExitCode::kCoreError, "--tts_wav_out is required when --tts_tokens_in is set.");
    }
    if (token2wav_model_path.empty()) {
      MLLM_ERROR_EXIT(mllm::ExitCode::kCoreError, "Missing token2wav model path (--tts_token2wav_model_path or --model_path).");
    }

    std::ifstream ifs(tts_tokens_in.get());
    if (!ifs.is_open()) { MLLM_ERROR_EXIT(mllm::ExitCode::kIOError, "Failed to open token file: {}", tts_tokens_in.get()); }
    std::vector<int64_t> token_ids;
    for (std::string line; std::getline(ifs, line);) {
      if (line.empty()) { continue; }
      std::stringstream ss(line);
      while (!ss.eof()) {
        int64_t token = 0;
        ss >> token;
        if (!ss.fail()) { token_ids.push_back(token); }
      }
    }
    if (token_ids.empty()) { MLLM_ERROR_EXIT(mllm::ExitCode::kCoreError, "No token id found in {}", tts_tokens_in.get()); }

    fmt::print("Loaded {} TTS token IDs from {}\n", token_ids.size(), tts_tokens_in.get());
    debug_log("Loading token2wav model and prompt cache...");
    auto token2wav_param = mllm::load(token2wav_model_path, token2wav_file_version);
    auto prompt_cache = mllm::models::minicpm_o45::loadMiniCPMO45Token2WavPromptCache(tts_prompt_cache.get());

    mllm::models::minicpm_o45::MiniCPMO45Token2WavModel token2wav("token2wav", {});
    token2wav.loadFromParameter(token2wav_param);
    debug_log("Native token2wav model loaded.");

    debug_log("Running native flow + HiFT...");
    auto wav = token2wav.infer(token_ids, prompt_cache, std::max(1, tts_token2wav_n_timesteps.get()));
    auto wav_i16 = wav * 32767.0f;
    wenet::WavWriter wav_writer(wav_i16.ptr<float>(), wav_i16.shape().back(), 1, 24000, 16);
    wav_writer.Write(tts_wav_out.get());
    fmt::print("Saved TTS waveform to {}\n", tts_wav_out.get());
    debug_log("Native token2wav finished.");
    mllm::shutdownContext();
    return 0;
  }

  if (model_path.get().empty() || tokenizer_path.get().empty() || config_path.get().empty()) {
    Argparse::printHelp();
    MLLM_ERROR_EXIT(mllm::ExitCode::kCoreError,
                    "Missing required arguments: --model_path, --tokenizer_path, --config_path");
  }

  auto cfg = mllm::models::minicpm_o45::MiniCPMO45Config(config_path.get());

  debug_log("Loading tokenizer and model modules...");
  auto tokenizer = mllm::models::minicpm_o45::MiniCPMO45Tokenizer(tokenizer_path.get(), cfg.vision_patch_size, cfg.audio_pool_step);
  auto model = mllm::models::minicpm_o45::MiniCPMO45ForCausalLM(cfg);

  debug_log("Loading model parameters...");
  auto param = mllm::load(model_path.get(), file_version);
  model.llm_.load(param);
  model.vpm_.load(param);
  model.resampler_.load(param);
  model.apm_.load(param);
  model.audio_projection_layer_.load(param);
  if (generate_tts_tokens.get()) { model.tts_.loadFromParameter(param); }
  debug_log("Model parameters loaded.");

  mllm::models::minicpm_o45::MiniCPMO45Message message;
  message.prompt = prompt.get();
  message.img_file_path = image_path.get();
  message.audio_file_path = audio_path.get();

  auto inputs = tokenizer.convertMessage(message, generate_tts_tokens.get());
  debug_log("Tokenizer convertMessage finished.");

  fmt::print("\n{:*^60}\n", " MiniCPM-o-4_5 CLI ");
  fmt::print("Prompt: {}\n", message.prompt);
  if (!message.img_file_path.empty()) { fmt::print("Image : {}\n", message.img_file_path); }
  if (!message.audio_file_path.empty()) { fmt::print("Audio : {}\n", message.audio_file_path); }

  if (!generate_tts_tokens.get()) {
    fmt::print("\nResponse: ");
    for (auto& step : model.chat(inputs)) {
      std::wcout << tokenizer.detokenize(step.cur_token_id) << std::flush;
    }
    fmt::print("\n");
  } else {
    auto tts_eos_id = tokenizer.lookupTokenId(L"<|tts_eos|>");
    auto im_end_id = tokenizer.lookupTokenId(L"<|im_end|>");
    auto eot_id = tokenizer.lookupTokenId(L"<|endoftext|>");

    std::vector<int64_t> stop_token_ids = {
        tts_eos_id,
        im_end_id,
        eot_id,
        cfg.eos_token_id,
    };

    debug_log("Start text generation for TTS conditioning...");
    auto text_out = model.generateTextWithHidden(
        inputs, text_max_new_tokens.get(), stop_token_ids, false, 1.0f, 0, 0.0f,
        [&](int32_t step, int64_t token_id) {
          auto interval = std::max(debug_interval.get(), 1);
          if (debug_progress.get() && (step == 1 || (step % interval) == 0)) {
            debug_log(fmt::format("Text generation step {} (token_id={})", step, token_id));
          }
        });
    debug_log(fmt::format("Text generation done, generated_tokens={}", text_out.generated_tokens.size()));

    fmt::print("\nGenerated text tokens: {}\n", text_out.generated_tokens.size());
    fmt::print("Text (for TTS conditioning): ");

    std::vector<int64_t> tts_text_tokens;
    std::vector<mllm::Tensor> tts_hidden_states;
    for (size_t i = 0; i < text_out.aligned_tokens.size() && i < text_out.aligned_hidden_states.size(); ++i) {
      auto token_id = text_out.aligned_tokens[i];
      if (token_id == tts_eos_id || token_id == im_end_id || token_id == eot_id || token_id == cfg.eos_token_id) { break; }
      tts_text_tokens.push_back(token_id);
      tts_hidden_states.push_back(text_out.aligned_hidden_states[i]);
      std::wcout << tokenizer.detokenize(token_id) << std::flush;
    }
    fmt::print("\n");

    if (tts_text_tokens.empty()) {
      MLLM_ERROR_EXIT(mllm::ExitCode::kCoreError,
                      "No text token available before <|tts_eos|>/<|im_end|>; cannot build TTS condition.");
    }

    auto condition_embeds = model.tts_.makeConditionEmbeddings(tts_text_tokens, tts_hidden_states);
    if (condition_embeds.isNil()) {
      MLLM_ERROR_EXIT(mllm::ExitCode::kCoreError, "Failed to build TTS conditioning embeddings.");
    }
    debug_log(fmt::format("Built TTS condition embeddings from {} text tokens.", tts_text_tokens.size()));

    mllm::models::minicpm_o45::MiniCPMO45TTSGenerationConfig tts_cfg;
    tts_cfg.max_new_tokens = tts_max_new_tokens.get();
    tts_cfg.min_new_tokens = tts_min_new_tokens.get();
    tts_cfg.force_no_stop = tts_force_no_stop.get();
    tts_cfg.do_sample = !tts_greedy.get();
    tts_cfg.temperature = {tts_temperature.get()};
    tts_cfg.top_k = tts_top_k.get();
    tts_cfg.top_p = tts_top_p.get();
    tts_cfg.repetition_penalty = tts_repetition_penalty.get();
    tts_cfg.repetition_penalty_window = tts_repetition_window.get();
    tts_cfg.debug_interval = std::max(debug_interval.get(), 1);
    if (debug_progress.get()) {
      tts_cfg.step_callback = [&](int32_t step, const std::vector<int64_t>& tokens, bool has_eos) {
        auto first_token = tokens.empty() ? -1 : tokens[0];
        debug_log(fmt::format("TTS generation step {} (first_vq_token={}, has_eos={})", step, first_token,
                              has_eos ? "true" : "false"));
      };
    }

    debug_log("Start TTS token generation...");
    auto tts_out = model.tts_.generate(condition_embeds, tts_cfg);
    debug_log("TTS token generation finished.");
    if (tts_out.new_ids.isNil()) {
      fmt::print("Generated TTS tokens: 0\n");
    } else {
      auto token_count = tts_out.new_ids.shape()[1];
      fmt::print("Generated TTS tokens: {} (finished={})\n", token_count, tts_out.finished ? "true" : "false");

      std::vector<int64_t> token_ids;
      token_ids.reserve(token_count);
      for (int32_t i = 0; i < token_count; ++i) { token_ids.push_back(tts_out.new_ids.at<int64_t>({0, i, 0})); }

      fmt::print("TTS token IDs:\n");
      for (size_t i = 0; i < token_ids.size(); ++i) {
        fmt::print("{}{}", token_ids[i], (i + 1 == token_ids.size() ? "\n" : " "));
      }

      if (!tts_tokens_out.get().empty()) {
        std::ofstream ofs(tts_tokens_out.get());
        if (!ofs.is_open()) {
          MLLM_ERROR_EXIT(mllm::ExitCode::kIOError, "Failed to open output file: {}", tts_tokens_out.get());
        }
        for (auto id : token_ids) { ofs << std::to_string(id) << '\n'; }
        fmt::print("Saved TTS token ids to {}\n", tts_tokens_out.get());
        debug_log(fmt::format("Saved token ids to {}", tts_tokens_out.get()));
      }

      if (!tts_wav_out.get().empty()) {
        debug_log("Loading token2wav model and prompt cache...");
        auto token2wav_param = mllm::load(token2wav_model_path, token2wav_file_version);
        auto prompt_cache = mllm::models::minicpm_o45::loadMiniCPMO45Token2WavPromptCache(tts_prompt_cache.get());

        mllm::models::minicpm_o45::MiniCPMO45Token2WavModel token2wav("token2wav", {});
        token2wav.loadFromParameter(token2wav_param);
        debug_log("Native token2wav model loaded.");

        debug_log("Running native flow + HiFT...");
        auto wav = token2wav.infer(token_ids, prompt_cache, std::max(1, tts_token2wav_n_timesteps.get()));
        auto wav_i16 = wav * 32767.0f;
        wenet::WavWriter wav_writer(wav_i16.ptr<float>(), wav_i16.shape().back(), 1, 24000, 16);
        wav_writer.Write(tts_wav_out.get());
        fmt::print("Saved TTS waveform to {}\n", tts_wav_out.get());
        debug_log("Native token2wav finished.");
      }
    }
  }

  model.perfSummary();
  mllm::memoryReport();
})
