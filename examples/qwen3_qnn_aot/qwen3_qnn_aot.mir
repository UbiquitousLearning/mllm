@main () -> () {
    graph.SubGraphOp @init <notype> {
        () -> () {
            tensor.CPU.register () -> (%106:tensor<[151936, 2048], Float32, CPU>[@model.embed_tokens.weight][symbol:model.embed_tokens.weight])[symbol:model.embed_tokens.weight]
            tensor.CPU.register () -> (%77:tensor<[2048, 2048], Float32, CPU>[@model.layers.0.self_attn.q_proj.weight][symbol:model.layers.0.self_attn.q_proj.weight])[symbol:model.layers.0.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%134:tensor<[1024, 2048], Float32, CPU>[@model.layers.0.self_attn.k_proj.weight][symbol:model.layers.0.self_attn.k_proj.weight])[symbol:model.layers.0.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%180:tensor<[1024, 2048], Float32, CPU>[@model.layers.0.self_attn.v_proj.weight][symbol:model.layers.0.self_attn.v_proj.weight])[symbol:model.layers.0.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%270:tensor<[2048, 2048], Float32, CPU>[@model.layers.0.self_attn.o_proj.weight][symbol:model.layers.0.self_attn.o_proj.weight])[symbol:model.layers.0.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%10:tensor<[6144, 2048], Float32, CPU>[@model.layers.0.mlp.gate_proj.weight][symbol:model.layers.0.mlp.gate_proj.weight])[symbol:model.layers.0.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%112:tensor<[6144, 2048], Float32, CPU>[@model.layers.0.mlp.up_proj.weight][symbol:model.layers.0.mlp.up_proj.weight])[symbol:model.layers.0.mlp.up_proj.weight]
            tensor.CPU.register () -> (%185:tensor<[2048, 6144], Float32, CPU>[@model.layers.0.mlp.down_proj.weight][symbol:model.layers.0.mlp.down_proj.weight])[symbol:model.layers.0.mlp.down_proj.weight]
            tensor.CPU.register () -> (%286:tensor<[2048, 2048], Float32, CPU>[@model.layers.1.self_attn.q_proj.weight][symbol:model.layers.1.self_attn.q_proj.weight])[symbol:model.layers.1.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%33:tensor<[1024, 2048], Float32, CPU>[@model.layers.1.self_attn.k_proj.weight][symbol:model.layers.1.self_attn.k_proj.weight])[symbol:model.layers.1.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%155:tensor<[1024, 2048], Float32, CPU>[@model.layers.1.self_attn.v_proj.weight][symbol:model.layers.1.self_attn.v_proj.weight])[symbol:model.layers.1.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%21:tensor<[2048, 2048], Float32, CPU>[@model.layers.1.self_attn.o_proj.weight][symbol:model.layers.1.self_attn.o_proj.weight])[symbol:model.layers.1.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%246:tensor<[6144, 2048], Float32, CPU>[@model.layers.1.mlp.gate_proj.weight][symbol:model.layers.1.mlp.gate_proj.weight])[symbol:model.layers.1.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%231:tensor<[6144, 2048], Float32, CPU>[@model.layers.1.mlp.up_proj.weight][symbol:model.layers.1.mlp.up_proj.weight])[symbol:model.layers.1.mlp.up_proj.weight]
            tensor.CPU.register () -> (%44:tensor<[2048, 6144], Float32, CPU>[@model.layers.1.mlp.down_proj.weight][symbol:model.layers.1.mlp.down_proj.weight])[symbol:model.layers.1.mlp.down_proj.weight]
            tensor.CPU.register () -> (%222:tensor<[2048, 2048], Float32, CPU>[@model.layers.2.self_attn.q_proj.weight][symbol:model.layers.2.self_attn.q_proj.weight])[symbol:model.layers.2.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%104:tensor<[1024, 2048], Float32, CPU>[@model.layers.2.self_attn.k_proj.weight][symbol:model.layers.2.self_attn.k_proj.weight])[symbol:model.layers.2.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%48:tensor<[1024, 2048], Float32, CPU>[@model.layers.2.self_attn.v_proj.weight][symbol:model.layers.2.self_attn.v_proj.weight])[symbol:model.layers.2.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%86:tensor<[2048, 2048], Float32, CPU>[@model.layers.2.self_attn.o_proj.weight][symbol:model.layers.2.self_attn.o_proj.weight])[symbol:model.layers.2.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%253:tensor<[6144, 2048], Float32, CPU>[@model.layers.2.mlp.gate_proj.weight][symbol:model.layers.2.mlp.gate_proj.weight])[symbol:model.layers.2.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%25:tensor<[6144, 2048], Float32, CPU>[@model.layers.2.mlp.up_proj.weight][symbol:model.layers.2.mlp.up_proj.weight])[symbol:model.layers.2.mlp.up_proj.weight]
            tensor.CPU.register () -> (%29:tensor<[2048, 6144], Float32, CPU>[@model.layers.2.mlp.down_proj.weight][symbol:model.layers.2.mlp.down_proj.weight])[symbol:model.layers.2.mlp.down_proj.weight]
            tensor.CPU.register () -> (%284:tensor<[2048, 2048], Float32, CPU>[@model.layers.3.self_attn.q_proj.weight][symbol:model.layers.3.self_attn.q_proj.weight])[symbol:model.layers.3.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%49:tensor<[1024, 2048], Float32, CPU>[@model.layers.3.self_attn.k_proj.weight][symbol:model.layers.3.self_attn.k_proj.weight])[symbol:model.layers.3.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%245:tensor<[1024, 2048], Float32, CPU>[@model.layers.3.self_attn.v_proj.weight][symbol:model.layers.3.self_attn.v_proj.weight])[symbol:model.layers.3.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%302:tensor<[2048, 2048], Float32, CPU>[@model.layers.3.self_attn.o_proj.weight][symbol:model.layers.3.self_attn.o_proj.weight])[symbol:model.layers.3.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%130:tensor<[6144, 2048], Float32, CPU>[@model.layers.3.mlp.gate_proj.weight][symbol:model.layers.3.mlp.gate_proj.weight])[symbol:model.layers.3.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%189:tensor<[6144, 2048], Float32, CPU>[@model.layers.3.mlp.up_proj.weight][symbol:model.layers.3.mlp.up_proj.weight])[symbol:model.layers.3.mlp.up_proj.weight]
            tensor.CPU.register () -> (%98:tensor<[2048, 6144], Float32, CPU>[@model.layers.3.mlp.down_proj.weight][symbol:model.layers.3.mlp.down_proj.weight])[symbol:model.layers.3.mlp.down_proj.weight]
            tensor.CPU.register () -> (%165:tensor<[2048, 2048], Float32, CPU>[@model.layers.4.self_attn.q_proj.weight][symbol:model.layers.4.self_attn.q_proj.weight])[symbol:model.layers.4.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%149:tensor<[1024, 2048], Float32, CPU>[@model.layers.4.self_attn.k_proj.weight][symbol:model.layers.4.self_attn.k_proj.weight])[symbol:model.layers.4.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%280:tensor<[1024, 2048], Float32, CPU>[@model.layers.4.self_attn.v_proj.weight][symbol:model.layers.4.self_attn.v_proj.weight])[symbol:model.layers.4.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%92:tensor<[2048, 2048], Float32, CPU>[@model.layers.4.self_attn.o_proj.weight][symbol:model.layers.4.self_attn.o_proj.weight])[symbol:model.layers.4.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%190:tensor<[6144, 2048], Float32, CPU>[@model.layers.4.mlp.gate_proj.weight][symbol:model.layers.4.mlp.gate_proj.weight])[symbol:model.layers.4.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%157:tensor<[6144, 2048], Float32, CPU>[@model.layers.4.mlp.up_proj.weight][symbol:model.layers.4.mlp.up_proj.weight])[symbol:model.layers.4.mlp.up_proj.weight]
            tensor.CPU.register () -> (%154:tensor<[2048, 6144], Float32, CPU>[@model.layers.4.mlp.down_proj.weight][symbol:model.layers.4.mlp.down_proj.weight])[symbol:model.layers.4.mlp.down_proj.weight]
            tensor.CPU.register () -> (%79:tensor<[2048, 2048], Float32, CPU>[@model.layers.5.self_attn.q_proj.weight][symbol:model.layers.5.self_attn.q_proj.weight])[symbol:model.layers.5.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%73:tensor<[1024, 2048], Float32, CPU>[@model.layers.5.self_attn.k_proj.weight][symbol:model.layers.5.self_attn.k_proj.weight])[symbol:model.layers.5.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%290:tensor<[1024, 2048], Float32, CPU>[@model.layers.5.self_attn.v_proj.weight][symbol:model.layers.5.self_attn.v_proj.weight])[symbol:model.layers.5.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%265:tensor<[2048, 2048], Float32, CPU>[@model.layers.5.self_attn.o_proj.weight][symbol:model.layers.5.self_attn.o_proj.weight])[symbol:model.layers.5.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%5:tensor<[6144, 2048], Float32, CPU>[@model.layers.5.mlp.gate_proj.weight][symbol:model.layers.5.mlp.gate_proj.weight])[symbol:model.layers.5.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%309:tensor<[6144, 2048], Float32, CPU>[@model.layers.5.mlp.up_proj.weight][symbol:model.layers.5.mlp.up_proj.weight])[symbol:model.layers.5.mlp.up_proj.weight]
            tensor.CPU.register () -> (%75:tensor<[2048, 6144], Float32, CPU>[@model.layers.5.mlp.down_proj.weight][symbol:model.layers.5.mlp.down_proj.weight])[symbol:model.layers.5.mlp.down_proj.weight]
            tensor.CPU.register () -> (%60:tensor<[2048, 2048], Float32, CPU>[@model.layers.6.self_attn.q_proj.weight][symbol:model.layers.6.self_attn.q_proj.weight])[symbol:model.layers.6.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%209:tensor<[1024, 2048], Float32, CPU>[@model.layers.6.self_attn.k_proj.weight][symbol:model.layers.6.self_attn.k_proj.weight])[symbol:model.layers.6.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%239:tensor<[1024, 2048], Float32, CPU>[@model.layers.6.self_attn.v_proj.weight][symbol:model.layers.6.self_attn.v_proj.weight])[symbol:model.layers.6.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%53:tensor<[2048, 2048], Float32, CPU>[@model.layers.6.self_attn.o_proj.weight][symbol:model.layers.6.self_attn.o_proj.weight])[symbol:model.layers.6.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%81:tensor<[6144, 2048], Float32, CPU>[@model.layers.6.mlp.gate_proj.weight][symbol:model.layers.6.mlp.gate_proj.weight])[symbol:model.layers.6.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%277:tensor<[6144, 2048], Float32, CPU>[@model.layers.6.mlp.up_proj.weight][symbol:model.layers.6.mlp.up_proj.weight])[symbol:model.layers.6.mlp.up_proj.weight]
            tensor.CPU.register () -> (%228:tensor<[2048, 6144], Float32, CPU>[@model.layers.6.mlp.down_proj.weight][symbol:model.layers.6.mlp.down_proj.weight])[symbol:model.layers.6.mlp.down_proj.weight]
            tensor.CPU.register () -> (%288:tensor<[2048, 2048], Float32, CPU>[@model.layers.7.self_attn.q_proj.weight][symbol:model.layers.7.self_attn.q_proj.weight])[symbol:model.layers.7.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%136:tensor<[1024, 2048], Float32, CPU>[@model.layers.7.self_attn.k_proj.weight][symbol:model.layers.7.self_attn.k_proj.weight])[symbol:model.layers.7.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%301:tensor<[1024, 2048], Float32, CPU>[@model.layers.7.self_attn.v_proj.weight][symbol:model.layers.7.self_attn.v_proj.weight])[symbol:model.layers.7.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%252:tensor<[2048, 2048], Float32, CPU>[@model.layers.7.self_attn.o_proj.weight][symbol:model.layers.7.self_attn.o_proj.weight])[symbol:model.layers.7.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%156:tensor<[6144, 2048], Float32, CPU>[@model.layers.7.mlp.gate_proj.weight][symbol:model.layers.7.mlp.gate_proj.weight])[symbol:model.layers.7.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%219:tensor<[6144, 2048], Float32, CPU>[@model.layers.7.mlp.up_proj.weight][symbol:model.layers.7.mlp.up_proj.weight])[symbol:model.layers.7.mlp.up_proj.weight]
            tensor.CPU.register () -> (%276:tensor<[2048, 6144], Float32, CPU>[@model.layers.7.mlp.down_proj.weight][symbol:model.layers.7.mlp.down_proj.weight])[symbol:model.layers.7.mlp.down_proj.weight]
            tensor.CPU.register () -> (%166:tensor<[2048, 2048], Float32, CPU>[@model.layers.8.self_attn.q_proj.weight][symbol:model.layers.8.self_attn.q_proj.weight])[symbol:model.layers.8.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%195:tensor<[1024, 2048], Float32, CPU>[@model.layers.8.self_attn.k_proj.weight][symbol:model.layers.8.self_attn.k_proj.weight])[symbol:model.layers.8.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%182:tensor<[1024, 2048], Float32, CPU>[@model.layers.8.self_attn.v_proj.weight][symbol:model.layers.8.self_attn.v_proj.weight])[symbol:model.layers.8.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%198:tensor<[2048, 2048], Float32, CPU>[@model.layers.8.self_attn.o_proj.weight][symbol:model.layers.8.self_attn.o_proj.weight])[symbol:model.layers.8.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%111:tensor<[6144, 2048], Float32, CPU>[@model.layers.8.mlp.gate_proj.weight][symbol:model.layers.8.mlp.gate_proj.weight])[symbol:model.layers.8.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%237:tensor<[6144, 2048], Float32, CPU>[@model.layers.8.mlp.up_proj.weight][symbol:model.layers.8.mlp.up_proj.weight])[symbol:model.layers.8.mlp.up_proj.weight]
            tensor.CPU.register () -> (%107:tensor<[2048, 6144], Float32, CPU>[@model.layers.8.mlp.down_proj.weight][symbol:model.layers.8.mlp.down_proj.weight])[symbol:model.layers.8.mlp.down_proj.weight]
            tensor.CPU.register () -> (%236:tensor<[2048, 2048], Float32, CPU>[@model.layers.9.self_attn.q_proj.weight][symbol:model.layers.9.self_attn.q_proj.weight])[symbol:model.layers.9.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%70:tensor<[1024, 2048], Float32, CPU>[@model.layers.9.self_attn.k_proj.weight][symbol:model.layers.9.self_attn.k_proj.weight])[symbol:model.layers.9.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%121:tensor<[1024, 2048], Float32, CPU>[@model.layers.9.self_attn.v_proj.weight][symbol:model.layers.9.self_attn.v_proj.weight])[symbol:model.layers.9.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%206:tensor<[2048, 2048], Float32, CPU>[@model.layers.9.self_attn.o_proj.weight][symbol:model.layers.9.self_attn.o_proj.weight])[symbol:model.layers.9.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%264:tensor<[6144, 2048], Float32, CPU>[@model.layers.9.mlp.gate_proj.weight][symbol:model.layers.9.mlp.gate_proj.weight])[symbol:model.layers.9.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%103:tensor<[6144, 2048], Float32, CPU>[@model.layers.9.mlp.up_proj.weight][symbol:model.layers.9.mlp.up_proj.weight])[symbol:model.layers.9.mlp.up_proj.weight]
            tensor.CPU.register () -> (%137:tensor<[2048, 6144], Float32, CPU>[@model.layers.9.mlp.down_proj.weight][symbol:model.layers.9.mlp.down_proj.weight])[symbol:model.layers.9.mlp.down_proj.weight]
            tensor.CPU.register () -> (%279:tensor<[2048, 2048], Float32, CPU>[@model.layers.10.self_attn.q_proj.weight][symbol:model.layers.10.self_attn.q_proj.weight])[symbol:model.layers.10.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%183:tensor<[1024, 2048], Float32, CPU>[@model.layers.10.self_attn.k_proj.weight][symbol:model.layers.10.self_attn.k_proj.weight])[symbol:model.layers.10.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%139:tensor<[1024, 2048], Float32, CPU>[@model.layers.10.self_attn.v_proj.weight][symbol:model.layers.10.self_attn.v_proj.weight])[symbol:model.layers.10.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%234:tensor<[2048, 2048], Float32, CPU>[@model.layers.10.self_attn.o_proj.weight][symbol:model.layers.10.self_attn.o_proj.weight])[symbol:model.layers.10.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%125:tensor<[6144, 2048], Float32, CPU>[@model.layers.10.mlp.gate_proj.weight][symbol:model.layers.10.mlp.gate_proj.weight])[symbol:model.layers.10.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%262:tensor<[6144, 2048], Float32, CPU>[@model.layers.10.mlp.up_proj.weight][symbol:model.layers.10.mlp.up_proj.weight])[symbol:model.layers.10.mlp.up_proj.weight]
            tensor.CPU.register () -> (%46:tensor<[2048, 6144], Float32, CPU>[@model.layers.10.mlp.down_proj.weight][symbol:model.layers.10.mlp.down_proj.weight])[symbol:model.layers.10.mlp.down_proj.weight]
            tensor.CPU.register () -> (%275:tensor<[2048, 2048], Float32, CPU>[@model.layers.11.self_attn.q_proj.weight][symbol:model.layers.11.self_attn.q_proj.weight])[symbol:model.layers.11.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%158:tensor<[1024, 2048], Float32, CPU>[@model.layers.11.self_attn.k_proj.weight][symbol:model.layers.11.self_attn.k_proj.weight])[symbol:model.layers.11.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%64:tensor<[1024, 2048], Float32, CPU>[@model.layers.11.self_attn.v_proj.weight][symbol:model.layers.11.self_attn.v_proj.weight])[symbol:model.layers.11.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%119:tensor<[2048, 2048], Float32, CPU>[@model.layers.11.self_attn.o_proj.weight][symbol:model.layers.11.self_attn.o_proj.weight])[symbol:model.layers.11.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%208:tensor<[6144, 2048], Float32, CPU>[@model.layers.11.mlp.gate_proj.weight][symbol:model.layers.11.mlp.gate_proj.weight])[symbol:model.layers.11.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%227:tensor<[6144, 2048], Float32, CPU>[@model.layers.11.mlp.up_proj.weight][symbol:model.layers.11.mlp.up_proj.weight])[symbol:model.layers.11.mlp.up_proj.weight]
            tensor.CPU.register () -> (%225:tensor<[2048, 6144], Float32, CPU>[@model.layers.11.mlp.down_proj.weight][symbol:model.layers.11.mlp.down_proj.weight])[symbol:model.layers.11.mlp.down_proj.weight]
            tensor.CPU.register () -> (%218:tensor<[2048, 2048], Float32, CPU>[@model.layers.12.self_attn.q_proj.weight][symbol:model.layers.12.self_attn.q_proj.weight])[symbol:model.layers.12.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%298:tensor<[1024, 2048], Float32, CPU>[@model.layers.12.self_attn.k_proj.weight][symbol:model.layers.12.self_attn.k_proj.weight])[symbol:model.layers.12.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%95:tensor<[1024, 2048], Float32, CPU>[@model.layers.12.self_attn.v_proj.weight][symbol:model.layers.12.self_attn.v_proj.weight])[symbol:model.layers.12.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%50:tensor<[2048, 2048], Float32, CPU>[@model.layers.12.self_attn.o_proj.weight][symbol:model.layers.12.self_attn.o_proj.weight])[symbol:model.layers.12.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%263:tensor<[6144, 2048], Float32, CPU>[@model.layers.12.mlp.gate_proj.weight][symbol:model.layers.12.mlp.gate_proj.weight])[symbol:model.layers.12.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%256:tensor<[6144, 2048], Float32, CPU>[@model.layers.12.mlp.up_proj.weight][symbol:model.layers.12.mlp.up_proj.weight])[symbol:model.layers.12.mlp.up_proj.weight]
            tensor.CPU.register () -> (%23:tensor<[2048, 6144], Float32, CPU>[@model.layers.12.mlp.down_proj.weight][symbol:model.layers.12.mlp.down_proj.weight])[symbol:model.layers.12.mlp.down_proj.weight]
            tensor.CPU.register () -> (%115:tensor<[2048, 2048], Float32, CPU>[@model.layers.13.self_attn.q_proj.weight][symbol:model.layers.13.self_attn.q_proj.weight])[symbol:model.layers.13.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%153:tensor<[1024, 2048], Float32, CPU>[@model.layers.13.self_attn.k_proj.weight][symbol:model.layers.13.self_attn.k_proj.weight])[symbol:model.layers.13.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%16:tensor<[1024, 2048], Float32, CPU>[@model.layers.13.self_attn.v_proj.weight][symbol:model.layers.13.self_attn.v_proj.weight])[symbol:model.layers.13.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%251:tensor<[2048, 2048], Float32, CPU>[@model.layers.13.self_attn.o_proj.weight][symbol:model.layers.13.self_attn.o_proj.weight])[symbol:model.layers.13.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%248:tensor<[6144, 2048], Float32, CPU>[@model.layers.13.mlp.gate_proj.weight][symbol:model.layers.13.mlp.gate_proj.weight])[symbol:model.layers.13.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%99:tensor<[6144, 2048], Float32, CPU>[@model.layers.13.mlp.up_proj.weight][symbol:model.layers.13.mlp.up_proj.weight])[symbol:model.layers.13.mlp.up_proj.weight]
            tensor.CPU.register () -> (%194:tensor<[2048, 6144], Float32, CPU>[@model.layers.13.mlp.down_proj.weight][symbol:model.layers.13.mlp.down_proj.weight])[symbol:model.layers.13.mlp.down_proj.weight]
            tensor.CPU.register () -> (%210:tensor<[2048, 2048], Float32, CPU>[@model.layers.14.self_attn.q_proj.weight][symbol:model.layers.14.self_attn.q_proj.weight])[symbol:model.layers.14.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%39:tensor<[1024, 2048], Float32, CPU>[@model.layers.14.self_attn.k_proj.weight][symbol:model.layers.14.self_attn.k_proj.weight])[symbol:model.layers.14.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%233:tensor<[1024, 2048], Float32, CPU>[@model.layers.14.self_attn.v_proj.weight][symbol:model.layers.14.self_attn.v_proj.weight])[symbol:model.layers.14.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%169:tensor<[2048, 2048], Float32, CPU>[@model.layers.14.self_attn.o_proj.weight][symbol:model.layers.14.self_attn.o_proj.weight])[symbol:model.layers.14.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%38:tensor<[6144, 2048], Float32, CPU>[@model.layers.14.mlp.gate_proj.weight][symbol:model.layers.14.mlp.gate_proj.weight])[symbol:model.layers.14.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%148:tensor<[6144, 2048], Float32, CPU>[@model.layers.14.mlp.up_proj.weight][symbol:model.layers.14.mlp.up_proj.weight])[symbol:model.layers.14.mlp.up_proj.weight]
            tensor.CPU.register () -> (%164:tensor<[2048, 6144], Float32, CPU>[@model.layers.14.mlp.down_proj.weight][symbol:model.layers.14.mlp.down_proj.weight])[symbol:model.layers.14.mlp.down_proj.weight]
            tensor.CPU.register () -> (%47:tensor<[2048, 2048], Float32, CPU>[@model.layers.15.self_attn.q_proj.weight][symbol:model.layers.15.self_attn.q_proj.weight])[symbol:model.layers.15.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%269:tensor<[1024, 2048], Float32, CPU>[@model.layers.15.self_attn.k_proj.weight][symbol:model.layers.15.self_attn.k_proj.weight])[symbol:model.layers.15.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%118:tensor<[1024, 2048], Float32, CPU>[@model.layers.15.self_attn.v_proj.weight][symbol:model.layers.15.self_attn.v_proj.weight])[symbol:model.layers.15.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%304:tensor<[2048, 2048], Float32, CPU>[@model.layers.15.self_attn.o_proj.weight][symbol:model.layers.15.self_attn.o_proj.weight])[symbol:model.layers.15.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%261:tensor<[6144, 2048], Float32, CPU>[@model.layers.15.mlp.gate_proj.weight][symbol:model.layers.15.mlp.gate_proj.weight])[symbol:model.layers.15.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%43:tensor<[6144, 2048], Float32, CPU>[@model.layers.15.mlp.up_proj.weight][symbol:model.layers.15.mlp.up_proj.weight])[symbol:model.layers.15.mlp.up_proj.weight]
            tensor.CPU.register () -> (%291:tensor<[2048, 6144], Float32, CPU>[@model.layers.15.mlp.down_proj.weight][symbol:model.layers.15.mlp.down_proj.weight])[symbol:model.layers.15.mlp.down_proj.weight]
            tensor.CPU.register () -> (%18:tensor<[2048, 2048], Float32, CPU>[@model.layers.16.self_attn.q_proj.weight][symbol:model.layers.16.self_attn.q_proj.weight])[symbol:model.layers.16.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%229:tensor<[1024, 2048], Float32, CPU>[@model.layers.16.self_attn.k_proj.weight][symbol:model.layers.16.self_attn.k_proj.weight])[symbol:model.layers.16.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%67:tensor<[1024, 2048], Float32, CPU>[@model.layers.16.self_attn.v_proj.weight][symbol:model.layers.16.self_attn.v_proj.weight])[symbol:model.layers.16.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%212:tensor<[2048, 2048], Float32, CPU>[@model.layers.16.self_attn.o_proj.weight][symbol:model.layers.16.self_attn.o_proj.weight])[symbol:model.layers.16.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%131:tensor<[6144, 2048], Float32, CPU>[@model.layers.16.mlp.gate_proj.weight][symbol:model.layers.16.mlp.gate_proj.weight])[symbol:model.layers.16.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%80:tensor<[6144, 2048], Float32, CPU>[@model.layers.16.mlp.up_proj.weight][symbol:model.layers.16.mlp.up_proj.weight])[symbol:model.layers.16.mlp.up_proj.weight]
            tensor.CPU.register () -> (%249:tensor<[2048, 6144], Float32, CPU>[@model.layers.16.mlp.down_proj.weight][symbol:model.layers.16.mlp.down_proj.weight])[symbol:model.layers.16.mlp.down_proj.weight]
            tensor.CPU.register () -> (%65:tensor<[2048, 2048], Float32, CPU>[@model.layers.17.self_attn.q_proj.weight][symbol:model.layers.17.self_attn.q_proj.weight])[symbol:model.layers.17.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%238:tensor<[1024, 2048], Float32, CPU>[@model.layers.17.self_attn.k_proj.weight][symbol:model.layers.17.self_attn.k_proj.weight])[symbol:model.layers.17.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%7:tensor<[1024, 2048], Float32, CPU>[@model.layers.17.self_attn.v_proj.weight][symbol:model.layers.17.self_attn.v_proj.weight])[symbol:model.layers.17.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%126:tensor<[2048, 2048], Float32, CPU>[@model.layers.17.self_attn.o_proj.weight][symbol:model.layers.17.self_attn.o_proj.weight])[symbol:model.layers.17.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%178:tensor<[6144, 2048], Float32, CPU>[@model.layers.17.mlp.gate_proj.weight][symbol:model.layers.17.mlp.gate_proj.weight])[symbol:model.layers.17.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%27:tensor<[6144, 2048], Float32, CPU>[@model.layers.17.mlp.up_proj.weight][symbol:model.layers.17.mlp.up_proj.weight])[symbol:model.layers.17.mlp.up_proj.weight]
            tensor.CPU.register () -> (%26:tensor<[2048, 6144], Float32, CPU>[@model.layers.17.mlp.down_proj.weight][symbol:model.layers.17.mlp.down_proj.weight])[symbol:model.layers.17.mlp.down_proj.weight]
            tensor.CPU.register () -> (%274:tensor<[2048, 2048], Float32, CPU>[@model.layers.18.self_attn.q_proj.weight][symbol:model.layers.18.self_attn.q_proj.weight])[symbol:model.layers.18.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%285:tensor<[1024, 2048], Float32, CPU>[@model.layers.18.self_attn.k_proj.weight][symbol:model.layers.18.self_attn.k_proj.weight])[symbol:model.layers.18.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%19:tensor<[1024, 2048], Float32, CPU>[@model.layers.18.self_attn.v_proj.weight][symbol:model.layers.18.self_attn.v_proj.weight])[symbol:model.layers.18.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%3:tensor<[2048, 2048], Float32, CPU>[@model.layers.18.self_attn.o_proj.weight][symbol:model.layers.18.self_attn.o_proj.weight])[symbol:model.layers.18.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%167:tensor<[6144, 2048], Float32, CPU>[@model.layers.18.mlp.gate_proj.weight][symbol:model.layers.18.mlp.gate_proj.weight])[symbol:model.layers.18.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%272:tensor<[6144, 2048], Float32, CPU>[@model.layers.18.mlp.up_proj.weight][symbol:model.layers.18.mlp.up_proj.weight])[symbol:model.layers.18.mlp.up_proj.weight]
            tensor.CPU.register () -> (%113:tensor<[2048, 6144], Float32, CPU>[@model.layers.18.mlp.down_proj.weight][symbol:model.layers.18.mlp.down_proj.weight])[symbol:model.layers.18.mlp.down_proj.weight]
            tensor.CPU.register () -> (%9:tensor<[2048, 2048], Float32, CPU>[@model.layers.19.self_attn.q_proj.weight][symbol:model.layers.19.self_attn.q_proj.weight])[symbol:model.layers.19.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%287:tensor<[1024, 2048], Float32, CPU>[@model.layers.19.self_attn.k_proj.weight][symbol:model.layers.19.self_attn.k_proj.weight])[symbol:model.layers.19.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%51:tensor<[1024, 2048], Float32, CPU>[@model.layers.19.self_attn.v_proj.weight][symbol:model.layers.19.self_attn.v_proj.weight])[symbol:model.layers.19.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%59:tensor<[2048, 2048], Float32, CPU>[@model.layers.19.self_attn.o_proj.weight][symbol:model.layers.19.self_attn.o_proj.weight])[symbol:model.layers.19.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%282:tensor<[6144, 2048], Float32, CPU>[@model.layers.19.mlp.gate_proj.weight][symbol:model.layers.19.mlp.gate_proj.weight])[symbol:model.layers.19.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%83:tensor<[6144, 2048], Float32, CPU>[@model.layers.19.mlp.up_proj.weight][symbol:model.layers.19.mlp.up_proj.weight])[symbol:model.layers.19.mlp.up_proj.weight]
            tensor.CPU.register () -> (%174:tensor<[2048, 6144], Float32, CPU>[@model.layers.19.mlp.down_proj.weight][symbol:model.layers.19.mlp.down_proj.weight])[symbol:model.layers.19.mlp.down_proj.weight]
            tensor.CPU.register () -> (%281:tensor<[2048, 2048], Float32, CPU>[@model.layers.20.self_attn.q_proj.weight][symbol:model.layers.20.self_attn.q_proj.weight])[symbol:model.layers.20.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%254:tensor<[1024, 2048], Float32, CPU>[@model.layers.20.self_attn.k_proj.weight][symbol:model.layers.20.self_attn.k_proj.weight])[symbol:model.layers.20.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%240:tensor<[1024, 2048], Float32, CPU>[@model.layers.20.self_attn.v_proj.weight][symbol:model.layers.20.self_attn.v_proj.weight])[symbol:model.layers.20.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%42:tensor<[2048, 2048], Float32, CPU>[@model.layers.20.self_attn.o_proj.weight][symbol:model.layers.20.self_attn.o_proj.weight])[symbol:model.layers.20.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%173:tensor<[6144, 2048], Float32, CPU>[@model.layers.20.mlp.gate_proj.weight][symbol:model.layers.20.mlp.gate_proj.weight])[symbol:model.layers.20.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%300:tensor<[6144, 2048], Float32, CPU>[@model.layers.20.mlp.up_proj.weight][symbol:model.layers.20.mlp.up_proj.weight])[symbol:model.layers.20.mlp.up_proj.weight]
            tensor.CPU.register () -> (%124:tensor<[2048, 6144], Float32, CPU>[@model.layers.20.mlp.down_proj.weight][symbol:model.layers.20.mlp.down_proj.weight])[symbol:model.layers.20.mlp.down_proj.weight]
            tensor.CPU.register () -> (%296:tensor<[2048, 2048], Float32, CPU>[@model.layers.21.self_attn.q_proj.weight][symbol:model.layers.21.self_attn.q_proj.weight])[symbol:model.layers.21.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%140:tensor<[1024, 2048], Float32, CPU>[@model.layers.21.self_attn.k_proj.weight][symbol:model.layers.21.self_attn.k_proj.weight])[symbol:model.layers.21.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%143:tensor<[1024, 2048], Float32, CPU>[@model.layers.21.self_attn.v_proj.weight][symbol:model.layers.21.self_attn.v_proj.weight])[symbol:model.layers.21.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%116:tensor<[2048, 2048], Float32, CPU>[@model.layers.21.self_attn.o_proj.weight][symbol:model.layers.21.self_attn.o_proj.weight])[symbol:model.layers.21.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%260:tensor<[6144, 2048], Float32, CPU>[@model.layers.21.mlp.gate_proj.weight][symbol:model.layers.21.mlp.gate_proj.weight])[symbol:model.layers.21.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%163:tensor<[6144, 2048], Float32, CPU>[@model.layers.21.mlp.up_proj.weight][symbol:model.layers.21.mlp.up_proj.weight])[symbol:model.layers.21.mlp.up_proj.weight]
            tensor.CPU.register () -> (%184:tensor<[2048, 6144], Float32, CPU>[@model.layers.21.mlp.down_proj.weight][symbol:model.layers.21.mlp.down_proj.weight])[symbol:model.layers.21.mlp.down_proj.weight]
            tensor.CPU.register () -> (%90:tensor<[2048, 2048], Float32, CPU>[@model.layers.22.self_attn.q_proj.weight][symbol:model.layers.22.self_attn.q_proj.weight])[symbol:model.layers.22.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%37:tensor<[1024, 2048], Float32, CPU>[@model.layers.22.self_attn.k_proj.weight][symbol:model.layers.22.self_attn.k_proj.weight])[symbol:model.layers.22.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%205:tensor<[1024, 2048], Float32, CPU>[@model.layers.22.self_attn.v_proj.weight][symbol:model.layers.22.self_attn.v_proj.weight])[symbol:model.layers.22.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%235:tensor<[2048, 2048], Float32, CPU>[@model.layers.22.self_attn.o_proj.weight][symbol:model.layers.22.self_attn.o_proj.weight])[symbol:model.layers.22.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%199:tensor<[6144, 2048], Float32, CPU>[@model.layers.22.mlp.gate_proj.weight][symbol:model.layers.22.mlp.gate_proj.weight])[symbol:model.layers.22.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%255:tensor<[6144, 2048], Float32, CPU>[@model.layers.22.mlp.up_proj.weight][symbol:model.layers.22.mlp.up_proj.weight])[symbol:model.layers.22.mlp.up_proj.weight]
            tensor.CPU.register () -> (%32:tensor<[2048, 6144], Float32, CPU>[@model.layers.22.mlp.down_proj.weight][symbol:model.layers.22.mlp.down_proj.weight])[symbol:model.layers.22.mlp.down_proj.weight]
            tensor.CPU.register () -> (%110:tensor<[2048, 2048], Float32, CPU>[@model.layers.23.self_attn.q_proj.weight][symbol:model.layers.23.self_attn.q_proj.weight])[symbol:model.layers.23.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%40:tensor<[1024, 2048], Float32, CPU>[@model.layers.23.self_attn.k_proj.weight][symbol:model.layers.23.self_attn.k_proj.weight])[symbol:model.layers.23.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%84:tensor<[1024, 2048], Float32, CPU>[@model.layers.23.self_attn.v_proj.weight][symbol:model.layers.23.self_attn.v_proj.weight])[symbol:model.layers.23.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%177:tensor<[2048, 2048], Float32, CPU>[@model.layers.23.self_attn.o_proj.weight][symbol:model.layers.23.self_attn.o_proj.weight])[symbol:model.layers.23.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%170:tensor<[6144, 2048], Float32, CPU>[@model.layers.23.mlp.gate_proj.weight][symbol:model.layers.23.mlp.gate_proj.weight])[symbol:model.layers.23.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%244:tensor<[6144, 2048], Float32, CPU>[@model.layers.23.mlp.up_proj.weight][symbol:model.layers.23.mlp.up_proj.weight])[symbol:model.layers.23.mlp.up_proj.weight]
            tensor.CPU.register () -> (%150:tensor<[2048, 6144], Float32, CPU>[@model.layers.23.mlp.down_proj.weight][symbol:model.layers.23.mlp.down_proj.weight])[symbol:model.layers.23.mlp.down_proj.weight]
            tensor.CPU.register () -> (%12:tensor<[2048, 2048], Float32, CPU>[@model.layers.24.self_attn.q_proj.weight][symbol:model.layers.24.self_attn.q_proj.weight])[symbol:model.layers.24.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%62:tensor<[1024, 2048], Float32, CPU>[@model.layers.24.self_attn.k_proj.weight][symbol:model.layers.24.self_attn.k_proj.weight])[symbol:model.layers.24.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%82:tensor<[1024, 2048], Float32, CPU>[@model.layers.24.self_attn.v_proj.weight][symbol:model.layers.24.self_attn.v_proj.weight])[symbol:model.layers.24.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%128:tensor<[2048, 2048], Float32, CPU>[@model.layers.24.self_attn.o_proj.weight][symbol:model.layers.24.self_attn.o_proj.weight])[symbol:model.layers.24.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%142:tensor<[6144, 2048], Float32, CPU>[@model.layers.24.mlp.gate_proj.weight][symbol:model.layers.24.mlp.gate_proj.weight])[symbol:model.layers.24.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%127:tensor<[6144, 2048], Float32, CPU>[@model.layers.24.mlp.up_proj.weight][symbol:model.layers.24.mlp.up_proj.weight])[symbol:model.layers.24.mlp.up_proj.weight]
            tensor.CPU.register () -> (%35:tensor<[2048, 6144], Float32, CPU>[@model.layers.24.mlp.down_proj.weight][symbol:model.layers.24.mlp.down_proj.weight])[symbol:model.layers.24.mlp.down_proj.weight]
            tensor.CPU.register () -> (%207:tensor<[2048, 2048], Float32, CPU>[@model.layers.25.self_attn.q_proj.weight][symbol:model.layers.25.self_attn.q_proj.weight])[symbol:model.layers.25.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%28:tensor<[1024, 2048], Float32, CPU>[@model.layers.25.self_attn.k_proj.weight][symbol:model.layers.25.self_attn.k_proj.weight])[symbol:model.layers.25.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%122:tensor<[1024, 2048], Float32, CPU>[@model.layers.25.self_attn.v_proj.weight][symbol:model.layers.25.self_attn.v_proj.weight])[symbol:model.layers.25.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%151:tensor<[2048, 2048], Float32, CPU>[@model.layers.25.self_attn.o_proj.weight][symbol:model.layers.25.self_attn.o_proj.weight])[symbol:model.layers.25.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%250:tensor<[6144, 2048], Float32, CPU>[@model.layers.25.mlp.gate_proj.weight][symbol:model.layers.25.mlp.gate_proj.weight])[symbol:model.layers.25.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%160:tensor<[6144, 2048], Float32, CPU>[@model.layers.25.mlp.up_proj.weight][symbol:model.layers.25.mlp.up_proj.weight])[symbol:model.layers.25.mlp.up_proj.weight]
            tensor.CPU.register () -> (%268:tensor<[2048, 6144], Float32, CPU>[@model.layers.25.mlp.down_proj.weight][symbol:model.layers.25.mlp.down_proj.weight])[symbol:model.layers.25.mlp.down_proj.weight]
            tensor.CPU.register () -> (%266:tensor<[2048, 2048], Float32, CPU>[@model.layers.26.self_attn.q_proj.weight][symbol:model.layers.26.self_attn.q_proj.weight])[symbol:model.layers.26.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%191:tensor<[1024, 2048], Float32, CPU>[@model.layers.26.self_attn.k_proj.weight][symbol:model.layers.26.self_attn.k_proj.weight])[symbol:model.layers.26.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%120:tensor<[1024, 2048], Float32, CPU>[@model.layers.26.self_attn.v_proj.weight][symbol:model.layers.26.self_attn.v_proj.weight])[symbol:model.layers.26.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%89:tensor<[2048, 2048], Float32, CPU>[@model.layers.26.self_attn.o_proj.weight][symbol:model.layers.26.self_attn.o_proj.weight])[symbol:model.layers.26.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%97:tensor<[6144, 2048], Float32, CPU>[@model.layers.26.mlp.gate_proj.weight][symbol:model.layers.26.mlp.gate_proj.weight])[symbol:model.layers.26.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%63:tensor<[6144, 2048], Float32, CPU>[@model.layers.26.mlp.up_proj.weight][symbol:model.layers.26.mlp.up_proj.weight])[symbol:model.layers.26.mlp.up_proj.weight]
            tensor.CPU.register () -> (%221:tensor<[2048, 6144], Float32, CPU>[@model.layers.26.mlp.down_proj.weight][symbol:model.layers.26.mlp.down_proj.weight])[symbol:model.layers.26.mlp.down_proj.weight]
            tensor.CPU.register () -> (%186:tensor<[2048, 2048], Float32, CPU>[@model.layers.27.self_attn.q_proj.weight][symbol:model.layers.27.self_attn.q_proj.weight])[symbol:model.layers.27.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%13:tensor<[1024, 2048], Float32, CPU>[@model.layers.27.self_attn.k_proj.weight][symbol:model.layers.27.self_attn.k_proj.weight])[symbol:model.layers.27.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%55:tensor<[1024, 2048], Float32, CPU>[@model.layers.27.self_attn.v_proj.weight][symbol:model.layers.27.self_attn.v_proj.weight])[symbol:model.layers.27.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%61:tensor<[2048, 2048], Float32, CPU>[@model.layers.27.self_attn.o_proj.weight][symbol:model.layers.27.self_attn.o_proj.weight])[symbol:model.layers.27.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%145:tensor<[6144, 2048], Float32, CPU>[@model.layers.27.mlp.gate_proj.weight][symbol:model.layers.27.mlp.gate_proj.weight])[symbol:model.layers.27.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%147:tensor<[6144, 2048], Float32, CPU>[@model.layers.27.mlp.up_proj.weight][symbol:model.layers.27.mlp.up_proj.weight])[symbol:model.layers.27.mlp.up_proj.weight]
            tensor.CPU.register () -> (%196:tensor<[2048, 6144], Float32, CPU>[@model.layers.27.mlp.down_proj.weight][symbol:model.layers.27.mlp.down_proj.weight])[symbol:model.layers.27.mlp.down_proj.weight]
            tensor.CPU.register () -> (%102:tensor<[151936, 2048], Float32, CPU>[@lm_head.weight][symbol:lm_head.weight])[symbol:lm_head.weight]
        }
    }
    graph.SubGraphOp @deinit <notype> {
        () -> () {
            
        }
    }
    graph.CallGraphOp @model (%312:tensor<[1, 32], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%1357:tensor<[1, 32, 2048], Float32, CPU>, %332:tensor<[1, 8, 128, 32], Float32, CPU>, %369:tensor<[1, 8, 128, 32], Float32, CPU>, %406:tensor<[1, 8, 128, 32], Float32, CPU>, %443:tensor<[1, 8, 128, 32], Float32, CPU>, %480:tensor<[1, 8, 128, 32], Float32, CPU>, %517:tensor<[1, 8, 128, 32], Float32, CPU>, %554:tensor<[1, 8, 128, 32], Float32, CPU>, %591:tensor<[1, 8, 128, 32], Float32, CPU>, %628:tensor<[1, 8, 128, 32], Float32, CPU>, %665:tensor<[1, 8, 128, 32], Float32, CPU>, %702:tensor<[1, 8, 128, 32], Float32, CPU>, %739:tensor<[1, 8, 128, 32], Float32, CPU>, %776:tensor<[1, 8, 128, 32], Float32, CPU>, %813:tensor<[1, 8, 128, 32], Float32, CPU>, %850:tensor<[1, 8, 128, 32], Float32, CPU>, %887:tensor<[1, 8, 128, 32], Float32, CPU>, %924:tensor<[1, 8, 128, 32], Float32, CPU>, %961:tensor<[1, 8, 128, 32], Float32, CPU>, %998:tensor<[1, 8, 128, 32], Float32, CPU>, %1035:tensor<[1, 8, 128, 32], Float32, CPU>, %1072:tensor<[1, 8, 128, 32], Float32, CPU>, %1109:tensor<[1, 8, 128, 32], Float32, CPU>, %1146:tensor<[1, 8, 128, 32], Float32, CPU>, %1183:tensor<[1, 8, 128, 32], Float32, CPU>, %1220:tensor<[1, 8, 128, 32], Float32, CPU>, %1257:tensor<[1, 8, 128, 32], Float32, CPU>, %1294:tensor<[1, 8, 128, 32], Float32, CPU>, %1331:tensor<[1, 8, 128, 32], Float32, CPU>, %327:tensor<[1, 8, 32, 128], Float32, CPU>, %364:tensor<[1, 8, 32, 128], Float32, CPU>, %401:tensor<[1, 8, 32, 128], Float32, CPU>, %438:tensor<[1, 8, 32, 128], Float32, CPU>, %475:tensor<[1, 8, 32, 128], Float32, CPU>, %512:tensor<[1, 8, 32, 128], Float32, CPU>, %549:tensor<[1, 8, 32, 128], Float32, CPU>, %586:tensor<[1, 8, 32, 128], Float32, CPU>, %623:tensor<[1, 8, 32, 128], Float32, CPU>, %660:tensor<[1, 8, 32, 128], Float32, CPU>, %697:tensor<[1, 8, 32, 128], Float32, CPU>, %734:tensor<[1, 8, 32, 128], Float32, CPU>, %771:tensor<[1, 8, 32, 128], Float32, CPU>, %808:tensor<[1, 8, 32, 128], Float32, CPU>, %845:tensor<[1, 8, 32, 128], Float32, CPU>, %882:tensor<[1, 8, 32, 128], Float32, CPU>, %919:tensor<[1, 8, 32, 128], Float32, CPU>, %956:tensor<[1, 8, 32, 128], Float32, CPU>, %993:tensor<[1, 8, 32, 128], Float32, CPU>, %1030:tensor<[1, 8, 32, 128], Float32, CPU>, %1067:tensor<[1, 8, 32, 128], Float32, CPU>, %1104:tensor<[1, 8, 32, 128], Float32, CPU>, %1141:tensor<[1, 8, 32, 128], Float32, CPU>, %1178:tensor<[1, 8, 32, 128], Float32, CPU>, %1215:tensor<[1, 8, 32, 128], Float32, CPU>, %1252:tensor<[1, 8, 32, 128], Float32, CPU>, %1289:tensor<[1, 8, 32, 128], Float32, CPU>, %1326:tensor<[1, 8, 32, 128], Float32, CPU>)
    graph.SubGraphOp @model <CPU> {
        (%312:tensor<[1, 32], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%1357:tensor<[1, 32, 2048], Float32, CPU>, %332:tensor<[1, 8, 128, 32], Float32, CPU>, %369:tensor<[1, 8, 128, 32], Float32, CPU>, %406:tensor<[1, 8, 128, 32], Float32, CPU>, %443:tensor<[1, 8, 128, 32], Float32, CPU>, %480:tensor<[1, 8, 128, 32], Float32, CPU>, %517:tensor<[1, 8, 128, 32], Float32, CPU>, %554:tensor<[1, 8, 128, 32], Float32, CPU>, %591:tensor<[1, 8, 128, 32], Float32, CPU>, %628:tensor<[1, 8, 128, 32], Float32, CPU>, %665:tensor<[1, 8, 128, 32], Float32, CPU>, %702:tensor<[1, 8, 128, 32], Float32, CPU>, %739:tensor<[1, 8, 128, 32], Float32, CPU>, %776:tensor<[1, 8, 128, 32], Float32, CPU>, %813:tensor<[1, 8, 128, 32], Float32, CPU>, %850:tensor<[1, 8, 128, 32], Float32, CPU>, %887:tensor<[1, 8, 128, 32], Float32, CPU>, %924:tensor<[1, 8, 128, 32], Float32, CPU>, %961:tensor<[1, 8, 128, 32], Float32, CPU>, %998:tensor<[1, 8, 128, 32], Float32, CPU>, %1035:tensor<[1, 8, 128, 32], Float32, CPU>, %1072:tensor<[1, 8, 128, 32], Float32, CPU>, %1109:tensor<[1, 8, 128, 32], Float32, CPU>, %1146:tensor<[1, 8, 128, 32], Float32, CPU>, %1183:tensor<[1, 8, 128, 32], Float32, CPU>, %1220:tensor<[1, 8, 128, 32], Float32, CPU>, %1257:tensor<[1, 8, 128, 32], Float32, CPU>, %1294:tensor<[1, 8, 128, 32], Float32, CPU>, %1331:tensor<[1, 8, 128, 32], Float32, CPU>, %327:tensor<[1, 8, 32, 128], Float32, CPU>, %364:tensor<[1, 8, 32, 128], Float32, CPU>, %401:tensor<[1, 8, 32, 128], Float32, CPU>, %438:tensor<[1, 8, 32, 128], Float32, CPU>, %475:tensor<[1, 8, 32, 128], Float32, CPU>, %512:tensor<[1, 8, 32, 128], Float32, CPU>, %549:tensor<[1, 8, 32, 128], Float32, CPU>, %586:tensor<[1, 8, 32, 128], Float32, CPU>, %623:tensor<[1, 8, 32, 128], Float32, CPU>, %660:tensor<[1, 8, 32, 128], Float32, CPU>, %697:tensor<[1, 8, 32, 128], Float32, CPU>, %734:tensor<[1, 8, 32, 128], Float32, CPU>, %771:tensor<[1, 8, 32, 128], Float32, CPU>, %808:tensor<[1, 8, 32, 128], Float32, CPU>, %845:tensor<[1, 8, 32, 128], Float32, CPU>, %882:tensor<[1, 8, 32, 128], Float32, CPU>, %919:tensor<[1, 8, 32, 128], Float32, CPU>, %956:tensor<[1, 8, 32, 128], Float32, CPU>, %993:tensor<[1, 8, 32, 128], Float32, CPU>, %1030:tensor<[1, 8, 32, 128], Float32, CPU>, %1067:tensor<[1, 8, 32, 128], Float32, CPU>, %1104:tensor<[1, 8, 32, 128], Float32, CPU>, %1141:tensor<[1, 8, 32, 128], Float32, CPU>, %1178:tensor<[1, 8, 32, 128], Float32, CPU>, %1215:tensor<[1, 8, 32, 128], Float32, CPU>, %1252:tensor<[1, 8, 32, 128], Float32, CPU>, %1289:tensor<[1, 8, 32, 128], Float32, CPU>, %1326:tensor<[1, 8, 32, 128], Float32, CPU>) {
            linalg.CPU.EmbeddingOp [name="model.embed_tokens"](%312:tensor<[1, 32], Float32, CPU>) -> (%320:tensor<[1, 32, 2048], Float32, CPU>)
            graph.CallGraphOp @model.layers.0 (%320:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%357:tensor<[1, 32, 2048], Float32, CPU>, %332:tensor<[1, 8, 128, 32], Float32, CPU>, %327:tensor<[1, 8, 32, 128], Float32, CPU>)
            graph.CallGraphOp @model.layers.1 (%357:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%394:tensor<[1, 32, 2048], Float32, CPU>, %369:tensor<[1, 8, 128, 32], Float32, CPU>, %364:tensor<[1, 8, 32, 128], Float32, CPU>)
            graph.CallGraphOp @model.layers.2 (%394:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%431:tensor<[1, 32, 2048], Float32, CPU>, %406:tensor<[1, 8, 128, 32], Float32, CPU>, %401:tensor<[1, 8, 32, 128], Float32, CPU>)
            graph.CallGraphOp @model.layers.3 (%431:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%468:tensor<[1, 32, 2048], Float32, CPU>, %443:tensor<[1, 8, 128, 32], Float32, CPU>, %438:tensor<[1, 8, 32, 128], Float32, CPU>)
            graph.CallGraphOp @model.layers.4 (%468:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%505:tensor<[1, 32, 2048], Float32, CPU>, %480:tensor<[1, 8, 128, 32], Float32, CPU>, %475:tensor<[1, 8, 32, 128], Float32, CPU>)
            graph.CallGraphOp @model.layers.5 (%505:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%542:tensor<[1, 32, 2048], Float32, CPU>, %517:tensor<[1, 8, 128, 32], Float32, CPU>, %512:tensor<[1, 8, 32, 128], Float32, CPU>)
            graph.CallGraphOp @model.layers.6 (%542:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%579:tensor<[1, 32, 2048], Float32, CPU>, %554:tensor<[1, 8, 128, 32], Float32, CPU>, %549:tensor<[1, 8, 32, 128], Float32, CPU>)
            graph.CallGraphOp @model.layers.7 (%579:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%616:tensor<[1, 32, 2048], Float32, CPU>, %591:tensor<[1, 8, 128, 32], Float32, CPU>, %586:tensor<[1, 8, 32, 128], Float32, CPU>)
            graph.CallGraphOp @model.layers.8 (%616:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%653:tensor<[1, 32, 2048], Float32, CPU>, %628:tensor<[1, 8, 128, 32], Float32, CPU>, %623:tensor<[1, 8, 32, 128], Float32, CPU>)
            graph.CallGraphOp @model.layers.9 (%653:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%690:tensor<[1, 32, 2048], Float32, CPU>, %665:tensor<[1, 8, 128, 32], Float32, CPU>, %660:tensor<[1, 8, 32, 128], Float32, CPU>)
            graph.CallGraphOp @model.layers.10 (%690:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%727:tensor<[1, 32, 2048], Float32, CPU>, %702:tensor<[1, 8, 128, 32], Float32, CPU>, %697:tensor<[1, 8, 32, 128], Float32, CPU>)
            graph.CallGraphOp @model.layers.11 (%727:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%764:tensor<[1, 32, 2048], Float32, CPU>, %739:tensor<[1, 8, 128, 32], Float32, CPU>, %734:tensor<[1, 8, 32, 128], Float32, CPU>)
            graph.CallGraphOp @model.layers.12 (%764:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%801:tensor<[1, 32, 2048], Float32, CPU>, %776:tensor<[1, 8, 128, 32], Float32, CPU>, %771:tensor<[1, 8, 32, 128], Float32, CPU>)
            graph.CallGraphOp @model.layers.13 (%801:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%838:tensor<[1, 32, 2048], Float32, CPU>, %813:tensor<[1, 8, 128, 32], Float32, CPU>, %808:tensor<[1, 8, 32, 128], Float32, CPU>)
            graph.CallGraphOp @model.layers.14 (%838:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%875:tensor<[1, 32, 2048], Float32, CPU>, %850:tensor<[1, 8, 128, 32], Float32, CPU>, %845:tensor<[1, 8, 32, 128], Float32, CPU>)
            graph.CallGraphOp @model.layers.15 (%875:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%912:tensor<[1, 32, 2048], Float32, CPU>, %887:tensor<[1, 8, 128, 32], Float32, CPU>, %882:tensor<[1, 8, 32, 128], Float32, CPU>)
            graph.CallGraphOp @model.layers.16 (%912:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%949:tensor<[1, 32, 2048], Float32, CPU>, %924:tensor<[1, 8, 128, 32], Float32, CPU>, %919:tensor<[1, 8, 32, 128], Float32, CPU>)
            graph.CallGraphOp @model.layers.17 (%949:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%986:tensor<[1, 32, 2048], Float32, CPU>, %961:tensor<[1, 8, 128, 32], Float32, CPU>, %956:tensor<[1, 8, 32, 128], Float32, CPU>)
            graph.CallGraphOp @model.layers.18 (%986:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%1023:tensor<[1, 32, 2048], Float32, CPU>, %998:tensor<[1, 8, 128, 32], Float32, CPU>, %993:tensor<[1, 8, 32, 128], Float32, CPU>)
            graph.CallGraphOp @model.layers.19 (%1023:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%1060:tensor<[1, 32, 2048], Float32, CPU>, %1035:tensor<[1, 8, 128, 32], Float32, CPU>, %1030:tensor<[1, 8, 32, 128], Float32, CPU>)
            graph.CallGraphOp @model.layers.20 (%1060:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%1097:tensor<[1, 32, 2048], Float32, CPU>, %1072:tensor<[1, 8, 128, 32], Float32, CPU>, %1067:tensor<[1, 8, 32, 128], Float32, CPU>)
            graph.CallGraphOp @model.layers.21 (%1097:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%1134:tensor<[1, 32, 2048], Float32, CPU>, %1109:tensor<[1, 8, 128, 32], Float32, CPU>, %1104:tensor<[1, 8, 32, 128], Float32, CPU>)
            graph.CallGraphOp @model.layers.22 (%1134:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%1171:tensor<[1, 32, 2048], Float32, CPU>, %1146:tensor<[1, 8, 128, 32], Float32, CPU>, %1141:tensor<[1, 8, 32, 128], Float32, CPU>)
            graph.CallGraphOp @model.layers.23 (%1171:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%1208:tensor<[1, 32, 2048], Float32, CPU>, %1183:tensor<[1, 8, 128, 32], Float32, CPU>, %1178:tensor<[1, 8, 32, 128], Float32, CPU>)
            graph.CallGraphOp @model.layers.24 (%1208:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%1245:tensor<[1, 32, 2048], Float32, CPU>, %1220:tensor<[1, 8, 128, 32], Float32, CPU>, %1215:tensor<[1, 8, 32, 128], Float32, CPU>)
            graph.CallGraphOp @model.layers.25 (%1245:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%1282:tensor<[1, 32, 2048], Float32, CPU>, %1257:tensor<[1, 8, 128, 32], Float32, CPU>, %1252:tensor<[1, 8, 32, 128], Float32, CPU>)
            graph.CallGraphOp @model.layers.26 (%1282:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%1319:tensor<[1, 32, 2048], Float32, CPU>, %1294:tensor<[1, 8, 128, 32], Float32, CPU>, %1289:tensor<[1, 8, 32, 128], Float32, CPU>)
            graph.CallGraphOp @model.layers.27 (%1319:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%1356:tensor<[1, 32, 2048], Float32, CPU>, %1331:tensor<[1, 8, 128, 32], Float32, CPU>, %1326:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.norm"](%1356:tensor<[1, 32, 2048], Float32, CPU>) -> (%1357:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%1357:tensor<[1, 32, 2048], Float32, CPU>, %332:tensor<[1, 8, 128, 32], Float32, CPU>, %369:tensor<[1, 8, 128, 32], Float32, CPU>, %406:tensor<[1, 8, 128, 32], Float32, CPU>, %443:tensor<[1, 8, 128, 32], Float32, CPU>, %480:tensor<[1, 8, 128, 32], Float32, CPU>, %517:tensor<[1, 8, 128, 32], Float32, CPU>, %554:tensor<[1, 8, 128, 32], Float32, CPU>, %591:tensor<[1, 8, 128, 32], Float32, CPU>, %628:tensor<[1, 8, 128, 32], Float32, CPU>, %665:tensor<[1, 8, 128, 32], Float32, CPU>, %702:tensor<[1, 8, 128, 32], Float32, CPU>, %739:tensor<[1, 8, 128, 32], Float32, CPU>, %776:tensor<[1, 8, 128, 32], Float32, CPU>, %813:tensor<[1, 8, 128, 32], Float32, CPU>, %850:tensor<[1, 8, 128, 32], Float32, CPU>, %887:tensor<[1, 8, 128, 32], Float32, CPU>, %924:tensor<[1, 8, 128, 32], Float32, CPU>, %961:tensor<[1, 8, 128, 32], Float32, CPU>, %998:tensor<[1, 8, 128, 32], Float32, CPU>, %1035:tensor<[1, 8, 128, 32], Float32, CPU>, %1072:tensor<[1, 8, 128, 32], Float32, CPU>, %1109:tensor<[1, 8, 128, 32], Float32, CPU>, %1146:tensor<[1, 8, 128, 32], Float32, CPU>, %1183:tensor<[1, 8, 128, 32], Float32, CPU>, %1220:tensor<[1, 8, 128, 32], Float32, CPU>, %1257:tensor<[1, 8, 128, 32], Float32, CPU>, %1294:tensor<[1, 8, 128, 32], Float32, CPU>, %1331:tensor<[1, 8, 128, 32], Float32, CPU>, %327:tensor<[1, 8, 32, 128], Float32, CPU>, %364:tensor<[1, 8, 32, 128], Float32, CPU>, %401:tensor<[1, 8, 32, 128], Float32, CPU>, %438:tensor<[1, 8, 32, 128], Float32, CPU>, %475:tensor<[1, 8, 32, 128], Float32, CPU>, %512:tensor<[1, 8, 32, 128], Float32, CPU>, %549:tensor<[1, 8, 32, 128], Float32, CPU>, %586:tensor<[1, 8, 32, 128], Float32, CPU>, %623:tensor<[1, 8, 32, 128], Float32, CPU>, %660:tensor<[1, 8, 32, 128], Float32, CPU>, %697:tensor<[1, 8, 32, 128], Float32, CPU>, %734:tensor<[1, 8, 32, 128], Float32, CPU>, %771:tensor<[1, 8, 32, 128], Float32, CPU>, %808:tensor<[1, 8, 32, 128], Float32, CPU>, %845:tensor<[1, 8, 32, 128], Float32, CPU>, %882:tensor<[1, 8, 32, 128], Float32, CPU>, %919:tensor<[1, 8, 32, 128], Float32, CPU>, %956:tensor<[1, 8, 32, 128], Float32, CPU>, %993:tensor<[1, 8, 32, 128], Float32, CPU>, %1030:tensor<[1, 8, 32, 128], Float32, CPU>, %1067:tensor<[1, 8, 32, 128], Float32, CPU>, %1104:tensor<[1, 8, 32, 128], Float32, CPU>, %1141:tensor<[1, 8, 32, 128], Float32, CPU>, %1178:tensor<[1, 8, 32, 128], Float32, CPU>, %1215:tensor<[1, 8, 32, 128], Float32, CPU>, %1252:tensor<[1, 8, 32, 128], Float32, CPU>, %1289:tensor<[1, 8, 32, 128], Float32, CPU>, %1326:tensor<[1, 8, 32, 128], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.0 <CPU> {
        (%320:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%357:tensor<[1, 32, 2048], Float32, CPU>, %332:tensor<[1, 8, 128, 32], Float32, CPU>, %327:tensor<[1, 8, 32, 128], Float32, CPU>) {
            linalg.CPU.RMSNormOp [name="model.layers.0.input_layernorm"](%320:tensor<[1, 32, 2048], Float32, CPU>) -> (%321:tensor<[1, 32, 2048], Float32, CPU>)
            graph.CallGraphOp @model.layers.0.self_attn (%321:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%349:tensor<[1, 32, 2048], Float32, CPU>, %332:tensor<[1, 8, 128, 32], Float32, CPU>, %327:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.AddOp(%349:tensor<[1, 32, 2048], Float32, CPU>, %320:tensor<[1, 32, 2048], Float32, CPU>) -> (%350:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.0.post_attention_layernorm"](%350:tensor<[1, 32, 2048], Float32, CPU>) -> (%351:tensor<[1, 32, 2048], Float32, CPU>)
            graph.CallGraphOp @model.layers.0.mlp (%351:tensor<[1, 32, 2048], Float32, CPU>) -> (%356:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.AddOp(%356:tensor<[1, 32, 2048], Float32, CPU>, %350:tensor<[1, 32, 2048], Float32, CPU>) -> (%357:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%357:tensor<[1, 32, 2048], Float32, CPU>, %332:tensor<[1, 8, 128, 32], Float32, CPU>, %327:tensor<[1, 8, 32, 128], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.0.self_attn <CPU> {
        (%321:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%349:tensor<[1, 32, 2048], Float32, CPU>, %332:tensor<[1, 8, 128, 32], Float32, CPU>, %327:tensor<[1, 8, 32, 128], Float32, CPU>) {
            linalg.CPU.LinearOp [name="model.layers.0.self_attn.q_proj"](%321:tensor<[1, 32, 2048], Float32, CPU>) -> (%322:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.0.self_attn.k_proj"](%321:tensor<[1, 32, 2048], Float32, CPU>) -> (%323:tensor<[1, 32, 1024], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.0.self_attn.v_proj"](%321:tensor<[1, 32, 2048], Float32, CPU>) -> (%324:tensor<[1, 32, 1024], Float32, CPU>)
            linalg.CPU.ViewOp(%322:tensor<[1, 32, 2048], Float32, CPU>) -> (%322:tensor<[1, 32, 16, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%322:tensor<[1, 32, 16, 128], Float32, CPU>) -> (%325:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%323:tensor<[1, 32, 1024], Float32, CPU>) -> (%323:tensor<[1, 32, 8, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%323:tensor<[1, 32, 8, 128], Float32, CPU>) -> (%326:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%324:tensor<[1, 32, 1024], Float32, CPU>) -> (%324:tensor<[1, 32, 8, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%324:tensor<[1, 32, 8, 128], Float32, CPU>) -> (%327:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.0.self_attn.q_norm"](%325:tensor<[1, 16, 32, 128], Float32, CPU>) -> (%328:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.0.self_attn.k_norm"](%326:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%329:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.RoPEOp [name="model.layers.0.self_attn.q_rope"](%328:tensor<[1, 16, 32, 128], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>) -> (%330:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.RoPEOp [name="model.layers.0.self_attn.k_rope"](%329:tensor<[1, 8, 32, 128], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>) -> (%331:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%331:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%332:tensor<[1, 8, 128, 32], Float32, CPU>)
            linalg.CPU.ConcatOp(%313:tensor<[1, 8, 128, 992], Float32, CPU>, %332:tensor<[1, 8, 128, 32], Float32, CPU>) -> (%333:tensor<[1, 8, 128, 1024], Float32, CPU>)
            linalg.CPU.ConcatOp(%314:tensor<[1, 8, 992, 128], Float32, CPU>, %327:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%334:tensor<[1, 8, 1024, 128], Float32, CPU>)
            linalg.CPU.RepeatOp(%333:tensor<[1, 8, 128, 1024], Float32, CPU>) -> (%335:tensor<[1, 16, 128, 1024], Float32, CPU>)
            linalg.CPU.RepeatOp(%334:tensor<[1, 8, 1024, 128], Float32, CPU>) -> (%336:tensor<[1, 16, 1024, 128], Float32, CPU>)
            linalg.CPU.MatMulOp(%330:tensor<[1, 16, 32, 128], Float32, CPU>, %335:tensor<[1, 16, 128, 1024], Float32, CPU>) -> (%337:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.MulOp(%337:tensor<[1, 16, 32, 1024], Float32, CPU>, %338:tensor<[1], Float32, CPU>[constant: [0.088388346]][constant:[0.088388346]]) -> (%339:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.ReduceMinOp(%339:tensor<[1, 16, 32, 1024], Float32, CPU>) -> (%340:tensor<[1, 16, 32, 1], Float32, CPU>)
            linalg.CPU.AddOp(%340:tensor<[1, 16, 32, 1], Float32, CPU>, %341:tensor<[1], Float32, CPU>[constant: [-20]][constant:[-20]]) -> (%342:tensor<[1, 16, 32, 1], Float32, CPU>)
            linalg.CPU.EqualOp(%315:tensor<[1, 1, 32, 1024], Float32, CPU>, %343:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%344:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp(%344:tensor<[1, 1, 32, 1024], UInt8, CPU>, %339:tensor<[1, 16, 32, 1024], Float32, CPU>, %342:tensor<[1, 16, 32, 1], Float32, CPU>) -> (%345:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%345:tensor<[1, 16, 32, 1024], Float32, CPU>) -> (%346:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.MatMulOp(%346:tensor<[1, 16, 32, 1024], Float32, CPU>, %336:tensor<[1, 16, 1024, 128], Float32, CPU>) -> (%347:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%347:tensor<[1, 16, 32, 128], Float32, CPU>) -> (%348:tensor<[1, 32, 16, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%348:tensor<[1, 32, 16, 128], Float32, CPU>) -> (%348:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.0.self_attn.o_proj"](%348:tensor<[1, 32, 2048], Float32, CPU>) -> (%349:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%349:tensor<[1, 32, 2048], Float32, CPU>, %332:tensor<[1, 8, 128, 32], Float32, CPU>, %327:tensor<[1, 8, 32, 128], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.0.mlp <CPU> {
        (%351:tensor<[1, 32, 2048], Float32, CPU>) -> (%356:tensor<[1, 32, 2048], Float32, CPU>) {
            linalg.CPU.LinearOp [name="model.layers.0.mlp.gate_proj"](%351:tensor<[1, 32, 2048], Float32, CPU>) -> (%352:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.SiLUOp [name="model.layers.0.mlp.act"](%352:tensor<[1, 32, 6144], Float32, CPU>) -> (%353:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.0.mlp.up_proj"](%351:tensor<[1, 32, 2048], Float32, CPU>) -> (%354:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.MulOp(%353:tensor<[1, 32, 6144], Float32, CPU>, %354:tensor<[1, 32, 6144], Float32, CPU>) -> (%355:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.0.mlp.down_proj"](%355:tensor<[1, 32, 6144], Float32, CPU>) -> (%356:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%356:tensor<[1, 32, 2048], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.1 <CPU> {
        (%357:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%394:tensor<[1, 32, 2048], Float32, CPU>, %369:tensor<[1, 8, 128, 32], Float32, CPU>, %364:tensor<[1, 8, 32, 128], Float32, CPU>) {
            linalg.CPU.RMSNormOp [name="model.layers.1.input_layernorm"](%357:tensor<[1, 32, 2048], Float32, CPU>) -> (%358:tensor<[1, 32, 2048], Float32, CPU>)
            graph.CallGraphOp @model.layers.1.self_attn (%358:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%386:tensor<[1, 32, 2048], Float32, CPU>, %369:tensor<[1, 8, 128, 32], Float32, CPU>, %364:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.AddOp(%386:tensor<[1, 32, 2048], Float32, CPU>, %357:tensor<[1, 32, 2048], Float32, CPU>) -> (%387:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.1.post_attention_layernorm"](%387:tensor<[1, 32, 2048], Float32, CPU>) -> (%388:tensor<[1, 32, 2048], Float32, CPU>)
            graph.CallGraphOp @model.layers.1.mlp (%388:tensor<[1, 32, 2048], Float32, CPU>) -> (%393:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.AddOp(%393:tensor<[1, 32, 2048], Float32, CPU>, %387:tensor<[1, 32, 2048], Float32, CPU>) -> (%394:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%394:tensor<[1, 32, 2048], Float32, CPU>, %369:tensor<[1, 8, 128, 32], Float32, CPU>, %364:tensor<[1, 8, 32, 128], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.1.self_attn <CPU> {
        (%358:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%386:tensor<[1, 32, 2048], Float32, CPU>, %369:tensor<[1, 8, 128, 32], Float32, CPU>, %364:tensor<[1, 8, 32, 128], Float32, CPU>) {
            linalg.CPU.LinearOp [name="model.layers.1.self_attn.q_proj"](%358:tensor<[1, 32, 2048], Float32, CPU>) -> (%359:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.1.self_attn.k_proj"](%358:tensor<[1, 32, 2048], Float32, CPU>) -> (%360:tensor<[1, 32, 1024], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.1.self_attn.v_proj"](%358:tensor<[1, 32, 2048], Float32, CPU>) -> (%361:tensor<[1, 32, 1024], Float32, CPU>)
            linalg.CPU.ViewOp(%359:tensor<[1, 32, 2048], Float32, CPU>) -> (%359:tensor<[1, 32, 16, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%359:tensor<[1, 32, 16, 128], Float32, CPU>) -> (%362:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%360:tensor<[1, 32, 1024], Float32, CPU>) -> (%360:tensor<[1, 32, 8, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%360:tensor<[1, 32, 8, 128], Float32, CPU>) -> (%363:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%361:tensor<[1, 32, 1024], Float32, CPU>) -> (%361:tensor<[1, 32, 8, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%361:tensor<[1, 32, 8, 128], Float32, CPU>) -> (%364:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.1.self_attn.q_norm"](%362:tensor<[1, 16, 32, 128], Float32, CPU>) -> (%365:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.1.self_attn.k_norm"](%363:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%366:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.RoPEOp [name="model.layers.1.self_attn.q_rope"](%365:tensor<[1, 16, 32, 128], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>) -> (%367:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.RoPEOp [name="model.layers.1.self_attn.k_rope"](%366:tensor<[1, 8, 32, 128], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>) -> (%368:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%368:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%369:tensor<[1, 8, 128, 32], Float32, CPU>)
            linalg.CPU.ConcatOp(%313:tensor<[1, 8, 128, 992], Float32, CPU>, %369:tensor<[1, 8, 128, 32], Float32, CPU>) -> (%370:tensor<[1, 8, 128, 1024], Float32, CPU>)
            linalg.CPU.ConcatOp(%314:tensor<[1, 8, 992, 128], Float32, CPU>, %364:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%371:tensor<[1, 8, 1024, 128], Float32, CPU>)
            linalg.CPU.RepeatOp(%370:tensor<[1, 8, 128, 1024], Float32, CPU>) -> (%372:tensor<[1, 16, 128, 1024], Float32, CPU>)
            linalg.CPU.RepeatOp(%371:tensor<[1, 8, 1024, 128], Float32, CPU>) -> (%373:tensor<[1, 16, 1024, 128], Float32, CPU>)
            linalg.CPU.MatMulOp(%367:tensor<[1, 16, 32, 128], Float32, CPU>, %372:tensor<[1, 16, 128, 1024], Float32, CPU>) -> (%374:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.MulOp(%374:tensor<[1, 16, 32, 1024], Float32, CPU>, %375:tensor<[1], Float32, CPU>[constant: [0.088388346]][constant:[0.088388346]]) -> (%376:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.ReduceMinOp(%376:tensor<[1, 16, 32, 1024], Float32, CPU>) -> (%377:tensor<[1, 16, 32, 1], Float32, CPU>)
            linalg.CPU.AddOp(%377:tensor<[1, 16, 32, 1], Float32, CPU>, %378:tensor<[1], Float32, CPU>[constant: [-20]][constant:[-20]]) -> (%379:tensor<[1, 16, 32, 1], Float32, CPU>)
            linalg.CPU.EqualOp(%315:tensor<[1, 1, 32, 1024], Float32, CPU>, %380:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%381:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp(%381:tensor<[1, 1, 32, 1024], UInt8, CPU>, %376:tensor<[1, 16, 32, 1024], Float32, CPU>, %379:tensor<[1, 16, 32, 1], Float32, CPU>) -> (%382:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%382:tensor<[1, 16, 32, 1024], Float32, CPU>) -> (%383:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.MatMulOp(%383:tensor<[1, 16, 32, 1024], Float32, CPU>, %373:tensor<[1, 16, 1024, 128], Float32, CPU>) -> (%384:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%384:tensor<[1, 16, 32, 128], Float32, CPU>) -> (%385:tensor<[1, 32, 16, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%385:tensor<[1, 32, 16, 128], Float32, CPU>) -> (%385:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.1.self_attn.o_proj"](%385:tensor<[1, 32, 2048], Float32, CPU>) -> (%386:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%386:tensor<[1, 32, 2048], Float32, CPU>, %369:tensor<[1, 8, 128, 32], Float32, CPU>, %364:tensor<[1, 8, 32, 128], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.1.mlp <CPU> {
        (%388:tensor<[1, 32, 2048], Float32, CPU>) -> (%393:tensor<[1, 32, 2048], Float32, CPU>) {
            linalg.CPU.LinearOp [name="model.layers.1.mlp.gate_proj"](%388:tensor<[1, 32, 2048], Float32, CPU>) -> (%389:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.SiLUOp [name="model.layers.1.mlp.act"](%389:tensor<[1, 32, 6144], Float32, CPU>) -> (%390:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.1.mlp.up_proj"](%388:tensor<[1, 32, 2048], Float32, CPU>) -> (%391:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.MulOp(%390:tensor<[1, 32, 6144], Float32, CPU>, %391:tensor<[1, 32, 6144], Float32, CPU>) -> (%392:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.1.mlp.down_proj"](%392:tensor<[1, 32, 6144], Float32, CPU>) -> (%393:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%393:tensor<[1, 32, 2048], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.2 <CPU> {
        (%394:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%431:tensor<[1, 32, 2048], Float32, CPU>, %406:tensor<[1, 8, 128, 32], Float32, CPU>, %401:tensor<[1, 8, 32, 128], Float32, CPU>) {
            linalg.CPU.RMSNormOp [name="model.layers.2.input_layernorm"](%394:tensor<[1, 32, 2048], Float32, CPU>) -> (%395:tensor<[1, 32, 2048], Float32, CPU>)
            graph.CallGraphOp @model.layers.2.self_attn (%395:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%423:tensor<[1, 32, 2048], Float32, CPU>, %406:tensor<[1, 8, 128, 32], Float32, CPU>, %401:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.AddOp(%423:tensor<[1, 32, 2048], Float32, CPU>, %394:tensor<[1, 32, 2048], Float32, CPU>) -> (%424:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.2.post_attention_layernorm"](%424:tensor<[1, 32, 2048], Float32, CPU>) -> (%425:tensor<[1, 32, 2048], Float32, CPU>)
            graph.CallGraphOp @model.layers.2.mlp (%425:tensor<[1, 32, 2048], Float32, CPU>) -> (%430:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.AddOp(%430:tensor<[1, 32, 2048], Float32, CPU>, %424:tensor<[1, 32, 2048], Float32, CPU>) -> (%431:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%431:tensor<[1, 32, 2048], Float32, CPU>, %406:tensor<[1, 8, 128, 32], Float32, CPU>, %401:tensor<[1, 8, 32, 128], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.2.self_attn <CPU> {
        (%395:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%423:tensor<[1, 32, 2048], Float32, CPU>, %406:tensor<[1, 8, 128, 32], Float32, CPU>, %401:tensor<[1, 8, 32, 128], Float32, CPU>) {
            linalg.CPU.LinearOp [name="model.layers.2.self_attn.q_proj"](%395:tensor<[1, 32, 2048], Float32, CPU>) -> (%396:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.2.self_attn.k_proj"](%395:tensor<[1, 32, 2048], Float32, CPU>) -> (%397:tensor<[1, 32, 1024], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.2.self_attn.v_proj"](%395:tensor<[1, 32, 2048], Float32, CPU>) -> (%398:tensor<[1, 32, 1024], Float32, CPU>)
            linalg.CPU.ViewOp(%396:tensor<[1, 32, 2048], Float32, CPU>) -> (%396:tensor<[1, 32, 16, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%396:tensor<[1, 32, 16, 128], Float32, CPU>) -> (%399:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%397:tensor<[1, 32, 1024], Float32, CPU>) -> (%397:tensor<[1, 32, 8, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%397:tensor<[1, 32, 8, 128], Float32, CPU>) -> (%400:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%398:tensor<[1, 32, 1024], Float32, CPU>) -> (%398:tensor<[1, 32, 8, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%398:tensor<[1, 32, 8, 128], Float32, CPU>) -> (%401:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.2.self_attn.q_norm"](%399:tensor<[1, 16, 32, 128], Float32, CPU>) -> (%402:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.2.self_attn.k_norm"](%400:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%403:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.RoPEOp [name="model.layers.2.self_attn.q_rope"](%402:tensor<[1, 16, 32, 128], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>) -> (%404:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.RoPEOp [name="model.layers.2.self_attn.k_rope"](%403:tensor<[1, 8, 32, 128], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>) -> (%405:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%405:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%406:tensor<[1, 8, 128, 32], Float32, CPU>)
            linalg.CPU.ConcatOp(%313:tensor<[1, 8, 128, 992], Float32, CPU>, %406:tensor<[1, 8, 128, 32], Float32, CPU>) -> (%407:tensor<[1, 8, 128, 1024], Float32, CPU>)
            linalg.CPU.ConcatOp(%314:tensor<[1, 8, 992, 128], Float32, CPU>, %401:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%408:tensor<[1, 8, 1024, 128], Float32, CPU>)
            linalg.CPU.RepeatOp(%407:tensor<[1, 8, 128, 1024], Float32, CPU>) -> (%409:tensor<[1, 16, 128, 1024], Float32, CPU>)
            linalg.CPU.RepeatOp(%408:tensor<[1, 8, 1024, 128], Float32, CPU>) -> (%410:tensor<[1, 16, 1024, 128], Float32, CPU>)
            linalg.CPU.MatMulOp(%404:tensor<[1, 16, 32, 128], Float32, CPU>, %409:tensor<[1, 16, 128, 1024], Float32, CPU>) -> (%411:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.MulOp(%411:tensor<[1, 16, 32, 1024], Float32, CPU>, %412:tensor<[1], Float32, CPU>[constant: [0.088388346]][constant:[0.088388346]]) -> (%413:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.ReduceMinOp(%413:tensor<[1, 16, 32, 1024], Float32, CPU>) -> (%414:tensor<[1, 16, 32, 1], Float32, CPU>)
            linalg.CPU.AddOp(%414:tensor<[1, 16, 32, 1], Float32, CPU>, %415:tensor<[1], Float32, CPU>[constant: [-20]][constant:[-20]]) -> (%416:tensor<[1, 16, 32, 1], Float32, CPU>)
            linalg.CPU.EqualOp(%315:tensor<[1, 1, 32, 1024], Float32, CPU>, %417:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%418:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp(%418:tensor<[1, 1, 32, 1024], UInt8, CPU>, %413:tensor<[1, 16, 32, 1024], Float32, CPU>, %416:tensor<[1, 16, 32, 1], Float32, CPU>) -> (%419:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%419:tensor<[1, 16, 32, 1024], Float32, CPU>) -> (%420:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.MatMulOp(%420:tensor<[1, 16, 32, 1024], Float32, CPU>, %410:tensor<[1, 16, 1024, 128], Float32, CPU>) -> (%421:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%421:tensor<[1, 16, 32, 128], Float32, CPU>) -> (%422:tensor<[1, 32, 16, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%422:tensor<[1, 32, 16, 128], Float32, CPU>) -> (%422:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.2.self_attn.o_proj"](%422:tensor<[1, 32, 2048], Float32, CPU>) -> (%423:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%423:tensor<[1, 32, 2048], Float32, CPU>, %406:tensor<[1, 8, 128, 32], Float32, CPU>, %401:tensor<[1, 8, 32, 128], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.2.mlp <CPU> {
        (%425:tensor<[1, 32, 2048], Float32, CPU>) -> (%430:tensor<[1, 32, 2048], Float32, CPU>) {
            linalg.CPU.LinearOp [name="model.layers.2.mlp.gate_proj"](%425:tensor<[1, 32, 2048], Float32, CPU>) -> (%426:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.SiLUOp [name="model.layers.2.mlp.act"](%426:tensor<[1, 32, 6144], Float32, CPU>) -> (%427:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.2.mlp.up_proj"](%425:tensor<[1, 32, 2048], Float32, CPU>) -> (%428:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.MulOp(%427:tensor<[1, 32, 6144], Float32, CPU>, %428:tensor<[1, 32, 6144], Float32, CPU>) -> (%429:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.2.mlp.down_proj"](%429:tensor<[1, 32, 6144], Float32, CPU>) -> (%430:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%430:tensor<[1, 32, 2048], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.3 <CPU> {
        (%431:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%468:tensor<[1, 32, 2048], Float32, CPU>, %443:tensor<[1, 8, 128, 32], Float32, CPU>, %438:tensor<[1, 8, 32, 128], Float32, CPU>) {
            linalg.CPU.RMSNormOp [name="model.layers.3.input_layernorm"](%431:tensor<[1, 32, 2048], Float32, CPU>) -> (%432:tensor<[1, 32, 2048], Float32, CPU>)
            graph.CallGraphOp @model.layers.3.self_attn (%432:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%460:tensor<[1, 32, 2048], Float32, CPU>, %443:tensor<[1, 8, 128, 32], Float32, CPU>, %438:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.AddOp(%460:tensor<[1, 32, 2048], Float32, CPU>, %431:tensor<[1, 32, 2048], Float32, CPU>) -> (%461:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.3.post_attention_layernorm"](%461:tensor<[1, 32, 2048], Float32, CPU>) -> (%462:tensor<[1, 32, 2048], Float32, CPU>)
            graph.CallGraphOp @model.layers.3.mlp (%462:tensor<[1, 32, 2048], Float32, CPU>) -> (%467:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.AddOp(%467:tensor<[1, 32, 2048], Float32, CPU>, %461:tensor<[1, 32, 2048], Float32, CPU>) -> (%468:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%468:tensor<[1, 32, 2048], Float32, CPU>, %443:tensor<[1, 8, 128, 32], Float32, CPU>, %438:tensor<[1, 8, 32, 128], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.3.self_attn <CPU> {
        (%432:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%460:tensor<[1, 32, 2048], Float32, CPU>, %443:tensor<[1, 8, 128, 32], Float32, CPU>, %438:tensor<[1, 8, 32, 128], Float32, CPU>) {
            linalg.CPU.LinearOp [name="model.layers.3.self_attn.q_proj"](%432:tensor<[1, 32, 2048], Float32, CPU>) -> (%433:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.3.self_attn.k_proj"](%432:tensor<[1, 32, 2048], Float32, CPU>) -> (%434:tensor<[1, 32, 1024], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.3.self_attn.v_proj"](%432:tensor<[1, 32, 2048], Float32, CPU>) -> (%435:tensor<[1, 32, 1024], Float32, CPU>)
            linalg.CPU.ViewOp(%433:tensor<[1, 32, 2048], Float32, CPU>) -> (%433:tensor<[1, 32, 16, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%433:tensor<[1, 32, 16, 128], Float32, CPU>) -> (%436:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%434:tensor<[1, 32, 1024], Float32, CPU>) -> (%434:tensor<[1, 32, 8, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%434:tensor<[1, 32, 8, 128], Float32, CPU>) -> (%437:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%435:tensor<[1, 32, 1024], Float32, CPU>) -> (%435:tensor<[1, 32, 8, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%435:tensor<[1, 32, 8, 128], Float32, CPU>) -> (%438:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.3.self_attn.q_norm"](%436:tensor<[1, 16, 32, 128], Float32, CPU>) -> (%439:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.3.self_attn.k_norm"](%437:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%440:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.RoPEOp [name="model.layers.3.self_attn.q_rope"](%439:tensor<[1, 16, 32, 128], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>) -> (%441:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.RoPEOp [name="model.layers.3.self_attn.k_rope"](%440:tensor<[1, 8, 32, 128], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>) -> (%442:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%442:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%443:tensor<[1, 8, 128, 32], Float32, CPU>)
            linalg.CPU.ConcatOp(%313:tensor<[1, 8, 128, 992], Float32, CPU>, %443:tensor<[1, 8, 128, 32], Float32, CPU>) -> (%444:tensor<[1, 8, 128, 1024], Float32, CPU>)
            linalg.CPU.ConcatOp(%314:tensor<[1, 8, 992, 128], Float32, CPU>, %438:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%445:tensor<[1, 8, 1024, 128], Float32, CPU>)
            linalg.CPU.RepeatOp(%444:tensor<[1, 8, 128, 1024], Float32, CPU>) -> (%446:tensor<[1, 16, 128, 1024], Float32, CPU>)
            linalg.CPU.RepeatOp(%445:tensor<[1, 8, 1024, 128], Float32, CPU>) -> (%447:tensor<[1, 16, 1024, 128], Float32, CPU>)
            linalg.CPU.MatMulOp(%441:tensor<[1, 16, 32, 128], Float32, CPU>, %446:tensor<[1, 16, 128, 1024], Float32, CPU>) -> (%448:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.MulOp(%448:tensor<[1, 16, 32, 1024], Float32, CPU>, %449:tensor<[1], Float32, CPU>[constant: [0.088388346]][constant:[0.088388346]]) -> (%450:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.ReduceMinOp(%450:tensor<[1, 16, 32, 1024], Float32, CPU>) -> (%451:tensor<[1, 16, 32, 1], Float32, CPU>)
            linalg.CPU.AddOp(%451:tensor<[1, 16, 32, 1], Float32, CPU>, %452:tensor<[1], Float32, CPU>[constant: [-20]][constant:[-20]]) -> (%453:tensor<[1, 16, 32, 1], Float32, CPU>)
            linalg.CPU.EqualOp(%315:tensor<[1, 1, 32, 1024], Float32, CPU>, %454:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%455:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp(%455:tensor<[1, 1, 32, 1024], UInt8, CPU>, %450:tensor<[1, 16, 32, 1024], Float32, CPU>, %453:tensor<[1, 16, 32, 1], Float32, CPU>) -> (%456:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%456:tensor<[1, 16, 32, 1024], Float32, CPU>) -> (%457:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.MatMulOp(%457:tensor<[1, 16, 32, 1024], Float32, CPU>, %447:tensor<[1, 16, 1024, 128], Float32, CPU>) -> (%458:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%458:tensor<[1, 16, 32, 128], Float32, CPU>) -> (%459:tensor<[1, 32, 16, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%459:tensor<[1, 32, 16, 128], Float32, CPU>) -> (%459:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.3.self_attn.o_proj"](%459:tensor<[1, 32, 2048], Float32, CPU>) -> (%460:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%460:tensor<[1, 32, 2048], Float32, CPU>, %443:tensor<[1, 8, 128, 32], Float32, CPU>, %438:tensor<[1, 8, 32, 128], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.3.mlp <CPU> {
        (%462:tensor<[1, 32, 2048], Float32, CPU>) -> (%467:tensor<[1, 32, 2048], Float32, CPU>) {
            linalg.CPU.LinearOp [name="model.layers.3.mlp.gate_proj"](%462:tensor<[1, 32, 2048], Float32, CPU>) -> (%463:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.SiLUOp [name="model.layers.3.mlp.act"](%463:tensor<[1, 32, 6144], Float32, CPU>) -> (%464:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.3.mlp.up_proj"](%462:tensor<[1, 32, 2048], Float32, CPU>) -> (%465:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.MulOp(%464:tensor<[1, 32, 6144], Float32, CPU>, %465:tensor<[1, 32, 6144], Float32, CPU>) -> (%466:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.3.mlp.down_proj"](%466:tensor<[1, 32, 6144], Float32, CPU>) -> (%467:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%467:tensor<[1, 32, 2048], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.4 <CPU> {
        (%468:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%505:tensor<[1, 32, 2048], Float32, CPU>, %480:tensor<[1, 8, 128, 32], Float32, CPU>, %475:tensor<[1, 8, 32, 128], Float32, CPU>) {
            linalg.CPU.RMSNormOp [name="model.layers.4.input_layernorm"](%468:tensor<[1, 32, 2048], Float32, CPU>) -> (%469:tensor<[1, 32, 2048], Float32, CPU>)
            graph.CallGraphOp @model.layers.4.self_attn (%469:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%497:tensor<[1, 32, 2048], Float32, CPU>, %480:tensor<[1, 8, 128, 32], Float32, CPU>, %475:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.AddOp(%497:tensor<[1, 32, 2048], Float32, CPU>, %468:tensor<[1, 32, 2048], Float32, CPU>) -> (%498:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.4.post_attention_layernorm"](%498:tensor<[1, 32, 2048], Float32, CPU>) -> (%499:tensor<[1, 32, 2048], Float32, CPU>)
            graph.CallGraphOp @model.layers.4.mlp (%499:tensor<[1, 32, 2048], Float32, CPU>) -> (%504:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.AddOp(%504:tensor<[1, 32, 2048], Float32, CPU>, %498:tensor<[1, 32, 2048], Float32, CPU>) -> (%505:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%505:tensor<[1, 32, 2048], Float32, CPU>, %480:tensor<[1, 8, 128, 32], Float32, CPU>, %475:tensor<[1, 8, 32, 128], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.4.self_attn <CPU> {
        (%469:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%497:tensor<[1, 32, 2048], Float32, CPU>, %480:tensor<[1, 8, 128, 32], Float32, CPU>, %475:tensor<[1, 8, 32, 128], Float32, CPU>) {
            linalg.CPU.LinearOp [name="model.layers.4.self_attn.q_proj"](%469:tensor<[1, 32, 2048], Float32, CPU>) -> (%470:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.4.self_attn.k_proj"](%469:tensor<[1, 32, 2048], Float32, CPU>) -> (%471:tensor<[1, 32, 1024], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.4.self_attn.v_proj"](%469:tensor<[1, 32, 2048], Float32, CPU>) -> (%472:tensor<[1, 32, 1024], Float32, CPU>)
            linalg.CPU.ViewOp(%470:tensor<[1, 32, 2048], Float32, CPU>) -> (%470:tensor<[1, 32, 16, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%470:tensor<[1, 32, 16, 128], Float32, CPU>) -> (%473:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%471:tensor<[1, 32, 1024], Float32, CPU>) -> (%471:tensor<[1, 32, 8, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%471:tensor<[1, 32, 8, 128], Float32, CPU>) -> (%474:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%472:tensor<[1, 32, 1024], Float32, CPU>) -> (%472:tensor<[1, 32, 8, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%472:tensor<[1, 32, 8, 128], Float32, CPU>) -> (%475:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.4.self_attn.q_norm"](%473:tensor<[1, 16, 32, 128], Float32, CPU>) -> (%476:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.4.self_attn.k_norm"](%474:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%477:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.RoPEOp [name="model.layers.4.self_attn.q_rope"](%476:tensor<[1, 16, 32, 128], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>) -> (%478:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.RoPEOp [name="model.layers.4.self_attn.k_rope"](%477:tensor<[1, 8, 32, 128], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>) -> (%479:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%479:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%480:tensor<[1, 8, 128, 32], Float32, CPU>)
            linalg.CPU.ConcatOp(%313:tensor<[1, 8, 128, 992], Float32, CPU>, %480:tensor<[1, 8, 128, 32], Float32, CPU>) -> (%481:tensor<[1, 8, 128, 1024], Float32, CPU>)
            linalg.CPU.ConcatOp(%314:tensor<[1, 8, 992, 128], Float32, CPU>, %475:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%482:tensor<[1, 8, 1024, 128], Float32, CPU>)
            linalg.CPU.RepeatOp(%481:tensor<[1, 8, 128, 1024], Float32, CPU>) -> (%483:tensor<[1, 16, 128, 1024], Float32, CPU>)
            linalg.CPU.RepeatOp(%482:tensor<[1, 8, 1024, 128], Float32, CPU>) -> (%484:tensor<[1, 16, 1024, 128], Float32, CPU>)
            linalg.CPU.MatMulOp(%478:tensor<[1, 16, 32, 128], Float32, CPU>, %483:tensor<[1, 16, 128, 1024], Float32, CPU>) -> (%485:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.MulOp(%485:tensor<[1, 16, 32, 1024], Float32, CPU>, %486:tensor<[1], Float32, CPU>[constant: [0.088388346]][constant:[0.088388346]]) -> (%487:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.ReduceMinOp(%487:tensor<[1, 16, 32, 1024], Float32, CPU>) -> (%488:tensor<[1, 16, 32, 1], Float32, CPU>)
            linalg.CPU.AddOp(%488:tensor<[1, 16, 32, 1], Float32, CPU>, %489:tensor<[1], Float32, CPU>[constant: [-20]][constant:[-20]]) -> (%490:tensor<[1, 16, 32, 1], Float32, CPU>)
            linalg.CPU.EqualOp(%315:tensor<[1, 1, 32, 1024], Float32, CPU>, %491:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%492:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp(%492:tensor<[1, 1, 32, 1024], UInt8, CPU>, %487:tensor<[1, 16, 32, 1024], Float32, CPU>, %490:tensor<[1, 16, 32, 1], Float32, CPU>) -> (%493:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%493:tensor<[1, 16, 32, 1024], Float32, CPU>) -> (%494:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.MatMulOp(%494:tensor<[1, 16, 32, 1024], Float32, CPU>, %484:tensor<[1, 16, 1024, 128], Float32, CPU>) -> (%495:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%495:tensor<[1, 16, 32, 128], Float32, CPU>) -> (%496:tensor<[1, 32, 16, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%496:tensor<[1, 32, 16, 128], Float32, CPU>) -> (%496:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.4.self_attn.o_proj"](%496:tensor<[1, 32, 2048], Float32, CPU>) -> (%497:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%497:tensor<[1, 32, 2048], Float32, CPU>, %480:tensor<[1, 8, 128, 32], Float32, CPU>, %475:tensor<[1, 8, 32, 128], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.4.mlp <CPU> {
        (%499:tensor<[1, 32, 2048], Float32, CPU>) -> (%504:tensor<[1, 32, 2048], Float32, CPU>) {
            linalg.CPU.LinearOp [name="model.layers.4.mlp.gate_proj"](%499:tensor<[1, 32, 2048], Float32, CPU>) -> (%500:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.SiLUOp [name="model.layers.4.mlp.act"](%500:tensor<[1, 32, 6144], Float32, CPU>) -> (%501:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.4.mlp.up_proj"](%499:tensor<[1, 32, 2048], Float32, CPU>) -> (%502:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.MulOp(%501:tensor<[1, 32, 6144], Float32, CPU>, %502:tensor<[1, 32, 6144], Float32, CPU>) -> (%503:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.4.mlp.down_proj"](%503:tensor<[1, 32, 6144], Float32, CPU>) -> (%504:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%504:tensor<[1, 32, 2048], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.5 <CPU> {
        (%505:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%542:tensor<[1, 32, 2048], Float32, CPU>, %517:tensor<[1, 8, 128, 32], Float32, CPU>, %512:tensor<[1, 8, 32, 128], Float32, CPU>) {
            linalg.CPU.RMSNormOp [name="model.layers.5.input_layernorm"](%505:tensor<[1, 32, 2048], Float32, CPU>) -> (%506:tensor<[1, 32, 2048], Float32, CPU>)
            graph.CallGraphOp @model.layers.5.self_attn (%506:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%534:tensor<[1, 32, 2048], Float32, CPU>, %517:tensor<[1, 8, 128, 32], Float32, CPU>, %512:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.AddOp(%534:tensor<[1, 32, 2048], Float32, CPU>, %505:tensor<[1, 32, 2048], Float32, CPU>) -> (%535:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.5.post_attention_layernorm"](%535:tensor<[1, 32, 2048], Float32, CPU>) -> (%536:tensor<[1, 32, 2048], Float32, CPU>)
            graph.CallGraphOp @model.layers.5.mlp (%536:tensor<[1, 32, 2048], Float32, CPU>) -> (%541:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.AddOp(%541:tensor<[1, 32, 2048], Float32, CPU>, %535:tensor<[1, 32, 2048], Float32, CPU>) -> (%542:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%542:tensor<[1, 32, 2048], Float32, CPU>, %517:tensor<[1, 8, 128, 32], Float32, CPU>, %512:tensor<[1, 8, 32, 128], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.5.self_attn <CPU> {
        (%506:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%534:tensor<[1, 32, 2048], Float32, CPU>, %517:tensor<[1, 8, 128, 32], Float32, CPU>, %512:tensor<[1, 8, 32, 128], Float32, CPU>) {
            linalg.CPU.LinearOp [name="model.layers.5.self_attn.q_proj"](%506:tensor<[1, 32, 2048], Float32, CPU>) -> (%507:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.5.self_attn.k_proj"](%506:tensor<[1, 32, 2048], Float32, CPU>) -> (%508:tensor<[1, 32, 1024], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.5.self_attn.v_proj"](%506:tensor<[1, 32, 2048], Float32, CPU>) -> (%509:tensor<[1, 32, 1024], Float32, CPU>)
            linalg.CPU.ViewOp(%507:tensor<[1, 32, 2048], Float32, CPU>) -> (%507:tensor<[1, 32, 16, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%507:tensor<[1, 32, 16, 128], Float32, CPU>) -> (%510:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%508:tensor<[1, 32, 1024], Float32, CPU>) -> (%508:tensor<[1, 32, 8, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%508:tensor<[1, 32, 8, 128], Float32, CPU>) -> (%511:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%509:tensor<[1, 32, 1024], Float32, CPU>) -> (%509:tensor<[1, 32, 8, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%509:tensor<[1, 32, 8, 128], Float32, CPU>) -> (%512:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.5.self_attn.q_norm"](%510:tensor<[1, 16, 32, 128], Float32, CPU>) -> (%513:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.5.self_attn.k_norm"](%511:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%514:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.RoPEOp [name="model.layers.5.self_attn.q_rope"](%513:tensor<[1, 16, 32, 128], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>) -> (%515:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.RoPEOp [name="model.layers.5.self_attn.k_rope"](%514:tensor<[1, 8, 32, 128], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>) -> (%516:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%516:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%517:tensor<[1, 8, 128, 32], Float32, CPU>)
            linalg.CPU.ConcatOp(%313:tensor<[1, 8, 128, 992], Float32, CPU>, %517:tensor<[1, 8, 128, 32], Float32, CPU>) -> (%518:tensor<[1, 8, 128, 1024], Float32, CPU>)
            linalg.CPU.ConcatOp(%314:tensor<[1, 8, 992, 128], Float32, CPU>, %512:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%519:tensor<[1, 8, 1024, 128], Float32, CPU>)
            linalg.CPU.RepeatOp(%518:tensor<[1, 8, 128, 1024], Float32, CPU>) -> (%520:tensor<[1, 16, 128, 1024], Float32, CPU>)
            linalg.CPU.RepeatOp(%519:tensor<[1, 8, 1024, 128], Float32, CPU>) -> (%521:tensor<[1, 16, 1024, 128], Float32, CPU>)
            linalg.CPU.MatMulOp(%515:tensor<[1, 16, 32, 128], Float32, CPU>, %520:tensor<[1, 16, 128, 1024], Float32, CPU>) -> (%522:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.MulOp(%522:tensor<[1, 16, 32, 1024], Float32, CPU>, %523:tensor<[1], Float32, CPU>[constant: [0.088388346]][constant:[0.088388346]]) -> (%524:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.ReduceMinOp(%524:tensor<[1, 16, 32, 1024], Float32, CPU>) -> (%525:tensor<[1, 16, 32, 1], Float32, CPU>)
            linalg.CPU.AddOp(%525:tensor<[1, 16, 32, 1], Float32, CPU>, %526:tensor<[1], Float32, CPU>[constant: [-20]][constant:[-20]]) -> (%527:tensor<[1, 16, 32, 1], Float32, CPU>)
            linalg.CPU.EqualOp(%315:tensor<[1, 1, 32, 1024], Float32, CPU>, %528:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%529:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp(%529:tensor<[1, 1, 32, 1024], UInt8, CPU>, %524:tensor<[1, 16, 32, 1024], Float32, CPU>, %527:tensor<[1, 16, 32, 1], Float32, CPU>) -> (%530:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%530:tensor<[1, 16, 32, 1024], Float32, CPU>) -> (%531:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.MatMulOp(%531:tensor<[1, 16, 32, 1024], Float32, CPU>, %521:tensor<[1, 16, 1024, 128], Float32, CPU>) -> (%532:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%532:tensor<[1, 16, 32, 128], Float32, CPU>) -> (%533:tensor<[1, 32, 16, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%533:tensor<[1, 32, 16, 128], Float32, CPU>) -> (%533:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.5.self_attn.o_proj"](%533:tensor<[1, 32, 2048], Float32, CPU>) -> (%534:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%534:tensor<[1, 32, 2048], Float32, CPU>, %517:tensor<[1, 8, 128, 32], Float32, CPU>, %512:tensor<[1, 8, 32, 128], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.5.mlp <CPU> {
        (%536:tensor<[1, 32, 2048], Float32, CPU>) -> (%541:tensor<[1, 32, 2048], Float32, CPU>) {
            linalg.CPU.LinearOp [name="model.layers.5.mlp.gate_proj"](%536:tensor<[1, 32, 2048], Float32, CPU>) -> (%537:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.SiLUOp [name="model.layers.5.mlp.act"](%537:tensor<[1, 32, 6144], Float32, CPU>) -> (%538:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.5.mlp.up_proj"](%536:tensor<[1, 32, 2048], Float32, CPU>) -> (%539:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.MulOp(%538:tensor<[1, 32, 6144], Float32, CPU>, %539:tensor<[1, 32, 6144], Float32, CPU>) -> (%540:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.5.mlp.down_proj"](%540:tensor<[1, 32, 6144], Float32, CPU>) -> (%541:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%541:tensor<[1, 32, 2048], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.6 <CPU> {
        (%542:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%579:tensor<[1, 32, 2048], Float32, CPU>, %554:tensor<[1, 8, 128, 32], Float32, CPU>, %549:tensor<[1, 8, 32, 128], Float32, CPU>) {
            linalg.CPU.RMSNormOp [name="model.layers.6.input_layernorm"](%542:tensor<[1, 32, 2048], Float32, CPU>) -> (%543:tensor<[1, 32, 2048], Float32, CPU>)
            graph.CallGraphOp @model.layers.6.self_attn (%543:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%571:tensor<[1, 32, 2048], Float32, CPU>, %554:tensor<[1, 8, 128, 32], Float32, CPU>, %549:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.AddOp(%571:tensor<[1, 32, 2048], Float32, CPU>, %542:tensor<[1, 32, 2048], Float32, CPU>) -> (%572:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.6.post_attention_layernorm"](%572:tensor<[1, 32, 2048], Float32, CPU>) -> (%573:tensor<[1, 32, 2048], Float32, CPU>)
            graph.CallGraphOp @model.layers.6.mlp (%573:tensor<[1, 32, 2048], Float32, CPU>) -> (%578:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.AddOp(%578:tensor<[1, 32, 2048], Float32, CPU>, %572:tensor<[1, 32, 2048], Float32, CPU>) -> (%579:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%579:tensor<[1, 32, 2048], Float32, CPU>, %554:tensor<[1, 8, 128, 32], Float32, CPU>, %549:tensor<[1, 8, 32, 128], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.6.self_attn <CPU> {
        (%543:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%571:tensor<[1, 32, 2048], Float32, CPU>, %554:tensor<[1, 8, 128, 32], Float32, CPU>, %549:tensor<[1, 8, 32, 128], Float32, CPU>) {
            linalg.CPU.LinearOp [name="model.layers.6.self_attn.q_proj"](%543:tensor<[1, 32, 2048], Float32, CPU>) -> (%544:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.6.self_attn.k_proj"](%543:tensor<[1, 32, 2048], Float32, CPU>) -> (%545:tensor<[1, 32, 1024], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.6.self_attn.v_proj"](%543:tensor<[1, 32, 2048], Float32, CPU>) -> (%546:tensor<[1, 32, 1024], Float32, CPU>)
            linalg.CPU.ViewOp(%544:tensor<[1, 32, 2048], Float32, CPU>) -> (%544:tensor<[1, 32, 16, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%544:tensor<[1, 32, 16, 128], Float32, CPU>) -> (%547:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%545:tensor<[1, 32, 1024], Float32, CPU>) -> (%545:tensor<[1, 32, 8, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%545:tensor<[1, 32, 8, 128], Float32, CPU>) -> (%548:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%546:tensor<[1, 32, 1024], Float32, CPU>) -> (%546:tensor<[1, 32, 8, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%546:tensor<[1, 32, 8, 128], Float32, CPU>) -> (%549:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.6.self_attn.q_norm"](%547:tensor<[1, 16, 32, 128], Float32, CPU>) -> (%550:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.6.self_attn.k_norm"](%548:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%551:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.RoPEOp [name="model.layers.6.self_attn.q_rope"](%550:tensor<[1, 16, 32, 128], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>) -> (%552:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.RoPEOp [name="model.layers.6.self_attn.k_rope"](%551:tensor<[1, 8, 32, 128], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>) -> (%553:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%553:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%554:tensor<[1, 8, 128, 32], Float32, CPU>)
            linalg.CPU.ConcatOp(%313:tensor<[1, 8, 128, 992], Float32, CPU>, %554:tensor<[1, 8, 128, 32], Float32, CPU>) -> (%555:tensor<[1, 8, 128, 1024], Float32, CPU>)
            linalg.CPU.ConcatOp(%314:tensor<[1, 8, 992, 128], Float32, CPU>, %549:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%556:tensor<[1, 8, 1024, 128], Float32, CPU>)
            linalg.CPU.RepeatOp(%555:tensor<[1, 8, 128, 1024], Float32, CPU>) -> (%557:tensor<[1, 16, 128, 1024], Float32, CPU>)
            linalg.CPU.RepeatOp(%556:tensor<[1, 8, 1024, 128], Float32, CPU>) -> (%558:tensor<[1, 16, 1024, 128], Float32, CPU>)
            linalg.CPU.MatMulOp(%552:tensor<[1, 16, 32, 128], Float32, CPU>, %557:tensor<[1, 16, 128, 1024], Float32, CPU>) -> (%559:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.MulOp(%559:tensor<[1, 16, 32, 1024], Float32, CPU>, %560:tensor<[1], Float32, CPU>[constant: [0.088388346]][constant:[0.088388346]]) -> (%561:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.ReduceMinOp(%561:tensor<[1, 16, 32, 1024], Float32, CPU>) -> (%562:tensor<[1, 16, 32, 1], Float32, CPU>)
            linalg.CPU.AddOp(%562:tensor<[1, 16, 32, 1], Float32, CPU>, %563:tensor<[1], Float32, CPU>[constant: [-20]][constant:[-20]]) -> (%564:tensor<[1, 16, 32, 1], Float32, CPU>)
            linalg.CPU.EqualOp(%315:tensor<[1, 1, 32, 1024], Float32, CPU>, %565:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%566:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp(%566:tensor<[1, 1, 32, 1024], UInt8, CPU>, %561:tensor<[1, 16, 32, 1024], Float32, CPU>, %564:tensor<[1, 16, 32, 1], Float32, CPU>) -> (%567:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%567:tensor<[1, 16, 32, 1024], Float32, CPU>) -> (%568:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.MatMulOp(%568:tensor<[1, 16, 32, 1024], Float32, CPU>, %558:tensor<[1, 16, 1024, 128], Float32, CPU>) -> (%569:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%569:tensor<[1, 16, 32, 128], Float32, CPU>) -> (%570:tensor<[1, 32, 16, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%570:tensor<[1, 32, 16, 128], Float32, CPU>) -> (%570:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.6.self_attn.o_proj"](%570:tensor<[1, 32, 2048], Float32, CPU>) -> (%571:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%571:tensor<[1, 32, 2048], Float32, CPU>, %554:tensor<[1, 8, 128, 32], Float32, CPU>, %549:tensor<[1, 8, 32, 128], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.6.mlp <CPU> {
        (%573:tensor<[1, 32, 2048], Float32, CPU>) -> (%578:tensor<[1, 32, 2048], Float32, CPU>) {
            linalg.CPU.LinearOp [name="model.layers.6.mlp.gate_proj"](%573:tensor<[1, 32, 2048], Float32, CPU>) -> (%574:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.SiLUOp [name="model.layers.6.mlp.act"](%574:tensor<[1, 32, 6144], Float32, CPU>) -> (%575:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.6.mlp.up_proj"](%573:tensor<[1, 32, 2048], Float32, CPU>) -> (%576:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.MulOp(%575:tensor<[1, 32, 6144], Float32, CPU>, %576:tensor<[1, 32, 6144], Float32, CPU>) -> (%577:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.6.mlp.down_proj"](%577:tensor<[1, 32, 6144], Float32, CPU>) -> (%578:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%578:tensor<[1, 32, 2048], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.7 <CPU> {
        (%579:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%616:tensor<[1, 32, 2048], Float32, CPU>, %591:tensor<[1, 8, 128, 32], Float32, CPU>, %586:tensor<[1, 8, 32, 128], Float32, CPU>) {
            linalg.CPU.RMSNormOp [name="model.layers.7.input_layernorm"](%579:tensor<[1, 32, 2048], Float32, CPU>) -> (%580:tensor<[1, 32, 2048], Float32, CPU>)
            graph.CallGraphOp @model.layers.7.self_attn (%580:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%608:tensor<[1, 32, 2048], Float32, CPU>, %591:tensor<[1, 8, 128, 32], Float32, CPU>, %586:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.AddOp(%608:tensor<[1, 32, 2048], Float32, CPU>, %579:tensor<[1, 32, 2048], Float32, CPU>) -> (%609:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.7.post_attention_layernorm"](%609:tensor<[1, 32, 2048], Float32, CPU>) -> (%610:tensor<[1, 32, 2048], Float32, CPU>)
            graph.CallGraphOp @model.layers.7.mlp (%610:tensor<[1, 32, 2048], Float32, CPU>) -> (%615:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.AddOp(%615:tensor<[1, 32, 2048], Float32, CPU>, %609:tensor<[1, 32, 2048], Float32, CPU>) -> (%616:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%616:tensor<[1, 32, 2048], Float32, CPU>, %591:tensor<[1, 8, 128, 32], Float32, CPU>, %586:tensor<[1, 8, 32, 128], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.7.self_attn <CPU> {
        (%580:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%608:tensor<[1, 32, 2048], Float32, CPU>, %591:tensor<[1, 8, 128, 32], Float32, CPU>, %586:tensor<[1, 8, 32, 128], Float32, CPU>) {
            linalg.CPU.LinearOp [name="model.layers.7.self_attn.q_proj"](%580:tensor<[1, 32, 2048], Float32, CPU>) -> (%581:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.7.self_attn.k_proj"](%580:tensor<[1, 32, 2048], Float32, CPU>) -> (%582:tensor<[1, 32, 1024], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.7.self_attn.v_proj"](%580:tensor<[1, 32, 2048], Float32, CPU>) -> (%583:tensor<[1, 32, 1024], Float32, CPU>)
            linalg.CPU.ViewOp(%581:tensor<[1, 32, 2048], Float32, CPU>) -> (%581:tensor<[1, 32, 16, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%581:tensor<[1, 32, 16, 128], Float32, CPU>) -> (%584:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%582:tensor<[1, 32, 1024], Float32, CPU>) -> (%582:tensor<[1, 32, 8, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%582:tensor<[1, 32, 8, 128], Float32, CPU>) -> (%585:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%583:tensor<[1, 32, 1024], Float32, CPU>) -> (%583:tensor<[1, 32, 8, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%583:tensor<[1, 32, 8, 128], Float32, CPU>) -> (%586:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.7.self_attn.q_norm"](%584:tensor<[1, 16, 32, 128], Float32, CPU>) -> (%587:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.7.self_attn.k_norm"](%585:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%588:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.RoPEOp [name="model.layers.7.self_attn.q_rope"](%587:tensor<[1, 16, 32, 128], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>) -> (%589:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.RoPEOp [name="model.layers.7.self_attn.k_rope"](%588:tensor<[1, 8, 32, 128], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>) -> (%590:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%590:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%591:tensor<[1, 8, 128, 32], Float32, CPU>)
            linalg.CPU.ConcatOp(%313:tensor<[1, 8, 128, 992], Float32, CPU>, %591:tensor<[1, 8, 128, 32], Float32, CPU>) -> (%592:tensor<[1, 8, 128, 1024], Float32, CPU>)
            linalg.CPU.ConcatOp(%314:tensor<[1, 8, 992, 128], Float32, CPU>, %586:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%593:tensor<[1, 8, 1024, 128], Float32, CPU>)
            linalg.CPU.RepeatOp(%592:tensor<[1, 8, 128, 1024], Float32, CPU>) -> (%594:tensor<[1, 16, 128, 1024], Float32, CPU>)
            linalg.CPU.RepeatOp(%593:tensor<[1, 8, 1024, 128], Float32, CPU>) -> (%595:tensor<[1, 16, 1024, 128], Float32, CPU>)
            linalg.CPU.MatMulOp(%589:tensor<[1, 16, 32, 128], Float32, CPU>, %594:tensor<[1, 16, 128, 1024], Float32, CPU>) -> (%596:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.MulOp(%596:tensor<[1, 16, 32, 1024], Float32, CPU>, %597:tensor<[1], Float32, CPU>[constant: [0.088388346]][constant:[0.088388346]]) -> (%598:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.ReduceMinOp(%598:tensor<[1, 16, 32, 1024], Float32, CPU>) -> (%599:tensor<[1, 16, 32, 1], Float32, CPU>)
            linalg.CPU.AddOp(%599:tensor<[1, 16, 32, 1], Float32, CPU>, %600:tensor<[1], Float32, CPU>[constant: [-20]][constant:[-20]]) -> (%601:tensor<[1, 16, 32, 1], Float32, CPU>)
            linalg.CPU.EqualOp(%315:tensor<[1, 1, 32, 1024], Float32, CPU>, %602:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%603:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp(%603:tensor<[1, 1, 32, 1024], UInt8, CPU>, %598:tensor<[1, 16, 32, 1024], Float32, CPU>, %601:tensor<[1, 16, 32, 1], Float32, CPU>) -> (%604:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%604:tensor<[1, 16, 32, 1024], Float32, CPU>) -> (%605:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.MatMulOp(%605:tensor<[1, 16, 32, 1024], Float32, CPU>, %595:tensor<[1, 16, 1024, 128], Float32, CPU>) -> (%606:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%606:tensor<[1, 16, 32, 128], Float32, CPU>) -> (%607:tensor<[1, 32, 16, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%607:tensor<[1, 32, 16, 128], Float32, CPU>) -> (%607:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.7.self_attn.o_proj"](%607:tensor<[1, 32, 2048], Float32, CPU>) -> (%608:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%608:tensor<[1, 32, 2048], Float32, CPU>, %591:tensor<[1, 8, 128, 32], Float32, CPU>, %586:tensor<[1, 8, 32, 128], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.7.mlp <CPU> {
        (%610:tensor<[1, 32, 2048], Float32, CPU>) -> (%615:tensor<[1, 32, 2048], Float32, CPU>) {
            linalg.CPU.LinearOp [name="model.layers.7.mlp.gate_proj"](%610:tensor<[1, 32, 2048], Float32, CPU>) -> (%611:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.SiLUOp [name="model.layers.7.mlp.act"](%611:tensor<[1, 32, 6144], Float32, CPU>) -> (%612:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.7.mlp.up_proj"](%610:tensor<[1, 32, 2048], Float32, CPU>) -> (%613:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.MulOp(%612:tensor<[1, 32, 6144], Float32, CPU>, %613:tensor<[1, 32, 6144], Float32, CPU>) -> (%614:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.7.mlp.down_proj"](%614:tensor<[1, 32, 6144], Float32, CPU>) -> (%615:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%615:tensor<[1, 32, 2048], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.8 <CPU> {
        (%616:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%653:tensor<[1, 32, 2048], Float32, CPU>, %628:tensor<[1, 8, 128, 32], Float32, CPU>, %623:tensor<[1, 8, 32, 128], Float32, CPU>) {
            linalg.CPU.RMSNormOp [name="model.layers.8.input_layernorm"](%616:tensor<[1, 32, 2048], Float32, CPU>) -> (%617:tensor<[1, 32, 2048], Float32, CPU>)
            graph.CallGraphOp @model.layers.8.self_attn (%617:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%645:tensor<[1, 32, 2048], Float32, CPU>, %628:tensor<[1, 8, 128, 32], Float32, CPU>, %623:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.AddOp(%645:tensor<[1, 32, 2048], Float32, CPU>, %616:tensor<[1, 32, 2048], Float32, CPU>) -> (%646:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.8.post_attention_layernorm"](%646:tensor<[1, 32, 2048], Float32, CPU>) -> (%647:tensor<[1, 32, 2048], Float32, CPU>)
            graph.CallGraphOp @model.layers.8.mlp (%647:tensor<[1, 32, 2048], Float32, CPU>) -> (%652:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.AddOp(%652:tensor<[1, 32, 2048], Float32, CPU>, %646:tensor<[1, 32, 2048], Float32, CPU>) -> (%653:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%653:tensor<[1, 32, 2048], Float32, CPU>, %628:tensor<[1, 8, 128, 32], Float32, CPU>, %623:tensor<[1, 8, 32, 128], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.8.self_attn <CPU> {
        (%617:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%645:tensor<[1, 32, 2048], Float32, CPU>, %628:tensor<[1, 8, 128, 32], Float32, CPU>, %623:tensor<[1, 8, 32, 128], Float32, CPU>) {
            linalg.CPU.LinearOp [name="model.layers.8.self_attn.q_proj"](%617:tensor<[1, 32, 2048], Float32, CPU>) -> (%618:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.8.self_attn.k_proj"](%617:tensor<[1, 32, 2048], Float32, CPU>) -> (%619:tensor<[1, 32, 1024], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.8.self_attn.v_proj"](%617:tensor<[1, 32, 2048], Float32, CPU>) -> (%620:tensor<[1, 32, 1024], Float32, CPU>)
            linalg.CPU.ViewOp(%618:tensor<[1, 32, 2048], Float32, CPU>) -> (%618:tensor<[1, 32, 16, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%618:tensor<[1, 32, 16, 128], Float32, CPU>) -> (%621:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%619:tensor<[1, 32, 1024], Float32, CPU>) -> (%619:tensor<[1, 32, 8, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%619:tensor<[1, 32, 8, 128], Float32, CPU>) -> (%622:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%620:tensor<[1, 32, 1024], Float32, CPU>) -> (%620:tensor<[1, 32, 8, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%620:tensor<[1, 32, 8, 128], Float32, CPU>) -> (%623:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.8.self_attn.q_norm"](%621:tensor<[1, 16, 32, 128], Float32, CPU>) -> (%624:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.8.self_attn.k_norm"](%622:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%625:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.RoPEOp [name="model.layers.8.self_attn.q_rope"](%624:tensor<[1, 16, 32, 128], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>) -> (%626:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.RoPEOp [name="model.layers.8.self_attn.k_rope"](%625:tensor<[1, 8, 32, 128], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>) -> (%627:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%627:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%628:tensor<[1, 8, 128, 32], Float32, CPU>)
            linalg.CPU.ConcatOp(%313:tensor<[1, 8, 128, 992], Float32, CPU>, %628:tensor<[1, 8, 128, 32], Float32, CPU>) -> (%629:tensor<[1, 8, 128, 1024], Float32, CPU>)
            linalg.CPU.ConcatOp(%314:tensor<[1, 8, 992, 128], Float32, CPU>, %623:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%630:tensor<[1, 8, 1024, 128], Float32, CPU>)
            linalg.CPU.RepeatOp(%629:tensor<[1, 8, 128, 1024], Float32, CPU>) -> (%631:tensor<[1, 16, 128, 1024], Float32, CPU>)
            linalg.CPU.RepeatOp(%630:tensor<[1, 8, 1024, 128], Float32, CPU>) -> (%632:tensor<[1, 16, 1024, 128], Float32, CPU>)
            linalg.CPU.MatMulOp(%626:tensor<[1, 16, 32, 128], Float32, CPU>, %631:tensor<[1, 16, 128, 1024], Float32, CPU>) -> (%633:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.MulOp(%633:tensor<[1, 16, 32, 1024], Float32, CPU>, %634:tensor<[1], Float32, CPU>[constant: [0.088388346]][constant:[0.088388346]]) -> (%635:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.ReduceMinOp(%635:tensor<[1, 16, 32, 1024], Float32, CPU>) -> (%636:tensor<[1, 16, 32, 1], Float32, CPU>)
            linalg.CPU.AddOp(%636:tensor<[1, 16, 32, 1], Float32, CPU>, %637:tensor<[1], Float32, CPU>[constant: [-20]][constant:[-20]]) -> (%638:tensor<[1, 16, 32, 1], Float32, CPU>)
            linalg.CPU.EqualOp(%315:tensor<[1, 1, 32, 1024], Float32, CPU>, %639:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%640:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp(%640:tensor<[1, 1, 32, 1024], UInt8, CPU>, %635:tensor<[1, 16, 32, 1024], Float32, CPU>, %638:tensor<[1, 16, 32, 1], Float32, CPU>) -> (%641:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%641:tensor<[1, 16, 32, 1024], Float32, CPU>) -> (%642:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.MatMulOp(%642:tensor<[1, 16, 32, 1024], Float32, CPU>, %632:tensor<[1, 16, 1024, 128], Float32, CPU>) -> (%643:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%643:tensor<[1, 16, 32, 128], Float32, CPU>) -> (%644:tensor<[1, 32, 16, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%644:tensor<[1, 32, 16, 128], Float32, CPU>) -> (%644:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.8.self_attn.o_proj"](%644:tensor<[1, 32, 2048], Float32, CPU>) -> (%645:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%645:tensor<[1, 32, 2048], Float32, CPU>, %628:tensor<[1, 8, 128, 32], Float32, CPU>, %623:tensor<[1, 8, 32, 128], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.8.mlp <CPU> {
        (%647:tensor<[1, 32, 2048], Float32, CPU>) -> (%652:tensor<[1, 32, 2048], Float32, CPU>) {
            linalg.CPU.LinearOp [name="model.layers.8.mlp.gate_proj"](%647:tensor<[1, 32, 2048], Float32, CPU>) -> (%648:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.SiLUOp [name="model.layers.8.mlp.act"](%648:tensor<[1, 32, 6144], Float32, CPU>) -> (%649:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.8.mlp.up_proj"](%647:tensor<[1, 32, 2048], Float32, CPU>) -> (%650:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.MulOp(%649:tensor<[1, 32, 6144], Float32, CPU>, %650:tensor<[1, 32, 6144], Float32, CPU>) -> (%651:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.8.mlp.down_proj"](%651:tensor<[1, 32, 6144], Float32, CPU>) -> (%652:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%652:tensor<[1, 32, 2048], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.9 <CPU> {
        (%653:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%690:tensor<[1, 32, 2048], Float32, CPU>, %665:tensor<[1, 8, 128, 32], Float32, CPU>, %660:tensor<[1, 8, 32, 128], Float32, CPU>) {
            linalg.CPU.RMSNormOp [name="model.layers.9.input_layernorm"](%653:tensor<[1, 32, 2048], Float32, CPU>) -> (%654:tensor<[1, 32, 2048], Float32, CPU>)
            graph.CallGraphOp @model.layers.9.self_attn (%654:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%682:tensor<[1, 32, 2048], Float32, CPU>, %665:tensor<[1, 8, 128, 32], Float32, CPU>, %660:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.AddOp(%682:tensor<[1, 32, 2048], Float32, CPU>, %653:tensor<[1, 32, 2048], Float32, CPU>) -> (%683:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.9.post_attention_layernorm"](%683:tensor<[1, 32, 2048], Float32, CPU>) -> (%684:tensor<[1, 32, 2048], Float32, CPU>)
            graph.CallGraphOp @model.layers.9.mlp (%684:tensor<[1, 32, 2048], Float32, CPU>) -> (%689:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.AddOp(%689:tensor<[1, 32, 2048], Float32, CPU>, %683:tensor<[1, 32, 2048], Float32, CPU>) -> (%690:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%690:tensor<[1, 32, 2048], Float32, CPU>, %665:tensor<[1, 8, 128, 32], Float32, CPU>, %660:tensor<[1, 8, 32, 128], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.9.self_attn <CPU> {
        (%654:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%682:tensor<[1, 32, 2048], Float32, CPU>, %665:tensor<[1, 8, 128, 32], Float32, CPU>, %660:tensor<[1, 8, 32, 128], Float32, CPU>) {
            linalg.CPU.LinearOp [name="model.layers.9.self_attn.q_proj"](%654:tensor<[1, 32, 2048], Float32, CPU>) -> (%655:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.9.self_attn.k_proj"](%654:tensor<[1, 32, 2048], Float32, CPU>) -> (%656:tensor<[1, 32, 1024], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.9.self_attn.v_proj"](%654:tensor<[1, 32, 2048], Float32, CPU>) -> (%657:tensor<[1, 32, 1024], Float32, CPU>)
            linalg.CPU.ViewOp(%655:tensor<[1, 32, 2048], Float32, CPU>) -> (%655:tensor<[1, 32, 16, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%655:tensor<[1, 32, 16, 128], Float32, CPU>) -> (%658:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%656:tensor<[1, 32, 1024], Float32, CPU>) -> (%656:tensor<[1, 32, 8, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%656:tensor<[1, 32, 8, 128], Float32, CPU>) -> (%659:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%657:tensor<[1, 32, 1024], Float32, CPU>) -> (%657:tensor<[1, 32, 8, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%657:tensor<[1, 32, 8, 128], Float32, CPU>) -> (%660:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.9.self_attn.q_norm"](%658:tensor<[1, 16, 32, 128], Float32, CPU>) -> (%661:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.9.self_attn.k_norm"](%659:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%662:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.RoPEOp [name="model.layers.9.self_attn.q_rope"](%661:tensor<[1, 16, 32, 128], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>) -> (%663:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.RoPEOp [name="model.layers.9.self_attn.k_rope"](%662:tensor<[1, 8, 32, 128], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>) -> (%664:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%664:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%665:tensor<[1, 8, 128, 32], Float32, CPU>)
            linalg.CPU.ConcatOp(%313:tensor<[1, 8, 128, 992], Float32, CPU>, %665:tensor<[1, 8, 128, 32], Float32, CPU>) -> (%666:tensor<[1, 8, 128, 1024], Float32, CPU>)
            linalg.CPU.ConcatOp(%314:tensor<[1, 8, 992, 128], Float32, CPU>, %660:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%667:tensor<[1, 8, 1024, 128], Float32, CPU>)
            linalg.CPU.RepeatOp(%666:tensor<[1, 8, 128, 1024], Float32, CPU>) -> (%668:tensor<[1, 16, 128, 1024], Float32, CPU>)
            linalg.CPU.RepeatOp(%667:tensor<[1, 8, 1024, 128], Float32, CPU>) -> (%669:tensor<[1, 16, 1024, 128], Float32, CPU>)
            linalg.CPU.MatMulOp(%663:tensor<[1, 16, 32, 128], Float32, CPU>, %668:tensor<[1, 16, 128, 1024], Float32, CPU>) -> (%670:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.MulOp(%670:tensor<[1, 16, 32, 1024], Float32, CPU>, %671:tensor<[1], Float32, CPU>[constant: [0.088388346]][constant:[0.088388346]]) -> (%672:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.ReduceMinOp(%672:tensor<[1, 16, 32, 1024], Float32, CPU>) -> (%673:tensor<[1, 16, 32, 1], Float32, CPU>)
            linalg.CPU.AddOp(%673:tensor<[1, 16, 32, 1], Float32, CPU>, %674:tensor<[1], Float32, CPU>[constant: [-20]][constant:[-20]]) -> (%675:tensor<[1, 16, 32, 1], Float32, CPU>)
            linalg.CPU.EqualOp(%315:tensor<[1, 1, 32, 1024], Float32, CPU>, %676:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%677:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp(%677:tensor<[1, 1, 32, 1024], UInt8, CPU>, %672:tensor<[1, 16, 32, 1024], Float32, CPU>, %675:tensor<[1, 16, 32, 1], Float32, CPU>) -> (%678:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%678:tensor<[1, 16, 32, 1024], Float32, CPU>) -> (%679:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.MatMulOp(%679:tensor<[1, 16, 32, 1024], Float32, CPU>, %669:tensor<[1, 16, 1024, 128], Float32, CPU>) -> (%680:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%680:tensor<[1, 16, 32, 128], Float32, CPU>) -> (%681:tensor<[1, 32, 16, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%681:tensor<[1, 32, 16, 128], Float32, CPU>) -> (%681:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.9.self_attn.o_proj"](%681:tensor<[1, 32, 2048], Float32, CPU>) -> (%682:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%682:tensor<[1, 32, 2048], Float32, CPU>, %665:tensor<[1, 8, 128, 32], Float32, CPU>, %660:tensor<[1, 8, 32, 128], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.9.mlp <CPU> {
        (%684:tensor<[1, 32, 2048], Float32, CPU>) -> (%689:tensor<[1, 32, 2048], Float32, CPU>) {
            linalg.CPU.LinearOp [name="model.layers.9.mlp.gate_proj"](%684:tensor<[1, 32, 2048], Float32, CPU>) -> (%685:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.SiLUOp [name="model.layers.9.mlp.act"](%685:tensor<[1, 32, 6144], Float32, CPU>) -> (%686:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.9.mlp.up_proj"](%684:tensor<[1, 32, 2048], Float32, CPU>) -> (%687:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.MulOp(%686:tensor<[1, 32, 6144], Float32, CPU>, %687:tensor<[1, 32, 6144], Float32, CPU>) -> (%688:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.9.mlp.down_proj"](%688:tensor<[1, 32, 6144], Float32, CPU>) -> (%689:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%689:tensor<[1, 32, 2048], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.10 <CPU> {
        (%690:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%727:tensor<[1, 32, 2048], Float32, CPU>, %702:tensor<[1, 8, 128, 32], Float32, CPU>, %697:tensor<[1, 8, 32, 128], Float32, CPU>) {
            linalg.CPU.RMSNormOp [name="model.layers.10.input_layernorm"](%690:tensor<[1, 32, 2048], Float32, CPU>) -> (%691:tensor<[1, 32, 2048], Float32, CPU>)
            graph.CallGraphOp @model.layers.10.self_attn (%691:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%719:tensor<[1, 32, 2048], Float32, CPU>, %702:tensor<[1, 8, 128, 32], Float32, CPU>, %697:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.AddOp(%719:tensor<[1, 32, 2048], Float32, CPU>, %690:tensor<[1, 32, 2048], Float32, CPU>) -> (%720:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.10.post_attention_layernorm"](%720:tensor<[1, 32, 2048], Float32, CPU>) -> (%721:tensor<[1, 32, 2048], Float32, CPU>)
            graph.CallGraphOp @model.layers.10.mlp (%721:tensor<[1, 32, 2048], Float32, CPU>) -> (%726:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.AddOp(%726:tensor<[1, 32, 2048], Float32, CPU>, %720:tensor<[1, 32, 2048], Float32, CPU>) -> (%727:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%727:tensor<[1, 32, 2048], Float32, CPU>, %702:tensor<[1, 8, 128, 32], Float32, CPU>, %697:tensor<[1, 8, 32, 128], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.10.self_attn <CPU> {
        (%691:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%719:tensor<[1, 32, 2048], Float32, CPU>, %702:tensor<[1, 8, 128, 32], Float32, CPU>, %697:tensor<[1, 8, 32, 128], Float32, CPU>) {
            linalg.CPU.LinearOp [name="model.layers.10.self_attn.q_proj"](%691:tensor<[1, 32, 2048], Float32, CPU>) -> (%692:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.10.self_attn.k_proj"](%691:tensor<[1, 32, 2048], Float32, CPU>) -> (%693:tensor<[1, 32, 1024], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.10.self_attn.v_proj"](%691:tensor<[1, 32, 2048], Float32, CPU>) -> (%694:tensor<[1, 32, 1024], Float32, CPU>)
            linalg.CPU.ViewOp(%692:tensor<[1, 32, 2048], Float32, CPU>) -> (%692:tensor<[1, 32, 16, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%692:tensor<[1, 32, 16, 128], Float32, CPU>) -> (%695:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%693:tensor<[1, 32, 1024], Float32, CPU>) -> (%693:tensor<[1, 32, 8, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%693:tensor<[1, 32, 8, 128], Float32, CPU>) -> (%696:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%694:tensor<[1, 32, 1024], Float32, CPU>) -> (%694:tensor<[1, 32, 8, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%694:tensor<[1, 32, 8, 128], Float32, CPU>) -> (%697:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.10.self_attn.q_norm"](%695:tensor<[1, 16, 32, 128], Float32, CPU>) -> (%698:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.10.self_attn.k_norm"](%696:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%699:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.RoPEOp [name="model.layers.10.self_attn.q_rope"](%698:tensor<[1, 16, 32, 128], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>) -> (%700:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.RoPEOp [name="model.layers.10.self_attn.k_rope"](%699:tensor<[1, 8, 32, 128], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>) -> (%701:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%701:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%702:tensor<[1, 8, 128, 32], Float32, CPU>)
            linalg.CPU.ConcatOp(%313:tensor<[1, 8, 128, 992], Float32, CPU>, %702:tensor<[1, 8, 128, 32], Float32, CPU>) -> (%703:tensor<[1, 8, 128, 1024], Float32, CPU>)
            linalg.CPU.ConcatOp(%314:tensor<[1, 8, 992, 128], Float32, CPU>, %697:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%704:tensor<[1, 8, 1024, 128], Float32, CPU>)
            linalg.CPU.RepeatOp(%703:tensor<[1, 8, 128, 1024], Float32, CPU>) -> (%705:tensor<[1, 16, 128, 1024], Float32, CPU>)
            linalg.CPU.RepeatOp(%704:tensor<[1, 8, 1024, 128], Float32, CPU>) -> (%706:tensor<[1, 16, 1024, 128], Float32, CPU>)
            linalg.CPU.MatMulOp(%700:tensor<[1, 16, 32, 128], Float32, CPU>, %705:tensor<[1, 16, 128, 1024], Float32, CPU>) -> (%707:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.MulOp(%707:tensor<[1, 16, 32, 1024], Float32, CPU>, %708:tensor<[1], Float32, CPU>[constant: [0.088388346]][constant:[0.088388346]]) -> (%709:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.ReduceMinOp(%709:tensor<[1, 16, 32, 1024], Float32, CPU>) -> (%710:tensor<[1, 16, 32, 1], Float32, CPU>)
            linalg.CPU.AddOp(%710:tensor<[1, 16, 32, 1], Float32, CPU>, %711:tensor<[1], Float32, CPU>[constant: [-20]][constant:[-20]]) -> (%712:tensor<[1, 16, 32, 1], Float32, CPU>)
            linalg.CPU.EqualOp(%315:tensor<[1, 1, 32, 1024], Float32, CPU>, %713:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%714:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp(%714:tensor<[1, 1, 32, 1024], UInt8, CPU>, %709:tensor<[1, 16, 32, 1024], Float32, CPU>, %712:tensor<[1, 16, 32, 1], Float32, CPU>) -> (%715:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%715:tensor<[1, 16, 32, 1024], Float32, CPU>) -> (%716:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.MatMulOp(%716:tensor<[1, 16, 32, 1024], Float32, CPU>, %706:tensor<[1, 16, 1024, 128], Float32, CPU>) -> (%717:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%717:tensor<[1, 16, 32, 128], Float32, CPU>) -> (%718:tensor<[1, 32, 16, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%718:tensor<[1, 32, 16, 128], Float32, CPU>) -> (%718:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.10.self_attn.o_proj"](%718:tensor<[1, 32, 2048], Float32, CPU>) -> (%719:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%719:tensor<[1, 32, 2048], Float32, CPU>, %702:tensor<[1, 8, 128, 32], Float32, CPU>, %697:tensor<[1, 8, 32, 128], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.10.mlp <CPU> {
        (%721:tensor<[1, 32, 2048], Float32, CPU>) -> (%726:tensor<[1, 32, 2048], Float32, CPU>) {
            linalg.CPU.LinearOp [name="model.layers.10.mlp.gate_proj"](%721:tensor<[1, 32, 2048], Float32, CPU>) -> (%722:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.SiLUOp [name="model.layers.10.mlp.act"](%722:tensor<[1, 32, 6144], Float32, CPU>) -> (%723:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.10.mlp.up_proj"](%721:tensor<[1, 32, 2048], Float32, CPU>) -> (%724:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.MulOp(%723:tensor<[1, 32, 6144], Float32, CPU>, %724:tensor<[1, 32, 6144], Float32, CPU>) -> (%725:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.10.mlp.down_proj"](%725:tensor<[1, 32, 6144], Float32, CPU>) -> (%726:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%726:tensor<[1, 32, 2048], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.11 <CPU> {
        (%727:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%764:tensor<[1, 32, 2048], Float32, CPU>, %739:tensor<[1, 8, 128, 32], Float32, CPU>, %734:tensor<[1, 8, 32, 128], Float32, CPU>) {
            linalg.CPU.RMSNormOp [name="model.layers.11.input_layernorm"](%727:tensor<[1, 32, 2048], Float32, CPU>) -> (%728:tensor<[1, 32, 2048], Float32, CPU>)
            graph.CallGraphOp @model.layers.11.self_attn (%728:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%756:tensor<[1, 32, 2048], Float32, CPU>, %739:tensor<[1, 8, 128, 32], Float32, CPU>, %734:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.AddOp(%756:tensor<[1, 32, 2048], Float32, CPU>, %727:tensor<[1, 32, 2048], Float32, CPU>) -> (%757:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.11.post_attention_layernorm"](%757:tensor<[1, 32, 2048], Float32, CPU>) -> (%758:tensor<[1, 32, 2048], Float32, CPU>)
            graph.CallGraphOp @model.layers.11.mlp (%758:tensor<[1, 32, 2048], Float32, CPU>) -> (%763:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.AddOp(%763:tensor<[1, 32, 2048], Float32, CPU>, %757:tensor<[1, 32, 2048], Float32, CPU>) -> (%764:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%764:tensor<[1, 32, 2048], Float32, CPU>, %739:tensor<[1, 8, 128, 32], Float32, CPU>, %734:tensor<[1, 8, 32, 128], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.11.self_attn <CPU> {
        (%728:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%756:tensor<[1, 32, 2048], Float32, CPU>, %739:tensor<[1, 8, 128, 32], Float32, CPU>, %734:tensor<[1, 8, 32, 128], Float32, CPU>) {
            linalg.CPU.LinearOp [name="model.layers.11.self_attn.q_proj"](%728:tensor<[1, 32, 2048], Float32, CPU>) -> (%729:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.11.self_attn.k_proj"](%728:tensor<[1, 32, 2048], Float32, CPU>) -> (%730:tensor<[1, 32, 1024], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.11.self_attn.v_proj"](%728:tensor<[1, 32, 2048], Float32, CPU>) -> (%731:tensor<[1, 32, 1024], Float32, CPU>)
            linalg.CPU.ViewOp(%729:tensor<[1, 32, 2048], Float32, CPU>) -> (%729:tensor<[1, 32, 16, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%729:tensor<[1, 32, 16, 128], Float32, CPU>) -> (%732:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%730:tensor<[1, 32, 1024], Float32, CPU>) -> (%730:tensor<[1, 32, 8, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%730:tensor<[1, 32, 8, 128], Float32, CPU>) -> (%733:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%731:tensor<[1, 32, 1024], Float32, CPU>) -> (%731:tensor<[1, 32, 8, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%731:tensor<[1, 32, 8, 128], Float32, CPU>) -> (%734:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.11.self_attn.q_norm"](%732:tensor<[1, 16, 32, 128], Float32, CPU>) -> (%735:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.11.self_attn.k_norm"](%733:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%736:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.RoPEOp [name="model.layers.11.self_attn.q_rope"](%735:tensor<[1, 16, 32, 128], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>) -> (%737:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.RoPEOp [name="model.layers.11.self_attn.k_rope"](%736:tensor<[1, 8, 32, 128], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>) -> (%738:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%738:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%739:tensor<[1, 8, 128, 32], Float32, CPU>)
            linalg.CPU.ConcatOp(%313:tensor<[1, 8, 128, 992], Float32, CPU>, %739:tensor<[1, 8, 128, 32], Float32, CPU>) -> (%740:tensor<[1, 8, 128, 1024], Float32, CPU>)
            linalg.CPU.ConcatOp(%314:tensor<[1, 8, 992, 128], Float32, CPU>, %734:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%741:tensor<[1, 8, 1024, 128], Float32, CPU>)
            linalg.CPU.RepeatOp(%740:tensor<[1, 8, 128, 1024], Float32, CPU>) -> (%742:tensor<[1, 16, 128, 1024], Float32, CPU>)
            linalg.CPU.RepeatOp(%741:tensor<[1, 8, 1024, 128], Float32, CPU>) -> (%743:tensor<[1, 16, 1024, 128], Float32, CPU>)
            linalg.CPU.MatMulOp(%737:tensor<[1, 16, 32, 128], Float32, CPU>, %742:tensor<[1, 16, 128, 1024], Float32, CPU>) -> (%744:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.MulOp(%744:tensor<[1, 16, 32, 1024], Float32, CPU>, %745:tensor<[1], Float32, CPU>[constant: [0.088388346]][constant:[0.088388346]]) -> (%746:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.ReduceMinOp(%746:tensor<[1, 16, 32, 1024], Float32, CPU>) -> (%747:tensor<[1, 16, 32, 1], Float32, CPU>)
            linalg.CPU.AddOp(%747:tensor<[1, 16, 32, 1], Float32, CPU>, %748:tensor<[1], Float32, CPU>[constant: [-20]][constant:[-20]]) -> (%749:tensor<[1, 16, 32, 1], Float32, CPU>)
            linalg.CPU.EqualOp(%315:tensor<[1, 1, 32, 1024], Float32, CPU>, %750:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%751:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp(%751:tensor<[1, 1, 32, 1024], UInt8, CPU>, %746:tensor<[1, 16, 32, 1024], Float32, CPU>, %749:tensor<[1, 16, 32, 1], Float32, CPU>) -> (%752:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%752:tensor<[1, 16, 32, 1024], Float32, CPU>) -> (%753:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.MatMulOp(%753:tensor<[1, 16, 32, 1024], Float32, CPU>, %743:tensor<[1, 16, 1024, 128], Float32, CPU>) -> (%754:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%754:tensor<[1, 16, 32, 128], Float32, CPU>) -> (%755:tensor<[1, 32, 16, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%755:tensor<[1, 32, 16, 128], Float32, CPU>) -> (%755:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.11.self_attn.o_proj"](%755:tensor<[1, 32, 2048], Float32, CPU>) -> (%756:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%756:tensor<[1, 32, 2048], Float32, CPU>, %739:tensor<[1, 8, 128, 32], Float32, CPU>, %734:tensor<[1, 8, 32, 128], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.11.mlp <CPU> {
        (%758:tensor<[1, 32, 2048], Float32, CPU>) -> (%763:tensor<[1, 32, 2048], Float32, CPU>) {
            linalg.CPU.LinearOp [name="model.layers.11.mlp.gate_proj"](%758:tensor<[1, 32, 2048], Float32, CPU>) -> (%759:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.SiLUOp [name="model.layers.11.mlp.act"](%759:tensor<[1, 32, 6144], Float32, CPU>) -> (%760:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.11.mlp.up_proj"](%758:tensor<[1, 32, 2048], Float32, CPU>) -> (%761:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.MulOp(%760:tensor<[1, 32, 6144], Float32, CPU>, %761:tensor<[1, 32, 6144], Float32, CPU>) -> (%762:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.11.mlp.down_proj"](%762:tensor<[1, 32, 6144], Float32, CPU>) -> (%763:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%763:tensor<[1, 32, 2048], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.12 <CPU> {
        (%764:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%801:tensor<[1, 32, 2048], Float32, CPU>, %776:tensor<[1, 8, 128, 32], Float32, CPU>, %771:tensor<[1, 8, 32, 128], Float32, CPU>) {
            linalg.CPU.RMSNormOp [name="model.layers.12.input_layernorm"](%764:tensor<[1, 32, 2048], Float32, CPU>) -> (%765:tensor<[1, 32, 2048], Float32, CPU>)
            graph.CallGraphOp @model.layers.12.self_attn (%765:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%793:tensor<[1, 32, 2048], Float32, CPU>, %776:tensor<[1, 8, 128, 32], Float32, CPU>, %771:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.AddOp(%793:tensor<[1, 32, 2048], Float32, CPU>, %764:tensor<[1, 32, 2048], Float32, CPU>) -> (%794:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.12.post_attention_layernorm"](%794:tensor<[1, 32, 2048], Float32, CPU>) -> (%795:tensor<[1, 32, 2048], Float32, CPU>)
            graph.CallGraphOp @model.layers.12.mlp (%795:tensor<[1, 32, 2048], Float32, CPU>) -> (%800:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.AddOp(%800:tensor<[1, 32, 2048], Float32, CPU>, %794:tensor<[1, 32, 2048], Float32, CPU>) -> (%801:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%801:tensor<[1, 32, 2048], Float32, CPU>, %776:tensor<[1, 8, 128, 32], Float32, CPU>, %771:tensor<[1, 8, 32, 128], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.12.self_attn <CPU> {
        (%765:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%793:tensor<[1, 32, 2048], Float32, CPU>, %776:tensor<[1, 8, 128, 32], Float32, CPU>, %771:tensor<[1, 8, 32, 128], Float32, CPU>) {
            linalg.CPU.LinearOp [name="model.layers.12.self_attn.q_proj"](%765:tensor<[1, 32, 2048], Float32, CPU>) -> (%766:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.12.self_attn.k_proj"](%765:tensor<[1, 32, 2048], Float32, CPU>) -> (%767:tensor<[1, 32, 1024], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.12.self_attn.v_proj"](%765:tensor<[1, 32, 2048], Float32, CPU>) -> (%768:tensor<[1, 32, 1024], Float32, CPU>)
            linalg.CPU.ViewOp(%766:tensor<[1, 32, 2048], Float32, CPU>) -> (%766:tensor<[1, 32, 16, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%766:tensor<[1, 32, 16, 128], Float32, CPU>) -> (%769:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%767:tensor<[1, 32, 1024], Float32, CPU>) -> (%767:tensor<[1, 32, 8, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%767:tensor<[1, 32, 8, 128], Float32, CPU>) -> (%770:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%768:tensor<[1, 32, 1024], Float32, CPU>) -> (%768:tensor<[1, 32, 8, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%768:tensor<[1, 32, 8, 128], Float32, CPU>) -> (%771:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.12.self_attn.q_norm"](%769:tensor<[1, 16, 32, 128], Float32, CPU>) -> (%772:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.12.self_attn.k_norm"](%770:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%773:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.RoPEOp [name="model.layers.12.self_attn.q_rope"](%772:tensor<[1, 16, 32, 128], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>) -> (%774:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.RoPEOp [name="model.layers.12.self_attn.k_rope"](%773:tensor<[1, 8, 32, 128], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>) -> (%775:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%775:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%776:tensor<[1, 8, 128, 32], Float32, CPU>)
            linalg.CPU.ConcatOp(%313:tensor<[1, 8, 128, 992], Float32, CPU>, %776:tensor<[1, 8, 128, 32], Float32, CPU>) -> (%777:tensor<[1, 8, 128, 1024], Float32, CPU>)
            linalg.CPU.ConcatOp(%314:tensor<[1, 8, 992, 128], Float32, CPU>, %771:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%778:tensor<[1, 8, 1024, 128], Float32, CPU>)
            linalg.CPU.RepeatOp(%777:tensor<[1, 8, 128, 1024], Float32, CPU>) -> (%779:tensor<[1, 16, 128, 1024], Float32, CPU>)
            linalg.CPU.RepeatOp(%778:tensor<[1, 8, 1024, 128], Float32, CPU>) -> (%780:tensor<[1, 16, 1024, 128], Float32, CPU>)
            linalg.CPU.MatMulOp(%774:tensor<[1, 16, 32, 128], Float32, CPU>, %779:tensor<[1, 16, 128, 1024], Float32, CPU>) -> (%781:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.MulOp(%781:tensor<[1, 16, 32, 1024], Float32, CPU>, %782:tensor<[1], Float32, CPU>[constant: [0.088388346]][constant:[0.088388346]]) -> (%783:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.ReduceMinOp(%783:tensor<[1, 16, 32, 1024], Float32, CPU>) -> (%784:tensor<[1, 16, 32, 1], Float32, CPU>)
            linalg.CPU.AddOp(%784:tensor<[1, 16, 32, 1], Float32, CPU>, %785:tensor<[1], Float32, CPU>[constant: [-20]][constant:[-20]]) -> (%786:tensor<[1, 16, 32, 1], Float32, CPU>)
            linalg.CPU.EqualOp(%315:tensor<[1, 1, 32, 1024], Float32, CPU>, %787:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%788:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp(%788:tensor<[1, 1, 32, 1024], UInt8, CPU>, %783:tensor<[1, 16, 32, 1024], Float32, CPU>, %786:tensor<[1, 16, 32, 1], Float32, CPU>) -> (%789:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%789:tensor<[1, 16, 32, 1024], Float32, CPU>) -> (%790:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.MatMulOp(%790:tensor<[1, 16, 32, 1024], Float32, CPU>, %780:tensor<[1, 16, 1024, 128], Float32, CPU>) -> (%791:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%791:tensor<[1, 16, 32, 128], Float32, CPU>) -> (%792:tensor<[1, 32, 16, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%792:tensor<[1, 32, 16, 128], Float32, CPU>) -> (%792:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.12.self_attn.o_proj"](%792:tensor<[1, 32, 2048], Float32, CPU>) -> (%793:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%793:tensor<[1, 32, 2048], Float32, CPU>, %776:tensor<[1, 8, 128, 32], Float32, CPU>, %771:tensor<[1, 8, 32, 128], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.12.mlp <CPU> {
        (%795:tensor<[1, 32, 2048], Float32, CPU>) -> (%800:tensor<[1, 32, 2048], Float32, CPU>) {
            linalg.CPU.LinearOp [name="model.layers.12.mlp.gate_proj"](%795:tensor<[1, 32, 2048], Float32, CPU>) -> (%796:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.SiLUOp [name="model.layers.12.mlp.act"](%796:tensor<[1, 32, 6144], Float32, CPU>) -> (%797:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.12.mlp.up_proj"](%795:tensor<[1, 32, 2048], Float32, CPU>) -> (%798:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.MulOp(%797:tensor<[1, 32, 6144], Float32, CPU>, %798:tensor<[1, 32, 6144], Float32, CPU>) -> (%799:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.12.mlp.down_proj"](%799:tensor<[1, 32, 6144], Float32, CPU>) -> (%800:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%800:tensor<[1, 32, 2048], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.13 <CPU> {
        (%801:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%838:tensor<[1, 32, 2048], Float32, CPU>, %813:tensor<[1, 8, 128, 32], Float32, CPU>, %808:tensor<[1, 8, 32, 128], Float32, CPU>) {
            linalg.CPU.RMSNormOp [name="model.layers.13.input_layernorm"](%801:tensor<[1, 32, 2048], Float32, CPU>) -> (%802:tensor<[1, 32, 2048], Float32, CPU>)
            graph.CallGraphOp @model.layers.13.self_attn (%802:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%830:tensor<[1, 32, 2048], Float32, CPU>, %813:tensor<[1, 8, 128, 32], Float32, CPU>, %808:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.AddOp(%830:tensor<[1, 32, 2048], Float32, CPU>, %801:tensor<[1, 32, 2048], Float32, CPU>) -> (%831:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.13.post_attention_layernorm"](%831:tensor<[1, 32, 2048], Float32, CPU>) -> (%832:tensor<[1, 32, 2048], Float32, CPU>)
            graph.CallGraphOp @model.layers.13.mlp (%832:tensor<[1, 32, 2048], Float32, CPU>) -> (%837:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.AddOp(%837:tensor<[1, 32, 2048], Float32, CPU>, %831:tensor<[1, 32, 2048], Float32, CPU>) -> (%838:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%838:tensor<[1, 32, 2048], Float32, CPU>, %813:tensor<[1, 8, 128, 32], Float32, CPU>, %808:tensor<[1, 8, 32, 128], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.13.self_attn <CPU> {
        (%802:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%830:tensor<[1, 32, 2048], Float32, CPU>, %813:tensor<[1, 8, 128, 32], Float32, CPU>, %808:tensor<[1, 8, 32, 128], Float32, CPU>) {
            linalg.CPU.LinearOp [name="model.layers.13.self_attn.q_proj"](%802:tensor<[1, 32, 2048], Float32, CPU>) -> (%803:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.13.self_attn.k_proj"](%802:tensor<[1, 32, 2048], Float32, CPU>) -> (%804:tensor<[1, 32, 1024], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.13.self_attn.v_proj"](%802:tensor<[1, 32, 2048], Float32, CPU>) -> (%805:tensor<[1, 32, 1024], Float32, CPU>)
            linalg.CPU.ViewOp(%803:tensor<[1, 32, 2048], Float32, CPU>) -> (%803:tensor<[1, 32, 16, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%803:tensor<[1, 32, 16, 128], Float32, CPU>) -> (%806:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%804:tensor<[1, 32, 1024], Float32, CPU>) -> (%804:tensor<[1, 32, 8, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%804:tensor<[1, 32, 8, 128], Float32, CPU>) -> (%807:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%805:tensor<[1, 32, 1024], Float32, CPU>) -> (%805:tensor<[1, 32, 8, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%805:tensor<[1, 32, 8, 128], Float32, CPU>) -> (%808:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.13.self_attn.q_norm"](%806:tensor<[1, 16, 32, 128], Float32, CPU>) -> (%809:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.13.self_attn.k_norm"](%807:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%810:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.RoPEOp [name="model.layers.13.self_attn.q_rope"](%809:tensor<[1, 16, 32, 128], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>) -> (%811:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.RoPEOp [name="model.layers.13.self_attn.k_rope"](%810:tensor<[1, 8, 32, 128], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>) -> (%812:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%812:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%813:tensor<[1, 8, 128, 32], Float32, CPU>)
            linalg.CPU.ConcatOp(%313:tensor<[1, 8, 128, 992], Float32, CPU>, %813:tensor<[1, 8, 128, 32], Float32, CPU>) -> (%814:tensor<[1, 8, 128, 1024], Float32, CPU>)
            linalg.CPU.ConcatOp(%314:tensor<[1, 8, 992, 128], Float32, CPU>, %808:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%815:tensor<[1, 8, 1024, 128], Float32, CPU>)
            linalg.CPU.RepeatOp(%814:tensor<[1, 8, 128, 1024], Float32, CPU>) -> (%816:tensor<[1, 16, 128, 1024], Float32, CPU>)
            linalg.CPU.RepeatOp(%815:tensor<[1, 8, 1024, 128], Float32, CPU>) -> (%817:tensor<[1, 16, 1024, 128], Float32, CPU>)
            linalg.CPU.MatMulOp(%811:tensor<[1, 16, 32, 128], Float32, CPU>, %816:tensor<[1, 16, 128, 1024], Float32, CPU>) -> (%818:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.MulOp(%818:tensor<[1, 16, 32, 1024], Float32, CPU>, %819:tensor<[1], Float32, CPU>[constant: [0.088388346]][constant:[0.088388346]]) -> (%820:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.ReduceMinOp(%820:tensor<[1, 16, 32, 1024], Float32, CPU>) -> (%821:tensor<[1, 16, 32, 1], Float32, CPU>)
            linalg.CPU.AddOp(%821:tensor<[1, 16, 32, 1], Float32, CPU>, %822:tensor<[1], Float32, CPU>[constant: [-20]][constant:[-20]]) -> (%823:tensor<[1, 16, 32, 1], Float32, CPU>)
            linalg.CPU.EqualOp(%315:tensor<[1, 1, 32, 1024], Float32, CPU>, %824:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%825:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp(%825:tensor<[1, 1, 32, 1024], UInt8, CPU>, %820:tensor<[1, 16, 32, 1024], Float32, CPU>, %823:tensor<[1, 16, 32, 1], Float32, CPU>) -> (%826:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%826:tensor<[1, 16, 32, 1024], Float32, CPU>) -> (%827:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.MatMulOp(%827:tensor<[1, 16, 32, 1024], Float32, CPU>, %817:tensor<[1, 16, 1024, 128], Float32, CPU>) -> (%828:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%828:tensor<[1, 16, 32, 128], Float32, CPU>) -> (%829:tensor<[1, 32, 16, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%829:tensor<[1, 32, 16, 128], Float32, CPU>) -> (%829:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.13.self_attn.o_proj"](%829:tensor<[1, 32, 2048], Float32, CPU>) -> (%830:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%830:tensor<[1, 32, 2048], Float32, CPU>, %813:tensor<[1, 8, 128, 32], Float32, CPU>, %808:tensor<[1, 8, 32, 128], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.13.mlp <CPU> {
        (%832:tensor<[1, 32, 2048], Float32, CPU>) -> (%837:tensor<[1, 32, 2048], Float32, CPU>) {
            linalg.CPU.LinearOp [name="model.layers.13.mlp.gate_proj"](%832:tensor<[1, 32, 2048], Float32, CPU>) -> (%833:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.SiLUOp [name="model.layers.13.mlp.act"](%833:tensor<[1, 32, 6144], Float32, CPU>) -> (%834:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.13.mlp.up_proj"](%832:tensor<[1, 32, 2048], Float32, CPU>) -> (%835:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.MulOp(%834:tensor<[1, 32, 6144], Float32, CPU>, %835:tensor<[1, 32, 6144], Float32, CPU>) -> (%836:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.13.mlp.down_proj"](%836:tensor<[1, 32, 6144], Float32, CPU>) -> (%837:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%837:tensor<[1, 32, 2048], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.14 <CPU> {
        (%838:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%875:tensor<[1, 32, 2048], Float32, CPU>, %850:tensor<[1, 8, 128, 32], Float32, CPU>, %845:tensor<[1, 8, 32, 128], Float32, CPU>) {
            linalg.CPU.RMSNormOp [name="model.layers.14.input_layernorm"](%838:tensor<[1, 32, 2048], Float32, CPU>) -> (%839:tensor<[1, 32, 2048], Float32, CPU>)
            graph.CallGraphOp @model.layers.14.self_attn (%839:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%867:tensor<[1, 32, 2048], Float32, CPU>, %850:tensor<[1, 8, 128, 32], Float32, CPU>, %845:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.AddOp(%867:tensor<[1, 32, 2048], Float32, CPU>, %838:tensor<[1, 32, 2048], Float32, CPU>) -> (%868:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.14.post_attention_layernorm"](%868:tensor<[1, 32, 2048], Float32, CPU>) -> (%869:tensor<[1, 32, 2048], Float32, CPU>)
            graph.CallGraphOp @model.layers.14.mlp (%869:tensor<[1, 32, 2048], Float32, CPU>) -> (%874:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.AddOp(%874:tensor<[1, 32, 2048], Float32, CPU>, %868:tensor<[1, 32, 2048], Float32, CPU>) -> (%875:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%875:tensor<[1, 32, 2048], Float32, CPU>, %850:tensor<[1, 8, 128, 32], Float32, CPU>, %845:tensor<[1, 8, 32, 128], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.14.self_attn <CPU> {
        (%839:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%867:tensor<[1, 32, 2048], Float32, CPU>, %850:tensor<[1, 8, 128, 32], Float32, CPU>, %845:tensor<[1, 8, 32, 128], Float32, CPU>) {
            linalg.CPU.LinearOp [name="model.layers.14.self_attn.q_proj"](%839:tensor<[1, 32, 2048], Float32, CPU>) -> (%840:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.14.self_attn.k_proj"](%839:tensor<[1, 32, 2048], Float32, CPU>) -> (%841:tensor<[1, 32, 1024], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.14.self_attn.v_proj"](%839:tensor<[1, 32, 2048], Float32, CPU>) -> (%842:tensor<[1, 32, 1024], Float32, CPU>)
            linalg.CPU.ViewOp(%840:tensor<[1, 32, 2048], Float32, CPU>) -> (%840:tensor<[1, 32, 16, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%840:tensor<[1, 32, 16, 128], Float32, CPU>) -> (%843:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%841:tensor<[1, 32, 1024], Float32, CPU>) -> (%841:tensor<[1, 32, 8, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%841:tensor<[1, 32, 8, 128], Float32, CPU>) -> (%844:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%842:tensor<[1, 32, 1024], Float32, CPU>) -> (%842:tensor<[1, 32, 8, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%842:tensor<[1, 32, 8, 128], Float32, CPU>) -> (%845:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.14.self_attn.q_norm"](%843:tensor<[1, 16, 32, 128], Float32, CPU>) -> (%846:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.14.self_attn.k_norm"](%844:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%847:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.RoPEOp [name="model.layers.14.self_attn.q_rope"](%846:tensor<[1, 16, 32, 128], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>) -> (%848:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.RoPEOp [name="model.layers.14.self_attn.k_rope"](%847:tensor<[1, 8, 32, 128], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>) -> (%849:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%849:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%850:tensor<[1, 8, 128, 32], Float32, CPU>)
            linalg.CPU.ConcatOp(%313:tensor<[1, 8, 128, 992], Float32, CPU>, %850:tensor<[1, 8, 128, 32], Float32, CPU>) -> (%851:tensor<[1, 8, 128, 1024], Float32, CPU>)
            linalg.CPU.ConcatOp(%314:tensor<[1, 8, 992, 128], Float32, CPU>, %845:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%852:tensor<[1, 8, 1024, 128], Float32, CPU>)
            linalg.CPU.RepeatOp(%851:tensor<[1, 8, 128, 1024], Float32, CPU>) -> (%853:tensor<[1, 16, 128, 1024], Float32, CPU>)
            linalg.CPU.RepeatOp(%852:tensor<[1, 8, 1024, 128], Float32, CPU>) -> (%854:tensor<[1, 16, 1024, 128], Float32, CPU>)
            linalg.CPU.MatMulOp(%848:tensor<[1, 16, 32, 128], Float32, CPU>, %853:tensor<[1, 16, 128, 1024], Float32, CPU>) -> (%855:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.MulOp(%855:tensor<[1, 16, 32, 1024], Float32, CPU>, %856:tensor<[1], Float32, CPU>[constant: [0.088388346]][constant:[0.088388346]]) -> (%857:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.ReduceMinOp(%857:tensor<[1, 16, 32, 1024], Float32, CPU>) -> (%858:tensor<[1, 16, 32, 1], Float32, CPU>)
            linalg.CPU.AddOp(%858:tensor<[1, 16, 32, 1], Float32, CPU>, %859:tensor<[1], Float32, CPU>[constant: [-20]][constant:[-20]]) -> (%860:tensor<[1, 16, 32, 1], Float32, CPU>)
            linalg.CPU.EqualOp(%315:tensor<[1, 1, 32, 1024], Float32, CPU>, %861:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%862:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp(%862:tensor<[1, 1, 32, 1024], UInt8, CPU>, %857:tensor<[1, 16, 32, 1024], Float32, CPU>, %860:tensor<[1, 16, 32, 1], Float32, CPU>) -> (%863:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%863:tensor<[1, 16, 32, 1024], Float32, CPU>) -> (%864:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.MatMulOp(%864:tensor<[1, 16, 32, 1024], Float32, CPU>, %854:tensor<[1, 16, 1024, 128], Float32, CPU>) -> (%865:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%865:tensor<[1, 16, 32, 128], Float32, CPU>) -> (%866:tensor<[1, 32, 16, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%866:tensor<[1, 32, 16, 128], Float32, CPU>) -> (%866:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.14.self_attn.o_proj"](%866:tensor<[1, 32, 2048], Float32, CPU>) -> (%867:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%867:tensor<[1, 32, 2048], Float32, CPU>, %850:tensor<[1, 8, 128, 32], Float32, CPU>, %845:tensor<[1, 8, 32, 128], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.14.mlp <CPU> {
        (%869:tensor<[1, 32, 2048], Float32, CPU>) -> (%874:tensor<[1, 32, 2048], Float32, CPU>) {
            linalg.CPU.LinearOp [name="model.layers.14.mlp.gate_proj"](%869:tensor<[1, 32, 2048], Float32, CPU>) -> (%870:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.SiLUOp [name="model.layers.14.mlp.act"](%870:tensor<[1, 32, 6144], Float32, CPU>) -> (%871:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.14.mlp.up_proj"](%869:tensor<[1, 32, 2048], Float32, CPU>) -> (%872:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.MulOp(%871:tensor<[1, 32, 6144], Float32, CPU>, %872:tensor<[1, 32, 6144], Float32, CPU>) -> (%873:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.14.mlp.down_proj"](%873:tensor<[1, 32, 6144], Float32, CPU>) -> (%874:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%874:tensor<[1, 32, 2048], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.15 <CPU> {
        (%875:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%912:tensor<[1, 32, 2048], Float32, CPU>, %887:tensor<[1, 8, 128, 32], Float32, CPU>, %882:tensor<[1, 8, 32, 128], Float32, CPU>) {
            linalg.CPU.RMSNormOp [name="model.layers.15.input_layernorm"](%875:tensor<[1, 32, 2048], Float32, CPU>) -> (%876:tensor<[1, 32, 2048], Float32, CPU>)
            graph.CallGraphOp @model.layers.15.self_attn (%876:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%904:tensor<[1, 32, 2048], Float32, CPU>, %887:tensor<[1, 8, 128, 32], Float32, CPU>, %882:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.AddOp(%904:tensor<[1, 32, 2048], Float32, CPU>, %875:tensor<[1, 32, 2048], Float32, CPU>) -> (%905:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.15.post_attention_layernorm"](%905:tensor<[1, 32, 2048], Float32, CPU>) -> (%906:tensor<[1, 32, 2048], Float32, CPU>)
            graph.CallGraphOp @model.layers.15.mlp (%906:tensor<[1, 32, 2048], Float32, CPU>) -> (%911:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.AddOp(%911:tensor<[1, 32, 2048], Float32, CPU>, %905:tensor<[1, 32, 2048], Float32, CPU>) -> (%912:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%912:tensor<[1, 32, 2048], Float32, CPU>, %887:tensor<[1, 8, 128, 32], Float32, CPU>, %882:tensor<[1, 8, 32, 128], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.15.self_attn <CPU> {
        (%876:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%904:tensor<[1, 32, 2048], Float32, CPU>, %887:tensor<[1, 8, 128, 32], Float32, CPU>, %882:tensor<[1, 8, 32, 128], Float32, CPU>) {
            linalg.CPU.LinearOp [name="model.layers.15.self_attn.q_proj"](%876:tensor<[1, 32, 2048], Float32, CPU>) -> (%877:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.15.self_attn.k_proj"](%876:tensor<[1, 32, 2048], Float32, CPU>) -> (%878:tensor<[1, 32, 1024], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.15.self_attn.v_proj"](%876:tensor<[1, 32, 2048], Float32, CPU>) -> (%879:tensor<[1, 32, 1024], Float32, CPU>)
            linalg.CPU.ViewOp(%877:tensor<[1, 32, 2048], Float32, CPU>) -> (%877:tensor<[1, 32, 16, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%877:tensor<[1, 32, 16, 128], Float32, CPU>) -> (%880:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%878:tensor<[1, 32, 1024], Float32, CPU>) -> (%878:tensor<[1, 32, 8, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%878:tensor<[1, 32, 8, 128], Float32, CPU>) -> (%881:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%879:tensor<[1, 32, 1024], Float32, CPU>) -> (%879:tensor<[1, 32, 8, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%879:tensor<[1, 32, 8, 128], Float32, CPU>) -> (%882:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.15.self_attn.q_norm"](%880:tensor<[1, 16, 32, 128], Float32, CPU>) -> (%883:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.15.self_attn.k_norm"](%881:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%884:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.RoPEOp [name="model.layers.15.self_attn.q_rope"](%883:tensor<[1, 16, 32, 128], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>) -> (%885:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.RoPEOp [name="model.layers.15.self_attn.k_rope"](%884:tensor<[1, 8, 32, 128], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>) -> (%886:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%886:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%887:tensor<[1, 8, 128, 32], Float32, CPU>)
            linalg.CPU.ConcatOp(%313:tensor<[1, 8, 128, 992], Float32, CPU>, %887:tensor<[1, 8, 128, 32], Float32, CPU>) -> (%888:tensor<[1, 8, 128, 1024], Float32, CPU>)
            linalg.CPU.ConcatOp(%314:tensor<[1, 8, 992, 128], Float32, CPU>, %882:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%889:tensor<[1, 8, 1024, 128], Float32, CPU>)
            linalg.CPU.RepeatOp(%888:tensor<[1, 8, 128, 1024], Float32, CPU>) -> (%890:tensor<[1, 16, 128, 1024], Float32, CPU>)
            linalg.CPU.RepeatOp(%889:tensor<[1, 8, 1024, 128], Float32, CPU>) -> (%891:tensor<[1, 16, 1024, 128], Float32, CPU>)
            linalg.CPU.MatMulOp(%885:tensor<[1, 16, 32, 128], Float32, CPU>, %890:tensor<[1, 16, 128, 1024], Float32, CPU>) -> (%892:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.MulOp(%892:tensor<[1, 16, 32, 1024], Float32, CPU>, %893:tensor<[1], Float32, CPU>[constant: [0.088388346]][constant:[0.088388346]]) -> (%894:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.ReduceMinOp(%894:tensor<[1, 16, 32, 1024], Float32, CPU>) -> (%895:tensor<[1, 16, 32, 1], Float32, CPU>)
            linalg.CPU.AddOp(%895:tensor<[1, 16, 32, 1], Float32, CPU>, %896:tensor<[1], Float32, CPU>[constant: [-20]][constant:[-20]]) -> (%897:tensor<[1, 16, 32, 1], Float32, CPU>)
            linalg.CPU.EqualOp(%315:tensor<[1, 1, 32, 1024], Float32, CPU>, %898:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%899:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp(%899:tensor<[1, 1, 32, 1024], UInt8, CPU>, %894:tensor<[1, 16, 32, 1024], Float32, CPU>, %897:tensor<[1, 16, 32, 1], Float32, CPU>) -> (%900:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%900:tensor<[1, 16, 32, 1024], Float32, CPU>) -> (%901:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.MatMulOp(%901:tensor<[1, 16, 32, 1024], Float32, CPU>, %891:tensor<[1, 16, 1024, 128], Float32, CPU>) -> (%902:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%902:tensor<[1, 16, 32, 128], Float32, CPU>) -> (%903:tensor<[1, 32, 16, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%903:tensor<[1, 32, 16, 128], Float32, CPU>) -> (%903:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.15.self_attn.o_proj"](%903:tensor<[1, 32, 2048], Float32, CPU>) -> (%904:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%904:tensor<[1, 32, 2048], Float32, CPU>, %887:tensor<[1, 8, 128, 32], Float32, CPU>, %882:tensor<[1, 8, 32, 128], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.15.mlp <CPU> {
        (%906:tensor<[1, 32, 2048], Float32, CPU>) -> (%911:tensor<[1, 32, 2048], Float32, CPU>) {
            linalg.CPU.LinearOp [name="model.layers.15.mlp.gate_proj"](%906:tensor<[1, 32, 2048], Float32, CPU>) -> (%907:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.SiLUOp [name="model.layers.15.mlp.act"](%907:tensor<[1, 32, 6144], Float32, CPU>) -> (%908:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.15.mlp.up_proj"](%906:tensor<[1, 32, 2048], Float32, CPU>) -> (%909:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.MulOp(%908:tensor<[1, 32, 6144], Float32, CPU>, %909:tensor<[1, 32, 6144], Float32, CPU>) -> (%910:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.15.mlp.down_proj"](%910:tensor<[1, 32, 6144], Float32, CPU>) -> (%911:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%911:tensor<[1, 32, 2048], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.16 <CPU> {
        (%912:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%949:tensor<[1, 32, 2048], Float32, CPU>, %924:tensor<[1, 8, 128, 32], Float32, CPU>, %919:tensor<[1, 8, 32, 128], Float32, CPU>) {
            linalg.CPU.RMSNormOp [name="model.layers.16.input_layernorm"](%912:tensor<[1, 32, 2048], Float32, CPU>) -> (%913:tensor<[1, 32, 2048], Float32, CPU>)
            graph.CallGraphOp @model.layers.16.self_attn (%913:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%941:tensor<[1, 32, 2048], Float32, CPU>, %924:tensor<[1, 8, 128, 32], Float32, CPU>, %919:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.AddOp(%941:tensor<[1, 32, 2048], Float32, CPU>, %912:tensor<[1, 32, 2048], Float32, CPU>) -> (%942:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.16.post_attention_layernorm"](%942:tensor<[1, 32, 2048], Float32, CPU>) -> (%943:tensor<[1, 32, 2048], Float32, CPU>)
            graph.CallGraphOp @model.layers.16.mlp (%943:tensor<[1, 32, 2048], Float32, CPU>) -> (%948:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.AddOp(%948:tensor<[1, 32, 2048], Float32, CPU>, %942:tensor<[1, 32, 2048], Float32, CPU>) -> (%949:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%949:tensor<[1, 32, 2048], Float32, CPU>, %924:tensor<[1, 8, 128, 32], Float32, CPU>, %919:tensor<[1, 8, 32, 128], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.16.self_attn <CPU> {
        (%913:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%941:tensor<[1, 32, 2048], Float32, CPU>, %924:tensor<[1, 8, 128, 32], Float32, CPU>, %919:tensor<[1, 8, 32, 128], Float32, CPU>) {
            linalg.CPU.LinearOp [name="model.layers.16.self_attn.q_proj"](%913:tensor<[1, 32, 2048], Float32, CPU>) -> (%914:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.16.self_attn.k_proj"](%913:tensor<[1, 32, 2048], Float32, CPU>) -> (%915:tensor<[1, 32, 1024], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.16.self_attn.v_proj"](%913:tensor<[1, 32, 2048], Float32, CPU>) -> (%916:tensor<[1, 32, 1024], Float32, CPU>)
            linalg.CPU.ViewOp(%914:tensor<[1, 32, 2048], Float32, CPU>) -> (%914:tensor<[1, 32, 16, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%914:tensor<[1, 32, 16, 128], Float32, CPU>) -> (%917:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%915:tensor<[1, 32, 1024], Float32, CPU>) -> (%915:tensor<[1, 32, 8, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%915:tensor<[1, 32, 8, 128], Float32, CPU>) -> (%918:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%916:tensor<[1, 32, 1024], Float32, CPU>) -> (%916:tensor<[1, 32, 8, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%916:tensor<[1, 32, 8, 128], Float32, CPU>) -> (%919:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.16.self_attn.q_norm"](%917:tensor<[1, 16, 32, 128], Float32, CPU>) -> (%920:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.16.self_attn.k_norm"](%918:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%921:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.RoPEOp [name="model.layers.16.self_attn.q_rope"](%920:tensor<[1, 16, 32, 128], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>) -> (%922:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.RoPEOp [name="model.layers.16.self_attn.k_rope"](%921:tensor<[1, 8, 32, 128], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>) -> (%923:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%923:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%924:tensor<[1, 8, 128, 32], Float32, CPU>)
            linalg.CPU.ConcatOp(%313:tensor<[1, 8, 128, 992], Float32, CPU>, %924:tensor<[1, 8, 128, 32], Float32, CPU>) -> (%925:tensor<[1, 8, 128, 1024], Float32, CPU>)
            linalg.CPU.ConcatOp(%314:tensor<[1, 8, 992, 128], Float32, CPU>, %919:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%926:tensor<[1, 8, 1024, 128], Float32, CPU>)
            linalg.CPU.RepeatOp(%925:tensor<[1, 8, 128, 1024], Float32, CPU>) -> (%927:tensor<[1, 16, 128, 1024], Float32, CPU>)
            linalg.CPU.RepeatOp(%926:tensor<[1, 8, 1024, 128], Float32, CPU>) -> (%928:tensor<[1, 16, 1024, 128], Float32, CPU>)
            linalg.CPU.MatMulOp(%922:tensor<[1, 16, 32, 128], Float32, CPU>, %927:tensor<[1, 16, 128, 1024], Float32, CPU>) -> (%929:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.MulOp(%929:tensor<[1, 16, 32, 1024], Float32, CPU>, %930:tensor<[1], Float32, CPU>[constant: [0.088388346]][constant:[0.088388346]]) -> (%931:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.ReduceMinOp(%931:tensor<[1, 16, 32, 1024], Float32, CPU>) -> (%932:tensor<[1, 16, 32, 1], Float32, CPU>)
            linalg.CPU.AddOp(%932:tensor<[1, 16, 32, 1], Float32, CPU>, %933:tensor<[1], Float32, CPU>[constant: [-20]][constant:[-20]]) -> (%934:tensor<[1, 16, 32, 1], Float32, CPU>)
            linalg.CPU.EqualOp(%315:tensor<[1, 1, 32, 1024], Float32, CPU>, %935:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%936:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp(%936:tensor<[1, 1, 32, 1024], UInt8, CPU>, %931:tensor<[1, 16, 32, 1024], Float32, CPU>, %934:tensor<[1, 16, 32, 1], Float32, CPU>) -> (%937:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%937:tensor<[1, 16, 32, 1024], Float32, CPU>) -> (%938:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.MatMulOp(%938:tensor<[1, 16, 32, 1024], Float32, CPU>, %928:tensor<[1, 16, 1024, 128], Float32, CPU>) -> (%939:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%939:tensor<[1, 16, 32, 128], Float32, CPU>) -> (%940:tensor<[1, 32, 16, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%940:tensor<[1, 32, 16, 128], Float32, CPU>) -> (%940:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.16.self_attn.o_proj"](%940:tensor<[1, 32, 2048], Float32, CPU>) -> (%941:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%941:tensor<[1, 32, 2048], Float32, CPU>, %924:tensor<[1, 8, 128, 32], Float32, CPU>, %919:tensor<[1, 8, 32, 128], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.16.mlp <CPU> {
        (%943:tensor<[1, 32, 2048], Float32, CPU>) -> (%948:tensor<[1, 32, 2048], Float32, CPU>) {
            linalg.CPU.LinearOp [name="model.layers.16.mlp.gate_proj"](%943:tensor<[1, 32, 2048], Float32, CPU>) -> (%944:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.SiLUOp [name="model.layers.16.mlp.act"](%944:tensor<[1, 32, 6144], Float32, CPU>) -> (%945:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.16.mlp.up_proj"](%943:tensor<[1, 32, 2048], Float32, CPU>) -> (%946:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.MulOp(%945:tensor<[1, 32, 6144], Float32, CPU>, %946:tensor<[1, 32, 6144], Float32, CPU>) -> (%947:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.16.mlp.down_proj"](%947:tensor<[1, 32, 6144], Float32, CPU>) -> (%948:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%948:tensor<[1, 32, 2048], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.17 <CPU> {
        (%949:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%986:tensor<[1, 32, 2048], Float32, CPU>, %961:tensor<[1, 8, 128, 32], Float32, CPU>, %956:tensor<[1, 8, 32, 128], Float32, CPU>) {
            linalg.CPU.RMSNormOp [name="model.layers.17.input_layernorm"](%949:tensor<[1, 32, 2048], Float32, CPU>) -> (%950:tensor<[1, 32, 2048], Float32, CPU>)
            graph.CallGraphOp @model.layers.17.self_attn (%950:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%978:tensor<[1, 32, 2048], Float32, CPU>, %961:tensor<[1, 8, 128, 32], Float32, CPU>, %956:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.AddOp(%978:tensor<[1, 32, 2048], Float32, CPU>, %949:tensor<[1, 32, 2048], Float32, CPU>) -> (%979:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.17.post_attention_layernorm"](%979:tensor<[1, 32, 2048], Float32, CPU>) -> (%980:tensor<[1, 32, 2048], Float32, CPU>)
            graph.CallGraphOp @model.layers.17.mlp (%980:tensor<[1, 32, 2048], Float32, CPU>) -> (%985:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.AddOp(%985:tensor<[1, 32, 2048], Float32, CPU>, %979:tensor<[1, 32, 2048], Float32, CPU>) -> (%986:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%986:tensor<[1, 32, 2048], Float32, CPU>, %961:tensor<[1, 8, 128, 32], Float32, CPU>, %956:tensor<[1, 8, 32, 128], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.17.self_attn <CPU> {
        (%950:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%978:tensor<[1, 32, 2048], Float32, CPU>, %961:tensor<[1, 8, 128, 32], Float32, CPU>, %956:tensor<[1, 8, 32, 128], Float32, CPU>) {
            linalg.CPU.LinearOp [name="model.layers.17.self_attn.q_proj"](%950:tensor<[1, 32, 2048], Float32, CPU>) -> (%951:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.17.self_attn.k_proj"](%950:tensor<[1, 32, 2048], Float32, CPU>) -> (%952:tensor<[1, 32, 1024], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.17.self_attn.v_proj"](%950:tensor<[1, 32, 2048], Float32, CPU>) -> (%953:tensor<[1, 32, 1024], Float32, CPU>)
            linalg.CPU.ViewOp(%951:tensor<[1, 32, 2048], Float32, CPU>) -> (%951:tensor<[1, 32, 16, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%951:tensor<[1, 32, 16, 128], Float32, CPU>) -> (%954:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%952:tensor<[1, 32, 1024], Float32, CPU>) -> (%952:tensor<[1, 32, 8, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%952:tensor<[1, 32, 8, 128], Float32, CPU>) -> (%955:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%953:tensor<[1, 32, 1024], Float32, CPU>) -> (%953:tensor<[1, 32, 8, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%953:tensor<[1, 32, 8, 128], Float32, CPU>) -> (%956:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.17.self_attn.q_norm"](%954:tensor<[1, 16, 32, 128], Float32, CPU>) -> (%957:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.17.self_attn.k_norm"](%955:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%958:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.RoPEOp [name="model.layers.17.self_attn.q_rope"](%957:tensor<[1, 16, 32, 128], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>) -> (%959:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.RoPEOp [name="model.layers.17.self_attn.k_rope"](%958:tensor<[1, 8, 32, 128], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>) -> (%960:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%960:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%961:tensor<[1, 8, 128, 32], Float32, CPU>)
            linalg.CPU.ConcatOp(%313:tensor<[1, 8, 128, 992], Float32, CPU>, %961:tensor<[1, 8, 128, 32], Float32, CPU>) -> (%962:tensor<[1, 8, 128, 1024], Float32, CPU>)
            linalg.CPU.ConcatOp(%314:tensor<[1, 8, 992, 128], Float32, CPU>, %956:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%963:tensor<[1, 8, 1024, 128], Float32, CPU>)
            linalg.CPU.RepeatOp(%962:tensor<[1, 8, 128, 1024], Float32, CPU>) -> (%964:tensor<[1, 16, 128, 1024], Float32, CPU>)
            linalg.CPU.RepeatOp(%963:tensor<[1, 8, 1024, 128], Float32, CPU>) -> (%965:tensor<[1, 16, 1024, 128], Float32, CPU>)
            linalg.CPU.MatMulOp(%959:tensor<[1, 16, 32, 128], Float32, CPU>, %964:tensor<[1, 16, 128, 1024], Float32, CPU>) -> (%966:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.MulOp(%966:tensor<[1, 16, 32, 1024], Float32, CPU>, %967:tensor<[1], Float32, CPU>[constant: [0.088388346]][constant:[0.088388346]]) -> (%968:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.ReduceMinOp(%968:tensor<[1, 16, 32, 1024], Float32, CPU>) -> (%969:tensor<[1, 16, 32, 1], Float32, CPU>)
            linalg.CPU.AddOp(%969:tensor<[1, 16, 32, 1], Float32, CPU>, %970:tensor<[1], Float32, CPU>[constant: [-20]][constant:[-20]]) -> (%971:tensor<[1, 16, 32, 1], Float32, CPU>)
            linalg.CPU.EqualOp(%315:tensor<[1, 1, 32, 1024], Float32, CPU>, %972:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%973:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp(%973:tensor<[1, 1, 32, 1024], UInt8, CPU>, %968:tensor<[1, 16, 32, 1024], Float32, CPU>, %971:tensor<[1, 16, 32, 1], Float32, CPU>) -> (%974:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%974:tensor<[1, 16, 32, 1024], Float32, CPU>) -> (%975:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.MatMulOp(%975:tensor<[1, 16, 32, 1024], Float32, CPU>, %965:tensor<[1, 16, 1024, 128], Float32, CPU>) -> (%976:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%976:tensor<[1, 16, 32, 128], Float32, CPU>) -> (%977:tensor<[1, 32, 16, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%977:tensor<[1, 32, 16, 128], Float32, CPU>) -> (%977:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.17.self_attn.o_proj"](%977:tensor<[1, 32, 2048], Float32, CPU>) -> (%978:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%978:tensor<[1, 32, 2048], Float32, CPU>, %961:tensor<[1, 8, 128, 32], Float32, CPU>, %956:tensor<[1, 8, 32, 128], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.17.mlp <CPU> {
        (%980:tensor<[1, 32, 2048], Float32, CPU>) -> (%985:tensor<[1, 32, 2048], Float32, CPU>) {
            linalg.CPU.LinearOp [name="model.layers.17.mlp.gate_proj"](%980:tensor<[1, 32, 2048], Float32, CPU>) -> (%981:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.SiLUOp [name="model.layers.17.mlp.act"](%981:tensor<[1, 32, 6144], Float32, CPU>) -> (%982:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.17.mlp.up_proj"](%980:tensor<[1, 32, 2048], Float32, CPU>) -> (%983:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.MulOp(%982:tensor<[1, 32, 6144], Float32, CPU>, %983:tensor<[1, 32, 6144], Float32, CPU>) -> (%984:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.17.mlp.down_proj"](%984:tensor<[1, 32, 6144], Float32, CPU>) -> (%985:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%985:tensor<[1, 32, 2048], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.18 <CPU> {
        (%986:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%1023:tensor<[1, 32, 2048], Float32, CPU>, %998:tensor<[1, 8, 128, 32], Float32, CPU>, %993:tensor<[1, 8, 32, 128], Float32, CPU>) {
            linalg.CPU.RMSNormOp [name="model.layers.18.input_layernorm"](%986:tensor<[1, 32, 2048], Float32, CPU>) -> (%987:tensor<[1, 32, 2048], Float32, CPU>)
            graph.CallGraphOp @model.layers.18.self_attn (%987:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%1015:tensor<[1, 32, 2048], Float32, CPU>, %998:tensor<[1, 8, 128, 32], Float32, CPU>, %993:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.AddOp(%1015:tensor<[1, 32, 2048], Float32, CPU>, %986:tensor<[1, 32, 2048], Float32, CPU>) -> (%1016:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.18.post_attention_layernorm"](%1016:tensor<[1, 32, 2048], Float32, CPU>) -> (%1017:tensor<[1, 32, 2048], Float32, CPU>)
            graph.CallGraphOp @model.layers.18.mlp (%1017:tensor<[1, 32, 2048], Float32, CPU>) -> (%1022:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.AddOp(%1022:tensor<[1, 32, 2048], Float32, CPU>, %1016:tensor<[1, 32, 2048], Float32, CPU>) -> (%1023:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%1023:tensor<[1, 32, 2048], Float32, CPU>, %998:tensor<[1, 8, 128, 32], Float32, CPU>, %993:tensor<[1, 8, 32, 128], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.18.self_attn <CPU> {
        (%987:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%1015:tensor<[1, 32, 2048], Float32, CPU>, %998:tensor<[1, 8, 128, 32], Float32, CPU>, %993:tensor<[1, 8, 32, 128], Float32, CPU>) {
            linalg.CPU.LinearOp [name="model.layers.18.self_attn.q_proj"](%987:tensor<[1, 32, 2048], Float32, CPU>) -> (%988:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.18.self_attn.k_proj"](%987:tensor<[1, 32, 2048], Float32, CPU>) -> (%989:tensor<[1, 32, 1024], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.18.self_attn.v_proj"](%987:tensor<[1, 32, 2048], Float32, CPU>) -> (%990:tensor<[1, 32, 1024], Float32, CPU>)
            linalg.CPU.ViewOp(%988:tensor<[1, 32, 2048], Float32, CPU>) -> (%988:tensor<[1, 32, 16, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%988:tensor<[1, 32, 16, 128], Float32, CPU>) -> (%991:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%989:tensor<[1, 32, 1024], Float32, CPU>) -> (%989:tensor<[1, 32, 8, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%989:tensor<[1, 32, 8, 128], Float32, CPU>) -> (%992:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%990:tensor<[1, 32, 1024], Float32, CPU>) -> (%990:tensor<[1, 32, 8, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%990:tensor<[1, 32, 8, 128], Float32, CPU>) -> (%993:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.18.self_attn.q_norm"](%991:tensor<[1, 16, 32, 128], Float32, CPU>) -> (%994:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.18.self_attn.k_norm"](%992:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%995:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.RoPEOp [name="model.layers.18.self_attn.q_rope"](%994:tensor<[1, 16, 32, 128], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>) -> (%996:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.RoPEOp [name="model.layers.18.self_attn.k_rope"](%995:tensor<[1, 8, 32, 128], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>) -> (%997:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%997:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%998:tensor<[1, 8, 128, 32], Float32, CPU>)
            linalg.CPU.ConcatOp(%313:tensor<[1, 8, 128, 992], Float32, CPU>, %998:tensor<[1, 8, 128, 32], Float32, CPU>) -> (%999:tensor<[1, 8, 128, 1024], Float32, CPU>)
            linalg.CPU.ConcatOp(%314:tensor<[1, 8, 992, 128], Float32, CPU>, %993:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%1000:tensor<[1, 8, 1024, 128], Float32, CPU>)
            linalg.CPU.RepeatOp(%999:tensor<[1, 8, 128, 1024], Float32, CPU>) -> (%1001:tensor<[1, 16, 128, 1024], Float32, CPU>)
            linalg.CPU.RepeatOp(%1000:tensor<[1, 8, 1024, 128], Float32, CPU>) -> (%1002:tensor<[1, 16, 1024, 128], Float32, CPU>)
            linalg.CPU.MatMulOp(%996:tensor<[1, 16, 32, 128], Float32, CPU>, %1001:tensor<[1, 16, 128, 1024], Float32, CPU>) -> (%1003:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.MulOp(%1003:tensor<[1, 16, 32, 1024], Float32, CPU>, %1004:tensor<[1], Float32, CPU>[constant: [0.088388346]][constant:[0.088388346]]) -> (%1005:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.ReduceMinOp(%1005:tensor<[1, 16, 32, 1024], Float32, CPU>) -> (%1006:tensor<[1, 16, 32, 1], Float32, CPU>)
            linalg.CPU.AddOp(%1006:tensor<[1, 16, 32, 1], Float32, CPU>, %1007:tensor<[1], Float32, CPU>[constant: [-20]][constant:[-20]]) -> (%1008:tensor<[1, 16, 32, 1], Float32, CPU>)
            linalg.CPU.EqualOp(%315:tensor<[1, 1, 32, 1024], Float32, CPU>, %1009:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%1010:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp(%1010:tensor<[1, 1, 32, 1024], UInt8, CPU>, %1005:tensor<[1, 16, 32, 1024], Float32, CPU>, %1008:tensor<[1, 16, 32, 1], Float32, CPU>) -> (%1011:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%1011:tensor<[1, 16, 32, 1024], Float32, CPU>) -> (%1012:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.MatMulOp(%1012:tensor<[1, 16, 32, 1024], Float32, CPU>, %1002:tensor<[1, 16, 1024, 128], Float32, CPU>) -> (%1013:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1013:tensor<[1, 16, 32, 128], Float32, CPU>) -> (%1014:tensor<[1, 32, 16, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1014:tensor<[1, 32, 16, 128], Float32, CPU>) -> (%1014:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.18.self_attn.o_proj"](%1014:tensor<[1, 32, 2048], Float32, CPU>) -> (%1015:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%1015:tensor<[1, 32, 2048], Float32, CPU>, %998:tensor<[1, 8, 128, 32], Float32, CPU>, %993:tensor<[1, 8, 32, 128], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.18.mlp <CPU> {
        (%1017:tensor<[1, 32, 2048], Float32, CPU>) -> (%1022:tensor<[1, 32, 2048], Float32, CPU>) {
            linalg.CPU.LinearOp [name="model.layers.18.mlp.gate_proj"](%1017:tensor<[1, 32, 2048], Float32, CPU>) -> (%1018:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.SiLUOp [name="model.layers.18.mlp.act"](%1018:tensor<[1, 32, 6144], Float32, CPU>) -> (%1019:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.18.mlp.up_proj"](%1017:tensor<[1, 32, 2048], Float32, CPU>) -> (%1020:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.MulOp(%1019:tensor<[1, 32, 6144], Float32, CPU>, %1020:tensor<[1, 32, 6144], Float32, CPU>) -> (%1021:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.18.mlp.down_proj"](%1021:tensor<[1, 32, 6144], Float32, CPU>) -> (%1022:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%1022:tensor<[1, 32, 2048], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.19 <CPU> {
        (%1023:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%1060:tensor<[1, 32, 2048], Float32, CPU>, %1035:tensor<[1, 8, 128, 32], Float32, CPU>, %1030:tensor<[1, 8, 32, 128], Float32, CPU>) {
            linalg.CPU.RMSNormOp [name="model.layers.19.input_layernorm"](%1023:tensor<[1, 32, 2048], Float32, CPU>) -> (%1024:tensor<[1, 32, 2048], Float32, CPU>)
            graph.CallGraphOp @model.layers.19.self_attn (%1024:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%1052:tensor<[1, 32, 2048], Float32, CPU>, %1035:tensor<[1, 8, 128, 32], Float32, CPU>, %1030:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.AddOp(%1052:tensor<[1, 32, 2048], Float32, CPU>, %1023:tensor<[1, 32, 2048], Float32, CPU>) -> (%1053:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.19.post_attention_layernorm"](%1053:tensor<[1, 32, 2048], Float32, CPU>) -> (%1054:tensor<[1, 32, 2048], Float32, CPU>)
            graph.CallGraphOp @model.layers.19.mlp (%1054:tensor<[1, 32, 2048], Float32, CPU>) -> (%1059:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.AddOp(%1059:tensor<[1, 32, 2048], Float32, CPU>, %1053:tensor<[1, 32, 2048], Float32, CPU>) -> (%1060:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%1060:tensor<[1, 32, 2048], Float32, CPU>, %1035:tensor<[1, 8, 128, 32], Float32, CPU>, %1030:tensor<[1, 8, 32, 128], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.19.self_attn <CPU> {
        (%1024:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%1052:tensor<[1, 32, 2048], Float32, CPU>, %1035:tensor<[1, 8, 128, 32], Float32, CPU>, %1030:tensor<[1, 8, 32, 128], Float32, CPU>) {
            linalg.CPU.LinearOp [name="model.layers.19.self_attn.q_proj"](%1024:tensor<[1, 32, 2048], Float32, CPU>) -> (%1025:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.19.self_attn.k_proj"](%1024:tensor<[1, 32, 2048], Float32, CPU>) -> (%1026:tensor<[1, 32, 1024], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.19.self_attn.v_proj"](%1024:tensor<[1, 32, 2048], Float32, CPU>) -> (%1027:tensor<[1, 32, 1024], Float32, CPU>)
            linalg.CPU.ViewOp(%1025:tensor<[1, 32, 2048], Float32, CPU>) -> (%1025:tensor<[1, 32, 16, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1025:tensor<[1, 32, 16, 128], Float32, CPU>) -> (%1028:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1026:tensor<[1, 32, 1024], Float32, CPU>) -> (%1026:tensor<[1, 32, 8, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1026:tensor<[1, 32, 8, 128], Float32, CPU>) -> (%1029:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1027:tensor<[1, 32, 1024], Float32, CPU>) -> (%1027:tensor<[1, 32, 8, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1027:tensor<[1, 32, 8, 128], Float32, CPU>) -> (%1030:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.19.self_attn.q_norm"](%1028:tensor<[1, 16, 32, 128], Float32, CPU>) -> (%1031:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.19.self_attn.k_norm"](%1029:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%1032:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.RoPEOp [name="model.layers.19.self_attn.q_rope"](%1031:tensor<[1, 16, 32, 128], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>) -> (%1033:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.RoPEOp [name="model.layers.19.self_attn.k_rope"](%1032:tensor<[1, 8, 32, 128], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>) -> (%1034:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1034:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%1035:tensor<[1, 8, 128, 32], Float32, CPU>)
            linalg.CPU.ConcatOp(%313:tensor<[1, 8, 128, 992], Float32, CPU>, %1035:tensor<[1, 8, 128, 32], Float32, CPU>) -> (%1036:tensor<[1, 8, 128, 1024], Float32, CPU>)
            linalg.CPU.ConcatOp(%314:tensor<[1, 8, 992, 128], Float32, CPU>, %1030:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%1037:tensor<[1, 8, 1024, 128], Float32, CPU>)
            linalg.CPU.RepeatOp(%1036:tensor<[1, 8, 128, 1024], Float32, CPU>) -> (%1038:tensor<[1, 16, 128, 1024], Float32, CPU>)
            linalg.CPU.RepeatOp(%1037:tensor<[1, 8, 1024, 128], Float32, CPU>) -> (%1039:tensor<[1, 16, 1024, 128], Float32, CPU>)
            linalg.CPU.MatMulOp(%1033:tensor<[1, 16, 32, 128], Float32, CPU>, %1038:tensor<[1, 16, 128, 1024], Float32, CPU>) -> (%1040:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.MulOp(%1040:tensor<[1, 16, 32, 1024], Float32, CPU>, %1041:tensor<[1], Float32, CPU>[constant: [0.088388346]][constant:[0.088388346]]) -> (%1042:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.ReduceMinOp(%1042:tensor<[1, 16, 32, 1024], Float32, CPU>) -> (%1043:tensor<[1, 16, 32, 1], Float32, CPU>)
            linalg.CPU.AddOp(%1043:tensor<[1, 16, 32, 1], Float32, CPU>, %1044:tensor<[1], Float32, CPU>[constant: [-20]][constant:[-20]]) -> (%1045:tensor<[1, 16, 32, 1], Float32, CPU>)
            linalg.CPU.EqualOp(%315:tensor<[1, 1, 32, 1024], Float32, CPU>, %1046:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%1047:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp(%1047:tensor<[1, 1, 32, 1024], UInt8, CPU>, %1042:tensor<[1, 16, 32, 1024], Float32, CPU>, %1045:tensor<[1, 16, 32, 1], Float32, CPU>) -> (%1048:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%1048:tensor<[1, 16, 32, 1024], Float32, CPU>) -> (%1049:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.MatMulOp(%1049:tensor<[1, 16, 32, 1024], Float32, CPU>, %1039:tensor<[1, 16, 1024, 128], Float32, CPU>) -> (%1050:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1050:tensor<[1, 16, 32, 128], Float32, CPU>) -> (%1051:tensor<[1, 32, 16, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1051:tensor<[1, 32, 16, 128], Float32, CPU>) -> (%1051:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.19.self_attn.o_proj"](%1051:tensor<[1, 32, 2048], Float32, CPU>) -> (%1052:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%1052:tensor<[1, 32, 2048], Float32, CPU>, %1035:tensor<[1, 8, 128, 32], Float32, CPU>, %1030:tensor<[1, 8, 32, 128], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.19.mlp <CPU> {
        (%1054:tensor<[1, 32, 2048], Float32, CPU>) -> (%1059:tensor<[1, 32, 2048], Float32, CPU>) {
            linalg.CPU.LinearOp [name="model.layers.19.mlp.gate_proj"](%1054:tensor<[1, 32, 2048], Float32, CPU>) -> (%1055:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.SiLUOp [name="model.layers.19.mlp.act"](%1055:tensor<[1, 32, 6144], Float32, CPU>) -> (%1056:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.19.mlp.up_proj"](%1054:tensor<[1, 32, 2048], Float32, CPU>) -> (%1057:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.MulOp(%1056:tensor<[1, 32, 6144], Float32, CPU>, %1057:tensor<[1, 32, 6144], Float32, CPU>) -> (%1058:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.19.mlp.down_proj"](%1058:tensor<[1, 32, 6144], Float32, CPU>) -> (%1059:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%1059:tensor<[1, 32, 2048], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.20 <CPU> {
        (%1060:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%1097:tensor<[1, 32, 2048], Float32, CPU>, %1072:tensor<[1, 8, 128, 32], Float32, CPU>, %1067:tensor<[1, 8, 32, 128], Float32, CPU>) {
            linalg.CPU.RMSNormOp [name="model.layers.20.input_layernorm"](%1060:tensor<[1, 32, 2048], Float32, CPU>) -> (%1061:tensor<[1, 32, 2048], Float32, CPU>)
            graph.CallGraphOp @model.layers.20.self_attn (%1061:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%1089:tensor<[1, 32, 2048], Float32, CPU>, %1072:tensor<[1, 8, 128, 32], Float32, CPU>, %1067:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.AddOp(%1089:tensor<[1, 32, 2048], Float32, CPU>, %1060:tensor<[1, 32, 2048], Float32, CPU>) -> (%1090:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.20.post_attention_layernorm"](%1090:tensor<[1, 32, 2048], Float32, CPU>) -> (%1091:tensor<[1, 32, 2048], Float32, CPU>)
            graph.CallGraphOp @model.layers.20.mlp (%1091:tensor<[1, 32, 2048], Float32, CPU>) -> (%1096:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.AddOp(%1096:tensor<[1, 32, 2048], Float32, CPU>, %1090:tensor<[1, 32, 2048], Float32, CPU>) -> (%1097:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%1097:tensor<[1, 32, 2048], Float32, CPU>, %1072:tensor<[1, 8, 128, 32], Float32, CPU>, %1067:tensor<[1, 8, 32, 128], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.20.self_attn <CPU> {
        (%1061:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%1089:tensor<[1, 32, 2048], Float32, CPU>, %1072:tensor<[1, 8, 128, 32], Float32, CPU>, %1067:tensor<[1, 8, 32, 128], Float32, CPU>) {
            linalg.CPU.LinearOp [name="model.layers.20.self_attn.q_proj"](%1061:tensor<[1, 32, 2048], Float32, CPU>) -> (%1062:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.20.self_attn.k_proj"](%1061:tensor<[1, 32, 2048], Float32, CPU>) -> (%1063:tensor<[1, 32, 1024], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.20.self_attn.v_proj"](%1061:tensor<[1, 32, 2048], Float32, CPU>) -> (%1064:tensor<[1, 32, 1024], Float32, CPU>)
            linalg.CPU.ViewOp(%1062:tensor<[1, 32, 2048], Float32, CPU>) -> (%1062:tensor<[1, 32, 16, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1062:tensor<[1, 32, 16, 128], Float32, CPU>) -> (%1065:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1063:tensor<[1, 32, 1024], Float32, CPU>) -> (%1063:tensor<[1, 32, 8, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1063:tensor<[1, 32, 8, 128], Float32, CPU>) -> (%1066:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1064:tensor<[1, 32, 1024], Float32, CPU>) -> (%1064:tensor<[1, 32, 8, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1064:tensor<[1, 32, 8, 128], Float32, CPU>) -> (%1067:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.20.self_attn.q_norm"](%1065:tensor<[1, 16, 32, 128], Float32, CPU>) -> (%1068:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.20.self_attn.k_norm"](%1066:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%1069:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.RoPEOp [name="model.layers.20.self_attn.q_rope"](%1068:tensor<[1, 16, 32, 128], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>) -> (%1070:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.RoPEOp [name="model.layers.20.self_attn.k_rope"](%1069:tensor<[1, 8, 32, 128], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>) -> (%1071:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1071:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%1072:tensor<[1, 8, 128, 32], Float32, CPU>)
            linalg.CPU.ConcatOp(%313:tensor<[1, 8, 128, 992], Float32, CPU>, %1072:tensor<[1, 8, 128, 32], Float32, CPU>) -> (%1073:tensor<[1, 8, 128, 1024], Float32, CPU>)
            linalg.CPU.ConcatOp(%314:tensor<[1, 8, 992, 128], Float32, CPU>, %1067:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%1074:tensor<[1, 8, 1024, 128], Float32, CPU>)
            linalg.CPU.RepeatOp(%1073:tensor<[1, 8, 128, 1024], Float32, CPU>) -> (%1075:tensor<[1, 16, 128, 1024], Float32, CPU>)
            linalg.CPU.RepeatOp(%1074:tensor<[1, 8, 1024, 128], Float32, CPU>) -> (%1076:tensor<[1, 16, 1024, 128], Float32, CPU>)
            linalg.CPU.MatMulOp(%1070:tensor<[1, 16, 32, 128], Float32, CPU>, %1075:tensor<[1, 16, 128, 1024], Float32, CPU>) -> (%1077:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.MulOp(%1077:tensor<[1, 16, 32, 1024], Float32, CPU>, %1078:tensor<[1], Float32, CPU>[constant: [0.088388346]][constant:[0.088388346]]) -> (%1079:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.ReduceMinOp(%1079:tensor<[1, 16, 32, 1024], Float32, CPU>) -> (%1080:tensor<[1, 16, 32, 1], Float32, CPU>)
            linalg.CPU.AddOp(%1080:tensor<[1, 16, 32, 1], Float32, CPU>, %1081:tensor<[1], Float32, CPU>[constant: [-20]][constant:[-20]]) -> (%1082:tensor<[1, 16, 32, 1], Float32, CPU>)
            linalg.CPU.EqualOp(%315:tensor<[1, 1, 32, 1024], Float32, CPU>, %1083:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%1084:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp(%1084:tensor<[1, 1, 32, 1024], UInt8, CPU>, %1079:tensor<[1, 16, 32, 1024], Float32, CPU>, %1082:tensor<[1, 16, 32, 1], Float32, CPU>) -> (%1085:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%1085:tensor<[1, 16, 32, 1024], Float32, CPU>) -> (%1086:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.MatMulOp(%1086:tensor<[1, 16, 32, 1024], Float32, CPU>, %1076:tensor<[1, 16, 1024, 128], Float32, CPU>) -> (%1087:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1087:tensor<[1, 16, 32, 128], Float32, CPU>) -> (%1088:tensor<[1, 32, 16, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1088:tensor<[1, 32, 16, 128], Float32, CPU>) -> (%1088:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.20.self_attn.o_proj"](%1088:tensor<[1, 32, 2048], Float32, CPU>) -> (%1089:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%1089:tensor<[1, 32, 2048], Float32, CPU>, %1072:tensor<[1, 8, 128, 32], Float32, CPU>, %1067:tensor<[1, 8, 32, 128], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.20.mlp <CPU> {
        (%1091:tensor<[1, 32, 2048], Float32, CPU>) -> (%1096:tensor<[1, 32, 2048], Float32, CPU>) {
            linalg.CPU.LinearOp [name="model.layers.20.mlp.gate_proj"](%1091:tensor<[1, 32, 2048], Float32, CPU>) -> (%1092:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.SiLUOp [name="model.layers.20.mlp.act"](%1092:tensor<[1, 32, 6144], Float32, CPU>) -> (%1093:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.20.mlp.up_proj"](%1091:tensor<[1, 32, 2048], Float32, CPU>) -> (%1094:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.MulOp(%1093:tensor<[1, 32, 6144], Float32, CPU>, %1094:tensor<[1, 32, 6144], Float32, CPU>) -> (%1095:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.20.mlp.down_proj"](%1095:tensor<[1, 32, 6144], Float32, CPU>) -> (%1096:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%1096:tensor<[1, 32, 2048], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.21 <CPU> {
        (%1097:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%1134:tensor<[1, 32, 2048], Float32, CPU>, %1109:tensor<[1, 8, 128, 32], Float32, CPU>, %1104:tensor<[1, 8, 32, 128], Float32, CPU>) {
            linalg.CPU.RMSNormOp [name="model.layers.21.input_layernorm"](%1097:tensor<[1, 32, 2048], Float32, CPU>) -> (%1098:tensor<[1, 32, 2048], Float32, CPU>)
            graph.CallGraphOp @model.layers.21.self_attn (%1098:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%1126:tensor<[1, 32, 2048], Float32, CPU>, %1109:tensor<[1, 8, 128, 32], Float32, CPU>, %1104:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.AddOp(%1126:tensor<[1, 32, 2048], Float32, CPU>, %1097:tensor<[1, 32, 2048], Float32, CPU>) -> (%1127:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.21.post_attention_layernorm"](%1127:tensor<[1, 32, 2048], Float32, CPU>) -> (%1128:tensor<[1, 32, 2048], Float32, CPU>)
            graph.CallGraphOp @model.layers.21.mlp (%1128:tensor<[1, 32, 2048], Float32, CPU>) -> (%1133:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.AddOp(%1133:tensor<[1, 32, 2048], Float32, CPU>, %1127:tensor<[1, 32, 2048], Float32, CPU>) -> (%1134:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%1134:tensor<[1, 32, 2048], Float32, CPU>, %1109:tensor<[1, 8, 128, 32], Float32, CPU>, %1104:tensor<[1, 8, 32, 128], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.21.self_attn <CPU> {
        (%1098:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%1126:tensor<[1, 32, 2048], Float32, CPU>, %1109:tensor<[1, 8, 128, 32], Float32, CPU>, %1104:tensor<[1, 8, 32, 128], Float32, CPU>) {
            linalg.CPU.LinearOp [name="model.layers.21.self_attn.q_proj"](%1098:tensor<[1, 32, 2048], Float32, CPU>) -> (%1099:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.21.self_attn.k_proj"](%1098:tensor<[1, 32, 2048], Float32, CPU>) -> (%1100:tensor<[1, 32, 1024], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.21.self_attn.v_proj"](%1098:tensor<[1, 32, 2048], Float32, CPU>) -> (%1101:tensor<[1, 32, 1024], Float32, CPU>)
            linalg.CPU.ViewOp(%1099:tensor<[1, 32, 2048], Float32, CPU>) -> (%1099:tensor<[1, 32, 16, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1099:tensor<[1, 32, 16, 128], Float32, CPU>) -> (%1102:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1100:tensor<[1, 32, 1024], Float32, CPU>) -> (%1100:tensor<[1, 32, 8, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1100:tensor<[1, 32, 8, 128], Float32, CPU>) -> (%1103:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1101:tensor<[1, 32, 1024], Float32, CPU>) -> (%1101:tensor<[1, 32, 8, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1101:tensor<[1, 32, 8, 128], Float32, CPU>) -> (%1104:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.21.self_attn.q_norm"](%1102:tensor<[1, 16, 32, 128], Float32, CPU>) -> (%1105:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.21.self_attn.k_norm"](%1103:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%1106:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.RoPEOp [name="model.layers.21.self_attn.q_rope"](%1105:tensor<[1, 16, 32, 128], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>) -> (%1107:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.RoPEOp [name="model.layers.21.self_attn.k_rope"](%1106:tensor<[1, 8, 32, 128], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>) -> (%1108:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1108:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%1109:tensor<[1, 8, 128, 32], Float32, CPU>)
            linalg.CPU.ConcatOp(%313:tensor<[1, 8, 128, 992], Float32, CPU>, %1109:tensor<[1, 8, 128, 32], Float32, CPU>) -> (%1110:tensor<[1, 8, 128, 1024], Float32, CPU>)
            linalg.CPU.ConcatOp(%314:tensor<[1, 8, 992, 128], Float32, CPU>, %1104:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%1111:tensor<[1, 8, 1024, 128], Float32, CPU>)
            linalg.CPU.RepeatOp(%1110:tensor<[1, 8, 128, 1024], Float32, CPU>) -> (%1112:tensor<[1, 16, 128, 1024], Float32, CPU>)
            linalg.CPU.RepeatOp(%1111:tensor<[1, 8, 1024, 128], Float32, CPU>) -> (%1113:tensor<[1, 16, 1024, 128], Float32, CPU>)
            linalg.CPU.MatMulOp(%1107:tensor<[1, 16, 32, 128], Float32, CPU>, %1112:tensor<[1, 16, 128, 1024], Float32, CPU>) -> (%1114:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.MulOp(%1114:tensor<[1, 16, 32, 1024], Float32, CPU>, %1115:tensor<[1], Float32, CPU>[constant: [0.088388346]][constant:[0.088388346]]) -> (%1116:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.ReduceMinOp(%1116:tensor<[1, 16, 32, 1024], Float32, CPU>) -> (%1117:tensor<[1, 16, 32, 1], Float32, CPU>)
            linalg.CPU.AddOp(%1117:tensor<[1, 16, 32, 1], Float32, CPU>, %1118:tensor<[1], Float32, CPU>[constant: [-20]][constant:[-20]]) -> (%1119:tensor<[1, 16, 32, 1], Float32, CPU>)
            linalg.CPU.EqualOp(%315:tensor<[1, 1, 32, 1024], Float32, CPU>, %1120:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%1121:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp(%1121:tensor<[1, 1, 32, 1024], UInt8, CPU>, %1116:tensor<[1, 16, 32, 1024], Float32, CPU>, %1119:tensor<[1, 16, 32, 1], Float32, CPU>) -> (%1122:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%1122:tensor<[1, 16, 32, 1024], Float32, CPU>) -> (%1123:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.MatMulOp(%1123:tensor<[1, 16, 32, 1024], Float32, CPU>, %1113:tensor<[1, 16, 1024, 128], Float32, CPU>) -> (%1124:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1124:tensor<[1, 16, 32, 128], Float32, CPU>) -> (%1125:tensor<[1, 32, 16, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1125:tensor<[1, 32, 16, 128], Float32, CPU>) -> (%1125:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.21.self_attn.o_proj"](%1125:tensor<[1, 32, 2048], Float32, CPU>) -> (%1126:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%1126:tensor<[1, 32, 2048], Float32, CPU>, %1109:tensor<[1, 8, 128, 32], Float32, CPU>, %1104:tensor<[1, 8, 32, 128], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.21.mlp <CPU> {
        (%1128:tensor<[1, 32, 2048], Float32, CPU>) -> (%1133:tensor<[1, 32, 2048], Float32, CPU>) {
            linalg.CPU.LinearOp [name="model.layers.21.mlp.gate_proj"](%1128:tensor<[1, 32, 2048], Float32, CPU>) -> (%1129:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.SiLUOp [name="model.layers.21.mlp.act"](%1129:tensor<[1, 32, 6144], Float32, CPU>) -> (%1130:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.21.mlp.up_proj"](%1128:tensor<[1, 32, 2048], Float32, CPU>) -> (%1131:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.MulOp(%1130:tensor<[1, 32, 6144], Float32, CPU>, %1131:tensor<[1, 32, 6144], Float32, CPU>) -> (%1132:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.21.mlp.down_proj"](%1132:tensor<[1, 32, 6144], Float32, CPU>) -> (%1133:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%1133:tensor<[1, 32, 2048], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.22 <CPU> {
        (%1134:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%1171:tensor<[1, 32, 2048], Float32, CPU>, %1146:tensor<[1, 8, 128, 32], Float32, CPU>, %1141:tensor<[1, 8, 32, 128], Float32, CPU>) {
            linalg.CPU.RMSNormOp [name="model.layers.22.input_layernorm"](%1134:tensor<[1, 32, 2048], Float32, CPU>) -> (%1135:tensor<[1, 32, 2048], Float32, CPU>)
            graph.CallGraphOp @model.layers.22.self_attn (%1135:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%1163:tensor<[1, 32, 2048], Float32, CPU>, %1146:tensor<[1, 8, 128, 32], Float32, CPU>, %1141:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.AddOp(%1163:tensor<[1, 32, 2048], Float32, CPU>, %1134:tensor<[1, 32, 2048], Float32, CPU>) -> (%1164:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.22.post_attention_layernorm"](%1164:tensor<[1, 32, 2048], Float32, CPU>) -> (%1165:tensor<[1, 32, 2048], Float32, CPU>)
            graph.CallGraphOp @model.layers.22.mlp (%1165:tensor<[1, 32, 2048], Float32, CPU>) -> (%1170:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.AddOp(%1170:tensor<[1, 32, 2048], Float32, CPU>, %1164:tensor<[1, 32, 2048], Float32, CPU>) -> (%1171:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%1171:tensor<[1, 32, 2048], Float32, CPU>, %1146:tensor<[1, 8, 128, 32], Float32, CPU>, %1141:tensor<[1, 8, 32, 128], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.22.self_attn <CPU> {
        (%1135:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%1163:tensor<[1, 32, 2048], Float32, CPU>, %1146:tensor<[1, 8, 128, 32], Float32, CPU>, %1141:tensor<[1, 8, 32, 128], Float32, CPU>) {
            linalg.CPU.LinearOp [name="model.layers.22.self_attn.q_proj"](%1135:tensor<[1, 32, 2048], Float32, CPU>) -> (%1136:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.22.self_attn.k_proj"](%1135:tensor<[1, 32, 2048], Float32, CPU>) -> (%1137:tensor<[1, 32, 1024], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.22.self_attn.v_proj"](%1135:tensor<[1, 32, 2048], Float32, CPU>) -> (%1138:tensor<[1, 32, 1024], Float32, CPU>)
            linalg.CPU.ViewOp(%1136:tensor<[1, 32, 2048], Float32, CPU>) -> (%1136:tensor<[1, 32, 16, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1136:tensor<[1, 32, 16, 128], Float32, CPU>) -> (%1139:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1137:tensor<[1, 32, 1024], Float32, CPU>) -> (%1137:tensor<[1, 32, 8, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1137:tensor<[1, 32, 8, 128], Float32, CPU>) -> (%1140:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1138:tensor<[1, 32, 1024], Float32, CPU>) -> (%1138:tensor<[1, 32, 8, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1138:tensor<[1, 32, 8, 128], Float32, CPU>) -> (%1141:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.22.self_attn.q_norm"](%1139:tensor<[1, 16, 32, 128], Float32, CPU>) -> (%1142:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.22.self_attn.k_norm"](%1140:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%1143:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.RoPEOp [name="model.layers.22.self_attn.q_rope"](%1142:tensor<[1, 16, 32, 128], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>) -> (%1144:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.RoPEOp [name="model.layers.22.self_attn.k_rope"](%1143:tensor<[1, 8, 32, 128], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>) -> (%1145:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1145:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%1146:tensor<[1, 8, 128, 32], Float32, CPU>)
            linalg.CPU.ConcatOp(%313:tensor<[1, 8, 128, 992], Float32, CPU>, %1146:tensor<[1, 8, 128, 32], Float32, CPU>) -> (%1147:tensor<[1, 8, 128, 1024], Float32, CPU>)
            linalg.CPU.ConcatOp(%314:tensor<[1, 8, 992, 128], Float32, CPU>, %1141:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%1148:tensor<[1, 8, 1024, 128], Float32, CPU>)
            linalg.CPU.RepeatOp(%1147:tensor<[1, 8, 128, 1024], Float32, CPU>) -> (%1149:tensor<[1, 16, 128, 1024], Float32, CPU>)
            linalg.CPU.RepeatOp(%1148:tensor<[1, 8, 1024, 128], Float32, CPU>) -> (%1150:tensor<[1, 16, 1024, 128], Float32, CPU>)
            linalg.CPU.MatMulOp(%1144:tensor<[1, 16, 32, 128], Float32, CPU>, %1149:tensor<[1, 16, 128, 1024], Float32, CPU>) -> (%1151:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.MulOp(%1151:tensor<[1, 16, 32, 1024], Float32, CPU>, %1152:tensor<[1], Float32, CPU>[constant: [0.088388346]][constant:[0.088388346]]) -> (%1153:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.ReduceMinOp(%1153:tensor<[1, 16, 32, 1024], Float32, CPU>) -> (%1154:tensor<[1, 16, 32, 1], Float32, CPU>)
            linalg.CPU.AddOp(%1154:tensor<[1, 16, 32, 1], Float32, CPU>, %1155:tensor<[1], Float32, CPU>[constant: [-20]][constant:[-20]]) -> (%1156:tensor<[1, 16, 32, 1], Float32, CPU>)
            linalg.CPU.EqualOp(%315:tensor<[1, 1, 32, 1024], Float32, CPU>, %1157:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%1158:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp(%1158:tensor<[1, 1, 32, 1024], UInt8, CPU>, %1153:tensor<[1, 16, 32, 1024], Float32, CPU>, %1156:tensor<[1, 16, 32, 1], Float32, CPU>) -> (%1159:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%1159:tensor<[1, 16, 32, 1024], Float32, CPU>) -> (%1160:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.MatMulOp(%1160:tensor<[1, 16, 32, 1024], Float32, CPU>, %1150:tensor<[1, 16, 1024, 128], Float32, CPU>) -> (%1161:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1161:tensor<[1, 16, 32, 128], Float32, CPU>) -> (%1162:tensor<[1, 32, 16, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1162:tensor<[1, 32, 16, 128], Float32, CPU>) -> (%1162:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.22.self_attn.o_proj"](%1162:tensor<[1, 32, 2048], Float32, CPU>) -> (%1163:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%1163:tensor<[1, 32, 2048], Float32, CPU>, %1146:tensor<[1, 8, 128, 32], Float32, CPU>, %1141:tensor<[1, 8, 32, 128], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.22.mlp <CPU> {
        (%1165:tensor<[1, 32, 2048], Float32, CPU>) -> (%1170:tensor<[1, 32, 2048], Float32, CPU>) {
            linalg.CPU.LinearOp [name="model.layers.22.mlp.gate_proj"](%1165:tensor<[1, 32, 2048], Float32, CPU>) -> (%1166:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.SiLUOp [name="model.layers.22.mlp.act"](%1166:tensor<[1, 32, 6144], Float32, CPU>) -> (%1167:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.22.mlp.up_proj"](%1165:tensor<[1, 32, 2048], Float32, CPU>) -> (%1168:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.MulOp(%1167:tensor<[1, 32, 6144], Float32, CPU>, %1168:tensor<[1, 32, 6144], Float32, CPU>) -> (%1169:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.22.mlp.down_proj"](%1169:tensor<[1, 32, 6144], Float32, CPU>) -> (%1170:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%1170:tensor<[1, 32, 2048], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.23 <CPU> {
        (%1171:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%1208:tensor<[1, 32, 2048], Float32, CPU>, %1183:tensor<[1, 8, 128, 32], Float32, CPU>, %1178:tensor<[1, 8, 32, 128], Float32, CPU>) {
            linalg.CPU.RMSNormOp [name="model.layers.23.input_layernorm"](%1171:tensor<[1, 32, 2048], Float32, CPU>) -> (%1172:tensor<[1, 32, 2048], Float32, CPU>)
            graph.CallGraphOp @model.layers.23.self_attn (%1172:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%1200:tensor<[1, 32, 2048], Float32, CPU>, %1183:tensor<[1, 8, 128, 32], Float32, CPU>, %1178:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.AddOp(%1200:tensor<[1, 32, 2048], Float32, CPU>, %1171:tensor<[1, 32, 2048], Float32, CPU>) -> (%1201:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.23.post_attention_layernorm"](%1201:tensor<[1, 32, 2048], Float32, CPU>) -> (%1202:tensor<[1, 32, 2048], Float32, CPU>)
            graph.CallGraphOp @model.layers.23.mlp (%1202:tensor<[1, 32, 2048], Float32, CPU>) -> (%1207:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.AddOp(%1207:tensor<[1, 32, 2048], Float32, CPU>, %1201:tensor<[1, 32, 2048], Float32, CPU>) -> (%1208:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%1208:tensor<[1, 32, 2048], Float32, CPU>, %1183:tensor<[1, 8, 128, 32], Float32, CPU>, %1178:tensor<[1, 8, 32, 128], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.23.self_attn <CPU> {
        (%1172:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%1200:tensor<[1, 32, 2048], Float32, CPU>, %1183:tensor<[1, 8, 128, 32], Float32, CPU>, %1178:tensor<[1, 8, 32, 128], Float32, CPU>) {
            linalg.CPU.LinearOp [name="model.layers.23.self_attn.q_proj"](%1172:tensor<[1, 32, 2048], Float32, CPU>) -> (%1173:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.23.self_attn.k_proj"](%1172:tensor<[1, 32, 2048], Float32, CPU>) -> (%1174:tensor<[1, 32, 1024], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.23.self_attn.v_proj"](%1172:tensor<[1, 32, 2048], Float32, CPU>) -> (%1175:tensor<[1, 32, 1024], Float32, CPU>)
            linalg.CPU.ViewOp(%1173:tensor<[1, 32, 2048], Float32, CPU>) -> (%1173:tensor<[1, 32, 16, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1173:tensor<[1, 32, 16, 128], Float32, CPU>) -> (%1176:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1174:tensor<[1, 32, 1024], Float32, CPU>) -> (%1174:tensor<[1, 32, 8, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1174:tensor<[1, 32, 8, 128], Float32, CPU>) -> (%1177:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1175:tensor<[1, 32, 1024], Float32, CPU>) -> (%1175:tensor<[1, 32, 8, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1175:tensor<[1, 32, 8, 128], Float32, CPU>) -> (%1178:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.23.self_attn.q_norm"](%1176:tensor<[1, 16, 32, 128], Float32, CPU>) -> (%1179:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.23.self_attn.k_norm"](%1177:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%1180:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.RoPEOp [name="model.layers.23.self_attn.q_rope"](%1179:tensor<[1, 16, 32, 128], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>) -> (%1181:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.RoPEOp [name="model.layers.23.self_attn.k_rope"](%1180:tensor<[1, 8, 32, 128], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>) -> (%1182:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1182:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%1183:tensor<[1, 8, 128, 32], Float32, CPU>)
            linalg.CPU.ConcatOp(%313:tensor<[1, 8, 128, 992], Float32, CPU>, %1183:tensor<[1, 8, 128, 32], Float32, CPU>) -> (%1184:tensor<[1, 8, 128, 1024], Float32, CPU>)
            linalg.CPU.ConcatOp(%314:tensor<[1, 8, 992, 128], Float32, CPU>, %1178:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%1185:tensor<[1, 8, 1024, 128], Float32, CPU>)
            linalg.CPU.RepeatOp(%1184:tensor<[1, 8, 128, 1024], Float32, CPU>) -> (%1186:tensor<[1, 16, 128, 1024], Float32, CPU>)
            linalg.CPU.RepeatOp(%1185:tensor<[1, 8, 1024, 128], Float32, CPU>) -> (%1187:tensor<[1, 16, 1024, 128], Float32, CPU>)
            linalg.CPU.MatMulOp(%1181:tensor<[1, 16, 32, 128], Float32, CPU>, %1186:tensor<[1, 16, 128, 1024], Float32, CPU>) -> (%1188:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.MulOp(%1188:tensor<[1, 16, 32, 1024], Float32, CPU>, %1189:tensor<[1], Float32, CPU>[constant: [0.088388346]][constant:[0.088388346]]) -> (%1190:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.ReduceMinOp(%1190:tensor<[1, 16, 32, 1024], Float32, CPU>) -> (%1191:tensor<[1, 16, 32, 1], Float32, CPU>)
            linalg.CPU.AddOp(%1191:tensor<[1, 16, 32, 1], Float32, CPU>, %1192:tensor<[1], Float32, CPU>[constant: [-20]][constant:[-20]]) -> (%1193:tensor<[1, 16, 32, 1], Float32, CPU>)
            linalg.CPU.EqualOp(%315:tensor<[1, 1, 32, 1024], Float32, CPU>, %1194:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%1195:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp(%1195:tensor<[1, 1, 32, 1024], UInt8, CPU>, %1190:tensor<[1, 16, 32, 1024], Float32, CPU>, %1193:tensor<[1, 16, 32, 1], Float32, CPU>) -> (%1196:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%1196:tensor<[1, 16, 32, 1024], Float32, CPU>) -> (%1197:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.MatMulOp(%1197:tensor<[1, 16, 32, 1024], Float32, CPU>, %1187:tensor<[1, 16, 1024, 128], Float32, CPU>) -> (%1198:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1198:tensor<[1, 16, 32, 128], Float32, CPU>) -> (%1199:tensor<[1, 32, 16, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1199:tensor<[1, 32, 16, 128], Float32, CPU>) -> (%1199:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.23.self_attn.o_proj"](%1199:tensor<[1, 32, 2048], Float32, CPU>) -> (%1200:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%1200:tensor<[1, 32, 2048], Float32, CPU>, %1183:tensor<[1, 8, 128, 32], Float32, CPU>, %1178:tensor<[1, 8, 32, 128], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.23.mlp <CPU> {
        (%1202:tensor<[1, 32, 2048], Float32, CPU>) -> (%1207:tensor<[1, 32, 2048], Float32, CPU>) {
            linalg.CPU.LinearOp [name="model.layers.23.mlp.gate_proj"](%1202:tensor<[1, 32, 2048], Float32, CPU>) -> (%1203:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.SiLUOp [name="model.layers.23.mlp.act"](%1203:tensor<[1, 32, 6144], Float32, CPU>) -> (%1204:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.23.mlp.up_proj"](%1202:tensor<[1, 32, 2048], Float32, CPU>) -> (%1205:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.MulOp(%1204:tensor<[1, 32, 6144], Float32, CPU>, %1205:tensor<[1, 32, 6144], Float32, CPU>) -> (%1206:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.23.mlp.down_proj"](%1206:tensor<[1, 32, 6144], Float32, CPU>) -> (%1207:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%1207:tensor<[1, 32, 2048], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.24 <CPU> {
        (%1208:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%1245:tensor<[1, 32, 2048], Float32, CPU>, %1220:tensor<[1, 8, 128, 32], Float32, CPU>, %1215:tensor<[1, 8, 32, 128], Float32, CPU>) {
            linalg.CPU.RMSNormOp [name="model.layers.24.input_layernorm"](%1208:tensor<[1, 32, 2048], Float32, CPU>) -> (%1209:tensor<[1, 32, 2048], Float32, CPU>)
            graph.CallGraphOp @model.layers.24.self_attn (%1209:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%1237:tensor<[1, 32, 2048], Float32, CPU>, %1220:tensor<[1, 8, 128, 32], Float32, CPU>, %1215:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.AddOp(%1237:tensor<[1, 32, 2048], Float32, CPU>, %1208:tensor<[1, 32, 2048], Float32, CPU>) -> (%1238:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.24.post_attention_layernorm"](%1238:tensor<[1, 32, 2048], Float32, CPU>) -> (%1239:tensor<[1, 32, 2048], Float32, CPU>)
            graph.CallGraphOp @model.layers.24.mlp (%1239:tensor<[1, 32, 2048], Float32, CPU>) -> (%1244:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.AddOp(%1244:tensor<[1, 32, 2048], Float32, CPU>, %1238:tensor<[1, 32, 2048], Float32, CPU>) -> (%1245:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%1245:tensor<[1, 32, 2048], Float32, CPU>, %1220:tensor<[1, 8, 128, 32], Float32, CPU>, %1215:tensor<[1, 8, 32, 128], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.24.self_attn <CPU> {
        (%1209:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%1237:tensor<[1, 32, 2048], Float32, CPU>, %1220:tensor<[1, 8, 128, 32], Float32, CPU>, %1215:tensor<[1, 8, 32, 128], Float32, CPU>) {
            linalg.CPU.LinearOp [name="model.layers.24.self_attn.q_proj"](%1209:tensor<[1, 32, 2048], Float32, CPU>) -> (%1210:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.24.self_attn.k_proj"](%1209:tensor<[1, 32, 2048], Float32, CPU>) -> (%1211:tensor<[1, 32, 1024], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.24.self_attn.v_proj"](%1209:tensor<[1, 32, 2048], Float32, CPU>) -> (%1212:tensor<[1, 32, 1024], Float32, CPU>)
            linalg.CPU.ViewOp(%1210:tensor<[1, 32, 2048], Float32, CPU>) -> (%1210:tensor<[1, 32, 16, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1210:tensor<[1, 32, 16, 128], Float32, CPU>) -> (%1213:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1211:tensor<[1, 32, 1024], Float32, CPU>) -> (%1211:tensor<[1, 32, 8, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1211:tensor<[1, 32, 8, 128], Float32, CPU>) -> (%1214:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1212:tensor<[1, 32, 1024], Float32, CPU>) -> (%1212:tensor<[1, 32, 8, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1212:tensor<[1, 32, 8, 128], Float32, CPU>) -> (%1215:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.24.self_attn.q_norm"](%1213:tensor<[1, 16, 32, 128], Float32, CPU>) -> (%1216:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.24.self_attn.k_norm"](%1214:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%1217:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.RoPEOp [name="model.layers.24.self_attn.q_rope"](%1216:tensor<[1, 16, 32, 128], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>) -> (%1218:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.RoPEOp [name="model.layers.24.self_attn.k_rope"](%1217:tensor<[1, 8, 32, 128], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>) -> (%1219:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1219:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%1220:tensor<[1, 8, 128, 32], Float32, CPU>)
            linalg.CPU.ConcatOp(%313:tensor<[1, 8, 128, 992], Float32, CPU>, %1220:tensor<[1, 8, 128, 32], Float32, CPU>) -> (%1221:tensor<[1, 8, 128, 1024], Float32, CPU>)
            linalg.CPU.ConcatOp(%314:tensor<[1, 8, 992, 128], Float32, CPU>, %1215:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%1222:tensor<[1, 8, 1024, 128], Float32, CPU>)
            linalg.CPU.RepeatOp(%1221:tensor<[1, 8, 128, 1024], Float32, CPU>) -> (%1223:tensor<[1, 16, 128, 1024], Float32, CPU>)
            linalg.CPU.RepeatOp(%1222:tensor<[1, 8, 1024, 128], Float32, CPU>) -> (%1224:tensor<[1, 16, 1024, 128], Float32, CPU>)
            linalg.CPU.MatMulOp(%1218:tensor<[1, 16, 32, 128], Float32, CPU>, %1223:tensor<[1, 16, 128, 1024], Float32, CPU>) -> (%1225:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.MulOp(%1225:tensor<[1, 16, 32, 1024], Float32, CPU>, %1226:tensor<[1], Float32, CPU>[constant: [0.088388346]][constant:[0.088388346]]) -> (%1227:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.ReduceMinOp(%1227:tensor<[1, 16, 32, 1024], Float32, CPU>) -> (%1228:tensor<[1, 16, 32, 1], Float32, CPU>)
            linalg.CPU.AddOp(%1228:tensor<[1, 16, 32, 1], Float32, CPU>, %1229:tensor<[1], Float32, CPU>[constant: [-20]][constant:[-20]]) -> (%1230:tensor<[1, 16, 32, 1], Float32, CPU>)
            linalg.CPU.EqualOp(%315:tensor<[1, 1, 32, 1024], Float32, CPU>, %1231:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%1232:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp(%1232:tensor<[1, 1, 32, 1024], UInt8, CPU>, %1227:tensor<[1, 16, 32, 1024], Float32, CPU>, %1230:tensor<[1, 16, 32, 1], Float32, CPU>) -> (%1233:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%1233:tensor<[1, 16, 32, 1024], Float32, CPU>) -> (%1234:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.MatMulOp(%1234:tensor<[1, 16, 32, 1024], Float32, CPU>, %1224:tensor<[1, 16, 1024, 128], Float32, CPU>) -> (%1235:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1235:tensor<[1, 16, 32, 128], Float32, CPU>) -> (%1236:tensor<[1, 32, 16, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1236:tensor<[1, 32, 16, 128], Float32, CPU>) -> (%1236:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.24.self_attn.o_proj"](%1236:tensor<[1, 32, 2048], Float32, CPU>) -> (%1237:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%1237:tensor<[1, 32, 2048], Float32, CPU>, %1220:tensor<[1, 8, 128, 32], Float32, CPU>, %1215:tensor<[1, 8, 32, 128], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.24.mlp <CPU> {
        (%1239:tensor<[1, 32, 2048], Float32, CPU>) -> (%1244:tensor<[1, 32, 2048], Float32, CPU>) {
            linalg.CPU.LinearOp [name="model.layers.24.mlp.gate_proj"](%1239:tensor<[1, 32, 2048], Float32, CPU>) -> (%1240:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.SiLUOp [name="model.layers.24.mlp.act"](%1240:tensor<[1, 32, 6144], Float32, CPU>) -> (%1241:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.24.mlp.up_proj"](%1239:tensor<[1, 32, 2048], Float32, CPU>) -> (%1242:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.MulOp(%1241:tensor<[1, 32, 6144], Float32, CPU>, %1242:tensor<[1, 32, 6144], Float32, CPU>) -> (%1243:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.24.mlp.down_proj"](%1243:tensor<[1, 32, 6144], Float32, CPU>) -> (%1244:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%1244:tensor<[1, 32, 2048], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.25 <CPU> {
        (%1245:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%1282:tensor<[1, 32, 2048], Float32, CPU>, %1257:tensor<[1, 8, 128, 32], Float32, CPU>, %1252:tensor<[1, 8, 32, 128], Float32, CPU>) {
            linalg.CPU.RMSNormOp [name="model.layers.25.input_layernorm"](%1245:tensor<[1, 32, 2048], Float32, CPU>) -> (%1246:tensor<[1, 32, 2048], Float32, CPU>)
            graph.CallGraphOp @model.layers.25.self_attn (%1246:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%1274:tensor<[1, 32, 2048], Float32, CPU>, %1257:tensor<[1, 8, 128, 32], Float32, CPU>, %1252:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.AddOp(%1274:tensor<[1, 32, 2048], Float32, CPU>, %1245:tensor<[1, 32, 2048], Float32, CPU>) -> (%1275:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.25.post_attention_layernorm"](%1275:tensor<[1, 32, 2048], Float32, CPU>) -> (%1276:tensor<[1, 32, 2048], Float32, CPU>)
            graph.CallGraphOp @model.layers.25.mlp (%1276:tensor<[1, 32, 2048], Float32, CPU>) -> (%1281:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.AddOp(%1281:tensor<[1, 32, 2048], Float32, CPU>, %1275:tensor<[1, 32, 2048], Float32, CPU>) -> (%1282:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%1282:tensor<[1, 32, 2048], Float32, CPU>, %1257:tensor<[1, 8, 128, 32], Float32, CPU>, %1252:tensor<[1, 8, 32, 128], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.25.self_attn <CPU> {
        (%1246:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%1274:tensor<[1, 32, 2048], Float32, CPU>, %1257:tensor<[1, 8, 128, 32], Float32, CPU>, %1252:tensor<[1, 8, 32, 128], Float32, CPU>) {
            linalg.CPU.LinearOp [name="model.layers.25.self_attn.q_proj"](%1246:tensor<[1, 32, 2048], Float32, CPU>) -> (%1247:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.25.self_attn.k_proj"](%1246:tensor<[1, 32, 2048], Float32, CPU>) -> (%1248:tensor<[1, 32, 1024], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.25.self_attn.v_proj"](%1246:tensor<[1, 32, 2048], Float32, CPU>) -> (%1249:tensor<[1, 32, 1024], Float32, CPU>)
            linalg.CPU.ViewOp(%1247:tensor<[1, 32, 2048], Float32, CPU>) -> (%1247:tensor<[1, 32, 16, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1247:tensor<[1, 32, 16, 128], Float32, CPU>) -> (%1250:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1248:tensor<[1, 32, 1024], Float32, CPU>) -> (%1248:tensor<[1, 32, 8, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1248:tensor<[1, 32, 8, 128], Float32, CPU>) -> (%1251:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1249:tensor<[1, 32, 1024], Float32, CPU>) -> (%1249:tensor<[1, 32, 8, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1249:tensor<[1, 32, 8, 128], Float32, CPU>) -> (%1252:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.25.self_attn.q_norm"](%1250:tensor<[1, 16, 32, 128], Float32, CPU>) -> (%1253:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.25.self_attn.k_norm"](%1251:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%1254:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.RoPEOp [name="model.layers.25.self_attn.q_rope"](%1253:tensor<[1, 16, 32, 128], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>) -> (%1255:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.RoPEOp [name="model.layers.25.self_attn.k_rope"](%1254:tensor<[1, 8, 32, 128], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>) -> (%1256:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1256:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%1257:tensor<[1, 8, 128, 32], Float32, CPU>)
            linalg.CPU.ConcatOp(%313:tensor<[1, 8, 128, 992], Float32, CPU>, %1257:tensor<[1, 8, 128, 32], Float32, CPU>) -> (%1258:tensor<[1, 8, 128, 1024], Float32, CPU>)
            linalg.CPU.ConcatOp(%314:tensor<[1, 8, 992, 128], Float32, CPU>, %1252:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%1259:tensor<[1, 8, 1024, 128], Float32, CPU>)
            linalg.CPU.RepeatOp(%1258:tensor<[1, 8, 128, 1024], Float32, CPU>) -> (%1260:tensor<[1, 16, 128, 1024], Float32, CPU>)
            linalg.CPU.RepeatOp(%1259:tensor<[1, 8, 1024, 128], Float32, CPU>) -> (%1261:tensor<[1, 16, 1024, 128], Float32, CPU>)
            linalg.CPU.MatMulOp(%1255:tensor<[1, 16, 32, 128], Float32, CPU>, %1260:tensor<[1, 16, 128, 1024], Float32, CPU>) -> (%1262:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.MulOp(%1262:tensor<[1, 16, 32, 1024], Float32, CPU>, %1263:tensor<[1], Float32, CPU>[constant: [0.088388346]][constant:[0.088388346]]) -> (%1264:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.ReduceMinOp(%1264:tensor<[1, 16, 32, 1024], Float32, CPU>) -> (%1265:tensor<[1, 16, 32, 1], Float32, CPU>)
            linalg.CPU.AddOp(%1265:tensor<[1, 16, 32, 1], Float32, CPU>, %1266:tensor<[1], Float32, CPU>[constant: [-20]][constant:[-20]]) -> (%1267:tensor<[1, 16, 32, 1], Float32, CPU>)
            linalg.CPU.EqualOp(%315:tensor<[1, 1, 32, 1024], Float32, CPU>, %1268:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%1269:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp(%1269:tensor<[1, 1, 32, 1024], UInt8, CPU>, %1264:tensor<[1, 16, 32, 1024], Float32, CPU>, %1267:tensor<[1, 16, 32, 1], Float32, CPU>) -> (%1270:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%1270:tensor<[1, 16, 32, 1024], Float32, CPU>) -> (%1271:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.MatMulOp(%1271:tensor<[1, 16, 32, 1024], Float32, CPU>, %1261:tensor<[1, 16, 1024, 128], Float32, CPU>) -> (%1272:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1272:tensor<[1, 16, 32, 128], Float32, CPU>) -> (%1273:tensor<[1, 32, 16, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1273:tensor<[1, 32, 16, 128], Float32, CPU>) -> (%1273:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.25.self_attn.o_proj"](%1273:tensor<[1, 32, 2048], Float32, CPU>) -> (%1274:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%1274:tensor<[1, 32, 2048], Float32, CPU>, %1257:tensor<[1, 8, 128, 32], Float32, CPU>, %1252:tensor<[1, 8, 32, 128], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.25.mlp <CPU> {
        (%1276:tensor<[1, 32, 2048], Float32, CPU>) -> (%1281:tensor<[1, 32, 2048], Float32, CPU>) {
            linalg.CPU.LinearOp [name="model.layers.25.mlp.gate_proj"](%1276:tensor<[1, 32, 2048], Float32, CPU>) -> (%1277:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.SiLUOp [name="model.layers.25.mlp.act"](%1277:tensor<[1, 32, 6144], Float32, CPU>) -> (%1278:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.25.mlp.up_proj"](%1276:tensor<[1, 32, 2048], Float32, CPU>) -> (%1279:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.MulOp(%1278:tensor<[1, 32, 6144], Float32, CPU>, %1279:tensor<[1, 32, 6144], Float32, CPU>) -> (%1280:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.25.mlp.down_proj"](%1280:tensor<[1, 32, 6144], Float32, CPU>) -> (%1281:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%1281:tensor<[1, 32, 2048], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.26 <CPU> {
        (%1282:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%1319:tensor<[1, 32, 2048], Float32, CPU>, %1294:tensor<[1, 8, 128, 32], Float32, CPU>, %1289:tensor<[1, 8, 32, 128], Float32, CPU>) {
            linalg.CPU.RMSNormOp [name="model.layers.26.input_layernorm"](%1282:tensor<[1, 32, 2048], Float32, CPU>) -> (%1283:tensor<[1, 32, 2048], Float32, CPU>)
            graph.CallGraphOp @model.layers.26.self_attn (%1283:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%1311:tensor<[1, 32, 2048], Float32, CPU>, %1294:tensor<[1, 8, 128, 32], Float32, CPU>, %1289:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.AddOp(%1311:tensor<[1, 32, 2048], Float32, CPU>, %1282:tensor<[1, 32, 2048], Float32, CPU>) -> (%1312:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.26.post_attention_layernorm"](%1312:tensor<[1, 32, 2048], Float32, CPU>) -> (%1313:tensor<[1, 32, 2048], Float32, CPU>)
            graph.CallGraphOp @model.layers.26.mlp (%1313:tensor<[1, 32, 2048], Float32, CPU>) -> (%1318:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.AddOp(%1318:tensor<[1, 32, 2048], Float32, CPU>, %1312:tensor<[1, 32, 2048], Float32, CPU>) -> (%1319:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%1319:tensor<[1, 32, 2048], Float32, CPU>, %1294:tensor<[1, 8, 128, 32], Float32, CPU>, %1289:tensor<[1, 8, 32, 128], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.26.self_attn <CPU> {
        (%1283:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%1311:tensor<[1, 32, 2048], Float32, CPU>, %1294:tensor<[1, 8, 128, 32], Float32, CPU>, %1289:tensor<[1, 8, 32, 128], Float32, CPU>) {
            linalg.CPU.LinearOp [name="model.layers.26.self_attn.q_proj"](%1283:tensor<[1, 32, 2048], Float32, CPU>) -> (%1284:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.26.self_attn.k_proj"](%1283:tensor<[1, 32, 2048], Float32, CPU>) -> (%1285:tensor<[1, 32, 1024], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.26.self_attn.v_proj"](%1283:tensor<[1, 32, 2048], Float32, CPU>) -> (%1286:tensor<[1, 32, 1024], Float32, CPU>)
            linalg.CPU.ViewOp(%1284:tensor<[1, 32, 2048], Float32, CPU>) -> (%1284:tensor<[1, 32, 16, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1284:tensor<[1, 32, 16, 128], Float32, CPU>) -> (%1287:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1285:tensor<[1, 32, 1024], Float32, CPU>) -> (%1285:tensor<[1, 32, 8, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1285:tensor<[1, 32, 8, 128], Float32, CPU>) -> (%1288:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1286:tensor<[1, 32, 1024], Float32, CPU>) -> (%1286:tensor<[1, 32, 8, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1286:tensor<[1, 32, 8, 128], Float32, CPU>) -> (%1289:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.26.self_attn.q_norm"](%1287:tensor<[1, 16, 32, 128], Float32, CPU>) -> (%1290:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.26.self_attn.k_norm"](%1288:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%1291:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.RoPEOp [name="model.layers.26.self_attn.q_rope"](%1290:tensor<[1, 16, 32, 128], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>) -> (%1292:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.RoPEOp [name="model.layers.26.self_attn.k_rope"](%1291:tensor<[1, 8, 32, 128], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>) -> (%1293:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1293:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%1294:tensor<[1, 8, 128, 32], Float32, CPU>)
            linalg.CPU.ConcatOp(%313:tensor<[1, 8, 128, 992], Float32, CPU>, %1294:tensor<[1, 8, 128, 32], Float32, CPU>) -> (%1295:tensor<[1, 8, 128, 1024], Float32, CPU>)
            linalg.CPU.ConcatOp(%314:tensor<[1, 8, 992, 128], Float32, CPU>, %1289:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%1296:tensor<[1, 8, 1024, 128], Float32, CPU>)
            linalg.CPU.RepeatOp(%1295:tensor<[1, 8, 128, 1024], Float32, CPU>) -> (%1297:tensor<[1, 16, 128, 1024], Float32, CPU>)
            linalg.CPU.RepeatOp(%1296:tensor<[1, 8, 1024, 128], Float32, CPU>) -> (%1298:tensor<[1, 16, 1024, 128], Float32, CPU>)
            linalg.CPU.MatMulOp(%1292:tensor<[1, 16, 32, 128], Float32, CPU>, %1297:tensor<[1, 16, 128, 1024], Float32, CPU>) -> (%1299:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.MulOp(%1299:tensor<[1, 16, 32, 1024], Float32, CPU>, %1300:tensor<[1], Float32, CPU>[constant: [0.088388346]][constant:[0.088388346]]) -> (%1301:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.ReduceMinOp(%1301:tensor<[1, 16, 32, 1024], Float32, CPU>) -> (%1302:tensor<[1, 16, 32, 1], Float32, CPU>)
            linalg.CPU.AddOp(%1302:tensor<[1, 16, 32, 1], Float32, CPU>, %1303:tensor<[1], Float32, CPU>[constant: [-20]][constant:[-20]]) -> (%1304:tensor<[1, 16, 32, 1], Float32, CPU>)
            linalg.CPU.EqualOp(%315:tensor<[1, 1, 32, 1024], Float32, CPU>, %1305:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%1306:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp(%1306:tensor<[1, 1, 32, 1024], UInt8, CPU>, %1301:tensor<[1, 16, 32, 1024], Float32, CPU>, %1304:tensor<[1, 16, 32, 1], Float32, CPU>) -> (%1307:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%1307:tensor<[1, 16, 32, 1024], Float32, CPU>) -> (%1308:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.MatMulOp(%1308:tensor<[1, 16, 32, 1024], Float32, CPU>, %1298:tensor<[1, 16, 1024, 128], Float32, CPU>) -> (%1309:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1309:tensor<[1, 16, 32, 128], Float32, CPU>) -> (%1310:tensor<[1, 32, 16, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1310:tensor<[1, 32, 16, 128], Float32, CPU>) -> (%1310:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.26.self_attn.o_proj"](%1310:tensor<[1, 32, 2048], Float32, CPU>) -> (%1311:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%1311:tensor<[1, 32, 2048], Float32, CPU>, %1294:tensor<[1, 8, 128, 32], Float32, CPU>, %1289:tensor<[1, 8, 32, 128], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.26.mlp <CPU> {
        (%1313:tensor<[1, 32, 2048], Float32, CPU>) -> (%1318:tensor<[1, 32, 2048], Float32, CPU>) {
            linalg.CPU.LinearOp [name="model.layers.26.mlp.gate_proj"](%1313:tensor<[1, 32, 2048], Float32, CPU>) -> (%1314:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.SiLUOp [name="model.layers.26.mlp.act"](%1314:tensor<[1, 32, 6144], Float32, CPU>) -> (%1315:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.26.mlp.up_proj"](%1313:tensor<[1, 32, 2048], Float32, CPU>) -> (%1316:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.MulOp(%1315:tensor<[1, 32, 6144], Float32, CPU>, %1316:tensor<[1, 32, 6144], Float32, CPU>) -> (%1317:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.26.mlp.down_proj"](%1317:tensor<[1, 32, 6144], Float32, CPU>) -> (%1318:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%1318:tensor<[1, 32, 2048], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.27 <CPU> {
        (%1319:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%1356:tensor<[1, 32, 2048], Float32, CPU>, %1331:tensor<[1, 8, 128, 32], Float32, CPU>, %1326:tensor<[1, 8, 32, 128], Float32, CPU>) {
            linalg.CPU.RMSNormOp [name="model.layers.27.input_layernorm"](%1319:tensor<[1, 32, 2048], Float32, CPU>) -> (%1320:tensor<[1, 32, 2048], Float32, CPU>)
            graph.CallGraphOp @model.layers.27.self_attn (%1320:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%1348:tensor<[1, 32, 2048], Float32, CPU>, %1331:tensor<[1, 8, 128, 32], Float32, CPU>, %1326:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.AddOp(%1348:tensor<[1, 32, 2048], Float32, CPU>, %1319:tensor<[1, 32, 2048], Float32, CPU>) -> (%1349:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.27.post_attention_layernorm"](%1349:tensor<[1, 32, 2048], Float32, CPU>) -> (%1350:tensor<[1, 32, 2048], Float32, CPU>)
            graph.CallGraphOp @model.layers.27.mlp (%1350:tensor<[1, 32, 2048], Float32, CPU>) -> (%1355:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.AddOp(%1355:tensor<[1, 32, 2048], Float32, CPU>, %1349:tensor<[1, 32, 2048], Float32, CPU>) -> (%1356:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%1356:tensor<[1, 32, 2048], Float32, CPU>, %1331:tensor<[1, 8, 128, 32], Float32, CPU>, %1326:tensor<[1, 8, 32, 128], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.27.self_attn <CPU> {
        (%1320:tensor<[1, 32, 2048], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>, %313:tensor<[1, 8, 128, 992], Float32, CPU>, %314:tensor<[1, 8, 992, 128], Float32, CPU>, %315:tensor<[1, 1, 32, 1024], Float32, CPU>) -> (%1348:tensor<[1, 32, 2048], Float32, CPU>, %1331:tensor<[1, 8, 128, 32], Float32, CPU>, %1326:tensor<[1, 8, 32, 128], Float32, CPU>) {
            linalg.CPU.LinearOp [name="model.layers.27.self_attn.q_proj"](%1320:tensor<[1, 32, 2048], Float32, CPU>) -> (%1321:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.27.self_attn.k_proj"](%1320:tensor<[1, 32, 2048], Float32, CPU>) -> (%1322:tensor<[1, 32, 1024], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.27.self_attn.v_proj"](%1320:tensor<[1, 32, 2048], Float32, CPU>) -> (%1323:tensor<[1, 32, 1024], Float32, CPU>)
            linalg.CPU.ViewOp(%1321:tensor<[1, 32, 2048], Float32, CPU>) -> (%1321:tensor<[1, 32, 16, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1321:tensor<[1, 32, 16, 128], Float32, CPU>) -> (%1324:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1322:tensor<[1, 32, 1024], Float32, CPU>) -> (%1322:tensor<[1, 32, 8, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1322:tensor<[1, 32, 8, 128], Float32, CPU>) -> (%1325:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1323:tensor<[1, 32, 1024], Float32, CPU>) -> (%1323:tensor<[1, 32, 8, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1323:tensor<[1, 32, 8, 128], Float32, CPU>) -> (%1326:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.27.self_attn.q_norm"](%1324:tensor<[1, 16, 32, 128], Float32, CPU>) -> (%1327:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.RMSNormOp [name="model.layers.27.self_attn.k_norm"](%1325:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%1328:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.RoPEOp [name="model.layers.27.self_attn.q_rope"](%1327:tensor<[1, 16, 32, 128], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>) -> (%1329:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.RoPEOp [name="model.layers.27.self_attn.k_rope"](%1328:tensor<[1, 8, 32, 128], Float32, CPU>, %318:tensor<[1, 32, 128], Float32, CPU>, %319:tensor<[1, 32, 128], Float32, CPU>) -> (%1330:tensor<[1, 8, 32, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1330:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%1331:tensor<[1, 8, 128, 32], Float32, CPU>)
            linalg.CPU.ConcatOp(%313:tensor<[1, 8, 128, 992], Float32, CPU>, %1331:tensor<[1, 8, 128, 32], Float32, CPU>) -> (%1332:tensor<[1, 8, 128, 1024], Float32, CPU>)
            linalg.CPU.ConcatOp(%314:tensor<[1, 8, 992, 128], Float32, CPU>, %1326:tensor<[1, 8, 32, 128], Float32, CPU>) -> (%1333:tensor<[1, 8, 1024, 128], Float32, CPU>)
            linalg.CPU.RepeatOp(%1332:tensor<[1, 8, 128, 1024], Float32, CPU>) -> (%1334:tensor<[1, 16, 128, 1024], Float32, CPU>)
            linalg.CPU.RepeatOp(%1333:tensor<[1, 8, 1024, 128], Float32, CPU>) -> (%1335:tensor<[1, 16, 1024, 128], Float32, CPU>)
            linalg.CPU.MatMulOp(%1329:tensor<[1, 16, 32, 128], Float32, CPU>, %1334:tensor<[1, 16, 128, 1024], Float32, CPU>) -> (%1336:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.MulOp(%1336:tensor<[1, 16, 32, 1024], Float32, CPU>, %1337:tensor<[1], Float32, CPU>[constant: [0.088388346]][constant:[0.088388346]]) -> (%1338:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.ReduceMinOp(%1338:tensor<[1, 16, 32, 1024], Float32, CPU>) -> (%1339:tensor<[1, 16, 32, 1], Float32, CPU>)
            linalg.CPU.AddOp(%1339:tensor<[1, 16, 32, 1], Float32, CPU>, %1340:tensor<[1], Float32, CPU>[constant: [-20]][constant:[-20]]) -> (%1341:tensor<[1, 16, 32, 1], Float32, CPU>)
            linalg.CPU.EqualOp(%315:tensor<[1, 1, 32, 1024], Float32, CPU>, %1342:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%1343:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp(%1343:tensor<[1, 1, 32, 1024], UInt8, CPU>, %1338:tensor<[1, 16, 32, 1024], Float32, CPU>, %1341:tensor<[1, 16, 32, 1], Float32, CPU>) -> (%1344:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.SoftmaxOp(%1344:tensor<[1, 16, 32, 1024], Float32, CPU>) -> (%1345:tensor<[1, 16, 32, 1024], Float32, CPU>)
            linalg.CPU.MatMulOp(%1345:tensor<[1, 16, 32, 1024], Float32, CPU>, %1335:tensor<[1, 16, 1024, 128], Float32, CPU>) -> (%1346:tensor<[1, 16, 32, 128], Float32, CPU>)
            linalg.CPU.TransposeOp(%1346:tensor<[1, 16, 32, 128], Float32, CPU>) -> (%1347:tensor<[1, 32, 16, 128], Float32, CPU>)
            linalg.CPU.ViewOp(%1347:tensor<[1, 32, 16, 128], Float32, CPU>) -> (%1347:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.27.self_attn.o_proj"](%1347:tensor<[1, 32, 2048], Float32, CPU>) -> (%1348:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%1348:tensor<[1, 32, 2048], Float32, CPU>, %1331:tensor<[1, 8, 128, 32], Float32, CPU>, %1326:tensor<[1, 8, 32, 128], Float32, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.27.mlp <CPU> {
        (%1350:tensor<[1, 32, 2048], Float32, CPU>) -> (%1355:tensor<[1, 32, 2048], Float32, CPU>) {
            linalg.CPU.LinearOp [name="model.layers.27.mlp.gate_proj"](%1350:tensor<[1, 32, 2048], Float32, CPU>) -> (%1351:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.SiLUOp [name="model.layers.27.mlp.act"](%1351:tensor<[1, 32, 6144], Float32, CPU>) -> (%1352:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.27.mlp.up_proj"](%1350:tensor<[1, 32, 2048], Float32, CPU>) -> (%1353:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.MulOp(%1352:tensor<[1, 32, 6144], Float32, CPU>, %1353:tensor<[1, 32, 6144], Float32, CPU>) -> (%1354:tensor<[1, 32, 6144], Float32, CPU>)
            linalg.CPU.LinearOp [name="model.layers.27.mlp.down_proj"](%1354:tensor<[1, 32, 6144], Float32, CPU>) -> (%1355:tensor<[1, 32, 2048], Float32, CPU>)
            cf.ReturnOp (%1355:tensor<[1, 32, 2048], Float32, CPU>) -> ()
        }
    }
    linalg.CPU.LinearOp [name="lm_head"](%1357:tensor<[1, 32, 2048], Float32, CPU>) -> (%1358:tensor<[1, 32, 151936], Float32, CPU>)
    //        
    //      o o    
    //            
    //       
    //             
    //        
}
 
