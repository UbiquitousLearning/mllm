@main () -> () {
    graph.SubGraphOp @init <notype> [symbol:init] {
        () -> () {
            tensor.CPU.register () -> (%105:tensor<[151936, 2048], Float32, CPU>[@model.embed_tokens.weight][symbol:model.embed_tokens.weight])[symbol:model.embed_tokens.weight]
            tensor.CPU.register () -> (%76:tensor<[2048, 2048], Float32, CPU>[@model.layers.0.self_attn.q_proj.weight][symbol:model.layers.0.self_attn.q_proj.weight])[symbol:model.layers.0.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%133:tensor<[1024, 2048], Float32, CPU>[@model.layers.0.self_attn.k_proj.weight][symbol:model.layers.0.self_attn.k_proj.weight])[symbol:model.layers.0.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%179:tensor<[1024, 2048], Float32, CPU>[@model.layers.0.self_attn.v_proj.weight][symbol:model.layers.0.self_attn.v_proj.weight])[symbol:model.layers.0.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%269:tensor<[2048, 2048], Float32, CPU>[@model.layers.0.self_attn.o_proj.weight][symbol:model.layers.0.self_attn.o_proj.weight])[symbol:model.layers.0.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%9:tensor<[6144, 2048], Float32, CPU>[@model.layers.0.mlp.gate_proj.weight][symbol:model.layers.0.mlp.gate_proj.weight])[symbol:model.layers.0.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%111:tensor<[6144, 2048], Float32, CPU>[@model.layers.0.mlp.up_proj.weight][symbol:model.layers.0.mlp.up_proj.weight])[symbol:model.layers.0.mlp.up_proj.weight]
            tensor.CPU.register () -> (%184:tensor<[2048, 6144], Float32, CPU>[@model.layers.0.mlp.down_proj.weight][symbol:model.layers.0.mlp.down_proj.weight])[symbol:model.layers.0.mlp.down_proj.weight]
            tensor.CPU.register () -> (%285:tensor<[2048, 2048], Float32, CPU>[@model.layers.1.self_attn.q_proj.weight][symbol:model.layers.1.self_attn.q_proj.weight])[symbol:model.layers.1.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%32:tensor<[1024, 2048], Float32, CPU>[@model.layers.1.self_attn.k_proj.weight][symbol:model.layers.1.self_attn.k_proj.weight])[symbol:model.layers.1.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%154:tensor<[1024, 2048], Float32, CPU>[@model.layers.1.self_attn.v_proj.weight][symbol:model.layers.1.self_attn.v_proj.weight])[symbol:model.layers.1.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%20:tensor<[2048, 2048], Float32, CPU>[@model.layers.1.self_attn.o_proj.weight][symbol:model.layers.1.self_attn.o_proj.weight])[symbol:model.layers.1.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%245:tensor<[6144, 2048], Float32, CPU>[@model.layers.1.mlp.gate_proj.weight][symbol:model.layers.1.mlp.gate_proj.weight])[symbol:model.layers.1.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%230:tensor<[6144, 2048], Float32, CPU>[@model.layers.1.mlp.up_proj.weight][symbol:model.layers.1.mlp.up_proj.weight])[symbol:model.layers.1.mlp.up_proj.weight]
            tensor.CPU.register () -> (%43:tensor<[2048, 6144], Float32, CPU>[@model.layers.1.mlp.down_proj.weight][symbol:model.layers.1.mlp.down_proj.weight])[symbol:model.layers.1.mlp.down_proj.weight]
            tensor.CPU.register () -> (%221:tensor<[2048, 2048], Float32, CPU>[@model.layers.2.self_attn.q_proj.weight][symbol:model.layers.2.self_attn.q_proj.weight])[symbol:model.layers.2.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%103:tensor<[1024, 2048], Float32, CPU>[@model.layers.2.self_attn.k_proj.weight][symbol:model.layers.2.self_attn.k_proj.weight])[symbol:model.layers.2.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%47:tensor<[1024, 2048], Float32, CPU>[@model.layers.2.self_attn.v_proj.weight][symbol:model.layers.2.self_attn.v_proj.weight])[symbol:model.layers.2.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%85:tensor<[2048, 2048], Float32, CPU>[@model.layers.2.self_attn.o_proj.weight][symbol:model.layers.2.self_attn.o_proj.weight])[symbol:model.layers.2.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%252:tensor<[6144, 2048], Float32, CPU>[@model.layers.2.mlp.gate_proj.weight][symbol:model.layers.2.mlp.gate_proj.weight])[symbol:model.layers.2.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%24:tensor<[6144, 2048], Float32, CPU>[@model.layers.2.mlp.up_proj.weight][symbol:model.layers.2.mlp.up_proj.weight])[symbol:model.layers.2.mlp.up_proj.weight]
            tensor.CPU.register () -> (%28:tensor<[2048, 6144], Float32, CPU>[@model.layers.2.mlp.down_proj.weight][symbol:model.layers.2.mlp.down_proj.weight])[symbol:model.layers.2.mlp.down_proj.weight]
            tensor.CPU.register () -> (%283:tensor<[2048, 2048], Float32, CPU>[@model.layers.3.self_attn.q_proj.weight][symbol:model.layers.3.self_attn.q_proj.weight])[symbol:model.layers.3.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%48:tensor<[1024, 2048], Float32, CPU>[@model.layers.3.self_attn.k_proj.weight][symbol:model.layers.3.self_attn.k_proj.weight])[symbol:model.layers.3.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%244:tensor<[1024, 2048], Float32, CPU>[@model.layers.3.self_attn.v_proj.weight][symbol:model.layers.3.self_attn.v_proj.weight])[symbol:model.layers.3.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%301:tensor<[2048, 2048], Float32, CPU>[@model.layers.3.self_attn.o_proj.weight][symbol:model.layers.3.self_attn.o_proj.weight])[symbol:model.layers.3.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%129:tensor<[6144, 2048], Float32, CPU>[@model.layers.3.mlp.gate_proj.weight][symbol:model.layers.3.mlp.gate_proj.weight])[symbol:model.layers.3.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%188:tensor<[6144, 2048], Float32, CPU>[@model.layers.3.mlp.up_proj.weight][symbol:model.layers.3.mlp.up_proj.weight])[symbol:model.layers.3.mlp.up_proj.weight]
            tensor.CPU.register () -> (%97:tensor<[2048, 6144], Float32, CPU>[@model.layers.3.mlp.down_proj.weight][symbol:model.layers.3.mlp.down_proj.weight])[symbol:model.layers.3.mlp.down_proj.weight]
            tensor.CPU.register () -> (%164:tensor<[2048, 2048], Float32, CPU>[@model.layers.4.self_attn.q_proj.weight][symbol:model.layers.4.self_attn.q_proj.weight])[symbol:model.layers.4.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%148:tensor<[1024, 2048], Float32, CPU>[@model.layers.4.self_attn.k_proj.weight][symbol:model.layers.4.self_attn.k_proj.weight])[symbol:model.layers.4.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%279:tensor<[1024, 2048], Float32, CPU>[@model.layers.4.self_attn.v_proj.weight][symbol:model.layers.4.self_attn.v_proj.weight])[symbol:model.layers.4.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%91:tensor<[2048, 2048], Float32, CPU>[@model.layers.4.self_attn.o_proj.weight][symbol:model.layers.4.self_attn.o_proj.weight])[symbol:model.layers.4.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%189:tensor<[6144, 2048], Float32, CPU>[@model.layers.4.mlp.gate_proj.weight][symbol:model.layers.4.mlp.gate_proj.weight])[symbol:model.layers.4.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%156:tensor<[6144, 2048], Float32, CPU>[@model.layers.4.mlp.up_proj.weight][symbol:model.layers.4.mlp.up_proj.weight])[symbol:model.layers.4.mlp.up_proj.weight]
            tensor.CPU.register () -> (%153:tensor<[2048, 6144], Float32, CPU>[@model.layers.4.mlp.down_proj.weight][symbol:model.layers.4.mlp.down_proj.weight])[symbol:model.layers.4.mlp.down_proj.weight]
            tensor.CPU.register () -> (%78:tensor<[2048, 2048], Float32, CPU>[@model.layers.5.self_attn.q_proj.weight][symbol:model.layers.5.self_attn.q_proj.weight])[symbol:model.layers.5.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%72:tensor<[1024, 2048], Float32, CPU>[@model.layers.5.self_attn.k_proj.weight][symbol:model.layers.5.self_attn.k_proj.weight])[symbol:model.layers.5.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%289:tensor<[1024, 2048], Float32, CPU>[@model.layers.5.self_attn.v_proj.weight][symbol:model.layers.5.self_attn.v_proj.weight])[symbol:model.layers.5.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%264:tensor<[2048, 2048], Float32, CPU>[@model.layers.5.self_attn.o_proj.weight][symbol:model.layers.5.self_attn.o_proj.weight])[symbol:model.layers.5.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%4:tensor<[6144, 2048], Float32, CPU>[@model.layers.5.mlp.gate_proj.weight][symbol:model.layers.5.mlp.gate_proj.weight])[symbol:model.layers.5.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%308:tensor<[6144, 2048], Float32, CPU>[@model.layers.5.mlp.up_proj.weight][symbol:model.layers.5.mlp.up_proj.weight])[symbol:model.layers.5.mlp.up_proj.weight]
            tensor.CPU.register () -> (%74:tensor<[2048, 6144], Float32, CPU>[@model.layers.5.mlp.down_proj.weight][symbol:model.layers.5.mlp.down_proj.weight])[symbol:model.layers.5.mlp.down_proj.weight]
            tensor.CPU.register () -> (%59:tensor<[2048, 2048], Float32, CPU>[@model.layers.6.self_attn.q_proj.weight][symbol:model.layers.6.self_attn.q_proj.weight])[symbol:model.layers.6.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%208:tensor<[1024, 2048], Float32, CPU>[@model.layers.6.self_attn.k_proj.weight][symbol:model.layers.6.self_attn.k_proj.weight])[symbol:model.layers.6.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%238:tensor<[1024, 2048], Float32, CPU>[@model.layers.6.self_attn.v_proj.weight][symbol:model.layers.6.self_attn.v_proj.weight])[symbol:model.layers.6.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%52:tensor<[2048, 2048], Float32, CPU>[@model.layers.6.self_attn.o_proj.weight][symbol:model.layers.6.self_attn.o_proj.weight])[symbol:model.layers.6.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%80:tensor<[6144, 2048], Float32, CPU>[@model.layers.6.mlp.gate_proj.weight][symbol:model.layers.6.mlp.gate_proj.weight])[symbol:model.layers.6.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%276:tensor<[6144, 2048], Float32, CPU>[@model.layers.6.mlp.up_proj.weight][symbol:model.layers.6.mlp.up_proj.weight])[symbol:model.layers.6.mlp.up_proj.weight]
            tensor.CPU.register () -> (%227:tensor<[2048, 6144], Float32, CPU>[@model.layers.6.mlp.down_proj.weight][symbol:model.layers.6.mlp.down_proj.weight])[symbol:model.layers.6.mlp.down_proj.weight]
            tensor.CPU.register () -> (%287:tensor<[2048, 2048], Float32, CPU>[@model.layers.7.self_attn.q_proj.weight][symbol:model.layers.7.self_attn.q_proj.weight])[symbol:model.layers.7.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%135:tensor<[1024, 2048], Float32, CPU>[@model.layers.7.self_attn.k_proj.weight][symbol:model.layers.7.self_attn.k_proj.weight])[symbol:model.layers.7.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%300:tensor<[1024, 2048], Float32, CPU>[@model.layers.7.self_attn.v_proj.weight][symbol:model.layers.7.self_attn.v_proj.weight])[symbol:model.layers.7.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%251:tensor<[2048, 2048], Float32, CPU>[@model.layers.7.self_attn.o_proj.weight][symbol:model.layers.7.self_attn.o_proj.weight])[symbol:model.layers.7.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%155:tensor<[6144, 2048], Float32, CPU>[@model.layers.7.mlp.gate_proj.weight][symbol:model.layers.7.mlp.gate_proj.weight])[symbol:model.layers.7.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%218:tensor<[6144, 2048], Float32, CPU>[@model.layers.7.mlp.up_proj.weight][symbol:model.layers.7.mlp.up_proj.weight])[symbol:model.layers.7.mlp.up_proj.weight]
            tensor.CPU.register () -> (%275:tensor<[2048, 6144], Float32, CPU>[@model.layers.7.mlp.down_proj.weight][symbol:model.layers.7.mlp.down_proj.weight])[symbol:model.layers.7.mlp.down_proj.weight]
            tensor.CPU.register () -> (%165:tensor<[2048, 2048], Float32, CPU>[@model.layers.8.self_attn.q_proj.weight][symbol:model.layers.8.self_attn.q_proj.weight])[symbol:model.layers.8.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%194:tensor<[1024, 2048], Float32, CPU>[@model.layers.8.self_attn.k_proj.weight][symbol:model.layers.8.self_attn.k_proj.weight])[symbol:model.layers.8.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%181:tensor<[1024, 2048], Float32, CPU>[@model.layers.8.self_attn.v_proj.weight][symbol:model.layers.8.self_attn.v_proj.weight])[symbol:model.layers.8.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%197:tensor<[2048, 2048], Float32, CPU>[@model.layers.8.self_attn.o_proj.weight][symbol:model.layers.8.self_attn.o_proj.weight])[symbol:model.layers.8.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%110:tensor<[6144, 2048], Float32, CPU>[@model.layers.8.mlp.gate_proj.weight][symbol:model.layers.8.mlp.gate_proj.weight])[symbol:model.layers.8.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%236:tensor<[6144, 2048], Float32, CPU>[@model.layers.8.mlp.up_proj.weight][symbol:model.layers.8.mlp.up_proj.weight])[symbol:model.layers.8.mlp.up_proj.weight]
            tensor.CPU.register () -> (%106:tensor<[2048, 6144], Float32, CPU>[@model.layers.8.mlp.down_proj.weight][symbol:model.layers.8.mlp.down_proj.weight])[symbol:model.layers.8.mlp.down_proj.weight]
            tensor.CPU.register () -> (%235:tensor<[2048, 2048], Float32, CPU>[@model.layers.9.self_attn.q_proj.weight][symbol:model.layers.9.self_attn.q_proj.weight])[symbol:model.layers.9.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%69:tensor<[1024, 2048], Float32, CPU>[@model.layers.9.self_attn.k_proj.weight][symbol:model.layers.9.self_attn.k_proj.weight])[symbol:model.layers.9.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%120:tensor<[1024, 2048], Float32, CPU>[@model.layers.9.self_attn.v_proj.weight][symbol:model.layers.9.self_attn.v_proj.weight])[symbol:model.layers.9.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%205:tensor<[2048, 2048], Float32, CPU>[@model.layers.9.self_attn.o_proj.weight][symbol:model.layers.9.self_attn.o_proj.weight])[symbol:model.layers.9.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%263:tensor<[6144, 2048], Float32, CPU>[@model.layers.9.mlp.gate_proj.weight][symbol:model.layers.9.mlp.gate_proj.weight])[symbol:model.layers.9.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%102:tensor<[6144, 2048], Float32, CPU>[@model.layers.9.mlp.up_proj.weight][symbol:model.layers.9.mlp.up_proj.weight])[symbol:model.layers.9.mlp.up_proj.weight]
            tensor.CPU.register () -> (%136:tensor<[2048, 6144], Float32, CPU>[@model.layers.9.mlp.down_proj.weight][symbol:model.layers.9.mlp.down_proj.weight])[symbol:model.layers.9.mlp.down_proj.weight]
            tensor.CPU.register () -> (%278:tensor<[2048, 2048], Float32, CPU>[@model.layers.10.self_attn.q_proj.weight][symbol:model.layers.10.self_attn.q_proj.weight])[symbol:model.layers.10.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%182:tensor<[1024, 2048], Float32, CPU>[@model.layers.10.self_attn.k_proj.weight][symbol:model.layers.10.self_attn.k_proj.weight])[symbol:model.layers.10.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%138:tensor<[1024, 2048], Float32, CPU>[@model.layers.10.self_attn.v_proj.weight][symbol:model.layers.10.self_attn.v_proj.weight])[symbol:model.layers.10.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%233:tensor<[2048, 2048], Float32, CPU>[@model.layers.10.self_attn.o_proj.weight][symbol:model.layers.10.self_attn.o_proj.weight])[symbol:model.layers.10.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%124:tensor<[6144, 2048], Float32, CPU>[@model.layers.10.mlp.gate_proj.weight][symbol:model.layers.10.mlp.gate_proj.weight])[symbol:model.layers.10.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%261:tensor<[6144, 2048], Float32, CPU>[@model.layers.10.mlp.up_proj.weight][symbol:model.layers.10.mlp.up_proj.weight])[symbol:model.layers.10.mlp.up_proj.weight]
            tensor.CPU.register () -> (%45:tensor<[2048, 6144], Float32, CPU>[@model.layers.10.mlp.down_proj.weight][symbol:model.layers.10.mlp.down_proj.weight])[symbol:model.layers.10.mlp.down_proj.weight]
            tensor.CPU.register () -> (%274:tensor<[2048, 2048], Float32, CPU>[@model.layers.11.self_attn.q_proj.weight][symbol:model.layers.11.self_attn.q_proj.weight])[symbol:model.layers.11.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%157:tensor<[1024, 2048], Float32, CPU>[@model.layers.11.self_attn.k_proj.weight][symbol:model.layers.11.self_attn.k_proj.weight])[symbol:model.layers.11.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%63:tensor<[1024, 2048], Float32, CPU>[@model.layers.11.self_attn.v_proj.weight][symbol:model.layers.11.self_attn.v_proj.weight])[symbol:model.layers.11.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%118:tensor<[2048, 2048], Float32, CPU>[@model.layers.11.self_attn.o_proj.weight][symbol:model.layers.11.self_attn.o_proj.weight])[symbol:model.layers.11.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%207:tensor<[6144, 2048], Float32, CPU>[@model.layers.11.mlp.gate_proj.weight][symbol:model.layers.11.mlp.gate_proj.weight])[symbol:model.layers.11.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%226:tensor<[6144, 2048], Float32, CPU>[@model.layers.11.mlp.up_proj.weight][symbol:model.layers.11.mlp.up_proj.weight])[symbol:model.layers.11.mlp.up_proj.weight]
            tensor.CPU.register () -> (%224:tensor<[2048, 6144], Float32, CPU>[@model.layers.11.mlp.down_proj.weight][symbol:model.layers.11.mlp.down_proj.weight])[symbol:model.layers.11.mlp.down_proj.weight]
            tensor.CPU.register () -> (%217:tensor<[2048, 2048], Float32, CPU>[@model.layers.12.self_attn.q_proj.weight][symbol:model.layers.12.self_attn.q_proj.weight])[symbol:model.layers.12.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%297:tensor<[1024, 2048], Float32, CPU>[@model.layers.12.self_attn.k_proj.weight][symbol:model.layers.12.self_attn.k_proj.weight])[symbol:model.layers.12.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%94:tensor<[1024, 2048], Float32, CPU>[@model.layers.12.self_attn.v_proj.weight][symbol:model.layers.12.self_attn.v_proj.weight])[symbol:model.layers.12.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%49:tensor<[2048, 2048], Float32, CPU>[@model.layers.12.self_attn.o_proj.weight][symbol:model.layers.12.self_attn.o_proj.weight])[symbol:model.layers.12.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%262:tensor<[6144, 2048], Float32, CPU>[@model.layers.12.mlp.gate_proj.weight][symbol:model.layers.12.mlp.gate_proj.weight])[symbol:model.layers.12.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%255:tensor<[6144, 2048], Float32, CPU>[@model.layers.12.mlp.up_proj.weight][symbol:model.layers.12.mlp.up_proj.weight])[symbol:model.layers.12.mlp.up_proj.weight]
            tensor.CPU.register () -> (%22:tensor<[2048, 6144], Float32, CPU>[@model.layers.12.mlp.down_proj.weight][symbol:model.layers.12.mlp.down_proj.weight])[symbol:model.layers.12.mlp.down_proj.weight]
            tensor.CPU.register () -> (%114:tensor<[2048, 2048], Float32, CPU>[@model.layers.13.self_attn.q_proj.weight][symbol:model.layers.13.self_attn.q_proj.weight])[symbol:model.layers.13.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%152:tensor<[1024, 2048], Float32, CPU>[@model.layers.13.self_attn.k_proj.weight][symbol:model.layers.13.self_attn.k_proj.weight])[symbol:model.layers.13.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%15:tensor<[1024, 2048], Float32, CPU>[@model.layers.13.self_attn.v_proj.weight][symbol:model.layers.13.self_attn.v_proj.weight])[symbol:model.layers.13.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%250:tensor<[2048, 2048], Float32, CPU>[@model.layers.13.self_attn.o_proj.weight][symbol:model.layers.13.self_attn.o_proj.weight])[symbol:model.layers.13.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%247:tensor<[6144, 2048], Float32, CPU>[@model.layers.13.mlp.gate_proj.weight][symbol:model.layers.13.mlp.gate_proj.weight])[symbol:model.layers.13.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%98:tensor<[6144, 2048], Float32, CPU>[@model.layers.13.mlp.up_proj.weight][symbol:model.layers.13.mlp.up_proj.weight])[symbol:model.layers.13.mlp.up_proj.weight]
            tensor.CPU.register () -> (%193:tensor<[2048, 6144], Float32, CPU>[@model.layers.13.mlp.down_proj.weight][symbol:model.layers.13.mlp.down_proj.weight])[symbol:model.layers.13.mlp.down_proj.weight]
            tensor.CPU.register () -> (%209:tensor<[2048, 2048], Float32, CPU>[@model.layers.14.self_attn.q_proj.weight][symbol:model.layers.14.self_attn.q_proj.weight])[symbol:model.layers.14.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%38:tensor<[1024, 2048], Float32, CPU>[@model.layers.14.self_attn.k_proj.weight][symbol:model.layers.14.self_attn.k_proj.weight])[symbol:model.layers.14.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%232:tensor<[1024, 2048], Float32, CPU>[@model.layers.14.self_attn.v_proj.weight][symbol:model.layers.14.self_attn.v_proj.weight])[symbol:model.layers.14.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%168:tensor<[2048, 2048], Float32, CPU>[@model.layers.14.self_attn.o_proj.weight][symbol:model.layers.14.self_attn.o_proj.weight])[symbol:model.layers.14.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%37:tensor<[6144, 2048], Float32, CPU>[@model.layers.14.mlp.gate_proj.weight][symbol:model.layers.14.mlp.gate_proj.weight])[symbol:model.layers.14.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%147:tensor<[6144, 2048], Float32, CPU>[@model.layers.14.mlp.up_proj.weight][symbol:model.layers.14.mlp.up_proj.weight])[symbol:model.layers.14.mlp.up_proj.weight]
            tensor.CPU.register () -> (%163:tensor<[2048, 6144], Float32, CPU>[@model.layers.14.mlp.down_proj.weight][symbol:model.layers.14.mlp.down_proj.weight])[symbol:model.layers.14.mlp.down_proj.weight]
            tensor.CPU.register () -> (%46:tensor<[2048, 2048], Float32, CPU>[@model.layers.15.self_attn.q_proj.weight][symbol:model.layers.15.self_attn.q_proj.weight])[symbol:model.layers.15.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%268:tensor<[1024, 2048], Float32, CPU>[@model.layers.15.self_attn.k_proj.weight][symbol:model.layers.15.self_attn.k_proj.weight])[symbol:model.layers.15.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%117:tensor<[1024, 2048], Float32, CPU>[@model.layers.15.self_attn.v_proj.weight][symbol:model.layers.15.self_attn.v_proj.weight])[symbol:model.layers.15.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%303:tensor<[2048, 2048], Float32, CPU>[@model.layers.15.self_attn.o_proj.weight][symbol:model.layers.15.self_attn.o_proj.weight])[symbol:model.layers.15.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%260:tensor<[6144, 2048], Float32, CPU>[@model.layers.15.mlp.gate_proj.weight][symbol:model.layers.15.mlp.gate_proj.weight])[symbol:model.layers.15.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%42:tensor<[6144, 2048], Float32, CPU>[@model.layers.15.mlp.up_proj.weight][symbol:model.layers.15.mlp.up_proj.weight])[symbol:model.layers.15.mlp.up_proj.weight]
            tensor.CPU.register () -> (%290:tensor<[2048, 6144], Float32, CPU>[@model.layers.15.mlp.down_proj.weight][symbol:model.layers.15.mlp.down_proj.weight])[symbol:model.layers.15.mlp.down_proj.weight]
            tensor.CPU.register () -> (%17:tensor<[2048, 2048], Float32, CPU>[@model.layers.16.self_attn.q_proj.weight][symbol:model.layers.16.self_attn.q_proj.weight])[symbol:model.layers.16.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%228:tensor<[1024, 2048], Float32, CPU>[@model.layers.16.self_attn.k_proj.weight][symbol:model.layers.16.self_attn.k_proj.weight])[symbol:model.layers.16.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%66:tensor<[1024, 2048], Float32, CPU>[@model.layers.16.self_attn.v_proj.weight][symbol:model.layers.16.self_attn.v_proj.weight])[symbol:model.layers.16.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%211:tensor<[2048, 2048], Float32, CPU>[@model.layers.16.self_attn.o_proj.weight][symbol:model.layers.16.self_attn.o_proj.weight])[symbol:model.layers.16.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%130:tensor<[6144, 2048], Float32, CPU>[@model.layers.16.mlp.gate_proj.weight][symbol:model.layers.16.mlp.gate_proj.weight])[symbol:model.layers.16.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%79:tensor<[6144, 2048], Float32, CPU>[@model.layers.16.mlp.up_proj.weight][symbol:model.layers.16.mlp.up_proj.weight])[symbol:model.layers.16.mlp.up_proj.weight]
            tensor.CPU.register () -> (%248:tensor<[2048, 6144], Float32, CPU>[@model.layers.16.mlp.down_proj.weight][symbol:model.layers.16.mlp.down_proj.weight])[symbol:model.layers.16.mlp.down_proj.weight]
            tensor.CPU.register () -> (%64:tensor<[2048, 2048], Float32, CPU>[@model.layers.17.self_attn.q_proj.weight][symbol:model.layers.17.self_attn.q_proj.weight])[symbol:model.layers.17.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%237:tensor<[1024, 2048], Float32, CPU>[@model.layers.17.self_attn.k_proj.weight][symbol:model.layers.17.self_attn.k_proj.weight])[symbol:model.layers.17.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%6:tensor<[1024, 2048], Float32, CPU>[@model.layers.17.self_attn.v_proj.weight][symbol:model.layers.17.self_attn.v_proj.weight])[symbol:model.layers.17.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%125:tensor<[2048, 2048], Float32, CPU>[@model.layers.17.self_attn.o_proj.weight][symbol:model.layers.17.self_attn.o_proj.weight])[symbol:model.layers.17.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%177:tensor<[6144, 2048], Float32, CPU>[@model.layers.17.mlp.gate_proj.weight][symbol:model.layers.17.mlp.gate_proj.weight])[symbol:model.layers.17.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%26:tensor<[6144, 2048], Float32, CPU>[@model.layers.17.mlp.up_proj.weight][symbol:model.layers.17.mlp.up_proj.weight])[symbol:model.layers.17.mlp.up_proj.weight]
            tensor.CPU.register () -> (%25:tensor<[2048, 6144], Float32, CPU>[@model.layers.17.mlp.down_proj.weight][symbol:model.layers.17.mlp.down_proj.weight])[symbol:model.layers.17.mlp.down_proj.weight]
            tensor.CPU.register () -> (%273:tensor<[2048, 2048], Float32, CPU>[@model.layers.18.self_attn.q_proj.weight][symbol:model.layers.18.self_attn.q_proj.weight])[symbol:model.layers.18.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%284:tensor<[1024, 2048], Float32, CPU>[@model.layers.18.self_attn.k_proj.weight][symbol:model.layers.18.self_attn.k_proj.weight])[symbol:model.layers.18.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%18:tensor<[1024, 2048], Float32, CPU>[@model.layers.18.self_attn.v_proj.weight][symbol:model.layers.18.self_attn.v_proj.weight])[symbol:model.layers.18.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%2:tensor<[2048, 2048], Float32, CPU>[@model.layers.18.self_attn.o_proj.weight][symbol:model.layers.18.self_attn.o_proj.weight])[symbol:model.layers.18.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%166:tensor<[6144, 2048], Float32, CPU>[@model.layers.18.mlp.gate_proj.weight][symbol:model.layers.18.mlp.gate_proj.weight])[symbol:model.layers.18.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%271:tensor<[6144, 2048], Float32, CPU>[@model.layers.18.mlp.up_proj.weight][symbol:model.layers.18.mlp.up_proj.weight])[symbol:model.layers.18.mlp.up_proj.weight]
            tensor.CPU.register () -> (%112:tensor<[2048, 6144], Float32, CPU>[@model.layers.18.mlp.down_proj.weight][symbol:model.layers.18.mlp.down_proj.weight])[symbol:model.layers.18.mlp.down_proj.weight]
            tensor.CPU.register () -> (%8:tensor<[2048, 2048], Float32, CPU>[@model.layers.19.self_attn.q_proj.weight][symbol:model.layers.19.self_attn.q_proj.weight])[symbol:model.layers.19.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%286:tensor<[1024, 2048], Float32, CPU>[@model.layers.19.self_attn.k_proj.weight][symbol:model.layers.19.self_attn.k_proj.weight])[symbol:model.layers.19.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%50:tensor<[1024, 2048], Float32, CPU>[@model.layers.19.self_attn.v_proj.weight][symbol:model.layers.19.self_attn.v_proj.weight])[symbol:model.layers.19.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%58:tensor<[2048, 2048], Float32, CPU>[@model.layers.19.self_attn.o_proj.weight][symbol:model.layers.19.self_attn.o_proj.weight])[symbol:model.layers.19.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%281:tensor<[6144, 2048], Float32, CPU>[@model.layers.19.mlp.gate_proj.weight][symbol:model.layers.19.mlp.gate_proj.weight])[symbol:model.layers.19.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%82:tensor<[6144, 2048], Float32, CPU>[@model.layers.19.mlp.up_proj.weight][symbol:model.layers.19.mlp.up_proj.weight])[symbol:model.layers.19.mlp.up_proj.weight]
            tensor.CPU.register () -> (%173:tensor<[2048, 6144], Float32, CPU>[@model.layers.19.mlp.down_proj.weight][symbol:model.layers.19.mlp.down_proj.weight])[symbol:model.layers.19.mlp.down_proj.weight]
            tensor.CPU.register () -> (%280:tensor<[2048, 2048], Float32, CPU>[@model.layers.20.self_attn.q_proj.weight][symbol:model.layers.20.self_attn.q_proj.weight])[symbol:model.layers.20.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%253:tensor<[1024, 2048], Float32, CPU>[@model.layers.20.self_attn.k_proj.weight][symbol:model.layers.20.self_attn.k_proj.weight])[symbol:model.layers.20.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%239:tensor<[1024, 2048], Float32, CPU>[@model.layers.20.self_attn.v_proj.weight][symbol:model.layers.20.self_attn.v_proj.weight])[symbol:model.layers.20.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%41:tensor<[2048, 2048], Float32, CPU>[@model.layers.20.self_attn.o_proj.weight][symbol:model.layers.20.self_attn.o_proj.weight])[symbol:model.layers.20.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%172:tensor<[6144, 2048], Float32, CPU>[@model.layers.20.mlp.gate_proj.weight][symbol:model.layers.20.mlp.gate_proj.weight])[symbol:model.layers.20.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%299:tensor<[6144, 2048], Float32, CPU>[@model.layers.20.mlp.up_proj.weight][symbol:model.layers.20.mlp.up_proj.weight])[symbol:model.layers.20.mlp.up_proj.weight]
            tensor.CPU.register () -> (%123:tensor<[2048, 6144], Float32, CPU>[@model.layers.20.mlp.down_proj.weight][symbol:model.layers.20.mlp.down_proj.weight])[symbol:model.layers.20.mlp.down_proj.weight]
            tensor.CPU.register () -> (%295:tensor<[2048, 2048], Float32, CPU>[@model.layers.21.self_attn.q_proj.weight][symbol:model.layers.21.self_attn.q_proj.weight])[symbol:model.layers.21.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%139:tensor<[1024, 2048], Float32, CPU>[@model.layers.21.self_attn.k_proj.weight][symbol:model.layers.21.self_attn.k_proj.weight])[symbol:model.layers.21.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%142:tensor<[1024, 2048], Float32, CPU>[@model.layers.21.self_attn.v_proj.weight][symbol:model.layers.21.self_attn.v_proj.weight])[symbol:model.layers.21.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%115:tensor<[2048, 2048], Float32, CPU>[@model.layers.21.self_attn.o_proj.weight][symbol:model.layers.21.self_attn.o_proj.weight])[symbol:model.layers.21.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%259:tensor<[6144, 2048], Float32, CPU>[@model.layers.21.mlp.gate_proj.weight][symbol:model.layers.21.mlp.gate_proj.weight])[symbol:model.layers.21.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%162:tensor<[6144, 2048], Float32, CPU>[@model.layers.21.mlp.up_proj.weight][symbol:model.layers.21.mlp.up_proj.weight])[symbol:model.layers.21.mlp.up_proj.weight]
            tensor.CPU.register () -> (%183:tensor<[2048, 6144], Float32, CPU>[@model.layers.21.mlp.down_proj.weight][symbol:model.layers.21.mlp.down_proj.weight])[symbol:model.layers.21.mlp.down_proj.weight]
            tensor.CPU.register () -> (%89:tensor<[2048, 2048], Float32, CPU>[@model.layers.22.self_attn.q_proj.weight][symbol:model.layers.22.self_attn.q_proj.weight])[symbol:model.layers.22.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%36:tensor<[1024, 2048], Float32, CPU>[@model.layers.22.self_attn.k_proj.weight][symbol:model.layers.22.self_attn.k_proj.weight])[symbol:model.layers.22.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%204:tensor<[1024, 2048], Float32, CPU>[@model.layers.22.self_attn.v_proj.weight][symbol:model.layers.22.self_attn.v_proj.weight])[symbol:model.layers.22.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%234:tensor<[2048, 2048], Float32, CPU>[@model.layers.22.self_attn.o_proj.weight][symbol:model.layers.22.self_attn.o_proj.weight])[symbol:model.layers.22.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%198:tensor<[6144, 2048], Float32, CPU>[@model.layers.22.mlp.gate_proj.weight][symbol:model.layers.22.mlp.gate_proj.weight])[symbol:model.layers.22.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%254:tensor<[6144, 2048], Float32, CPU>[@model.layers.22.mlp.up_proj.weight][symbol:model.layers.22.mlp.up_proj.weight])[symbol:model.layers.22.mlp.up_proj.weight]
            tensor.CPU.register () -> (%31:tensor<[2048, 6144], Float32, CPU>[@model.layers.22.mlp.down_proj.weight][symbol:model.layers.22.mlp.down_proj.weight])[symbol:model.layers.22.mlp.down_proj.weight]
            tensor.CPU.register () -> (%109:tensor<[2048, 2048], Float32, CPU>[@model.layers.23.self_attn.q_proj.weight][symbol:model.layers.23.self_attn.q_proj.weight])[symbol:model.layers.23.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%39:tensor<[1024, 2048], Float32, CPU>[@model.layers.23.self_attn.k_proj.weight][symbol:model.layers.23.self_attn.k_proj.weight])[symbol:model.layers.23.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%83:tensor<[1024, 2048], Float32, CPU>[@model.layers.23.self_attn.v_proj.weight][symbol:model.layers.23.self_attn.v_proj.weight])[symbol:model.layers.23.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%176:tensor<[2048, 2048], Float32, CPU>[@model.layers.23.self_attn.o_proj.weight][symbol:model.layers.23.self_attn.o_proj.weight])[symbol:model.layers.23.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%169:tensor<[6144, 2048], Float32, CPU>[@model.layers.23.mlp.gate_proj.weight][symbol:model.layers.23.mlp.gate_proj.weight])[symbol:model.layers.23.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%243:tensor<[6144, 2048], Float32, CPU>[@model.layers.23.mlp.up_proj.weight][symbol:model.layers.23.mlp.up_proj.weight])[symbol:model.layers.23.mlp.up_proj.weight]
            tensor.CPU.register () -> (%149:tensor<[2048, 6144], Float32, CPU>[@model.layers.23.mlp.down_proj.weight][symbol:model.layers.23.mlp.down_proj.weight])[symbol:model.layers.23.mlp.down_proj.weight]
            tensor.CPU.register () -> (%11:tensor<[2048, 2048], Float32, CPU>[@model.layers.24.self_attn.q_proj.weight][symbol:model.layers.24.self_attn.q_proj.weight])[symbol:model.layers.24.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%61:tensor<[1024, 2048], Float32, CPU>[@model.layers.24.self_attn.k_proj.weight][symbol:model.layers.24.self_attn.k_proj.weight])[symbol:model.layers.24.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%81:tensor<[1024, 2048], Float32, CPU>[@model.layers.24.self_attn.v_proj.weight][symbol:model.layers.24.self_attn.v_proj.weight])[symbol:model.layers.24.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%127:tensor<[2048, 2048], Float32, CPU>[@model.layers.24.self_attn.o_proj.weight][symbol:model.layers.24.self_attn.o_proj.weight])[symbol:model.layers.24.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%141:tensor<[6144, 2048], Float32, CPU>[@model.layers.24.mlp.gate_proj.weight][symbol:model.layers.24.mlp.gate_proj.weight])[symbol:model.layers.24.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%126:tensor<[6144, 2048], Float32, CPU>[@model.layers.24.mlp.up_proj.weight][symbol:model.layers.24.mlp.up_proj.weight])[symbol:model.layers.24.mlp.up_proj.weight]
            tensor.CPU.register () -> (%34:tensor<[2048, 6144], Float32, CPU>[@model.layers.24.mlp.down_proj.weight][symbol:model.layers.24.mlp.down_proj.weight])[symbol:model.layers.24.mlp.down_proj.weight]
            tensor.CPU.register () -> (%206:tensor<[2048, 2048], Float32, CPU>[@model.layers.25.self_attn.q_proj.weight][symbol:model.layers.25.self_attn.q_proj.weight])[symbol:model.layers.25.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%27:tensor<[1024, 2048], Float32, CPU>[@model.layers.25.self_attn.k_proj.weight][symbol:model.layers.25.self_attn.k_proj.weight])[symbol:model.layers.25.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%121:tensor<[1024, 2048], Float32, CPU>[@model.layers.25.self_attn.v_proj.weight][symbol:model.layers.25.self_attn.v_proj.weight])[symbol:model.layers.25.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%150:tensor<[2048, 2048], Float32, CPU>[@model.layers.25.self_attn.o_proj.weight][symbol:model.layers.25.self_attn.o_proj.weight])[symbol:model.layers.25.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%249:tensor<[6144, 2048], Float32, CPU>[@model.layers.25.mlp.gate_proj.weight][symbol:model.layers.25.mlp.gate_proj.weight])[symbol:model.layers.25.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%159:tensor<[6144, 2048], Float32, CPU>[@model.layers.25.mlp.up_proj.weight][symbol:model.layers.25.mlp.up_proj.weight])[symbol:model.layers.25.mlp.up_proj.weight]
            tensor.CPU.register () -> (%267:tensor<[2048, 6144], Float32, CPU>[@model.layers.25.mlp.down_proj.weight][symbol:model.layers.25.mlp.down_proj.weight])[symbol:model.layers.25.mlp.down_proj.weight]
            tensor.CPU.register () -> (%265:tensor<[2048, 2048], Float32, CPU>[@model.layers.26.self_attn.q_proj.weight][symbol:model.layers.26.self_attn.q_proj.weight])[symbol:model.layers.26.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%190:tensor<[1024, 2048], Float32, CPU>[@model.layers.26.self_attn.k_proj.weight][symbol:model.layers.26.self_attn.k_proj.weight])[symbol:model.layers.26.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%119:tensor<[1024, 2048], Float32, CPU>[@model.layers.26.self_attn.v_proj.weight][symbol:model.layers.26.self_attn.v_proj.weight])[symbol:model.layers.26.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%88:tensor<[2048, 2048], Float32, CPU>[@model.layers.26.self_attn.o_proj.weight][symbol:model.layers.26.self_attn.o_proj.weight])[symbol:model.layers.26.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%96:tensor<[6144, 2048], Float32, CPU>[@model.layers.26.mlp.gate_proj.weight][symbol:model.layers.26.mlp.gate_proj.weight])[symbol:model.layers.26.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%62:tensor<[6144, 2048], Float32, CPU>[@model.layers.26.mlp.up_proj.weight][symbol:model.layers.26.mlp.up_proj.weight])[symbol:model.layers.26.mlp.up_proj.weight]
            tensor.CPU.register () -> (%220:tensor<[2048, 6144], Float32, CPU>[@model.layers.26.mlp.down_proj.weight][symbol:model.layers.26.mlp.down_proj.weight])[symbol:model.layers.26.mlp.down_proj.weight]
            tensor.CPU.register () -> (%185:tensor<[2048, 2048], Float32, CPU>[@model.layers.27.self_attn.q_proj.weight][symbol:model.layers.27.self_attn.q_proj.weight])[symbol:model.layers.27.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%12:tensor<[1024, 2048], Float32, CPU>[@model.layers.27.self_attn.k_proj.weight][symbol:model.layers.27.self_attn.k_proj.weight])[symbol:model.layers.27.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%54:tensor<[1024, 2048], Float32, CPU>[@model.layers.27.self_attn.v_proj.weight][symbol:model.layers.27.self_attn.v_proj.weight])[symbol:model.layers.27.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%60:tensor<[2048, 2048], Float32, CPU>[@model.layers.27.self_attn.o_proj.weight][symbol:model.layers.27.self_attn.o_proj.weight])[symbol:model.layers.27.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%144:tensor<[6144, 2048], Float32, CPU>[@model.layers.27.mlp.gate_proj.weight][symbol:model.layers.27.mlp.gate_proj.weight])[symbol:model.layers.27.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%146:tensor<[6144, 2048], Float32, CPU>[@model.layers.27.mlp.up_proj.weight][symbol:model.layers.27.mlp.up_proj.weight])[symbol:model.layers.27.mlp.up_proj.weight]
            tensor.CPU.register () -> (%195:tensor<[2048, 6144], Float32, CPU>[@model.layers.27.mlp.down_proj.weight][symbol:model.layers.27.mlp.down_proj.weight])[symbol:model.layers.27.mlp.down_proj.weight]
            tensor.CPU.register () -> (%101:tensor<[151936, 2048], Float32, CPU>[@lm_head.weight][symbol:lm_head.weight])[symbol:lm_head.weight]
        }
    }
    graph.SubGraphOp @deinit <notype> [symbol:deinit] {
        () -> () {
            
        }
    }
    graph.CallGraphOp @model (%318:tensor<[1, 32], Float32, CPU>[qnn_graph_inputs:true], %376:tensor<[1, 32], Int64, CPU>[qnn_graph_inputs:true], %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %320:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %322:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %324:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %326:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %328:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %330:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %332:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %334:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %336:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %338:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %340:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %342:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %344:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %346:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %348:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %350:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %352:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %354:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %356:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %358:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %360:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %362:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %364:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %366:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %368:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %370:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %372:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %374:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %321:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %323:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %325:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %327:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %329:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %331:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %333:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %335:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %337:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %339:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %341:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %343:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %345:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %347:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %349:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %351:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %353:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %355:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %357:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %359:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %361:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %363:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %365:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %367:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %369:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %371:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %373:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %375:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%1501:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %434:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %474:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %514:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %554:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %594:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %634:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %674:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %714:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %754:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %794:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %834:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %874:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %914:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %954:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %994:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1034:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1074:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1114:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1154:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1194:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1234:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1274:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1314:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1354:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1434:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1474:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %395:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %435:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %475:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %515:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %555:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %595:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %635:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %675:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %715:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %755:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %795:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %835:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %875:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %915:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %955:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %995:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1035:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1075:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1115:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1155:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1195:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1235:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1275:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1315:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1355:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1395:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1435:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1475:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
    graph.SubGraphOp @model <CPU> [using_qnn:true, symbol:model] {
        (%318:tensor<[1, 32], Float32, CPU>[qnn_graph_inputs:true], %376:tensor<[1, 32], Int64, CPU>[qnn_graph_inputs:true], %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %320:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %322:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %324:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %326:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %328:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %330:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %332:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %334:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %336:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %338:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %340:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %342:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %344:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %346:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %348:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %350:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %352:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %354:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %356:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %358:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %360:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %362:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %364:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %366:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %368:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %370:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %372:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %374:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %321:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %323:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %325:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %327:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %329:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %331:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %333:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %335:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %337:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %339:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %341:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %343:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %345:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %347:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %349:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %351:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %353:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %355:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %357:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %359:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %361:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %363:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %365:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %367:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %369:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %371:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %373:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %375:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%1501:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %434:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %474:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %514:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %554:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %594:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %634:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %674:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %714:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %754:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %794:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %834:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %874:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %914:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %954:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %994:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1034:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1074:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1114:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1154:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1194:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1234:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1274:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1314:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1354:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1434:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1474:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %395:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %435:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %475:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %515:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %555:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %595:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %635:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %675:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %715:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %755:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %795:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %835:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %875:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %915:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %955:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %995:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1035:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1075:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1115:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1155:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1195:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1235:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1275:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1315:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1355:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1395:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1435:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1475:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) {
            linalg.CPU.EmbeddingOp <name="model.embed_tokens">(%318:tensor<[1, 32], Float32, CPU>[qnn_graph_inputs:true]) -> (%377:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.CastTypeOp <name="model.CastType.0">(%377:tensor<[1, 32, 2048], Float32, CPU>) -> (%378:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.View.0">(%376:tensor<[1, 32], Int64, CPU>[qnn_graph_inputs:true]) -> (%376:tensor<[32], Int64, CPU>)
            linalg.CPU.IndexOp <name="model.Index.0">(%316:tensor<[1, 1024, 128], Int16PerTensor, CPU>) -> (%379:tensor<[1, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.IndexOp <name="model.Index.1">(%317:tensor<[1, 1024, 128], Int16PerTensor, CPU>) -> (%380:tensor<[1, 32, 128], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.0 (%378:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %320:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %321:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%420:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %395:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            graph.CallGraphOp @model.layers.1 (%420:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %322:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %323:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%460:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %434:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %435:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            graph.CallGraphOp @model.layers.2 (%460:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %324:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %325:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%500:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %474:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %475:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            graph.CallGraphOp @model.layers.3 (%500:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %326:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %327:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%540:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %514:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %515:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            graph.CallGraphOp @model.layers.4 (%540:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %328:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %329:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%580:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %554:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %555:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            graph.CallGraphOp @model.layers.5 (%580:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %330:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %331:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%620:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %594:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %595:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            graph.CallGraphOp @model.layers.6 (%620:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %332:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %333:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%660:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %634:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %635:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            graph.CallGraphOp @model.layers.7 (%660:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %334:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %335:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%700:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %674:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %675:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            graph.CallGraphOp @model.layers.8 (%700:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %336:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %337:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%740:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %714:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %715:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            graph.CallGraphOp @model.layers.9 (%740:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %338:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %339:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%780:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %754:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %755:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            graph.CallGraphOp @model.layers.10 (%780:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %340:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %341:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%820:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %794:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %795:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            graph.CallGraphOp @model.layers.11 (%820:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %342:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %343:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%860:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %834:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %835:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            graph.CallGraphOp @model.layers.12 (%860:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %344:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %345:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%900:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %874:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %875:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            graph.CallGraphOp @model.layers.13 (%900:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %346:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %347:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%940:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %914:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %915:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            graph.CallGraphOp @model.layers.14 (%940:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %348:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %349:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%980:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %954:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %955:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            graph.CallGraphOp @model.layers.15 (%980:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %350:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %351:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%1020:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %994:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %995:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            graph.CallGraphOp @model.layers.16 (%1020:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %352:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %353:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%1060:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1034:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1035:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            graph.CallGraphOp @model.layers.17 (%1060:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %354:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %355:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%1100:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1074:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1075:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            graph.CallGraphOp @model.layers.18 (%1100:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %356:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %357:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%1140:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1114:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1115:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            graph.CallGraphOp @model.layers.19 (%1140:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %358:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %359:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%1180:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1154:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1155:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            graph.CallGraphOp @model.layers.20 (%1180:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %360:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %361:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%1220:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1194:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1195:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            graph.CallGraphOp @model.layers.21 (%1220:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %362:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %363:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%1260:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1234:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1235:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            graph.CallGraphOp @model.layers.22 (%1260:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %364:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %365:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%1300:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1274:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1275:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            graph.CallGraphOp @model.layers.23 (%1300:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %366:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %367:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%1340:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1314:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1315:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            graph.CallGraphOp @model.layers.24 (%1340:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %368:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %369:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%1380:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1354:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1355:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            graph.CallGraphOp @model.layers.25 (%1380:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %370:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %371:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%1420:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1395:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            graph.CallGraphOp @model.layers.26 (%1420:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %372:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %373:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%1460:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1434:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1435:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            graph.CallGraphOp @model.layers.27 (%1460:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %374:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %375:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%1500:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1474:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1475:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.RMSNormOp <name="model.norm">(%1500:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1501:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1501:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %434:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %474:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %514:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %554:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %594:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %634:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %674:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %714:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %754:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %794:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %834:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %874:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %914:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %954:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %994:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1034:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1074:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1114:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1154:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1194:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1234:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1274:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1314:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1354:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1434:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1474:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %395:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %435:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %475:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %515:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %555:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %595:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %635:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %675:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %715:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %755:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %795:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %835:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %875:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %915:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %955:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %995:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1035:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1075:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1115:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1155:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1195:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1235:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1275:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1315:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1355:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1395:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1435:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1475:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.0 <CPU> [using_qnn:true, symbol:model.layers.0] {
        (%378:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %320:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %321:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%420:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %395:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) {
            linalg.CPU.RMSNormOp <name="model.layers.0.input_layernorm">(%378:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%381:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.0.self_attn (%381:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %320:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %321:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%412:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %395:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.AddOp <name="model.layers.0.Add.0">(%412:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %378:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%413:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.0.post_attention_layernorm">(%413:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%414:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.0.mlp (%414:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%419:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.0.Add.1">(%419:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %413:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%420:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%420:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %395:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.0.self_attn <CPU> [using_qnn:true, symbol:model.layers.0.self_attn] {
        (%381:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %320:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %321:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%412:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %395:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) {
            linalg.CPU.LinearOp <name="model.layers.0.self_attn.q_proj">(%381:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%382:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.0.self_attn.k_proj">(%381:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%383:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.0.self_attn.v_proj">(%381:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%384:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.0.self_attn.View.0">(%382:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%382:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.0.self_attn.Transpose.0">(%382:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%385:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.0.self_attn.View.1">(%383:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%383:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.0.self_attn.Transpose.1">(%383:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%386:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.0.self_attn.View.2">(%384:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%384:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.0.self_attn.Transpose.2">(%384:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%387:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.0.self_attn.q_norm">(%385:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%388:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.0.self_attn.k_norm">(%386:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%389:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.0.self_attn.q_rope">(%388:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%390:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.0.self_attn.k_rope">(%389:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%391:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.0.self_attn.CastType.0">(%391:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%392:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.0.self_attn.CastType.1">(%392:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%393:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.0.self_attn.Transpose.3">(%393:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.CastTypeOp <name="model.layers.0.self_attn.CastType.2">(%387:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%395:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.ConcatOp <name="model.layers.0.self_attn.Concat.0">(%320:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> (%396:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.0.self_attn.Concat.1">(%321:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %395:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> (%397:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.0.self_attn.Repeat.0">(%396:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%398:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.0.self_attn.Repeat.1">(%397:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%399:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.0.self_attn.MatMul.0">(%390:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %398:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%400:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.0.self_attn.Mul.0">(%400:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %401:tensor<[1], Int16PerTensor, CPU>[constant:[0]]) -> (%402:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.0.self_attn.ReduceMin.0">(%402:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%403:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.0.self_attn.Add.0">(%403:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %404:tensor<[1], Int16PerTensor, CPU>[constant:[9.1807e-41]]) -> (%405:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.0.self_attn.Equal.0">(%319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %406:tensor<[1], Float32, CPU>[constant:[0]]) -> (%407:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.0.self_attn.Where.0">(%407:tensor<[1, 1, 32, 1024], UInt8, CPU>, %402:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %405:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%408:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.0.self_attn.Softmax.0">(%408:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%409:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.0.self_attn.MatMul.1">(%409:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %399:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%410:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.0.self_attn.Transpose.4">(%410:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%411:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.0.self_attn.View.3">(%411:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%411:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.0.self_attn.o_proj">(%411:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%412:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%412:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %395:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.0.mlp <CPU> [using_qnn:true, symbol:model.layers.0.mlp] {
        (%414:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%419:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.0.mlp.gate_proj">(%414:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%415:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.0.mlp.act">(%415:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%416:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.0.mlp.up_proj">(%414:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%417:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.0.mlp.Mul.0">(%416:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %417:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%418:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.0.mlp.down_proj">(%418:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%419:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%419:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.1 <CPU> [using_qnn:true, symbol:model.layers.1] {
        (%420:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %322:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %323:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%460:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %434:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %435:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) {
            linalg.CPU.RMSNormOp <name="model.layers.1.input_layernorm">(%420:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%421:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.1.self_attn (%421:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %322:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %323:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%452:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %434:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %435:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.AddOp <name="model.layers.1.Add.0">(%452:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %420:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%453:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.1.post_attention_layernorm">(%453:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%454:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.1.mlp (%454:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%459:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.1.Add.1">(%459:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %453:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%460:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%460:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %434:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %435:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.1.self_attn <CPU> [using_qnn:true, symbol:model.layers.1.self_attn] {
        (%421:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %322:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %323:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%452:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %434:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %435:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) {
            linalg.CPU.LinearOp <name="model.layers.1.self_attn.q_proj">(%421:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%422:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.1.self_attn.k_proj">(%421:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%423:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.1.self_attn.v_proj">(%421:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%424:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.1.self_attn.View.0">(%422:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%422:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.1.self_attn.Transpose.0">(%422:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%425:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.1.self_attn.View.1">(%423:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%423:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.1.self_attn.Transpose.1">(%423:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%426:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.1.self_attn.View.2">(%424:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%424:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.1.self_attn.Transpose.2">(%424:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%427:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.1.self_attn.q_norm">(%425:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%428:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.1.self_attn.k_norm">(%426:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%429:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.1.self_attn.q_rope">(%428:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%430:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.1.self_attn.k_rope">(%429:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%431:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.1.self_attn.CastType.0">(%431:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%432:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.1.self_attn.CastType.1">(%432:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%433:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.1.self_attn.Transpose.3">(%433:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%434:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.CastTypeOp <name="model.layers.1.self_attn.CastType.2">(%427:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%435:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.ConcatOp <name="model.layers.1.self_attn.Concat.0">(%322:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %434:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> (%436:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.1.self_attn.Concat.1">(%323:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %435:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> (%437:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.1.self_attn.Repeat.0">(%436:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%438:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.1.self_attn.Repeat.1">(%437:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%439:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.1.self_attn.MatMul.0">(%430:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %438:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%440:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.1.self_attn.Mul.0">(%440:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %441:tensor<[1], Int16PerTensor, CPU>[constant:[0]]) -> (%442:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.1.self_attn.ReduceMin.0">(%442:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%443:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.1.self_attn.Add.0">(%443:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %444:tensor<[1], Int16PerTensor, CPU>[constant:[9.1807e-41]]) -> (%445:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.1.self_attn.Equal.0">(%319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %446:tensor<[1], Float32, CPU>[constant:[0]]) -> (%447:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.1.self_attn.Where.0">(%447:tensor<[1, 1, 32, 1024], UInt8, CPU>, %442:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %445:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%448:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.1.self_attn.Softmax.0">(%448:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%449:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.1.self_attn.MatMul.1">(%449:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %439:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%450:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.1.self_attn.Transpose.4">(%450:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%451:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.1.self_attn.View.3">(%451:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%451:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.1.self_attn.o_proj">(%451:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%452:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%452:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %434:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %435:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.1.mlp <CPU> [using_qnn:true, symbol:model.layers.1.mlp] {
        (%454:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%459:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.1.mlp.gate_proj">(%454:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%455:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.1.mlp.act">(%455:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%456:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.1.mlp.up_proj">(%454:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%457:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.1.mlp.Mul.0">(%456:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %457:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%458:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.1.mlp.down_proj">(%458:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%459:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%459:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.2 <CPU> [using_qnn:true, symbol:model.layers.2] {
        (%460:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %324:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %325:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%500:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %474:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %475:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) {
            linalg.CPU.RMSNormOp <name="model.layers.2.input_layernorm">(%460:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%461:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.2.self_attn (%461:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %324:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %325:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%492:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %474:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %475:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.AddOp <name="model.layers.2.Add.0">(%492:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %460:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%493:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.2.post_attention_layernorm">(%493:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%494:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.2.mlp (%494:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%499:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.2.Add.1">(%499:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %493:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%500:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%500:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %474:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %475:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.2.self_attn <CPU> [using_qnn:true, symbol:model.layers.2.self_attn] {
        (%461:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %324:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %325:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%492:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %474:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %475:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) {
            linalg.CPU.LinearOp <name="model.layers.2.self_attn.q_proj">(%461:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%462:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.2.self_attn.k_proj">(%461:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%463:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.2.self_attn.v_proj">(%461:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%464:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.2.self_attn.View.0">(%462:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%462:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.2.self_attn.Transpose.0">(%462:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%465:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.2.self_attn.View.1">(%463:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%463:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.2.self_attn.Transpose.1">(%463:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%466:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.2.self_attn.View.2">(%464:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%464:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.2.self_attn.Transpose.2">(%464:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%467:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.2.self_attn.q_norm">(%465:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%468:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.2.self_attn.k_norm">(%466:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%469:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.2.self_attn.q_rope">(%468:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%470:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.2.self_attn.k_rope">(%469:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%471:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.2.self_attn.CastType.0">(%471:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%472:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.2.self_attn.CastType.1">(%472:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%473:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.2.self_attn.Transpose.3">(%473:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%474:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.CastTypeOp <name="model.layers.2.self_attn.CastType.2">(%467:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%475:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.ConcatOp <name="model.layers.2.self_attn.Concat.0">(%324:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %474:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> (%476:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.2.self_attn.Concat.1">(%325:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %475:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> (%477:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.2.self_attn.Repeat.0">(%476:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%478:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.2.self_attn.Repeat.1">(%477:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%479:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.2.self_attn.MatMul.0">(%470:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %478:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%480:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.2.self_attn.Mul.0">(%480:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %481:tensor<[1], Int16PerTensor, CPU>[constant:[1]]) -> (%482:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.2.self_attn.ReduceMin.0">(%482:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%483:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.2.self_attn.Add.0">(%483:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %484:tensor<[1], Int16PerTensor, CPU>[constant:[9.1807e-41]]) -> (%485:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.2.self_attn.Equal.0">(%319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %486:tensor<[1], Float32, CPU>[constant:[0]]) -> (%487:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.2.self_attn.Where.0">(%487:tensor<[1, 1, 32, 1024], UInt8, CPU>, %482:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %485:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%488:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.2.self_attn.Softmax.0">(%488:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%489:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.2.self_attn.MatMul.1">(%489:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %479:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%490:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.2.self_attn.Transpose.4">(%490:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%491:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.2.self_attn.View.3">(%491:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%491:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.2.self_attn.o_proj">(%491:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%492:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%492:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %474:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %475:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.2.mlp <CPU> [using_qnn:true, symbol:model.layers.2.mlp] {
        (%494:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%499:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.2.mlp.gate_proj">(%494:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%495:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.2.mlp.act">(%495:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%496:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.2.mlp.up_proj">(%494:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%497:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.2.mlp.Mul.0">(%496:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %497:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%498:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.2.mlp.down_proj">(%498:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%499:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%499:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.3 <CPU> [using_qnn:true, symbol:model.layers.3] {
        (%500:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %326:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %327:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%540:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %514:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %515:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) {
            linalg.CPU.RMSNormOp <name="model.layers.3.input_layernorm">(%500:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%501:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.3.self_attn (%501:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %326:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %327:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%532:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %514:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %515:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.AddOp <name="model.layers.3.Add.0">(%532:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %500:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%533:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.3.post_attention_layernorm">(%533:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%534:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.3.mlp (%534:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%539:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.3.Add.1">(%539:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %533:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%540:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%540:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %514:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %515:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.3.self_attn <CPU> [using_qnn:true, symbol:model.layers.3.self_attn] {
        (%501:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %326:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %327:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%532:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %514:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %515:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) {
            linalg.CPU.LinearOp <name="model.layers.3.self_attn.q_proj">(%501:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%502:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.3.self_attn.k_proj">(%501:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%503:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.3.self_attn.v_proj">(%501:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%504:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.3.self_attn.View.0">(%502:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%502:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.3.self_attn.Transpose.0">(%502:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%505:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.3.self_attn.View.1">(%503:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%503:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.3.self_attn.Transpose.1">(%503:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%506:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.3.self_attn.View.2">(%504:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%504:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.3.self_attn.Transpose.2">(%504:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%507:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.3.self_attn.q_norm">(%505:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%508:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.3.self_attn.k_norm">(%506:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%509:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.3.self_attn.q_rope">(%508:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%510:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.3.self_attn.k_rope">(%509:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%511:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.3.self_attn.CastType.0">(%511:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%512:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.3.self_attn.CastType.1">(%512:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%513:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.3.self_attn.Transpose.3">(%513:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%514:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.CastTypeOp <name="model.layers.3.self_attn.CastType.2">(%507:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%515:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.ConcatOp <name="model.layers.3.self_attn.Concat.0">(%326:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %514:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> (%516:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.3.self_attn.Concat.1">(%327:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %515:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> (%517:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.3.self_attn.Repeat.0">(%516:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%518:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.3.self_attn.Repeat.1">(%517:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%519:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.3.self_attn.MatMul.0">(%510:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %518:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%520:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.3.self_attn.Mul.0">(%520:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %521:tensor<[1], Int16PerTensor, CPU>[constant:[0]]) -> (%522:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.3.self_attn.ReduceMin.0">(%522:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%523:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.3.self_attn.Add.0">(%523:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %524:tensor<[1], Int16PerTensor, CPU>[constant:[9.1807e-41]]) -> (%525:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.3.self_attn.Equal.0">(%319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %526:tensor<[1], Float32, CPU>[constant:[0]]) -> (%527:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.3.self_attn.Where.0">(%527:tensor<[1, 1, 32, 1024], UInt8, CPU>, %522:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %525:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%528:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.3.self_attn.Softmax.0">(%528:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%529:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.3.self_attn.MatMul.1">(%529:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %519:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%530:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.3.self_attn.Transpose.4">(%530:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%531:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.3.self_attn.View.3">(%531:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%531:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.3.self_attn.o_proj">(%531:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%532:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%532:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %514:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %515:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.3.mlp <CPU> [using_qnn:true, symbol:model.layers.3.mlp] {
        (%534:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%539:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.3.mlp.gate_proj">(%534:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%535:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.3.mlp.act">(%535:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%536:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.3.mlp.up_proj">(%534:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%537:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.3.mlp.Mul.0">(%536:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %537:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%538:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.3.mlp.down_proj">(%538:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%539:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%539:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.4 <CPU> [using_qnn:true, symbol:model.layers.4] {
        (%540:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %328:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %329:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%580:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %554:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %555:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) {
            linalg.CPU.RMSNormOp <name="model.layers.4.input_layernorm">(%540:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%541:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.4.self_attn (%541:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %328:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %329:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%572:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %554:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %555:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.AddOp <name="model.layers.4.Add.0">(%572:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %540:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%573:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.4.post_attention_layernorm">(%573:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%574:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.4.mlp (%574:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%579:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.4.Add.1">(%579:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %573:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%580:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%580:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %554:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %555:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.4.self_attn <CPU> [using_qnn:true, symbol:model.layers.4.self_attn] {
        (%541:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %328:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %329:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%572:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %554:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %555:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) {
            linalg.CPU.LinearOp <name="model.layers.4.self_attn.q_proj">(%541:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%542:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.4.self_attn.k_proj">(%541:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%543:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.4.self_attn.v_proj">(%541:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%544:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.4.self_attn.View.0">(%542:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%542:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.4.self_attn.Transpose.0">(%542:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%545:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.4.self_attn.View.1">(%543:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%543:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.4.self_attn.Transpose.1">(%543:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%546:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.4.self_attn.View.2">(%544:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%544:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.4.self_attn.Transpose.2">(%544:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%547:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.4.self_attn.q_norm">(%545:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%548:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.4.self_attn.k_norm">(%546:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%549:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.4.self_attn.q_rope">(%548:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%550:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.4.self_attn.k_rope">(%549:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%551:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.4.self_attn.CastType.0">(%551:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%552:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.4.self_attn.CastType.1">(%552:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%553:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.4.self_attn.Transpose.3">(%553:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%554:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.CastTypeOp <name="model.layers.4.self_attn.CastType.2">(%547:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%555:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.ConcatOp <name="model.layers.4.self_attn.Concat.0">(%328:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %554:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> (%556:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.4.self_attn.Concat.1">(%329:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %555:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> (%557:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.4.self_attn.Repeat.0">(%556:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%558:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.4.self_attn.Repeat.1">(%557:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%559:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.4.self_attn.MatMul.0">(%550:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %558:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%560:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.4.self_attn.Mul.0">(%560:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %561:tensor<[1], Int16PerTensor, CPU>[constant:[0]]) -> (%562:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.4.self_attn.ReduceMin.0">(%562:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%563:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.4.self_attn.Add.0">(%563:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %564:tensor<[1], Int16PerTensor, CPU>[constant:[9.1807e-41]]) -> (%565:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.4.self_attn.Equal.0">(%319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %566:tensor<[1], Float32, CPU>[constant:[0]]) -> (%567:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.4.self_attn.Where.0">(%567:tensor<[1, 1, 32, 1024], UInt8, CPU>, %562:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %565:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%568:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.4.self_attn.Softmax.0">(%568:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%569:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.4.self_attn.MatMul.1">(%569:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %559:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%570:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.4.self_attn.Transpose.4">(%570:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%571:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.4.self_attn.View.3">(%571:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%571:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.4.self_attn.o_proj">(%571:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%572:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%572:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %554:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %555:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.4.mlp <CPU> [using_qnn:true, symbol:model.layers.4.mlp] {
        (%574:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%579:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.4.mlp.gate_proj">(%574:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%575:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.4.mlp.act">(%575:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%576:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.4.mlp.up_proj">(%574:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%577:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.4.mlp.Mul.0">(%576:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %577:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%578:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.4.mlp.down_proj">(%578:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%579:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%579:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.5 <CPU> [using_qnn:true, symbol:model.layers.5] {
        (%580:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %330:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %331:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%620:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %594:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %595:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) {
            linalg.CPU.RMSNormOp <name="model.layers.5.input_layernorm">(%580:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%581:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.5.self_attn (%581:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %330:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %331:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%612:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %594:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %595:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.AddOp <name="model.layers.5.Add.0">(%612:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %580:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%613:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.5.post_attention_layernorm">(%613:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%614:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.5.mlp (%614:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%619:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.5.Add.1">(%619:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %613:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%620:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%620:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %594:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %595:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.5.self_attn <CPU> [using_qnn:true, symbol:model.layers.5.self_attn] {
        (%581:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %330:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %331:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%612:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %594:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %595:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) {
            linalg.CPU.LinearOp <name="model.layers.5.self_attn.q_proj">(%581:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%582:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.5.self_attn.k_proj">(%581:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%583:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.5.self_attn.v_proj">(%581:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%584:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.5.self_attn.View.0">(%582:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%582:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.5.self_attn.Transpose.0">(%582:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%585:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.5.self_attn.View.1">(%583:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%583:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.5.self_attn.Transpose.1">(%583:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%586:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.5.self_attn.View.2">(%584:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%584:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.5.self_attn.Transpose.2">(%584:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%587:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.5.self_attn.q_norm">(%585:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%588:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.5.self_attn.k_norm">(%586:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%589:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.5.self_attn.q_rope">(%588:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%590:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.5.self_attn.k_rope">(%589:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%591:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.5.self_attn.CastType.0">(%591:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%592:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.5.self_attn.CastType.1">(%592:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%593:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.5.self_attn.Transpose.3">(%593:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%594:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.CastTypeOp <name="model.layers.5.self_attn.CastType.2">(%587:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%595:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.ConcatOp <name="model.layers.5.self_attn.Concat.0">(%330:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %594:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> (%596:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.5.self_attn.Concat.1">(%331:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %595:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> (%597:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.5.self_attn.Repeat.0">(%596:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%598:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.5.self_attn.Repeat.1">(%597:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%599:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.5.self_attn.MatMul.0">(%590:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %598:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%600:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.5.self_attn.Mul.0">(%600:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %601:tensor<[1], Int16PerTensor, CPU>[constant:[0]]) -> (%602:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.5.self_attn.ReduceMin.0">(%602:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%603:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.5.self_attn.Add.0">(%603:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %604:tensor<[1], Int16PerTensor, CPU>[constant:[9.1807e-41]]) -> (%605:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.5.self_attn.Equal.0">(%319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %606:tensor<[1], Float32, CPU>[constant:[0]]) -> (%607:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.5.self_attn.Where.0">(%607:tensor<[1, 1, 32, 1024], UInt8, CPU>, %602:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %605:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%608:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.5.self_attn.Softmax.0">(%608:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%609:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.5.self_attn.MatMul.1">(%609:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %599:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%610:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.5.self_attn.Transpose.4">(%610:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%611:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.5.self_attn.View.3">(%611:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%611:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.5.self_attn.o_proj">(%611:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%612:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%612:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %594:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %595:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.5.mlp <CPU> [using_qnn:true, symbol:model.layers.5.mlp] {
        (%614:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%619:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.5.mlp.gate_proj">(%614:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%615:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.5.mlp.act">(%615:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%616:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.5.mlp.up_proj">(%614:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%617:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.5.mlp.Mul.0">(%616:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %617:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%618:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.5.mlp.down_proj">(%618:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%619:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%619:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.6 <CPU> [using_qnn:true, symbol:model.layers.6] {
        (%620:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %332:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %333:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%660:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %634:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %635:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) {
            linalg.CPU.RMSNormOp <name="model.layers.6.input_layernorm">(%620:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%621:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.6.self_attn (%621:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %332:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %333:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%652:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %634:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %635:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.AddOp <name="model.layers.6.Add.0">(%652:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %620:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%653:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.6.post_attention_layernorm">(%653:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%654:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.6.mlp (%654:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%659:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.6.Add.1">(%659:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %653:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%660:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%660:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %634:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %635:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.6.self_attn <CPU> [using_qnn:true, symbol:model.layers.6.self_attn] {
        (%621:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %332:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %333:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%652:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %634:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %635:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) {
            linalg.CPU.LinearOp <name="model.layers.6.self_attn.q_proj">(%621:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%622:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.6.self_attn.k_proj">(%621:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%623:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.6.self_attn.v_proj">(%621:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%624:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.6.self_attn.View.0">(%622:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%622:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.6.self_attn.Transpose.0">(%622:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%625:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.6.self_attn.View.1">(%623:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%623:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.6.self_attn.Transpose.1">(%623:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%626:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.6.self_attn.View.2">(%624:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%624:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.6.self_attn.Transpose.2">(%624:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%627:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.6.self_attn.q_norm">(%625:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%628:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.6.self_attn.k_norm">(%626:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%629:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.6.self_attn.q_rope">(%628:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%630:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.6.self_attn.k_rope">(%629:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%631:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.6.self_attn.CastType.0">(%631:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%632:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.6.self_attn.CastType.1">(%632:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%633:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.6.self_attn.Transpose.3">(%633:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%634:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.CastTypeOp <name="model.layers.6.self_attn.CastType.2">(%627:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%635:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.ConcatOp <name="model.layers.6.self_attn.Concat.0">(%332:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %634:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> (%636:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.6.self_attn.Concat.1">(%333:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %635:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> (%637:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.6.self_attn.Repeat.0">(%636:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%638:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.6.self_attn.Repeat.1">(%637:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%639:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.6.self_attn.MatMul.0">(%630:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %638:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%640:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.6.self_attn.Mul.0">(%640:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %641:tensor<[1], Int16PerTensor, CPU>[constant:[0]]) -> (%642:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.6.self_attn.ReduceMin.0">(%642:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%643:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.6.self_attn.Add.0">(%643:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %644:tensor<[1], Int16PerTensor, CPU>[constant:[9.1807e-41]]) -> (%645:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.6.self_attn.Equal.0">(%319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %646:tensor<[1], Float32, CPU>[constant:[0]]) -> (%647:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.6.self_attn.Where.0">(%647:tensor<[1, 1, 32, 1024], UInt8, CPU>, %642:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %645:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%648:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.6.self_attn.Softmax.0">(%648:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%649:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.6.self_attn.MatMul.1">(%649:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %639:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%650:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.6.self_attn.Transpose.4">(%650:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%651:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.6.self_attn.View.3">(%651:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%651:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.6.self_attn.o_proj">(%651:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%652:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%652:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %634:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %635:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.6.mlp <CPU> [using_qnn:true, symbol:model.layers.6.mlp] {
        (%654:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%659:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.6.mlp.gate_proj">(%654:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%655:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.6.mlp.act">(%655:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%656:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.6.mlp.up_proj">(%654:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%657:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.6.mlp.Mul.0">(%656:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %657:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%658:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.6.mlp.down_proj">(%658:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%659:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%659:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.7 <CPU> [using_qnn:true, symbol:model.layers.7] {
        (%660:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %334:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %335:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%700:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %674:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %675:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) {
            linalg.CPU.RMSNormOp <name="model.layers.7.input_layernorm">(%660:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%661:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.7.self_attn (%661:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %334:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %335:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%692:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %674:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %675:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.AddOp <name="model.layers.7.Add.0">(%692:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %660:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%693:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.7.post_attention_layernorm">(%693:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%694:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.7.mlp (%694:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%699:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.7.Add.1">(%699:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %693:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%700:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%700:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %674:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %675:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.7.self_attn <CPU> [using_qnn:true, symbol:model.layers.7.self_attn] {
        (%661:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %334:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %335:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%692:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %674:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %675:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) {
            linalg.CPU.LinearOp <name="model.layers.7.self_attn.q_proj">(%661:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%662:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.7.self_attn.k_proj">(%661:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%663:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.7.self_attn.v_proj">(%661:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%664:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.7.self_attn.View.0">(%662:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%662:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.7.self_attn.Transpose.0">(%662:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%665:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.7.self_attn.View.1">(%663:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%663:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.7.self_attn.Transpose.1">(%663:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%666:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.7.self_attn.View.2">(%664:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%664:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.7.self_attn.Transpose.2">(%664:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%667:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.7.self_attn.q_norm">(%665:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%668:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.7.self_attn.k_norm">(%666:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%669:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.7.self_attn.q_rope">(%668:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%670:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.7.self_attn.k_rope">(%669:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%671:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.7.self_attn.CastType.0">(%671:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%672:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.7.self_attn.CastType.1">(%672:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%673:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.7.self_attn.Transpose.3">(%673:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%674:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.CastTypeOp <name="model.layers.7.self_attn.CastType.2">(%667:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%675:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.ConcatOp <name="model.layers.7.self_attn.Concat.0">(%334:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %674:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> (%676:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.7.self_attn.Concat.1">(%335:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %675:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> (%677:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.7.self_attn.Repeat.0">(%676:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%678:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.7.self_attn.Repeat.1">(%677:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%679:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.7.self_attn.MatMul.0">(%670:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %678:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%680:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.7.self_attn.Mul.0">(%680:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %681:tensor<[1], Int16PerTensor, CPU>[constant:[0]]) -> (%682:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.7.self_attn.ReduceMin.0">(%682:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%683:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.7.self_attn.Add.0">(%683:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %684:tensor<[1], Int16PerTensor, CPU>[constant:[1.0078101]]) -> (%685:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.7.self_attn.Equal.0">(%319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %686:tensor<[1], Float32, CPU>[constant:[0]]) -> (%687:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.7.self_attn.Where.0">(%687:tensor<[1, 1, 32, 1024], UInt8, CPU>, %682:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %685:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%688:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.7.self_attn.Softmax.0">(%688:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%689:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.7.self_attn.MatMul.1">(%689:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %679:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%690:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.7.self_attn.Transpose.4">(%690:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%691:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.7.self_attn.View.3">(%691:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%691:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.7.self_attn.o_proj">(%691:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%692:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%692:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %674:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %675:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.7.mlp <CPU> [using_qnn:true, symbol:model.layers.7.mlp] {
        (%694:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%699:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.7.mlp.gate_proj">(%694:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%695:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.7.mlp.act">(%695:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%696:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.7.mlp.up_proj">(%694:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%697:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.7.mlp.Mul.0">(%696:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %697:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%698:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.7.mlp.down_proj">(%698:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%699:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%699:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.8 <CPU> [using_qnn:true, symbol:model.layers.8] {
        (%700:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %336:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %337:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%740:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %714:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %715:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) {
            linalg.CPU.RMSNormOp <name="model.layers.8.input_layernorm">(%700:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%701:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.8.self_attn (%701:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %336:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %337:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%732:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %714:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %715:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.AddOp <name="model.layers.8.Add.0">(%732:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %700:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%733:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.8.post_attention_layernorm">(%733:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%734:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.8.mlp (%734:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%739:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.8.Add.1">(%739:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %733:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%740:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%740:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %714:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %715:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.8.self_attn <CPU> [using_qnn:true, symbol:model.layers.8.self_attn] {
        (%701:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %336:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %337:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%732:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %714:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %715:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) {
            linalg.CPU.LinearOp <name="model.layers.8.self_attn.q_proj">(%701:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%702:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.8.self_attn.k_proj">(%701:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%703:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.8.self_attn.v_proj">(%701:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%704:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.8.self_attn.View.0">(%702:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%702:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.8.self_attn.Transpose.0">(%702:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%705:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.8.self_attn.View.1">(%703:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%703:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.8.self_attn.Transpose.1">(%703:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%706:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.8.self_attn.View.2">(%704:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%704:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.8.self_attn.Transpose.2">(%704:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%707:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.8.self_attn.q_norm">(%705:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%708:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.8.self_attn.k_norm">(%706:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%709:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.8.self_attn.q_rope">(%708:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%710:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.8.self_attn.k_rope">(%709:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%711:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.8.self_attn.CastType.0">(%711:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%712:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.8.self_attn.CastType.1">(%712:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%713:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.8.self_attn.Transpose.3">(%713:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%714:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.CastTypeOp <name="model.layers.8.self_attn.CastType.2">(%707:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%715:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.ConcatOp <name="model.layers.8.self_attn.Concat.0">(%336:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %714:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> (%716:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.8.self_attn.Concat.1">(%337:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %715:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> (%717:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.8.self_attn.Repeat.0">(%716:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%718:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.8.self_attn.Repeat.1">(%717:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%719:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.8.self_attn.MatMul.0">(%710:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %718:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%720:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.8.self_attn.Mul.0">(%720:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %721:tensor<[1], Int16PerTensor, CPU>[constant:[0.390625]]) -> (%722:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.8.self_attn.ReduceMin.0">(%722:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%723:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.8.self_attn.Add.0">(%723:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %724:tensor<[1], Int16PerTensor, CPU>[constant:[-0.18066376]]) -> (%725:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.8.self_attn.Equal.0">(%319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %726:tensor<[1], Float32, CPU>[constant:[0]]) -> (%727:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.8.self_attn.Where.0">(%727:tensor<[1, 1, 32, 1024], UInt8, CPU>, %722:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %725:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%728:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.8.self_attn.Softmax.0">(%728:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%729:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.8.self_attn.MatMul.1">(%729:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %719:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%730:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.8.self_attn.Transpose.4">(%730:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%731:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.8.self_attn.View.3">(%731:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%731:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.8.self_attn.o_proj">(%731:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%732:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%732:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %714:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %715:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.8.mlp <CPU> [using_qnn:true, symbol:model.layers.8.mlp] {
        (%734:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%739:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.8.mlp.gate_proj">(%734:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%735:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.8.mlp.act">(%735:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%736:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.8.mlp.up_proj">(%734:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%737:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.8.mlp.Mul.0">(%736:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %737:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%738:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.8.mlp.down_proj">(%738:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%739:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%739:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.9 <CPU> [using_qnn:true, symbol:model.layers.9] {
        (%740:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %338:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %339:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%780:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %754:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %755:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) {
            linalg.CPU.RMSNormOp <name="model.layers.9.input_layernorm">(%740:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%741:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.9.self_attn (%741:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %338:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %339:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%772:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %754:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %755:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.AddOp <name="model.layers.9.Add.0">(%772:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %740:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%773:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.9.post_attention_layernorm">(%773:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%774:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.9.mlp (%774:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%779:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.9.Add.1">(%779:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %773:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%780:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%780:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %754:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %755:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.9.self_attn <CPU> [using_qnn:true, symbol:model.layers.9.self_attn] {
        (%741:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %338:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %339:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%772:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %754:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %755:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) {
            linalg.CPU.LinearOp <name="model.layers.9.self_attn.q_proj">(%741:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%742:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.9.self_attn.k_proj">(%741:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%743:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.9.self_attn.v_proj">(%741:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%744:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.9.self_attn.View.0">(%742:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%742:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.9.self_attn.Transpose.0">(%742:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%745:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.9.self_attn.View.1">(%743:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%743:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.9.self_attn.Transpose.1">(%743:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%746:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.9.self_attn.View.2">(%744:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%744:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.9.self_attn.Transpose.2">(%744:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%747:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.9.self_attn.q_norm">(%745:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%748:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.9.self_attn.k_norm">(%746:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%749:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.9.self_attn.q_rope">(%748:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%750:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.9.self_attn.k_rope">(%749:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%751:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.9.self_attn.CastType.0">(%751:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%752:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.9.self_attn.CastType.1">(%752:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%753:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.9.self_attn.Transpose.3">(%753:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%754:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.CastTypeOp <name="model.layers.9.self_attn.CastType.2">(%747:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%755:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.ConcatOp <name="model.layers.9.self_attn.Concat.0">(%338:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %754:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> (%756:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.9.self_attn.Concat.1">(%339:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %755:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> (%757:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.9.self_attn.Repeat.0">(%756:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%758:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.9.self_attn.Repeat.1">(%757:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%759:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.9.self_attn.MatMul.0">(%750:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %758:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%760:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.9.self_attn.Mul.0">(%760:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %761:tensor<[1], Int16PerTensor, CPU>[constant:[-0.97265625]]) -> (%762:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.9.self_attn.ReduceMin.0">(%762:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%763:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.9.self_attn.Add.0">(%763:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %764:tensor<[1], Int16PerTensor, CPU>[constant:[-0.9374988]]) -> (%765:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.9.self_attn.Equal.0">(%319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %766:tensor<[1], Float32, CPU>[constant:[0]]) -> (%767:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.9.self_attn.Where.0">(%767:tensor<[1, 1, 32, 1024], UInt8, CPU>, %762:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %765:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%768:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.9.self_attn.Softmax.0">(%768:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%769:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.9.self_attn.MatMul.1">(%769:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %759:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%770:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.9.self_attn.Transpose.4">(%770:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%771:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.9.self_attn.View.3">(%771:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%771:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.9.self_attn.o_proj">(%771:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%772:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%772:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %754:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %755:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.9.mlp <CPU> [using_qnn:true, symbol:model.layers.9.mlp] {
        (%774:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%779:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.9.mlp.gate_proj">(%774:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%775:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.9.mlp.act">(%775:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%776:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.9.mlp.up_proj">(%774:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%777:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.9.mlp.Mul.0">(%776:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %777:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%778:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.9.mlp.down_proj">(%778:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%779:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%779:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.10 <CPU> [using_qnn:true, symbol:model.layers.10] {
        (%780:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %340:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %341:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%820:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %794:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %795:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) {
            linalg.CPU.RMSNormOp <name="model.layers.10.input_layernorm">(%780:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%781:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.10.self_attn (%781:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %340:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %341:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%812:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %794:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %795:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.AddOp <name="model.layers.10.Add.0">(%812:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %780:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%813:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.10.post_attention_layernorm">(%813:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%814:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.10.mlp (%814:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%819:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.10.Add.1">(%819:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %813:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%820:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%820:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %794:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %795:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.10.self_attn <CPU> [using_qnn:true, symbol:model.layers.10.self_attn] {
        (%781:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %340:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %341:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%812:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %794:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %795:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) {
            linalg.CPU.LinearOp <name="model.layers.10.self_attn.q_proj">(%781:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%782:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.10.self_attn.k_proj">(%781:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%783:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.10.self_attn.v_proj">(%781:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%784:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.10.self_attn.View.0">(%782:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%782:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.10.self_attn.Transpose.0">(%782:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%785:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.10.self_attn.View.1">(%783:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%783:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.10.self_attn.Transpose.1">(%783:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%786:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.10.self_attn.View.2">(%784:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%784:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.10.self_attn.Transpose.2">(%784:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%787:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.10.self_attn.q_norm">(%785:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%788:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.10.self_attn.k_norm">(%786:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%789:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.10.self_attn.q_rope">(%788:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%790:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.10.self_attn.k_rope">(%789:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%791:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.10.self_attn.CastType.0">(%791:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%792:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.10.self_attn.CastType.1">(%792:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%793:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.10.self_attn.Transpose.3">(%793:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%794:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.CastTypeOp <name="model.layers.10.self_attn.CastType.2">(%787:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%795:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.ConcatOp <name="model.layers.10.self_attn.Concat.0">(%340:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %794:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> (%796:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.10.self_attn.Concat.1">(%341:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %795:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> (%797:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.10.self_attn.Repeat.0">(%796:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%798:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.10.self_attn.Repeat.1">(%797:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%799:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.10.self_attn.MatMul.0">(%790:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %798:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%800:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.10.self_attn.Mul.0">(%800:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %801:tensor<[1], Int16PerTensor, CPU>[constant:[-0.03955078]]) -> (%802:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.10.self_attn.ReduceMin.0">(%802:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%803:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.10.self_attn.Add.0">(%803:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %804:tensor<[1], Int16PerTensor, CPU>[constant:[0.51953006]]) -> (%805:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.10.self_attn.Equal.0">(%319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %806:tensor<[1], Float32, CPU>[constant:[0]]) -> (%807:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.10.self_attn.Where.0">(%807:tensor<[1, 1, 32, 1024], UInt8, CPU>, %802:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %805:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%808:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.10.self_attn.Softmax.0">(%808:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%809:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.10.self_attn.MatMul.1">(%809:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %799:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%810:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.10.self_attn.Transpose.4">(%810:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%811:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.10.self_attn.View.3">(%811:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%811:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.10.self_attn.o_proj">(%811:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%812:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%812:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %794:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %795:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.10.mlp <CPU> [using_qnn:true, symbol:model.layers.10.mlp] {
        (%814:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%819:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.10.mlp.gate_proj">(%814:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%815:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.10.mlp.act">(%815:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%816:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.10.mlp.up_proj">(%814:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%817:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.10.mlp.Mul.0">(%816:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %817:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%818:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.10.mlp.down_proj">(%818:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%819:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%819:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.11 <CPU> [using_qnn:true, symbol:model.layers.11] {
        (%820:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %342:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %343:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%860:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %834:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %835:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) {
            linalg.CPU.RMSNormOp <name="model.layers.11.input_layernorm">(%820:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%821:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.11.self_attn (%821:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %342:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %343:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%852:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %834:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %835:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.AddOp <name="model.layers.11.Add.0">(%852:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %820:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%853:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.11.post_attention_layernorm">(%853:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%854:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.11.mlp (%854:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%859:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.11.Add.1">(%859:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %853:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%860:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%860:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %834:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %835:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.11.self_attn <CPU> [using_qnn:true, symbol:model.layers.11.self_attn] {
        (%821:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %342:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %343:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%852:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %834:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %835:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) {
            linalg.CPU.LinearOp <name="model.layers.11.self_attn.q_proj">(%821:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%822:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.11.self_attn.k_proj">(%821:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%823:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.11.self_attn.v_proj">(%821:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%824:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.11.self_attn.View.0">(%822:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%822:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.11.self_attn.Transpose.0">(%822:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%825:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.11.self_attn.View.1">(%823:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%823:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.11.self_attn.Transpose.1">(%823:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%826:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.11.self_attn.View.2">(%824:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%824:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.11.self_attn.Transpose.2">(%824:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%827:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.11.self_attn.q_norm">(%825:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%828:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.11.self_attn.k_norm">(%826:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%829:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.11.self_attn.q_rope">(%828:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%830:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.11.self_attn.k_rope">(%829:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%831:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.11.self_attn.CastType.0">(%831:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%832:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.11.self_attn.CastType.1">(%832:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%833:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.11.self_attn.Transpose.3">(%833:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%834:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.CastTypeOp <name="model.layers.11.self_attn.CastType.2">(%827:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%835:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.ConcatOp <name="model.layers.11.self_attn.Concat.0">(%342:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %834:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> (%836:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.11.self_attn.Concat.1">(%343:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %835:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> (%837:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.11.self_attn.Repeat.0">(%836:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%838:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.11.self_attn.Repeat.1">(%837:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%839:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.11.self_attn.MatMul.0">(%830:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %838:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%840:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.11.self_attn.Mul.0">(%840:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %841:tensor<[1], Int16PerTensor, CPU>[constant:[0.98828125]]) -> (%842:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.11.self_attn.ReduceMin.0">(%842:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%843:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.11.self_attn.Add.0">(%843:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %844:tensor<[1], Int16PerTensor, CPU>[constant:[0.7499988]]) -> (%845:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.11.self_attn.Equal.0">(%319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %846:tensor<[1], Float32, CPU>[constant:[0]]) -> (%847:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.11.self_attn.Where.0">(%847:tensor<[1, 1, 32, 1024], UInt8, CPU>, %842:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %845:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%848:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.11.self_attn.Softmax.0">(%848:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%849:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.11.self_attn.MatMul.1">(%849:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %839:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%850:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.11.self_attn.Transpose.4">(%850:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%851:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.11.self_attn.View.3">(%851:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%851:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.11.self_attn.o_proj">(%851:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%852:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%852:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %834:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %835:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.11.mlp <CPU> [using_qnn:true, symbol:model.layers.11.mlp] {
        (%854:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%859:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.11.mlp.gate_proj">(%854:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%855:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.11.mlp.act">(%855:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%856:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.11.mlp.up_proj">(%854:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%857:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.11.mlp.Mul.0">(%856:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %857:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%858:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.11.mlp.down_proj">(%858:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%859:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%859:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.12 <CPU> [using_qnn:true, symbol:model.layers.12] {
        (%860:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %344:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %345:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%900:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %874:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %875:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) {
            linalg.CPU.RMSNormOp <name="model.layers.12.input_layernorm">(%860:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%861:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.12.self_attn (%861:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %344:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %345:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%892:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %874:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %875:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.AddOp <name="model.layers.12.Add.0">(%892:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %860:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%893:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.12.post_attention_layernorm">(%893:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%894:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.12.mlp (%894:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%899:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.12.Add.1">(%899:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %893:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%900:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%900:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %874:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %875:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.12.self_attn <CPU> [using_qnn:true, symbol:model.layers.12.self_attn] {
        (%861:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %344:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %345:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%892:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %874:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %875:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) {
            linalg.CPU.LinearOp <name="model.layers.12.self_attn.q_proj">(%861:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%862:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.12.self_attn.k_proj">(%861:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%863:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.12.self_attn.v_proj">(%861:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%864:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.12.self_attn.View.0">(%862:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%862:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.12.self_attn.Transpose.0">(%862:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%865:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.12.self_attn.View.1">(%863:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%863:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.12.self_attn.Transpose.1">(%863:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%866:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.12.self_attn.View.2">(%864:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%864:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.12.self_attn.Transpose.2">(%864:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%867:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.12.self_attn.q_norm">(%865:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%868:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.12.self_attn.k_norm">(%866:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%869:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.12.self_attn.q_rope">(%868:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%870:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.12.self_attn.k_rope">(%869:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%871:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.12.self_attn.CastType.0">(%871:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%872:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.12.self_attn.CastType.1">(%872:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%873:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.12.self_attn.Transpose.3">(%873:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%874:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.CastTypeOp <name="model.layers.12.self_attn.CastType.2">(%867:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%875:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.ConcatOp <name="model.layers.12.self_attn.Concat.0">(%344:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %874:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> (%876:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.12.self_attn.Concat.1">(%345:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %875:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> (%877:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.12.self_attn.Repeat.0">(%876:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%878:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.12.self_attn.Repeat.1">(%877:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%879:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.12.self_attn.MatMul.0">(%870:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %878:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%880:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.12.self_attn.Mul.0">(%880:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %881:tensor<[1], Int16PerTensor, CPU>[constant:[-0.31640625]]) -> (%882:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.12.self_attn.ReduceMin.0">(%882:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%883:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.12.self_attn.Add.0">(%883:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %884:tensor<[1], Int16PerTensor, CPU>[constant:[-0.7890613]]) -> (%885:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.12.self_attn.Equal.0">(%319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %886:tensor<[1], Float32, CPU>[constant:[0]]) -> (%887:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.12.self_attn.Where.0">(%887:tensor<[1, 1, 32, 1024], UInt8, CPU>, %882:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %885:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%888:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.12.self_attn.Softmax.0">(%888:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%889:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.12.self_attn.MatMul.1">(%889:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %879:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%890:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.12.self_attn.Transpose.4">(%890:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%891:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.12.self_attn.View.3">(%891:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%891:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.12.self_attn.o_proj">(%891:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%892:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%892:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %874:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %875:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.12.mlp <CPU> [using_qnn:true, symbol:model.layers.12.mlp] {
        (%894:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%899:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.12.mlp.gate_proj">(%894:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%895:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.12.mlp.act">(%895:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%896:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.12.mlp.up_proj">(%894:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%897:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.12.mlp.Mul.0">(%896:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %897:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%898:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.12.mlp.down_proj">(%898:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%899:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%899:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.13 <CPU> [using_qnn:true, symbol:model.layers.13] {
        (%900:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %346:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %347:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%940:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %914:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %915:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) {
            linalg.CPU.RMSNormOp <name="model.layers.13.input_layernorm">(%900:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%901:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.13.self_attn (%901:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %346:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %347:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%932:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %914:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %915:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.AddOp <name="model.layers.13.Add.0">(%932:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %900:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%933:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.13.post_attention_layernorm">(%933:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%934:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.13.mlp (%934:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%939:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.13.Add.1">(%939:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %933:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%940:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%940:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %914:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %915:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.13.self_attn <CPU> [using_qnn:true, symbol:model.layers.13.self_attn] {
        (%901:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %346:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %347:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%932:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %914:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %915:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) {
            linalg.CPU.LinearOp <name="model.layers.13.self_attn.q_proj">(%901:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%902:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.13.self_attn.k_proj">(%901:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%903:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.13.self_attn.v_proj">(%901:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%904:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.13.self_attn.View.0">(%902:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%902:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.13.self_attn.Transpose.0">(%902:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%905:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.13.self_attn.View.1">(%903:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%903:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.13.self_attn.Transpose.1">(%903:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%906:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.13.self_attn.View.2">(%904:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%904:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.13.self_attn.Transpose.2">(%904:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%907:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.13.self_attn.q_norm">(%905:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%908:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.13.self_attn.k_norm">(%906:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%909:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.13.self_attn.q_rope">(%908:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%910:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.13.self_attn.k_rope">(%909:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%911:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.13.self_attn.CastType.0">(%911:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%912:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.13.self_attn.CastType.1">(%912:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%913:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.13.self_attn.Transpose.3">(%913:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%914:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.CastTypeOp <name="model.layers.13.self_attn.CastType.2">(%907:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%915:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.ConcatOp <name="model.layers.13.self_attn.Concat.0">(%346:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %914:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> (%916:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.13.self_attn.Concat.1">(%347:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %915:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> (%917:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.13.self_attn.Repeat.0">(%916:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%918:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.13.self_attn.Repeat.1">(%917:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%919:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.13.self_attn.MatMul.0">(%910:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %918:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%920:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.13.self_attn.Mul.0">(%920:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %921:tensor<[1], Int16PerTensor, CPU>[constant:[-0.875]]) -> (%922:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.13.self_attn.ReduceMin.0">(%922:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%923:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.13.self_attn.Add.0">(%923:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %924:tensor<[1], Int16PerTensor, CPU>[constant:[-0.46484315]]) -> (%925:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.13.self_attn.Equal.0">(%319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %926:tensor<[1], Float32, CPU>[constant:[0]]) -> (%927:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.13.self_attn.Where.0">(%927:tensor<[1, 1, 32, 1024], UInt8, CPU>, %922:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %925:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%928:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.13.self_attn.Softmax.0">(%928:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%929:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.13.self_attn.MatMul.1">(%929:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %919:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%930:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.13.self_attn.Transpose.4">(%930:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%931:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.13.self_attn.View.3">(%931:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%931:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.13.self_attn.o_proj">(%931:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%932:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%932:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %914:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %915:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.13.mlp <CPU> [using_qnn:true, symbol:model.layers.13.mlp] {
        (%934:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%939:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.13.mlp.gate_proj">(%934:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%935:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.13.mlp.act">(%935:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%936:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.13.mlp.up_proj">(%934:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%937:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.13.mlp.Mul.0">(%936:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %937:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%938:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.13.mlp.down_proj">(%938:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%939:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%939:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.14 <CPU> [using_qnn:true, symbol:model.layers.14] {
        (%940:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %348:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %349:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%980:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %954:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %955:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) {
            linalg.CPU.RMSNormOp <name="model.layers.14.input_layernorm">(%940:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%941:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.14.self_attn (%941:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %348:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %349:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%972:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %954:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %955:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.AddOp <name="model.layers.14.Add.0">(%972:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %940:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%973:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.14.post_attention_layernorm">(%973:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%974:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.14.mlp (%974:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%979:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.14.Add.1">(%979:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %973:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%980:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%980:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %954:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %955:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.14.self_attn <CPU> [using_qnn:true, symbol:model.layers.14.self_attn] {
        (%941:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %348:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %349:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%972:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %954:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %955:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) {
            linalg.CPU.LinearOp <name="model.layers.14.self_attn.q_proj">(%941:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%942:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.14.self_attn.k_proj">(%941:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%943:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.14.self_attn.v_proj">(%941:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%944:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.14.self_attn.View.0">(%942:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%942:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.14.self_attn.Transpose.0">(%942:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%945:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.14.self_attn.View.1">(%943:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%943:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.14.self_attn.Transpose.1">(%943:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%946:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.14.self_attn.View.2">(%944:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%944:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.14.self_attn.Transpose.2">(%944:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%947:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.14.self_attn.q_norm">(%945:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%948:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.14.self_attn.k_norm">(%946:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%949:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.14.self_attn.q_rope">(%948:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%950:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.14.self_attn.k_rope">(%949:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%951:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.14.self_attn.CastType.0">(%951:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%952:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.14.self_attn.CastType.1">(%952:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%953:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.14.self_attn.Transpose.3">(%953:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%954:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.CastTypeOp <name="model.layers.14.self_attn.CastType.2">(%947:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%955:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.ConcatOp <name="model.layers.14.self_attn.Concat.0">(%348:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %954:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> (%956:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.14.self_attn.Concat.1">(%349:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %955:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> (%957:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.14.self_attn.Repeat.0">(%956:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%958:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.14.self_attn.Repeat.1">(%957:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%959:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.14.self_attn.MatMul.0">(%950:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %958:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%960:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.14.self_attn.Mul.0">(%960:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %961:tensor<[1], Int16PerTensor, CPU>[constant:[0.6328125]]) -> (%962:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.14.self_attn.ReduceMin.0">(%962:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%963:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.14.self_attn.Add.0">(%963:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %964:tensor<[1], Int16PerTensor, CPU>[constant:[0.95703006]]) -> (%965:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.14.self_attn.Equal.0">(%319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %966:tensor<[1], Float32, CPU>[constant:[0]]) -> (%967:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.14.self_attn.Where.0">(%967:tensor<[1, 1, 32, 1024], UInt8, CPU>, %962:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %965:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%968:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.14.self_attn.Softmax.0">(%968:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%969:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.14.self_attn.MatMul.1">(%969:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %959:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%970:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.14.self_attn.Transpose.4">(%970:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%971:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.14.self_attn.View.3">(%971:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%971:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.14.self_attn.o_proj">(%971:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%972:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%972:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %954:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %955:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.14.mlp <CPU> [using_qnn:true, symbol:model.layers.14.mlp] {
        (%974:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%979:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.14.mlp.gate_proj">(%974:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%975:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.14.mlp.act">(%975:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%976:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.14.mlp.up_proj">(%974:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%977:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.14.mlp.Mul.0">(%976:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %977:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%978:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.14.mlp.down_proj">(%978:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%979:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%979:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.15 <CPU> [using_qnn:true, symbol:model.layers.15] {
        (%980:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %350:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %351:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%1020:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %994:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %995:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) {
            linalg.CPU.RMSNormOp <name="model.layers.15.input_layernorm">(%980:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%981:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.15.self_attn (%981:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %350:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %351:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%1012:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %994:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %995:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.AddOp <name="model.layers.15.Add.0">(%1012:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %980:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1013:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.15.post_attention_layernorm">(%1013:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1014:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.15.mlp (%1014:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1019:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.15.Add.1">(%1019:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1013:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1020:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1020:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %994:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %995:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.15.self_attn <CPU> [using_qnn:true, symbol:model.layers.15.self_attn] {
        (%981:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %350:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %351:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%1012:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %994:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %995:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) {
            linalg.CPU.LinearOp <name="model.layers.15.self_attn.q_proj">(%981:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%982:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.15.self_attn.k_proj">(%981:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%983:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.15.self_attn.v_proj">(%981:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%984:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.15.self_attn.View.0">(%982:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%982:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.15.self_attn.Transpose.0">(%982:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%985:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.15.self_attn.View.1">(%983:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%983:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.15.self_attn.Transpose.1">(%983:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%986:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.15.self_attn.View.2">(%984:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%984:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.15.self_attn.Transpose.2">(%984:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%987:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.15.self_attn.q_norm">(%985:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%988:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.15.self_attn.k_norm">(%986:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%989:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.15.self_attn.q_rope">(%988:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%990:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.15.self_attn.k_rope">(%989:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%991:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.15.self_attn.CastType.0">(%991:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%992:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.15.self_attn.CastType.1">(%992:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%993:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.15.self_attn.Transpose.3">(%993:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%994:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.CastTypeOp <name="model.layers.15.self_attn.CastType.2">(%987:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%995:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.ConcatOp <name="model.layers.15.self_attn.Concat.0">(%350:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %994:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> (%996:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.15.self_attn.Concat.1">(%351:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %995:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> (%997:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.15.self_attn.Repeat.0">(%996:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%998:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.15.self_attn.Repeat.1">(%997:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%999:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.15.self_attn.MatMul.0">(%990:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %998:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%1000:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.15.self_attn.Mul.0">(%1000:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1001:tensor<[1], Int16PerTensor, CPU>[constant:[0.64453125]]) -> (%1002:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.15.self_attn.ReduceMin.0">(%1002:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1003:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.15.self_attn.Add.0">(%1003:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %1004:tensor<[1], Int16PerTensor, CPU>[constant:[0.119140476]]) -> (%1005:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.15.self_attn.Equal.0">(%319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %1006:tensor<[1], Float32, CPU>[constant:[0]]) -> (%1007:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.15.self_attn.Where.0">(%1007:tensor<[1, 1, 32, 1024], UInt8, CPU>, %1002:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1005:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%1008:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.15.self_attn.Softmax.0">(%1008:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1009:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.15.self_attn.MatMul.1">(%1009:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %999:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%1010:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.15.self_attn.Transpose.4">(%1010:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1011:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.15.self_attn.View.3">(%1011:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1011:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.15.self_attn.o_proj">(%1011:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1012:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1012:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %994:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %995:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.15.mlp <CPU> [using_qnn:true, symbol:model.layers.15.mlp] {
        (%1014:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1019:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.15.mlp.gate_proj">(%1014:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1015:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.15.mlp.act">(%1015:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1016:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.15.mlp.up_proj">(%1014:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1017:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.15.mlp.Mul.0">(%1016:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %1017:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1018:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.15.mlp.down_proj">(%1018:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1019:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1019:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.16 <CPU> [using_qnn:true, symbol:model.layers.16] {
        (%1020:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %352:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %353:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%1060:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1034:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1035:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) {
            linalg.CPU.RMSNormOp <name="model.layers.16.input_layernorm">(%1020:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1021:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.16.self_attn (%1021:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %352:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %353:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%1052:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1034:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1035:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.AddOp <name="model.layers.16.Add.0">(%1052:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1020:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1053:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.16.post_attention_layernorm">(%1053:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1054:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.16.mlp (%1054:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1059:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.16.Add.1">(%1059:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1053:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1060:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1060:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1034:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1035:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.16.self_attn <CPU> [using_qnn:true, symbol:model.layers.16.self_attn] {
        (%1021:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %352:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %353:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%1052:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1034:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1035:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) {
            linalg.CPU.LinearOp <name="model.layers.16.self_attn.q_proj">(%1021:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1022:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.16.self_attn.k_proj">(%1021:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1023:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.16.self_attn.v_proj">(%1021:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1024:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.16.self_attn.View.0">(%1022:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1022:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.16.self_attn.Transpose.0">(%1022:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1025:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.16.self_attn.View.1">(%1023:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1023:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.16.self_attn.Transpose.1">(%1023:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1026:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.16.self_attn.View.2">(%1024:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1024:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.16.self_attn.Transpose.2">(%1024:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1027:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.16.self_attn.q_norm">(%1025:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1028:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.16.self_attn.k_norm">(%1026:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1029:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.16.self_attn.q_rope">(%1028:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1030:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.16.self_attn.k_rope">(%1029:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1031:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.16.self_attn.CastType.0">(%1031:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1032:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.16.self_attn.CastType.1">(%1032:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1033:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.16.self_attn.Transpose.3">(%1033:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1034:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.CastTypeOp <name="model.layers.16.self_attn.CastType.2">(%1027:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1035:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.ConcatOp <name="model.layers.16.self_attn.Concat.0">(%352:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %1034:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> (%1036:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.16.self_attn.Concat.1">(%353:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %1035:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> (%1037:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.16.self_attn.Repeat.0">(%1036:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%1038:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.16.self_attn.Repeat.1">(%1037:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%1039:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.16.self_attn.MatMul.0">(%1030:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %1038:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%1040:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.16.self_attn.Mul.0">(%1040:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1041:tensor<[1], Int16PerTensor, CPU>[constant:[-0.86328125]]) -> (%1042:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.16.self_attn.ReduceMin.0">(%1042:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1043:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.16.self_attn.Add.0">(%1043:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %1044:tensor<[1], Int16PerTensor, CPU>[constant:[-0.9999988]]) -> (%1045:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.16.self_attn.Equal.0">(%319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %1046:tensor<[1], Float32, CPU>[constant:[0]]) -> (%1047:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.16.self_attn.Where.0">(%1047:tensor<[1, 1, 32, 1024], UInt8, CPU>, %1042:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1045:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%1048:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.16.self_attn.Softmax.0">(%1048:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1049:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.16.self_attn.MatMul.1">(%1049:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1039:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%1050:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.16.self_attn.Transpose.4">(%1050:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1051:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.16.self_attn.View.3">(%1051:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1051:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.16.self_attn.o_proj">(%1051:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1052:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1052:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1034:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1035:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.16.mlp <CPU> [using_qnn:true, symbol:model.layers.16.mlp] {
        (%1054:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1059:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.16.mlp.gate_proj">(%1054:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1055:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.16.mlp.act">(%1055:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1056:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.16.mlp.up_proj">(%1054:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1057:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.16.mlp.Mul.0">(%1056:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %1057:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1058:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.16.mlp.down_proj">(%1058:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1059:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1059:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.17 <CPU> [using_qnn:true, symbol:model.layers.17] {
        (%1060:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %354:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %355:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%1100:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1074:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1075:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) {
            linalg.CPU.RMSNormOp <name="model.layers.17.input_layernorm">(%1060:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1061:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.17.self_attn (%1061:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %354:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %355:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%1092:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1074:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1075:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.AddOp <name="model.layers.17.Add.0">(%1092:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1060:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1093:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.17.post_attention_layernorm">(%1093:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1094:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.17.mlp (%1094:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1099:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.17.Add.1">(%1099:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1093:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1100:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1100:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1074:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1075:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.17.self_attn <CPU> [using_qnn:true, symbol:model.layers.17.self_attn] {
        (%1061:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %354:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %355:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%1092:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1074:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1075:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) {
            linalg.CPU.LinearOp <name="model.layers.17.self_attn.q_proj">(%1061:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1062:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.17.self_attn.k_proj">(%1061:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1063:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.17.self_attn.v_proj">(%1061:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1064:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.17.self_attn.View.0">(%1062:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1062:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.17.self_attn.Transpose.0">(%1062:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1065:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.17.self_attn.View.1">(%1063:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1063:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.17.self_attn.Transpose.1">(%1063:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1066:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.17.self_attn.View.2">(%1064:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1064:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.17.self_attn.Transpose.2">(%1064:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1067:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.17.self_attn.q_norm">(%1065:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1068:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.17.self_attn.k_norm">(%1066:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1069:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.17.self_attn.q_rope">(%1068:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1070:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.17.self_attn.k_rope">(%1069:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1071:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.17.self_attn.CastType.0">(%1071:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1072:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.17.self_attn.CastType.1">(%1072:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1073:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.17.self_attn.Transpose.3">(%1073:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1074:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.CastTypeOp <name="model.layers.17.self_attn.CastType.2">(%1067:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1075:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.ConcatOp <name="model.layers.17.self_attn.Concat.0">(%354:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %1074:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> (%1076:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.17.self_attn.Concat.1">(%355:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %1075:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> (%1077:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.17.self_attn.Repeat.0">(%1076:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%1078:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.17.self_attn.Repeat.1">(%1077:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%1079:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.17.self_attn.MatMul.0">(%1070:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %1078:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%1080:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.17.self_attn.Mul.0">(%1080:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1081:tensor<[1], Int16PerTensor, CPU>[constant:[-0.33398438]]) -> (%1082:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.17.self_attn.ReduceMin.0">(%1082:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1083:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.17.self_attn.Add.0">(%1083:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %1084:tensor<[1], Int16PerTensor, CPU>[constant:[0.24121064]]) -> (%1085:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.17.self_attn.Equal.0">(%319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %1086:tensor<[1], Float32, CPU>[constant:[0]]) -> (%1087:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.17.self_attn.Where.0">(%1087:tensor<[1, 1, 32, 1024], UInt8, CPU>, %1082:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1085:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%1088:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.17.self_attn.Softmax.0">(%1088:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1089:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.17.self_attn.MatMul.1">(%1089:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1079:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%1090:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.17.self_attn.Transpose.4">(%1090:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1091:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.17.self_attn.View.3">(%1091:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1091:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.17.self_attn.o_proj">(%1091:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1092:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1092:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1074:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1075:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.17.mlp <CPU> [using_qnn:true, symbol:model.layers.17.mlp] {
        (%1094:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1099:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.17.mlp.gate_proj">(%1094:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1095:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.17.mlp.act">(%1095:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1096:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.17.mlp.up_proj">(%1094:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1097:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.17.mlp.Mul.0">(%1096:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %1097:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1098:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.17.mlp.down_proj">(%1098:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1099:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1099:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.18 <CPU> [using_qnn:true, symbol:model.layers.18] {
        (%1100:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %356:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %357:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%1140:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1114:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1115:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) {
            linalg.CPU.RMSNormOp <name="model.layers.18.input_layernorm">(%1100:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1101:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.18.self_attn (%1101:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %356:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %357:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%1132:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1114:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1115:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.AddOp <name="model.layers.18.Add.0">(%1132:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1100:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1133:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.18.post_attention_layernorm">(%1133:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1134:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.18.mlp (%1134:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1139:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.18.Add.1">(%1139:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1133:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1140:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1140:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1114:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1115:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.18.self_attn <CPU> [using_qnn:true, symbol:model.layers.18.self_attn] {
        (%1101:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %356:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %357:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%1132:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1114:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1115:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) {
            linalg.CPU.LinearOp <name="model.layers.18.self_attn.q_proj">(%1101:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1102:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.18.self_attn.k_proj">(%1101:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1103:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.18.self_attn.v_proj">(%1101:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1104:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.18.self_attn.View.0">(%1102:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1102:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.18.self_attn.Transpose.0">(%1102:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1105:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.18.self_attn.View.1">(%1103:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1103:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.18.self_attn.Transpose.1">(%1103:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1106:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.18.self_attn.View.2">(%1104:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1104:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.18.self_attn.Transpose.2">(%1104:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1107:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.18.self_attn.q_norm">(%1105:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1108:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.18.self_attn.k_norm">(%1106:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1109:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.18.self_attn.q_rope">(%1108:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1110:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.18.self_attn.k_rope">(%1109:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1111:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.18.self_attn.CastType.0">(%1111:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1112:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.18.self_attn.CastType.1">(%1112:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1113:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.18.self_attn.Transpose.3">(%1113:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1114:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.CastTypeOp <name="model.layers.18.self_attn.CastType.2">(%1107:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1115:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.ConcatOp <name="model.layers.18.self_attn.Concat.0">(%356:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %1114:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> (%1116:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.18.self_attn.Concat.1">(%357:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %1115:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> (%1117:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.18.self_attn.Repeat.0">(%1116:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%1118:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.18.self_attn.Repeat.1">(%1117:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%1119:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.18.self_attn.MatMul.0">(%1110:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %1118:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%1120:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.18.self_attn.Mul.0">(%1120:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1121:tensor<[1], Int16PerTensor, CPU>[constant:[0]]) -> (%1122:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.18.self_attn.ReduceMin.0">(%1122:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1123:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.18.self_attn.Add.0">(%1123:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %1124:tensor<[1], Int16PerTensor, CPU>[constant:[0.5546863]]) -> (%1125:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.18.self_attn.Equal.0">(%319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %1126:tensor<[1], Float32, CPU>[constant:[0]]) -> (%1127:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.18.self_attn.Where.0">(%1127:tensor<[1, 1, 32, 1024], UInt8, CPU>, %1122:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1125:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%1128:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.18.self_attn.Softmax.0">(%1128:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1129:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.18.self_attn.MatMul.1">(%1129:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1119:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%1130:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.18.self_attn.Transpose.4">(%1130:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1131:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.18.self_attn.View.3">(%1131:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1131:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.18.self_attn.o_proj">(%1131:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1132:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1132:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1114:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1115:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.18.mlp <CPU> [using_qnn:true, symbol:model.layers.18.mlp] {
        (%1134:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1139:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.18.mlp.gate_proj">(%1134:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1135:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.18.mlp.act">(%1135:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1136:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.18.mlp.up_proj">(%1134:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1137:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.18.mlp.Mul.0">(%1136:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %1137:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1138:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.18.mlp.down_proj">(%1138:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1139:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1139:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.19 <CPU> [using_qnn:true, symbol:model.layers.19] {
        (%1140:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %358:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %359:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%1180:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1154:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1155:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) {
            linalg.CPU.RMSNormOp <name="model.layers.19.input_layernorm">(%1140:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1141:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.19.self_attn (%1141:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %358:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %359:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%1172:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1154:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1155:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.AddOp <name="model.layers.19.Add.0">(%1172:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1140:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1173:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.19.post_attention_layernorm">(%1173:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1174:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.19.mlp (%1174:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1179:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.19.Add.1">(%1179:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1173:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1180:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1180:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1154:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1155:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.19.self_attn <CPU> [using_qnn:true, symbol:model.layers.19.self_attn] {
        (%1141:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %358:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %359:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%1172:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1154:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1155:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) {
            linalg.CPU.LinearOp <name="model.layers.19.self_attn.q_proj">(%1141:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1142:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.19.self_attn.k_proj">(%1141:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1143:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.19.self_attn.v_proj">(%1141:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1144:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.19.self_attn.View.0">(%1142:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1142:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.19.self_attn.Transpose.0">(%1142:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1145:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.19.self_attn.View.1">(%1143:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1143:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.19.self_attn.Transpose.1">(%1143:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1146:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.19.self_attn.View.2">(%1144:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1144:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.19.self_attn.Transpose.2">(%1144:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1147:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.19.self_attn.q_norm">(%1145:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1148:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.19.self_attn.k_norm">(%1146:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1149:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.19.self_attn.q_rope">(%1148:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1150:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.19.self_attn.k_rope">(%1149:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1151:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.19.self_attn.CastType.0">(%1151:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1152:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.19.self_attn.CastType.1">(%1152:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1153:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.19.self_attn.Transpose.3">(%1153:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1154:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.CastTypeOp <name="model.layers.19.self_attn.CastType.2">(%1147:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1155:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.ConcatOp <name="model.layers.19.self_attn.Concat.0">(%358:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %1154:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> (%1156:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.19.self_attn.Concat.1">(%359:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %1155:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> (%1157:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.19.self_attn.Repeat.0">(%1156:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%1158:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.19.self_attn.Repeat.1">(%1157:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%1159:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.19.self_attn.MatMul.0">(%1150:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %1158:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%1160:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.19.self_attn.Mul.0">(%1160:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1161:tensor<[1], Int16PerTensor, CPU>[constant:[0.98046875]]) -> (%1162:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.19.self_attn.ReduceMin.0">(%1162:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1163:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.19.self_attn.Add.0">(%1163:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %1164:tensor<[1], Int16PerTensor, CPU>[constant:[0.72265506]]) -> (%1165:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.19.self_attn.Equal.0">(%319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %1166:tensor<[1], Float32, CPU>[constant:[0]]) -> (%1167:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.19.self_attn.Where.0">(%1167:tensor<[1, 1, 32, 1024], UInt8, CPU>, %1162:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1165:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%1168:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.19.self_attn.Softmax.0">(%1168:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1169:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.19.self_attn.MatMul.1">(%1169:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1159:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%1170:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.19.self_attn.Transpose.4">(%1170:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1171:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.19.self_attn.View.3">(%1171:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1171:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.19.self_attn.o_proj">(%1171:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1172:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1172:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1154:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1155:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.19.mlp <CPU> [using_qnn:true, symbol:model.layers.19.mlp] {
        (%1174:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1179:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.19.mlp.gate_proj">(%1174:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1175:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.19.mlp.act">(%1175:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1176:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.19.mlp.up_proj">(%1174:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1177:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.19.mlp.Mul.0">(%1176:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %1177:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1178:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.19.mlp.down_proj">(%1178:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1179:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1179:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.20 <CPU> [using_qnn:true, symbol:model.layers.20] {
        (%1180:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %360:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %361:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%1220:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1194:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1195:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) {
            linalg.CPU.RMSNormOp <name="model.layers.20.input_layernorm">(%1180:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1181:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.20.self_attn (%1181:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %360:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %361:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%1212:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1194:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1195:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.AddOp <name="model.layers.20.Add.0">(%1212:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1180:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1213:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.20.post_attention_layernorm">(%1213:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1214:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.20.mlp (%1214:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1219:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.20.Add.1">(%1219:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1213:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1220:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1220:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1194:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1195:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.20.self_attn <CPU> [using_qnn:true, symbol:model.layers.20.self_attn] {
        (%1181:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %360:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %361:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%1212:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1194:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1195:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) {
            linalg.CPU.LinearOp <name="model.layers.20.self_attn.q_proj">(%1181:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1182:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.20.self_attn.k_proj">(%1181:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1183:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.20.self_attn.v_proj">(%1181:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1184:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.20.self_attn.View.0">(%1182:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1182:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.20.self_attn.Transpose.0">(%1182:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1185:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.20.self_attn.View.1">(%1183:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1183:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.20.self_attn.Transpose.1">(%1183:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1186:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.20.self_attn.View.2">(%1184:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1184:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.20.self_attn.Transpose.2">(%1184:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1187:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.20.self_attn.q_norm">(%1185:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1188:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.20.self_attn.k_norm">(%1186:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1189:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.20.self_attn.q_rope">(%1188:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1190:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.20.self_attn.k_rope">(%1189:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1191:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.20.self_attn.CastType.0">(%1191:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1192:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.20.self_attn.CastType.1">(%1192:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1193:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.20.self_attn.Transpose.3">(%1193:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1194:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.CastTypeOp <name="model.layers.20.self_attn.CastType.2">(%1187:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1195:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.ConcatOp <name="model.layers.20.self_attn.Concat.0">(%360:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %1194:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> (%1196:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.20.self_attn.Concat.1">(%361:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %1195:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> (%1197:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.20.self_attn.Repeat.0">(%1196:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%1198:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.20.self_attn.Repeat.1">(%1197:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%1199:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.20.self_attn.MatMul.0">(%1190:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %1198:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%1200:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.20.self_attn.Mul.0">(%1200:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1201:tensor<[1], Int16PerTensor, CPU>[constant:[-0.35351562]]) -> (%1202:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.20.self_attn.ReduceMin.0">(%1202:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1203:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.20.self_attn.Add.0">(%1203:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %1204:tensor<[1], Int16PerTensor, CPU>[constant:[-0.8124988]]) -> (%1205:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.20.self_attn.Equal.0">(%319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %1206:tensor<[1], Float32, CPU>[constant:[0]]) -> (%1207:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.20.self_attn.Where.0">(%1207:tensor<[1, 1, 32, 1024], UInt8, CPU>, %1202:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1205:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%1208:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.20.self_attn.Softmax.0">(%1208:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1209:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.20.self_attn.MatMul.1">(%1209:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1199:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%1210:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.20.self_attn.Transpose.4">(%1210:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1211:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.20.self_attn.View.3">(%1211:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1211:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.20.self_attn.o_proj">(%1211:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1212:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1212:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1194:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1195:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.20.mlp <CPU> [using_qnn:true, symbol:model.layers.20.mlp] {
        (%1214:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1219:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.20.mlp.gate_proj">(%1214:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1215:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.20.mlp.act">(%1215:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1216:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.20.mlp.up_proj">(%1214:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1217:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.20.mlp.Mul.0">(%1216:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %1217:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1218:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.20.mlp.down_proj">(%1218:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1219:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1219:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.21 <CPU> [using_qnn:true, symbol:model.layers.21] {
        (%1220:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %362:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %363:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%1260:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1234:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1235:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) {
            linalg.CPU.RMSNormOp <name="model.layers.21.input_layernorm">(%1220:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1221:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.21.self_attn (%1221:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %362:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %363:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%1252:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1234:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1235:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.AddOp <name="model.layers.21.Add.0">(%1252:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1220:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1253:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.21.post_attention_layernorm">(%1253:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1254:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.21.mlp (%1254:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1259:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.21.Add.1">(%1259:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1253:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1260:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1260:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1234:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1235:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.21.self_attn <CPU> [using_qnn:true, symbol:model.layers.21.self_attn] {
        (%1221:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %362:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %363:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%1252:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1234:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1235:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) {
            linalg.CPU.LinearOp <name="model.layers.21.self_attn.q_proj">(%1221:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1222:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.21.self_attn.k_proj">(%1221:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1223:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.21.self_attn.v_proj">(%1221:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1224:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.21.self_attn.View.0">(%1222:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1222:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.21.self_attn.Transpose.0">(%1222:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1225:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.21.self_attn.View.1">(%1223:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1223:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.21.self_attn.Transpose.1">(%1223:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1226:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.21.self_attn.View.2">(%1224:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1224:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.21.self_attn.Transpose.2">(%1224:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1227:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.21.self_attn.q_norm">(%1225:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1228:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.21.self_attn.k_norm">(%1226:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1229:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.21.self_attn.q_rope">(%1228:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1230:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.21.self_attn.k_rope">(%1229:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1231:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.21.self_attn.CastType.0">(%1231:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1232:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.21.self_attn.CastType.1">(%1232:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1233:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.21.self_attn.Transpose.3">(%1233:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1234:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.CastTypeOp <name="model.layers.21.self_attn.CastType.2">(%1227:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1235:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.ConcatOp <name="model.layers.21.self_attn.Concat.0">(%362:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %1234:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> (%1236:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.21.self_attn.Concat.1">(%363:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %1235:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> (%1237:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.21.self_attn.Repeat.0">(%1236:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%1238:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.21.self_attn.Repeat.1">(%1237:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%1239:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.21.self_attn.MatMul.0">(%1230:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %1238:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%1240:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.21.self_attn.Mul.0">(%1240:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1241:tensor<[1], Int16PerTensor, CPU>[constant:[-0.85546875]]) -> (%1242:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.21.self_attn.ReduceMin.0">(%1242:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1243:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.21.self_attn.Add.0">(%1243:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %1244:tensor<[1], Int16PerTensor, CPU>[constant:[-0.4296869]]) -> (%1245:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.21.self_attn.Equal.0">(%319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %1246:tensor<[1], Float32, CPU>[constant:[0]]) -> (%1247:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.21.self_attn.Where.0">(%1247:tensor<[1, 1, 32, 1024], UInt8, CPU>, %1242:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1245:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%1248:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.21.self_attn.Softmax.0">(%1248:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1249:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.21.self_attn.MatMul.1">(%1249:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1239:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%1250:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.21.self_attn.Transpose.4">(%1250:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1251:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.21.self_attn.View.3">(%1251:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1251:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.21.self_attn.o_proj">(%1251:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1252:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1252:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1234:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1235:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.21.mlp <CPU> [using_qnn:true, symbol:model.layers.21.mlp] {
        (%1254:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1259:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.21.mlp.gate_proj">(%1254:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1255:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.21.mlp.act">(%1255:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1256:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.21.mlp.up_proj">(%1254:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1257:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.21.mlp.Mul.0">(%1256:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %1257:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1258:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.21.mlp.down_proj">(%1258:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1259:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1259:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.22 <CPU> [using_qnn:true, symbol:model.layers.22] {
        (%1260:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %364:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %365:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%1300:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1274:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1275:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) {
            linalg.CPU.RMSNormOp <name="model.layers.22.input_layernorm">(%1260:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1261:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.22.self_attn (%1261:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %364:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %365:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%1292:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1274:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1275:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.AddOp <name="model.layers.22.Add.0">(%1292:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1260:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1293:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.22.post_attention_layernorm">(%1293:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1294:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.22.mlp (%1294:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1299:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.22.Add.1">(%1299:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1293:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1300:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1300:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1274:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1275:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.22.self_attn <CPU> [using_qnn:true, symbol:model.layers.22.self_attn] {
        (%1261:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %364:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %365:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%1292:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1274:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1275:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) {
            linalg.CPU.LinearOp <name="model.layers.22.self_attn.q_proj">(%1261:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1262:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.22.self_attn.k_proj">(%1261:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1263:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.22.self_attn.v_proj">(%1261:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1264:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.22.self_attn.View.0">(%1262:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1262:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.22.self_attn.Transpose.0">(%1262:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1265:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.22.self_attn.View.1">(%1263:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1263:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.22.self_attn.Transpose.1">(%1263:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1266:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.22.self_attn.View.2">(%1264:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1264:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.22.self_attn.Transpose.2">(%1264:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1267:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.22.self_attn.q_norm">(%1265:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1268:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.22.self_attn.k_norm">(%1266:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1269:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.22.self_attn.q_rope">(%1268:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1270:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.22.self_attn.k_rope">(%1269:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1271:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.22.self_attn.CastType.0">(%1271:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1272:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.22.self_attn.CastType.1">(%1272:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1273:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.22.self_attn.Transpose.3">(%1273:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1274:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.CastTypeOp <name="model.layers.22.self_attn.CastType.2">(%1267:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1275:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.ConcatOp <name="model.layers.22.self_attn.Concat.0">(%364:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %1274:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> (%1276:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.22.self_attn.Concat.1">(%365:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %1275:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> (%1277:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.22.self_attn.Repeat.0">(%1276:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%1278:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.22.self_attn.Repeat.1">(%1277:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%1279:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.22.self_attn.MatMul.0">(%1270:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %1278:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%1280:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.22.self_attn.Mul.0">(%1280:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1281:tensor<[1], Int16PerTensor, CPU>[constant:[0.66015625]]) -> (%1282:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.22.self_attn.ReduceMin.0">(%1282:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1283:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.22.self_attn.Add.0">(%1283:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %1284:tensor<[1], Int16PerTensor, CPU>[constant:[0.9687488]]) -> (%1285:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.22.self_attn.Equal.0">(%319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %1286:tensor<[1], Float32, CPU>[constant:[0]]) -> (%1287:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.22.self_attn.Where.0">(%1287:tensor<[1, 1, 32, 1024], UInt8, CPU>, %1282:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1285:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%1288:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.22.self_attn.Softmax.0">(%1288:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1289:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.22.self_attn.MatMul.1">(%1289:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1279:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%1290:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.22.self_attn.Transpose.4">(%1290:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1291:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.22.self_attn.View.3">(%1291:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1291:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.22.self_attn.o_proj">(%1291:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1292:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1292:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1274:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1275:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.22.mlp <CPU> [using_qnn:true, symbol:model.layers.22.mlp] {
        (%1294:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1299:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.22.mlp.gate_proj">(%1294:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1295:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.22.mlp.act">(%1295:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1296:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.22.mlp.up_proj">(%1294:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1297:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.22.mlp.Mul.0">(%1296:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %1297:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1298:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.22.mlp.down_proj">(%1298:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1299:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1299:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.23 <CPU> [using_qnn:true, symbol:model.layers.23] {
        (%1300:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %366:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %367:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%1340:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1314:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1315:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) {
            linalg.CPU.RMSNormOp <name="model.layers.23.input_layernorm">(%1300:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1301:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.23.self_attn (%1301:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %366:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %367:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%1332:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1314:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1315:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.AddOp <name="model.layers.23.Add.0">(%1332:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1300:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1333:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.23.post_attention_layernorm">(%1333:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1334:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.23.mlp (%1334:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1339:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.23.Add.1">(%1339:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1333:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1340:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1340:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1314:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1315:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.23.self_attn <CPU> [using_qnn:true, symbol:model.layers.23.self_attn] {
        (%1301:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %366:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %367:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%1332:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1314:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1315:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) {
            linalg.CPU.LinearOp <name="model.layers.23.self_attn.q_proj">(%1301:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1302:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.23.self_attn.k_proj">(%1301:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1303:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.23.self_attn.v_proj">(%1301:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1304:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.23.self_attn.View.0">(%1302:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1302:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.23.self_attn.Transpose.0">(%1302:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1305:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.23.self_attn.View.1">(%1303:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1303:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.23.self_attn.Transpose.1">(%1303:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1306:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.23.self_attn.View.2">(%1304:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1304:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.23.self_attn.Transpose.2">(%1304:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1307:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.23.self_attn.q_norm">(%1305:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1308:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.23.self_attn.k_norm">(%1306:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1309:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.23.self_attn.q_rope">(%1308:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1310:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.23.self_attn.k_rope">(%1309:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1311:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.23.self_attn.CastType.0">(%1311:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1312:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.23.self_attn.CastType.1">(%1312:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1313:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.23.self_attn.Transpose.3">(%1313:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1314:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.CastTypeOp <name="model.layers.23.self_attn.CastType.2">(%1307:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1315:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.ConcatOp <name="model.layers.23.self_attn.Concat.0">(%366:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %1314:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> (%1316:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.23.self_attn.Concat.1">(%367:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %1315:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> (%1317:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.23.self_attn.Repeat.0">(%1316:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%1318:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.23.self_attn.Repeat.1">(%1317:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%1319:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.23.self_attn.MatMul.0">(%1310:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %1318:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%1320:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.23.self_attn.Mul.0">(%1320:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1321:tensor<[1], Int16PerTensor, CPU>[constant:[0.61328125]]) -> (%1322:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.23.self_attn.ReduceMin.0">(%1322:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1323:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.23.self_attn.Add.0">(%1323:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %1324:tensor<[1], Int16PerTensor, CPU>[constant:[0.079589695]]) -> (%1325:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.23.self_attn.Equal.0">(%319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %1326:tensor<[1], Float32, CPU>[constant:[0]]) -> (%1327:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.23.self_attn.Where.0">(%1327:tensor<[1, 1, 32, 1024], UInt8, CPU>, %1322:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1325:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%1328:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.23.self_attn.Softmax.0">(%1328:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1329:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.23.self_attn.MatMul.1">(%1329:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1319:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%1330:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.23.self_attn.Transpose.4">(%1330:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1331:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.23.self_attn.View.3">(%1331:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1331:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.23.self_attn.o_proj">(%1331:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1332:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1332:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1314:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1315:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.23.mlp <CPU> [using_qnn:true, symbol:model.layers.23.mlp] {
        (%1334:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1339:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.23.mlp.gate_proj">(%1334:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1335:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.23.mlp.act">(%1335:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1336:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.23.mlp.up_proj">(%1334:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1337:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.23.mlp.Mul.0">(%1336:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %1337:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1338:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.23.mlp.down_proj">(%1338:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1339:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1339:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.24 <CPU> [using_qnn:true, symbol:model.layers.24] {
        (%1340:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %368:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %369:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%1380:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1354:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1355:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) {
            linalg.CPU.RMSNormOp <name="model.layers.24.input_layernorm">(%1340:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1341:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.24.self_attn (%1341:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %368:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %369:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%1372:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1354:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1355:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.AddOp <name="model.layers.24.Add.0">(%1372:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1340:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1373:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.24.post_attention_layernorm">(%1373:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1374:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.24.mlp (%1374:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1379:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.24.Add.1">(%1379:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1373:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1380:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1380:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1354:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1355:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.24.self_attn <CPU> [using_qnn:true, symbol:model.layers.24.self_attn] {
        (%1341:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %368:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %369:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%1372:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1354:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1355:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) {
            linalg.CPU.LinearOp <name="model.layers.24.self_attn.q_proj">(%1341:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1342:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.24.self_attn.k_proj">(%1341:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1343:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.24.self_attn.v_proj">(%1341:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1344:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.24.self_attn.View.0">(%1342:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1342:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.24.self_attn.Transpose.0">(%1342:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1345:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.24.self_attn.View.1">(%1343:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1343:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.24.self_attn.Transpose.1">(%1343:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1346:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.24.self_attn.View.2">(%1344:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1344:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.24.self_attn.Transpose.2">(%1344:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1347:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.24.self_attn.q_norm">(%1345:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1348:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.24.self_attn.k_norm">(%1346:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1349:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.24.self_attn.q_rope">(%1348:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1350:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.24.self_attn.k_rope">(%1349:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1351:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.24.self_attn.CastType.0">(%1351:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1352:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.24.self_attn.CastType.1">(%1352:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1353:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.24.self_attn.Transpose.3">(%1353:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1354:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.CastTypeOp <name="model.layers.24.self_attn.CastType.2">(%1347:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1355:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.ConcatOp <name="model.layers.24.self_attn.Concat.0">(%368:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %1354:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> (%1356:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.24.self_attn.Concat.1">(%369:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %1355:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> (%1357:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.24.self_attn.Repeat.0">(%1356:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%1358:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.24.self_attn.Repeat.1">(%1357:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%1359:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.24.self_attn.MatMul.0">(%1350:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %1358:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%1360:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.24.self_attn.Mul.0">(%1360:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1361:tensor<[1], Int16PerTensor, CPU>[constant:[-0.8828125]]) -> (%1362:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.24.self_attn.ReduceMin.0">(%1362:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1363:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.24.self_attn.Add.0">(%1363:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %1364:tensor<[1], Int16PerTensor, CPU>[constant:[-0.99609256]]) -> (%1365:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.24.self_attn.Equal.0">(%319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %1366:tensor<[1], Float32, CPU>[constant:[0]]) -> (%1367:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.24.self_attn.Where.0">(%1367:tensor<[1, 1, 32, 1024], UInt8, CPU>, %1362:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1365:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%1368:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.24.self_attn.Softmax.0">(%1368:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1369:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.24.self_attn.MatMul.1">(%1369:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1359:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%1370:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.24.self_attn.Transpose.4">(%1370:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1371:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.24.self_attn.View.3">(%1371:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1371:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.24.self_attn.o_proj">(%1371:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1372:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1372:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1354:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1355:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.24.mlp <CPU> [using_qnn:true, symbol:model.layers.24.mlp] {
        (%1374:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1379:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.24.mlp.gate_proj">(%1374:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1375:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.24.mlp.act">(%1375:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1376:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.24.mlp.up_proj">(%1374:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1377:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.24.mlp.Mul.0">(%1376:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %1377:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1378:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.24.mlp.down_proj">(%1378:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1379:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1379:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.25 <CPU> [using_qnn:true, symbol:model.layers.25] {
        (%1380:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %370:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %371:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%1420:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1395:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) {
            linalg.CPU.RMSNormOp <name="model.layers.25.input_layernorm">(%1380:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1381:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.25.self_attn (%1381:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %370:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %371:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%1412:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1395:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.AddOp <name="model.layers.25.Add.0">(%1412:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1380:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1413:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.25.post_attention_layernorm">(%1413:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1414:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.25.mlp (%1414:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1419:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.25.Add.1">(%1419:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1413:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1420:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1420:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1395:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.25.self_attn <CPU> [using_qnn:true, symbol:model.layers.25.self_attn] {
        (%1381:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %370:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %371:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%1412:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1395:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) {
            linalg.CPU.LinearOp <name="model.layers.25.self_attn.q_proj">(%1381:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1382:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.25.self_attn.k_proj">(%1381:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1383:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.25.self_attn.v_proj">(%1381:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1384:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.25.self_attn.View.0">(%1382:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1382:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.25.self_attn.Transpose.0">(%1382:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1385:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.25.self_attn.View.1">(%1383:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1383:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.25.self_attn.Transpose.1">(%1383:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1386:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.25.self_attn.View.2">(%1384:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1384:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.25.self_attn.Transpose.2">(%1384:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1387:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.25.self_attn.q_norm">(%1385:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1388:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.25.self_attn.k_norm">(%1386:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1389:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.25.self_attn.q_rope">(%1388:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1390:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.25.self_attn.k_rope">(%1389:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1391:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.25.self_attn.CastType.0">(%1391:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1392:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.25.self_attn.CastType.1">(%1392:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1393:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.25.self_attn.Transpose.3">(%1393:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.CastTypeOp <name="model.layers.25.self_attn.CastType.2">(%1387:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1395:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.ConcatOp <name="model.layers.25.self_attn.Concat.0">(%370:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %1394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> (%1396:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.25.self_attn.Concat.1">(%371:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %1395:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> (%1397:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.25.self_attn.Repeat.0">(%1396:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%1398:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.25.self_attn.Repeat.1">(%1397:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%1399:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.25.self_attn.MatMul.0">(%1390:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %1398:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%1400:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.25.self_attn.Mul.0">(%1400:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1401:tensor<[1], Int16PerTensor, CPU>[constant:[-0.29492188]]) -> (%1402:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.25.self_attn.ReduceMin.0">(%1402:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1403:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.25.self_attn.Add.0">(%1403:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %1404:tensor<[1], Int16PerTensor, CPU>[constant:[0.2812494]]) -> (%1405:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.25.self_attn.Equal.0">(%319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %1406:tensor<[1], Float32, CPU>[constant:[0]]) -> (%1407:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.25.self_attn.Where.0">(%1407:tensor<[1, 1, 32, 1024], UInt8, CPU>, %1402:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1405:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%1408:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.25.self_attn.Softmax.0">(%1408:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1409:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.25.self_attn.MatMul.1">(%1409:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1399:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%1410:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.25.self_attn.Transpose.4">(%1410:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1411:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.25.self_attn.View.3">(%1411:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1411:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.25.self_attn.o_proj">(%1411:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1412:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1412:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1395:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.25.mlp <CPU> [using_qnn:true, symbol:model.layers.25.mlp] {
        (%1414:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1419:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.25.mlp.gate_proj">(%1414:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1415:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.25.mlp.act">(%1415:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1416:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.25.mlp.up_proj">(%1414:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1417:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.25.mlp.Mul.0">(%1416:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %1417:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1418:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.25.mlp.down_proj">(%1418:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1419:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1419:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.26 <CPU> [using_qnn:true, symbol:model.layers.26] {
        (%1420:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %372:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %373:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%1460:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1434:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1435:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) {
            linalg.CPU.RMSNormOp <name="model.layers.26.input_layernorm">(%1420:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1421:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.26.self_attn (%1421:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %372:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %373:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%1452:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1434:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1435:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.AddOp <name="model.layers.26.Add.0">(%1452:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1420:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1453:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.26.post_attention_layernorm">(%1453:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1454:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.26.mlp (%1454:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1459:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.26.Add.1">(%1459:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1453:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1460:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1460:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1434:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1435:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.26.self_attn <CPU> [using_qnn:true, symbol:model.layers.26.self_attn] {
        (%1421:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %372:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %373:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%1452:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1434:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1435:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) {
            linalg.CPU.LinearOp <name="model.layers.26.self_attn.q_proj">(%1421:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1422:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.26.self_attn.k_proj">(%1421:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1423:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.26.self_attn.v_proj">(%1421:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1424:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.26.self_attn.View.0">(%1422:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1422:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.26.self_attn.Transpose.0">(%1422:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1425:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.26.self_attn.View.1">(%1423:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1423:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.26.self_attn.Transpose.1">(%1423:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1426:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.26.self_attn.View.2">(%1424:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1424:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.26.self_attn.Transpose.2">(%1424:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1427:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.26.self_attn.q_norm">(%1425:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1428:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.26.self_attn.k_norm">(%1426:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1429:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.26.self_attn.q_rope">(%1428:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1430:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.26.self_attn.k_rope">(%1429:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1431:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.26.self_attn.CastType.0">(%1431:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1432:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.26.self_attn.CastType.1">(%1432:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1433:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.26.self_attn.Transpose.3">(%1433:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1434:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.CastTypeOp <name="model.layers.26.self_attn.CastType.2">(%1427:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1435:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.ConcatOp <name="model.layers.26.self_attn.Concat.0">(%372:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %1434:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> (%1436:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.26.self_attn.Concat.1">(%373:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %1435:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> (%1437:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.26.self_attn.Repeat.0">(%1436:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%1438:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.26.self_attn.Repeat.1">(%1437:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%1439:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.26.self_attn.MatMul.0">(%1430:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %1438:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%1440:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.26.self_attn.Mul.0">(%1440:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1441:tensor<[1], Int16PerTensor, CPU>[constant:[0.9921875]]) -> (%1442:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.26.self_attn.ReduceMin.0">(%1442:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1443:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.26.self_attn.Add.0">(%1443:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %1444:tensor<[1], Int16PerTensor, CPU>[constant:[0.89453006]]) -> (%1445:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.26.self_attn.Equal.0">(%319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %1446:tensor<[1], Float32, CPU>[constant:[0]]) -> (%1447:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.26.self_attn.Where.0">(%1447:tensor<[1, 1, 32, 1024], UInt8, CPU>, %1442:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1445:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%1448:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.26.self_attn.Softmax.0">(%1448:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1449:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.26.self_attn.MatMul.1">(%1449:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1439:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%1450:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.26.self_attn.Transpose.4">(%1450:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1451:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.26.self_attn.View.3">(%1451:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1451:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.26.self_attn.o_proj">(%1451:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1452:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1452:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1434:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1435:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.26.mlp <CPU> [using_qnn:true, symbol:model.layers.26.mlp] {
        (%1454:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1459:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.26.mlp.gate_proj">(%1454:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1455:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.26.mlp.act">(%1455:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1456:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.26.mlp.up_proj">(%1454:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1457:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.26.mlp.Mul.0">(%1456:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %1457:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1458:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.26.mlp.down_proj">(%1458:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1459:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1459:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.27 <CPU> [using_qnn:true, symbol:model.layers.27] {
        (%1460:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %374:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %375:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%1500:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1474:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1475:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) {
            linalg.CPU.RMSNormOp <name="model.layers.27.input_layernorm">(%1460:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1461:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.27.self_attn (%1461:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %374:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %375:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%1492:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1474:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1475:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.AddOp <name="model.layers.27.Add.0">(%1492:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1460:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1493:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.27.post_attention_layernorm">(%1493:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1494:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.27.mlp (%1494:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1499:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.27.Add.1">(%1499:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1493:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1500:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1500:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1474:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1475:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.27.self_attn <CPU> [using_qnn:true, symbol:model.layers.27.self_attn] {
        (%1461:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %374:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %375:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true]) -> (%1492:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1474:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1475:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) {
            linalg.CPU.LinearOp <name="model.layers.27.self_attn.q_proj">(%1461:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1462:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.27.self_attn.k_proj">(%1461:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1463:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.27.self_attn.v_proj">(%1461:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1464:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.27.self_attn.View.0">(%1462:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1462:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.27.self_attn.Transpose.0">(%1462:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1465:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.27.self_attn.View.1">(%1463:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1463:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.27.self_attn.Transpose.1">(%1463:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1466:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.27.self_attn.View.2">(%1464:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1464:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.27.self_attn.Transpose.2">(%1464:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1467:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.27.self_attn.q_norm">(%1465:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1468:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.27.self_attn.k_norm">(%1466:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1469:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.27.self_attn.q_rope">(%1468:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1470:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.27.self_attn.k_rope">(%1469:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1471:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.27.self_attn.CastType.0">(%1471:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1472:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.27.self_attn.CastType.1">(%1472:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1473:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.27.self_attn.Transpose.3">(%1473:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1474:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.CastTypeOp <name="model.layers.27.self_attn.CastType.2">(%1467:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1475:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true])
            linalg.CPU.ConcatOp <name="model.layers.27.self_attn.Concat.0">(%374:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[qnn_graph_inputs:true], %1474:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> (%1476:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.27.self_attn.Concat.1">(%375:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[qnn_graph_inputs:true], %1475:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> (%1477:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.27.self_attn.Repeat.0">(%1476:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%1478:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.27.self_attn.Repeat.1">(%1477:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%1479:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.27.self_attn.MatMul.0">(%1470:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %1478:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%1480:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.27.self_attn.Mul.0">(%1480:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1481:tensor<[1], Int16PerTensor, CPU>[constant:[-0.061767578]]) -> (%1482:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.27.self_attn.ReduceMin.0">(%1482:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1483:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.27.self_attn.Add.0">(%1483:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %1484:tensor<[1], Int16PerTensor, CPU>[constant:[-0.60546756]]) -> (%1485:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.27.self_attn.Equal.0">(%319:tensor<[1, 1, 32, 1024], Float32, CPU>[qnn_graph_inputs:true], %1486:tensor<[1], Float32, CPU>[constant:[0]]) -> (%1487:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.27.self_attn.Where.0">(%1487:tensor<[1, 1, 32, 1024], UInt8, CPU>, %1482:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1485:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%1488:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.27.self_attn.Softmax.0">(%1488:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1489:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.27.self_attn.MatMul.1">(%1489:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1479:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%1490:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.27.self_attn.Transpose.4">(%1490:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1491:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.27.self_attn.View.3">(%1491:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1491:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.27.self_attn.o_proj">(%1491:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1492:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1492:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1474:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[qnn_graph_outputs:true], %1475:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[qnn_graph_outputs:true]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.27.mlp <CPU> [using_qnn:true, symbol:model.layers.27.mlp] {
        (%1494:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1499:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.27.mlp.gate_proj">(%1494:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1495:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.27.mlp.act">(%1495:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1496:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.27.mlp.up_proj">(%1494:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1497:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.27.mlp.Mul.0">(%1496:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %1497:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1498:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.27.mlp.down_proj">(%1498:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1499:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1499:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    linalg.CPU.LinearOp <name="lm_head"> [using_qnn:true] (%1501:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1502:tensor<[1, 32, 151936], Int16PerTensor, CPU>[qnn_graph_outputs:true])
    //        
    //      o o    
    //            
    //       
    //             
    //        
}
 
