@main () -> () {
    graph.SubGraphOp @init <notype> [symbol:init] {
        () -> () {
            tensor.CPU.register () -> (%7516:tensor<[151936, 2048], Float32, CPU>[@model.embed_tokens.weight][quant_recipe:QuantSpec(Raw(type: Float32), uuid=61), symbol:model.embed_tokens.weight])[symbol:model.embed_tokens.weight]
            tensor.CPU.register () -> (%8011:tensor<[1, 1024, 128], Int16PerTensor, CPU>[@rope_sin][symbol:rope_sin])[symbol:rope_sin]
            tensor.CPU.register () -> (%8012:tensor<[1, 1024, 128], Int16PerTensor, CPU>[@rope_cos][symbol:rope_cos])[symbol:rope_cos]
            tensor.CPU.register () -> (%6662:tensor<[2048], Float32, CPU>[@model.layers.0.input_layernorm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=67), symbol:model.layers.0.input_layernorm.weight])[symbol:model.layers.0.input_layernorm.weight]
            tensor.CPU.register () -> (%7778:tensor<[2048, 2048], Float32, CPU>[@model.layers.0.self_attn.q_proj.weight][symbol:model.layers.0.self_attn.q_proj.weight])[symbol:model.layers.0.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%61:tensor<[1024, 2048], Float32, CPU>[@model.layers.0.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=68), symbol:model.layers.0.self_attn.k_proj.weight])[symbol:model.layers.0.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%5178:tensor<[1024, 2048], Float32, CPU>[@model.layers.0.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=70), symbol:model.layers.0.self_attn.v_proj.weight])[symbol:model.layers.0.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%1867:tensor<[128], Float32, CPU>[@model.layers.0.self_attn.q_norm.weight][quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=74), symbol:model.layers.0.self_attn.q_norm.weight])[symbol:model.layers.0.self_attn.q_norm.weight]
            tensor.CPU.register () -> (%7469:tensor<[128], Float32, CPU>[@model.layers.0.self_attn.k_norm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=76), symbol:model.layers.0.self_attn.k_norm.weight])[symbol:model.layers.0.self_attn.k_norm.weight]
            tensor.CPU.register () -> (%7880:tensor<[2048, 2048], Float32, CPU>[@model.layers.0.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=89), symbol:model.layers.0.self_attn.o_proj.weight])[symbol:model.layers.0.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%3163:tensor<[2048], Float32, CPU>[@model.layers.0.post_attention_layernorm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=92), symbol:model.layers.0.post_attention_layernorm.weight])[symbol:model.layers.0.post_attention_layernorm.weight]
            tensor.CPU.register () -> (%3038:tensor<[6144, 2048], Float32, CPU>[@model.layers.0.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=93), symbol:model.layers.0.mlp.gate_proj.weight])[symbol:model.layers.0.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%184:tensor<[6144, 2048], Float32, CPU>[@model.layers.0.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=96), symbol:model.layers.0.mlp.up_proj.weight])[symbol:model.layers.0.mlp.up_proj.weight]
            tensor.CPU.register () -> (%7449:tensor<[2048, 6144], Float32, CPU>[@model.layers.0.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=98), symbol:model.layers.0.mlp.down_proj.weight])[symbol:model.layers.0.mlp.down_proj.weight]
            tensor.CPU.register () -> (%3526:tensor<[2048], Float32, CPU>[@model.layers.1.input_layernorm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=101), symbol:model.layers.1.input_layernorm.weight])[symbol:model.layers.1.input_layernorm.weight]
            tensor.CPU.register () -> (%2471:tensor<[2048, 2048], Float32, CPU>[@model.layers.1.self_attn.q_proj.weight][symbol:model.layers.1.self_attn.q_proj.weight])[symbol:model.layers.1.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%5492:tensor<[1024, 2048], Float32, CPU>[@model.layers.1.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=102), symbol:model.layers.1.self_attn.k_proj.weight])[symbol:model.layers.1.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%554:tensor<[1024, 2048], Float32, CPU>[@model.layers.1.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=104), symbol:model.layers.1.self_attn.v_proj.weight])[symbol:model.layers.1.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%5159:tensor<[128], Float32, CPU>[@model.layers.1.self_attn.q_norm.weight][quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=108), symbol:model.layers.1.self_attn.q_norm.weight])[symbol:model.layers.1.self_attn.q_norm.weight]
            tensor.CPU.register () -> (%6337:tensor<[128], Float32, CPU>[@model.layers.1.self_attn.k_norm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=110), symbol:model.layers.1.self_attn.k_norm.weight])[symbol:model.layers.1.self_attn.k_norm.weight]
            tensor.CPU.register () -> (%3431:tensor<[2048, 2048], Float32, CPU>[@model.layers.1.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=123), symbol:model.layers.1.self_attn.o_proj.weight])[symbol:model.layers.1.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%7183:tensor<[2048], Float32, CPU>[@model.layers.1.post_attention_layernorm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=126), symbol:model.layers.1.post_attention_layernorm.weight])[symbol:model.layers.1.post_attention_layernorm.weight]
            tensor.CPU.register () -> (%6960:tensor<[6144, 2048], Float32, CPU>[@model.layers.1.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=127), symbol:model.layers.1.mlp.gate_proj.weight])[symbol:model.layers.1.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%7251:tensor<[6144, 2048], Float32, CPU>[@model.layers.1.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=130), symbol:model.layers.1.mlp.up_proj.weight])[symbol:model.layers.1.mlp.up_proj.weight]
            tensor.CPU.register () -> (%6256:tensor<[2048, 6144], Float32, CPU>[@model.layers.1.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=132), symbol:model.layers.1.mlp.down_proj.weight])[symbol:model.layers.1.mlp.down_proj.weight]
            tensor.CPU.register () -> (%7411:tensor<[2048], Float32, CPU>[@model.layers.2.input_layernorm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=135), symbol:model.layers.2.input_layernorm.weight])[symbol:model.layers.2.input_layernorm.weight]
            tensor.CPU.register () -> (%4879:tensor<[2048, 2048], Float32, CPU>[@model.layers.2.self_attn.q_proj.weight][symbol:model.layers.2.self_attn.q_proj.weight])[symbol:model.layers.2.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%725:tensor<[1024, 2048], Float32, CPU>[@model.layers.2.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=136), symbol:model.layers.2.self_attn.k_proj.weight])[symbol:model.layers.2.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%2701:tensor<[1024, 2048], Float32, CPU>[@model.layers.2.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=138), symbol:model.layers.2.self_attn.v_proj.weight])[symbol:model.layers.2.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%7660:tensor<[128], Float32, CPU>[@model.layers.2.self_attn.q_norm.weight][quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=142), symbol:model.layers.2.self_attn.q_norm.weight])[symbol:model.layers.2.self_attn.q_norm.weight]
            tensor.CPU.register () -> (%5749:tensor<[128], Float32, CPU>[@model.layers.2.self_attn.k_norm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=144), symbol:model.layers.2.self_attn.k_norm.weight])[symbol:model.layers.2.self_attn.k_norm.weight]
            tensor.CPU.register () -> (%1525:tensor<[2048, 2048], Float32, CPU>[@model.layers.2.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=157), symbol:model.layers.2.self_attn.o_proj.weight])[symbol:model.layers.2.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%6444:tensor<[2048], Float32, CPU>[@model.layers.2.post_attention_layernorm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=160), symbol:model.layers.2.post_attention_layernorm.weight])[symbol:model.layers.2.post_attention_layernorm.weight]
            tensor.CPU.register () -> (%3201:tensor<[6144, 2048], Float32, CPU>[@model.layers.2.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=161), symbol:model.layers.2.mlp.gate_proj.weight])[symbol:model.layers.2.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%4120:tensor<[6144, 2048], Float32, CPU>[@model.layers.2.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=164), symbol:model.layers.2.mlp.up_proj.weight])[symbol:model.layers.2.mlp.up_proj.weight]
            tensor.CPU.register () -> (%1962:tensor<[2048, 6144], Float32, CPU>[@model.layers.2.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=166), symbol:model.layers.2.mlp.down_proj.weight])[symbol:model.layers.2.mlp.down_proj.weight]
            tensor.CPU.register () -> (%3250:tensor<[2048], Float32, CPU>[@model.layers.3.input_layernorm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=169), symbol:model.layers.3.input_layernorm.weight])[symbol:model.layers.3.input_layernorm.weight]
            tensor.CPU.register () -> (%5564:tensor<[2048, 2048], Float32, CPU>[@model.layers.3.self_attn.q_proj.weight][symbol:model.layers.3.self_attn.q_proj.weight])[symbol:model.layers.3.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%3502:tensor<[1024, 2048], Float32, CPU>[@model.layers.3.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=170), symbol:model.layers.3.self_attn.k_proj.weight])[symbol:model.layers.3.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%2402:tensor<[1024, 2048], Float32, CPU>[@model.layers.3.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=172), symbol:model.layers.3.self_attn.v_proj.weight])[symbol:model.layers.3.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%1747:tensor<[128], Float32, CPU>[@model.layers.3.self_attn.q_norm.weight][quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=176), symbol:model.layers.3.self_attn.q_norm.weight])[symbol:model.layers.3.self_attn.q_norm.weight]
            tensor.CPU.register () -> (%4846:tensor<[128], Float32, CPU>[@model.layers.3.self_attn.k_norm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=178), symbol:model.layers.3.self_attn.k_norm.weight])[symbol:model.layers.3.self_attn.k_norm.weight]
            tensor.CPU.register () -> (%3109:tensor<[2048, 2048], Float32, CPU>[@model.layers.3.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=191), symbol:model.layers.3.self_attn.o_proj.weight])[symbol:model.layers.3.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%7221:tensor<[2048], Float32, CPU>[@model.layers.3.post_attention_layernorm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=194), symbol:model.layers.3.post_attention_layernorm.weight])[symbol:model.layers.3.post_attention_layernorm.weight]
            tensor.CPU.register () -> (%7181:tensor<[6144, 2048], Float32, CPU>[@model.layers.3.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=195), symbol:model.layers.3.mlp.gate_proj.weight])[symbol:model.layers.3.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%2714:tensor<[6144, 2048], Float32, CPU>[@model.layers.3.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=198), symbol:model.layers.3.mlp.up_proj.weight])[symbol:model.layers.3.mlp.up_proj.weight]
            tensor.CPU.register () -> (%4573:tensor<[2048, 6144], Float32, CPU>[@model.layers.3.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=200), symbol:model.layers.3.mlp.down_proj.weight])[symbol:model.layers.3.mlp.down_proj.weight]
            tensor.CPU.register () -> (%5536:tensor<[2048], Float32, CPU>[@model.layers.4.input_layernorm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=203), symbol:model.layers.4.input_layernorm.weight])[symbol:model.layers.4.input_layernorm.weight]
            tensor.CPU.register () -> (%463:tensor<[2048, 2048], Float32, CPU>[@model.layers.4.self_attn.q_proj.weight][symbol:model.layers.4.self_attn.q_proj.weight])[symbol:model.layers.4.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%5989:tensor<[1024, 2048], Float32, CPU>[@model.layers.4.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=204), symbol:model.layers.4.self_attn.k_proj.weight])[symbol:model.layers.4.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%3443:tensor<[1024, 2048], Float32, CPU>[@model.layers.4.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=206), symbol:model.layers.4.self_attn.v_proj.weight])[symbol:model.layers.4.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%926:tensor<[128], Float32, CPU>[@model.layers.4.self_attn.q_norm.weight][quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=210), symbol:model.layers.4.self_attn.q_norm.weight])[symbol:model.layers.4.self_attn.q_norm.weight]
            tensor.CPU.register () -> (%5648:tensor<[128], Float32, CPU>[@model.layers.4.self_attn.k_norm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=212), symbol:model.layers.4.self_attn.k_norm.weight])[symbol:model.layers.4.self_attn.k_norm.weight]
            tensor.CPU.register () -> (%256:tensor<[2048, 2048], Float32, CPU>[@model.layers.4.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=225), symbol:model.layers.4.self_attn.o_proj.weight])[symbol:model.layers.4.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%3101:tensor<[2048], Float32, CPU>[@model.layers.4.post_attention_layernorm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=228), symbol:model.layers.4.post_attention_layernorm.weight])[symbol:model.layers.4.post_attention_layernorm.weight]
            tensor.CPU.register () -> (%15:tensor<[6144, 2048], Float32, CPU>[@model.layers.4.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=229), symbol:model.layers.4.mlp.gate_proj.weight])[symbol:model.layers.4.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%3494:tensor<[6144, 2048], Float32, CPU>[@model.layers.4.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=232), symbol:model.layers.4.mlp.up_proj.weight])[symbol:model.layers.4.mlp.up_proj.weight]
            tensor.CPU.register () -> (%6518:tensor<[2048, 6144], Float32, CPU>[@model.layers.4.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=234), symbol:model.layers.4.mlp.down_proj.weight])[symbol:model.layers.4.mlp.down_proj.weight]
            tensor.CPU.register () -> (%7246:tensor<[2048], Float32, CPU>[@model.layers.5.input_layernorm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=237), symbol:model.layers.5.input_layernorm.weight])[symbol:model.layers.5.input_layernorm.weight]
            tensor.CPU.register () -> (%3752:tensor<[2048, 2048], Float32, CPU>[@model.layers.5.self_attn.q_proj.weight][symbol:model.layers.5.self_attn.q_proj.weight])[symbol:model.layers.5.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%2143:tensor<[1024, 2048], Float32, CPU>[@model.layers.5.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=238), symbol:model.layers.5.self_attn.k_proj.weight])[symbol:model.layers.5.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%5753:tensor<[1024, 2048], Float32, CPU>[@model.layers.5.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=240), symbol:model.layers.5.self_attn.v_proj.weight])[symbol:model.layers.5.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%4774:tensor<[128], Float32, CPU>[@model.layers.5.self_attn.q_norm.weight][quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=244), symbol:model.layers.5.self_attn.q_norm.weight])[symbol:model.layers.5.self_attn.q_norm.weight]
            tensor.CPU.register () -> (%1215:tensor<[128], Float32, CPU>[@model.layers.5.self_attn.k_norm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=246), symbol:model.layers.5.self_attn.k_norm.weight])[symbol:model.layers.5.self_attn.k_norm.weight]
            tensor.CPU.register () -> (%2076:tensor<[2048, 2048], Float32, CPU>[@model.layers.5.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=259), symbol:model.layers.5.self_attn.o_proj.weight])[symbol:model.layers.5.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%6883:tensor<[2048], Float32, CPU>[@model.layers.5.post_attention_layernorm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=262), symbol:model.layers.5.post_attention_layernorm.weight])[symbol:model.layers.5.post_attention_layernorm.weight]
            tensor.CPU.register () -> (%5485:tensor<[6144, 2048], Float32, CPU>[@model.layers.5.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=263), symbol:model.layers.5.mlp.gate_proj.weight])[symbol:model.layers.5.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%759:tensor<[6144, 2048], Float32, CPU>[@model.layers.5.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=266), symbol:model.layers.5.mlp.up_proj.weight])[symbol:model.layers.5.mlp.up_proj.weight]
            tensor.CPU.register () -> (%6315:tensor<[2048, 6144], Float32, CPU>[@model.layers.5.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=268), symbol:model.layers.5.mlp.down_proj.weight])[symbol:model.layers.5.mlp.down_proj.weight]
            tensor.CPU.register () -> (%7090:tensor<[2048], Float32, CPU>[@model.layers.6.input_layernorm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=271), symbol:model.layers.6.input_layernorm.weight])[symbol:model.layers.6.input_layernorm.weight]
            tensor.CPU.register () -> (%3125:tensor<[2048, 2048], Float32, CPU>[@model.layers.6.self_attn.q_proj.weight][symbol:model.layers.6.self_attn.q_proj.weight])[symbol:model.layers.6.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%1798:tensor<[1024, 2048], Float32, CPU>[@model.layers.6.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=272), symbol:model.layers.6.self_attn.k_proj.weight])[symbol:model.layers.6.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%1047:tensor<[1024, 2048], Float32, CPU>[@model.layers.6.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=274), symbol:model.layers.6.self_attn.v_proj.weight])[symbol:model.layers.6.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%7385:tensor<[128], Float32, CPU>[@model.layers.6.self_attn.q_norm.weight][quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=278), symbol:model.layers.6.self_attn.q_norm.weight])[symbol:model.layers.6.self_attn.q_norm.weight]
            tensor.CPU.register () -> (%5603:tensor<[128], Float32, CPU>[@model.layers.6.self_attn.k_norm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=280), symbol:model.layers.6.self_attn.k_norm.weight])[symbol:model.layers.6.self_attn.k_norm.weight]
            tensor.CPU.register () -> (%6862:tensor<[2048, 2048], Float32, CPU>[@model.layers.6.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=293), symbol:model.layers.6.self_attn.o_proj.weight])[symbol:model.layers.6.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%4161:tensor<[2048], Float32, CPU>[@model.layers.6.post_attention_layernorm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=296), symbol:model.layers.6.post_attention_layernorm.weight])[symbol:model.layers.6.post_attention_layernorm.weight]
            tensor.CPU.register () -> (%5295:tensor<[6144, 2048], Float32, CPU>[@model.layers.6.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=297), symbol:model.layers.6.mlp.gate_proj.weight])[symbol:model.layers.6.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%4710:tensor<[6144, 2048], Float32, CPU>[@model.layers.6.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=300), symbol:model.layers.6.mlp.up_proj.weight])[symbol:model.layers.6.mlp.up_proj.weight]
            tensor.CPU.register () -> (%4929:tensor<[2048, 6144], Float32, CPU>[@model.layers.6.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=302), symbol:model.layers.6.mlp.down_proj.weight])[symbol:model.layers.6.mlp.down_proj.weight]
            tensor.CPU.register () -> (%4605:tensor<[2048], Float32, CPU>[@model.layers.7.input_layernorm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=305), symbol:model.layers.7.input_layernorm.weight])[symbol:model.layers.7.input_layernorm.weight]
            tensor.CPU.register () -> (%4585:tensor<[2048, 2048], Float32, CPU>[@model.layers.7.self_attn.q_proj.weight][symbol:model.layers.7.self_attn.q_proj.weight])[symbol:model.layers.7.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%1:tensor<[1024, 2048], Float32, CPU>[@model.layers.7.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=306), symbol:model.layers.7.self_attn.k_proj.weight])[symbol:model.layers.7.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%2341:tensor<[1024, 2048], Float32, CPU>[@model.layers.7.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=308), symbol:model.layers.7.self_attn.v_proj.weight])[symbol:model.layers.7.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%5151:tensor<[128], Float32, CPU>[@model.layers.7.self_attn.q_norm.weight][quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=312), symbol:model.layers.7.self_attn.q_norm.weight])[symbol:model.layers.7.self_attn.q_norm.weight]
            tensor.CPU.register () -> (%3437:tensor<[128], Float32, CPU>[@model.layers.7.self_attn.k_norm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=314), symbol:model.layers.7.self_attn.k_norm.weight])[symbol:model.layers.7.self_attn.k_norm.weight]
            tensor.CPU.register () -> (%3368:tensor<[2048, 2048], Float32, CPU>[@model.layers.7.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=327), symbol:model.layers.7.self_attn.o_proj.weight])[symbol:model.layers.7.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%68:tensor<[2048], Float32, CPU>[@model.layers.7.post_attention_layernorm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=330), symbol:model.layers.7.post_attention_layernorm.weight])[symbol:model.layers.7.post_attention_layernorm.weight]
            tensor.CPU.register () -> (%324:tensor<[6144, 2048], Float32, CPU>[@model.layers.7.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=331), symbol:model.layers.7.mlp.gate_proj.weight])[symbol:model.layers.7.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%5551:tensor<[6144, 2048], Float32, CPU>[@model.layers.7.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=334), symbol:model.layers.7.mlp.up_proj.weight])[symbol:model.layers.7.mlp.up_proj.weight]
            tensor.CPU.register () -> (%7894:tensor<[2048, 6144], Float32, CPU>[@model.layers.7.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=336), symbol:model.layers.7.mlp.down_proj.weight])[symbol:model.layers.7.mlp.down_proj.weight]
            tensor.CPU.register () -> (%3851:tensor<[2048], Float32, CPU>[@model.layers.8.input_layernorm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=339), symbol:model.layers.8.input_layernorm.weight])[symbol:model.layers.8.input_layernorm.weight]
            tensor.CPU.register () -> (%5874:tensor<[2048, 2048], Float32, CPU>[@model.layers.8.self_attn.q_proj.weight][symbol:model.layers.8.self_attn.q_proj.weight])[symbol:model.layers.8.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%1863:tensor<[1024, 2048], Float32, CPU>[@model.layers.8.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=340), symbol:model.layers.8.self_attn.k_proj.weight])[symbol:model.layers.8.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%3204:tensor<[1024, 2048], Float32, CPU>[@model.layers.8.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=342), symbol:model.layers.8.self_attn.v_proj.weight])[symbol:model.layers.8.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%2301:tensor<[128], Float32, CPU>[@model.layers.8.self_attn.q_norm.weight][quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=346), symbol:model.layers.8.self_attn.q_norm.weight])[symbol:model.layers.8.self_attn.q_norm.weight]
            tensor.CPU.register () -> (%7373:tensor<[128], Float32, CPU>[@model.layers.8.self_attn.k_norm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=348), symbol:model.layers.8.self_attn.k_norm.weight])[symbol:model.layers.8.self_attn.k_norm.weight]
            tensor.CPU.register () -> (%6303:tensor<[2048, 2048], Float32, CPU>[@model.layers.8.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=361), symbol:model.layers.8.self_attn.o_proj.weight])[symbol:model.layers.8.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%1997:tensor<[2048], Float32, CPU>[@model.layers.8.post_attention_layernorm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=364), symbol:model.layers.8.post_attention_layernorm.weight])[symbol:model.layers.8.post_attention_layernorm.weight]
            tensor.CPU.register () -> (%6731:tensor<[6144, 2048], Float32, CPU>[@model.layers.8.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=365), symbol:model.layers.8.mlp.gate_proj.weight])[symbol:model.layers.8.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%5478:tensor<[6144, 2048], Float32, CPU>[@model.layers.8.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=368), symbol:model.layers.8.mlp.up_proj.weight])[symbol:model.layers.8.mlp.up_proj.weight]
            tensor.CPU.register () -> (%4734:tensor<[2048, 6144], Float32, CPU>[@model.layers.8.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=370), symbol:model.layers.8.mlp.down_proj.weight])[symbol:model.layers.8.mlp.down_proj.weight]
            tensor.CPU.register () -> (%4963:tensor<[2048], Float32, CPU>[@model.layers.9.input_layernorm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=373), symbol:model.layers.9.input_layernorm.weight])[symbol:model.layers.9.input_layernorm.weight]
            tensor.CPU.register () -> (%137:tensor<[2048, 2048], Float32, CPU>[@model.layers.9.self_attn.q_proj.weight][symbol:model.layers.9.self_attn.q_proj.weight])[symbol:model.layers.9.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%2689:tensor<[1024, 2048], Float32, CPU>[@model.layers.9.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=374), symbol:model.layers.9.self_attn.k_proj.weight])[symbol:model.layers.9.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%4027:tensor<[1024, 2048], Float32, CPU>[@model.layers.9.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=376), symbol:model.layers.9.self_attn.v_proj.weight])[symbol:model.layers.9.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%1375:tensor<[128], Float32, CPU>[@model.layers.9.self_attn.q_norm.weight][quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=380), symbol:model.layers.9.self_attn.q_norm.weight])[symbol:model.layers.9.self_attn.q_norm.weight]
            tensor.CPU.register () -> (%4962:tensor<[128], Float32, CPU>[@model.layers.9.self_attn.k_norm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=382), symbol:model.layers.9.self_attn.k_norm.weight])[symbol:model.layers.9.self_attn.k_norm.weight]
            tensor.CPU.register () -> (%6399:tensor<[2048, 2048], Float32, CPU>[@model.layers.9.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=395), symbol:model.layers.9.self_attn.o_proj.weight])[symbol:model.layers.9.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%2594:tensor<[2048], Float32, CPU>[@model.layers.9.post_attention_layernorm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=398), symbol:model.layers.9.post_attention_layernorm.weight])[symbol:model.layers.9.post_attention_layernorm.weight]
            tensor.CPU.register () -> (%3833:tensor<[6144, 2048], Float32, CPU>[@model.layers.9.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=399), symbol:model.layers.9.mlp.gate_proj.weight])[symbol:model.layers.9.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%2358:tensor<[6144, 2048], Float32, CPU>[@model.layers.9.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=402), symbol:model.layers.9.mlp.up_proj.weight])[symbol:model.layers.9.mlp.up_proj.weight]
            tensor.CPU.register () -> (%3947:tensor<[2048, 6144], Float32, CPU>[@model.layers.9.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=404), symbol:model.layers.9.mlp.down_proj.weight])[symbol:model.layers.9.mlp.down_proj.weight]
            tensor.CPU.register () -> (%3229:tensor<[2048], Float32, CPU>[@model.layers.10.input_layernorm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=407), symbol:model.layers.10.input_layernorm.weight])[symbol:model.layers.10.input_layernorm.weight]
            tensor.CPU.register () -> (%5022:tensor<[2048, 2048], Float32, CPU>[@model.layers.10.self_attn.q_proj.weight][symbol:model.layers.10.self_attn.q_proj.weight])[symbol:model.layers.10.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%2867:tensor<[1024, 2048], Float32, CPU>[@model.layers.10.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=408), symbol:model.layers.10.self_attn.k_proj.weight])[symbol:model.layers.10.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%567:tensor<[1024, 2048], Float32, CPU>[@model.layers.10.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=410), symbol:model.layers.10.self_attn.v_proj.weight])[symbol:model.layers.10.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%7008:tensor<[128], Float32, CPU>[@model.layers.10.self_attn.q_norm.weight][quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=414), symbol:model.layers.10.self_attn.q_norm.weight])[symbol:model.layers.10.self_attn.q_norm.weight]
            tensor.CPU.register () -> (%6953:tensor<[128], Float32, CPU>[@model.layers.10.self_attn.k_norm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=416), symbol:model.layers.10.self_attn.k_norm.weight])[symbol:model.layers.10.self_attn.k_norm.weight]
            tensor.CPU.register () -> (%5479:tensor<[2048, 2048], Float32, CPU>[@model.layers.10.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=429), symbol:model.layers.10.self_attn.o_proj.weight])[symbol:model.layers.10.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%3177:tensor<[2048], Float32, CPU>[@model.layers.10.post_attention_layernorm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=432), symbol:model.layers.10.post_attention_layernorm.weight])[symbol:model.layers.10.post_attention_layernorm.weight]
            tensor.CPU.register () -> (%7857:tensor<[6144, 2048], Float32, CPU>[@model.layers.10.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=433), symbol:model.layers.10.mlp.gate_proj.weight])[symbol:model.layers.10.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%3620:tensor<[6144, 2048], Float32, CPU>[@model.layers.10.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=436), symbol:model.layers.10.mlp.up_proj.weight])[symbol:model.layers.10.mlp.up_proj.weight]
            tensor.CPU.register () -> (%4172:tensor<[2048, 6144], Float32, CPU>[@model.layers.10.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=438), symbol:model.layers.10.mlp.down_proj.weight])[symbol:model.layers.10.mlp.down_proj.weight]
            tensor.CPU.register () -> (%1820:tensor<[2048], Float32, CPU>[@model.layers.11.input_layernorm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=441), symbol:model.layers.11.input_layernorm.weight])[symbol:model.layers.11.input_layernorm.weight]
            tensor.CPU.register () -> (%4375:tensor<[2048, 2048], Float32, CPU>[@model.layers.11.self_attn.q_proj.weight][symbol:model.layers.11.self_attn.q_proj.weight])[symbol:model.layers.11.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%3805:tensor<[1024, 2048], Float32, CPU>[@model.layers.11.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=442), symbol:model.layers.11.self_attn.k_proj.weight])[symbol:model.layers.11.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%5348:tensor<[1024, 2048], Float32, CPU>[@model.layers.11.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=444), symbol:model.layers.11.self_attn.v_proj.weight])[symbol:model.layers.11.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%1018:tensor<[128], Float32, CPU>[@model.layers.11.self_attn.q_norm.weight][quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=448), symbol:model.layers.11.self_attn.q_norm.weight])[symbol:model.layers.11.self_attn.q_norm.weight]
            tensor.CPU.register () -> (%5323:tensor<[128], Float32, CPU>[@model.layers.11.self_attn.k_norm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=450), symbol:model.layers.11.self_attn.k_norm.weight])[symbol:model.layers.11.self_attn.k_norm.weight]
            tensor.CPU.register () -> (%6587:tensor<[2048, 2048], Float32, CPU>[@model.layers.11.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=463), symbol:model.layers.11.self_attn.o_proj.weight])[symbol:model.layers.11.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%2072:tensor<[2048], Float32, CPU>[@model.layers.11.post_attention_layernorm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=466), symbol:model.layers.11.post_attention_layernorm.weight])[symbol:model.layers.11.post_attention_layernorm.weight]
            tensor.CPU.register () -> (%5180:tensor<[6144, 2048], Float32, CPU>[@model.layers.11.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=467), symbol:model.layers.11.mlp.gate_proj.weight])[symbol:model.layers.11.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%1917:tensor<[6144, 2048], Float32, CPU>[@model.layers.11.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=470), symbol:model.layers.11.mlp.up_proj.weight])[symbol:model.layers.11.mlp.up_proj.weight]
            tensor.CPU.register () -> (%2810:tensor<[2048, 6144], Float32, CPU>[@model.layers.11.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=472), symbol:model.layers.11.mlp.down_proj.weight])[symbol:model.layers.11.mlp.down_proj.weight]
            tensor.CPU.register () -> (%4945:tensor<[2048], Float32, CPU>[@model.layers.12.input_layernorm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=475), symbol:model.layers.12.input_layernorm.weight])[symbol:model.layers.12.input_layernorm.weight]
            tensor.CPU.register () -> (%6926:tensor<[2048, 2048], Float32, CPU>[@model.layers.12.self_attn.q_proj.weight][symbol:model.layers.12.self_attn.q_proj.weight])[symbol:model.layers.12.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%2741:tensor<[1024, 2048], Float32, CPU>[@model.layers.12.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=476), symbol:model.layers.12.self_attn.k_proj.weight])[symbol:model.layers.12.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%3690:tensor<[1024, 2048], Float32, CPU>[@model.layers.12.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=478), symbol:model.layers.12.self_attn.v_proj.weight])[symbol:model.layers.12.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%5447:tensor<[128], Float32, CPU>[@model.layers.12.self_attn.q_norm.weight][quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=482), symbol:model.layers.12.self_attn.q_norm.weight])[symbol:model.layers.12.self_attn.q_norm.weight]
            tensor.CPU.register () -> (%5437:tensor<[128], Float32, CPU>[@model.layers.12.self_attn.k_norm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=484), symbol:model.layers.12.self_attn.k_norm.weight])[symbol:model.layers.12.self_attn.k_norm.weight]
            tensor.CPU.register () -> (%4785:tensor<[2048, 2048], Float32, CPU>[@model.layers.12.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=497), symbol:model.layers.12.self_attn.o_proj.weight])[symbol:model.layers.12.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%1343:tensor<[2048], Float32, CPU>[@model.layers.12.post_attention_layernorm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=500), symbol:model.layers.12.post_attention_layernorm.weight])[symbol:model.layers.12.post_attention_layernorm.weight]
            tensor.CPU.register () -> (%3306:tensor<[6144, 2048], Float32, CPU>[@model.layers.12.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=501), symbol:model.layers.12.mlp.gate_proj.weight])[symbol:model.layers.12.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%2123:tensor<[6144, 2048], Float32, CPU>[@model.layers.12.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=504), symbol:model.layers.12.mlp.up_proj.weight])[symbol:model.layers.12.mlp.up_proj.weight]
            tensor.CPU.register () -> (%2005:tensor<[2048, 6144], Float32, CPU>[@model.layers.12.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=506), symbol:model.layers.12.mlp.down_proj.weight])[symbol:model.layers.12.mlp.down_proj.weight]
            tensor.CPU.register () -> (%1812:tensor<[2048], Float32, CPU>[@model.layers.13.input_layernorm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=509), symbol:model.layers.13.input_layernorm.weight])[symbol:model.layers.13.input_layernorm.weight]
            tensor.CPU.register () -> (%7043:tensor<[2048, 2048], Float32, CPU>[@model.layers.13.self_attn.q_proj.weight][symbol:model.layers.13.self_attn.q_proj.weight])[symbol:model.layers.13.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%229:tensor<[1024, 2048], Float32, CPU>[@model.layers.13.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=510), symbol:model.layers.13.self_attn.k_proj.weight])[symbol:model.layers.13.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%1019:tensor<[1024, 2048], Float32, CPU>[@model.layers.13.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=512), symbol:model.layers.13.self_attn.v_proj.weight])[symbol:model.layers.13.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%3318:tensor<[128], Float32, CPU>[@model.layers.13.self_attn.q_norm.weight][quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=516), symbol:model.layers.13.self_attn.q_norm.weight])[symbol:model.layers.13.self_attn.q_norm.weight]
            tensor.CPU.register () -> (%2503:tensor<[128], Float32, CPU>[@model.layers.13.self_attn.k_norm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=518), symbol:model.layers.13.self_attn.k_norm.weight])[symbol:model.layers.13.self_attn.k_norm.weight]
            tensor.CPU.register () -> (%3883:tensor<[2048, 2048], Float32, CPU>[@model.layers.13.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=531), symbol:model.layers.13.self_attn.o_proj.weight])[symbol:model.layers.13.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%6904:tensor<[2048], Float32, CPU>[@model.layers.13.post_attention_layernorm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=534), symbol:model.layers.13.post_attention_layernorm.weight])[symbol:model.layers.13.post_attention_layernorm.weight]
            tensor.CPU.register () -> (%5444:tensor<[6144, 2048], Float32, CPU>[@model.layers.13.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=535), symbol:model.layers.13.mlp.gate_proj.weight])[symbol:model.layers.13.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%3100:tensor<[6144, 2048], Float32, CPU>[@model.layers.13.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=538), symbol:model.layers.13.mlp.up_proj.weight])[symbol:model.layers.13.mlp.up_proj.weight]
            tensor.CPU.register () -> (%6631:tensor<[2048, 6144], Float32, CPU>[@model.layers.13.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=540), symbol:model.layers.13.mlp.down_proj.weight])[symbol:model.layers.13.mlp.down_proj.weight]
            tensor.CPU.register () -> (%5555:tensor<[2048], Float32, CPU>[@model.layers.14.input_layernorm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=543), symbol:model.layers.14.input_layernorm.weight])[symbol:model.layers.14.input_layernorm.weight]
            tensor.CPU.register () -> (%1210:tensor<[2048, 2048], Float32, CPU>[@model.layers.14.self_attn.q_proj.weight][symbol:model.layers.14.self_attn.q_proj.weight])[symbol:model.layers.14.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%3756:tensor<[1024, 2048], Float32, CPU>[@model.layers.14.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=544), symbol:model.layers.14.self_attn.k_proj.weight])[symbol:model.layers.14.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%5243:tensor<[1024, 2048], Float32, CPU>[@model.layers.14.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=546), symbol:model.layers.14.self_attn.v_proj.weight])[symbol:model.layers.14.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%3796:tensor<[128], Float32, CPU>[@model.layers.14.self_attn.q_norm.weight][quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=550), symbol:model.layers.14.self_attn.q_norm.weight])[symbol:model.layers.14.self_attn.q_norm.weight]
            tensor.CPU.register () -> (%3974:tensor<[128], Float32, CPU>[@model.layers.14.self_attn.k_norm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=552), symbol:model.layers.14.self_attn.k_norm.weight])[symbol:model.layers.14.self_attn.k_norm.weight]
            tensor.CPU.register () -> (%3797:tensor<[2048, 2048], Float32, CPU>[@model.layers.14.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=565), symbol:model.layers.14.self_attn.o_proj.weight])[symbol:model.layers.14.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%4508:tensor<[2048], Float32, CPU>[@model.layers.14.post_attention_layernorm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=568), symbol:model.layers.14.post_attention_layernorm.weight])[symbol:model.layers.14.post_attention_layernorm.weight]
            tensor.CPU.register () -> (%7092:tensor<[6144, 2048], Float32, CPU>[@model.layers.14.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=569), symbol:model.layers.14.mlp.gate_proj.weight])[symbol:model.layers.14.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%7164:tensor<[6144, 2048], Float32, CPU>[@model.layers.14.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=572), symbol:model.layers.14.mlp.up_proj.weight])[symbol:model.layers.14.mlp.up_proj.weight]
            tensor.CPU.register () -> (%4419:tensor<[2048, 6144], Float32, CPU>[@model.layers.14.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=574), symbol:model.layers.14.mlp.down_proj.weight])[symbol:model.layers.14.mlp.down_proj.weight]
            tensor.CPU.register () -> (%5590:tensor<[2048], Float32, CPU>[@model.layers.15.input_layernorm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=577), symbol:model.layers.15.input_layernorm.weight])[symbol:model.layers.15.input_layernorm.weight]
            tensor.CPU.register () -> (%5843:tensor<[2048, 2048], Float32, CPU>[@model.layers.15.self_attn.q_proj.weight][symbol:model.layers.15.self_attn.q_proj.weight])[symbol:model.layers.15.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%938:tensor<[1024, 2048], Float32, CPU>[@model.layers.15.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=578), symbol:model.layers.15.self_attn.k_proj.weight])[symbol:model.layers.15.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%3967:tensor<[1024, 2048], Float32, CPU>[@model.layers.15.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=580), symbol:model.layers.15.self_attn.v_proj.weight])[symbol:model.layers.15.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%3289:tensor<[128], Float32, CPU>[@model.layers.15.self_attn.q_norm.weight][quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=584), symbol:model.layers.15.self_attn.q_norm.weight])[symbol:model.layers.15.self_attn.q_norm.weight]
            tensor.CPU.register () -> (%6756:tensor<[128], Float32, CPU>[@model.layers.15.self_attn.k_norm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=586), symbol:model.layers.15.self_attn.k_norm.weight])[symbol:model.layers.15.self_attn.k_norm.weight]
            tensor.CPU.register () -> (%4838:tensor<[2048, 2048], Float32, CPU>[@model.layers.15.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=599), symbol:model.layers.15.self_attn.o_proj.weight])[symbol:model.layers.15.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%6774:tensor<[2048], Float32, CPU>[@model.layers.15.post_attention_layernorm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=602), symbol:model.layers.15.post_attention_layernorm.weight])[symbol:model.layers.15.post_attention_layernorm.weight]
            tensor.CPU.register () -> (%2819:tensor<[6144, 2048], Float32, CPU>[@model.layers.15.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=603), symbol:model.layers.15.mlp.gate_proj.weight])[symbol:model.layers.15.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%1377:tensor<[6144, 2048], Float32, CPU>[@model.layers.15.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=606), symbol:model.layers.15.mlp.up_proj.weight])[symbol:model.layers.15.mlp.up_proj.weight]
            tensor.CPU.register () -> (%526:tensor<[2048, 6144], Float32, CPU>[@model.layers.15.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=608), symbol:model.layers.15.mlp.down_proj.weight])[symbol:model.layers.15.mlp.down_proj.weight]
            tensor.CPU.register () -> (%369:tensor<[2048], Float32, CPU>[@model.layers.16.input_layernorm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=611), symbol:model.layers.16.input_layernorm.weight])[symbol:model.layers.16.input_layernorm.weight]
            tensor.CPU.register () -> (%2345:tensor<[2048, 2048], Float32, CPU>[@model.layers.16.self_attn.q_proj.weight][symbol:model.layers.16.self_attn.q_proj.weight])[symbol:model.layers.16.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%3022:tensor<[1024, 2048], Float32, CPU>[@model.layers.16.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=612), symbol:model.layers.16.self_attn.k_proj.weight])[symbol:model.layers.16.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%2931:tensor<[1024, 2048], Float32, CPU>[@model.layers.16.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=614), symbol:model.layers.16.self_attn.v_proj.weight])[symbol:model.layers.16.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%1150:tensor<[128], Float32, CPU>[@model.layers.16.self_attn.q_norm.weight][quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=618), symbol:model.layers.16.self_attn.q_norm.weight])[symbol:model.layers.16.self_attn.q_norm.weight]
            tensor.CPU.register () -> (%5521:tensor<[128], Float32, CPU>[@model.layers.16.self_attn.k_norm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=620), symbol:model.layers.16.self_attn.k_norm.weight])[symbol:model.layers.16.self_attn.k_norm.weight]
            tensor.CPU.register () -> (%672:tensor<[2048, 2048], Float32, CPU>[@model.layers.16.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=633), symbol:model.layers.16.self_attn.o_proj.weight])[symbol:model.layers.16.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%6793:tensor<[2048], Float32, CPU>[@model.layers.16.post_attention_layernorm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=636), symbol:model.layers.16.post_attention_layernorm.weight])[symbol:model.layers.16.post_attention_layernorm.weight]
            tensor.CPU.register () -> (%993:tensor<[6144, 2048], Float32, CPU>[@model.layers.16.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=637), symbol:model.layers.16.mlp.gate_proj.weight])[symbol:model.layers.16.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%226:tensor<[6144, 2048], Float32, CPU>[@model.layers.16.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=640), symbol:model.layers.16.mlp.up_proj.weight])[symbol:model.layers.16.mlp.up_proj.weight]
            tensor.CPU.register () -> (%7287:tensor<[2048, 6144], Float32, CPU>[@model.layers.16.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=642), symbol:model.layers.16.mlp.down_proj.weight])[symbol:model.layers.16.mlp.down_proj.weight]
            tensor.CPU.register () -> (%7811:tensor<[2048], Float32, CPU>[@model.layers.17.input_layernorm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=645), symbol:model.layers.17.input_layernorm.weight])[symbol:model.layers.17.input_layernorm.weight]
            tensor.CPU.register () -> (%5758:tensor<[2048, 2048], Float32, CPU>[@model.layers.17.self_attn.q_proj.weight][symbol:model.layers.17.self_attn.q_proj.weight])[symbol:model.layers.17.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%2828:tensor<[1024, 2048], Float32, CPU>[@model.layers.17.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=646), symbol:model.layers.17.self_attn.k_proj.weight])[symbol:model.layers.17.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%417:tensor<[1024, 2048], Float32, CPU>[@model.layers.17.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=648), symbol:model.layers.17.self_attn.v_proj.weight])[symbol:model.layers.17.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%59:tensor<[128], Float32, CPU>[@model.layers.17.self_attn.q_norm.weight][quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=652), symbol:model.layers.17.self_attn.q_norm.weight])[symbol:model.layers.17.self_attn.q_norm.weight]
            tensor.CPU.register () -> (%7588:tensor<[128], Float32, CPU>[@model.layers.17.self_attn.k_norm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=654), symbol:model.layers.17.self_attn.k_norm.weight])[symbol:model.layers.17.self_attn.k_norm.weight]
            tensor.CPU.register () -> (%5285:tensor<[2048, 2048], Float32, CPU>[@model.layers.17.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=667), symbol:model.layers.17.self_attn.o_proj.weight])[symbol:model.layers.17.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%3787:tensor<[2048], Float32, CPU>[@model.layers.17.post_attention_layernorm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=670), symbol:model.layers.17.post_attention_layernorm.weight])[symbol:model.layers.17.post_attention_layernorm.weight]
            tensor.CPU.register () -> (%4841:tensor<[6144, 2048], Float32, CPU>[@model.layers.17.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=671), symbol:model.layers.17.mlp.gate_proj.weight])[symbol:model.layers.17.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%4784:tensor<[6144, 2048], Float32, CPU>[@model.layers.17.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=674), symbol:model.layers.17.mlp.up_proj.weight])[symbol:model.layers.17.mlp.up_proj.weight]
            tensor.CPU.register () -> (%1908:tensor<[2048, 6144], Float32, CPU>[@model.layers.17.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=676), symbol:model.layers.17.mlp.down_proj.weight])[symbol:model.layers.17.mlp.down_proj.weight]
            tensor.CPU.register () -> (%310:tensor<[2048], Float32, CPU>[@model.layers.18.input_layernorm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=679), symbol:model.layers.18.input_layernorm.weight])[symbol:model.layers.18.input_layernorm.weight]
            tensor.CPU.register () -> (%7352:tensor<[2048, 2048], Float32, CPU>[@model.layers.18.self_attn.q_proj.weight][symbol:model.layers.18.self_attn.q_proj.weight])[symbol:model.layers.18.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%6436:tensor<[1024, 2048], Float32, CPU>[@model.layers.18.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=680), symbol:model.layers.18.self_attn.k_proj.weight])[symbol:model.layers.18.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%6164:tensor<[1024, 2048], Float32, CPU>[@model.layers.18.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=682), symbol:model.layers.18.self_attn.v_proj.weight])[symbol:model.layers.18.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%2747:tensor<[128], Float32, CPU>[@model.layers.18.self_attn.q_norm.weight][quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=686), symbol:model.layers.18.self_attn.q_norm.weight])[symbol:model.layers.18.self_attn.q_norm.weight]
            tensor.CPU.register () -> (%5281:tensor<[128], Float32, CPU>[@model.layers.18.self_attn.k_norm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=688), symbol:model.layers.18.self_attn.k_norm.weight])[symbol:model.layers.18.self_attn.k_norm.weight]
            tensor.CPU.register () -> (%7646:tensor<[2048, 2048], Float32, CPU>[@model.layers.18.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=701), symbol:model.layers.18.self_attn.o_proj.weight])[symbol:model.layers.18.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%2540:tensor<[2048], Float32, CPU>[@model.layers.18.post_attention_layernorm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=704), symbol:model.layers.18.post_attention_layernorm.weight])[symbol:model.layers.18.post_attention_layernorm.weight]
            tensor.CPU.register () -> (%6101:tensor<[6144, 2048], Float32, CPU>[@model.layers.18.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=705), symbol:model.layers.18.mlp.gate_proj.weight])[symbol:model.layers.18.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%2195:tensor<[6144, 2048], Float32, CPU>[@model.layers.18.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=708), symbol:model.layers.18.mlp.up_proj.weight])[symbol:model.layers.18.mlp.up_proj.weight]
            tensor.CPU.register () -> (%3651:tensor<[2048, 6144], Float32, CPU>[@model.layers.18.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=710), symbol:model.layers.18.mlp.down_proj.weight])[symbol:model.layers.18.mlp.down_proj.weight]
            tensor.CPU.register () -> (%3722:tensor<[2048], Float32, CPU>[@model.layers.19.input_layernorm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=713), symbol:model.layers.19.input_layernorm.weight])[symbol:model.layers.19.input_layernorm.weight]
            tensor.CPU.register () -> (%1141:tensor<[2048, 2048], Float32, CPU>[@model.layers.19.self_attn.q_proj.weight][symbol:model.layers.19.self_attn.q_proj.weight])[symbol:model.layers.19.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%651:tensor<[1024, 2048], Float32, CPU>[@model.layers.19.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=714), symbol:model.layers.19.self_attn.k_proj.weight])[symbol:model.layers.19.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%254:tensor<[1024, 2048], Float32, CPU>[@model.layers.19.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=716), symbol:model.layers.19.self_attn.v_proj.weight])[symbol:model.layers.19.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%610:tensor<[128], Float32, CPU>[@model.layers.19.self_attn.q_norm.weight][quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=720), symbol:model.layers.19.self_attn.q_norm.weight])[symbol:model.layers.19.self_attn.q_norm.weight]
            tensor.CPU.register () -> (%3691:tensor<[128], Float32, CPU>[@model.layers.19.self_attn.k_norm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=722), symbol:model.layers.19.self_attn.k_norm.weight])[symbol:model.layers.19.self_attn.k_norm.weight]
            tensor.CPU.register () -> (%7002:tensor<[2048, 2048], Float32, CPU>[@model.layers.19.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=735), symbol:model.layers.19.self_attn.o_proj.weight])[symbol:model.layers.19.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%3446:tensor<[2048], Float32, CPU>[@model.layers.19.post_attention_layernorm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=738), symbol:model.layers.19.post_attention_layernorm.weight])[symbol:model.layers.19.post_attention_layernorm.weight]
            tensor.CPU.register () -> (%2118:tensor<[6144, 2048], Float32, CPU>[@model.layers.19.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=739), symbol:model.layers.19.mlp.gate_proj.weight])[symbol:model.layers.19.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%283:tensor<[6144, 2048], Float32, CPU>[@model.layers.19.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=742), symbol:model.layers.19.mlp.up_proj.weight])[symbol:model.layers.19.mlp.up_proj.weight]
            tensor.CPU.register () -> (%1264:tensor<[2048, 6144], Float32, CPU>[@model.layers.19.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=744), symbol:model.layers.19.mlp.down_proj.weight])[symbol:model.layers.19.mlp.down_proj.weight]
            tensor.CPU.register () -> (%5183:tensor<[2048], Float32, CPU>[@model.layers.20.input_layernorm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=747), symbol:model.layers.20.input_layernorm.weight])[symbol:model.layers.20.input_layernorm.weight]
            tensor.CPU.register () -> (%6004:tensor<[2048, 2048], Float32, CPU>[@model.layers.20.self_attn.q_proj.weight][symbol:model.layers.20.self_attn.q_proj.weight])[symbol:model.layers.20.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%4764:tensor<[1024, 2048], Float32, CPU>[@model.layers.20.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=748), symbol:model.layers.20.self_attn.k_proj.weight])[symbol:model.layers.20.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%3516:tensor<[1024, 2048], Float32, CPU>[@model.layers.20.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=750), symbol:model.layers.20.self_attn.v_proj.weight])[symbol:model.layers.20.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%2042:tensor<[128], Float32, CPU>[@model.layers.20.self_attn.q_norm.weight][quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=754), symbol:model.layers.20.self_attn.q_norm.weight])[symbol:model.layers.20.self_attn.q_norm.weight]
            tensor.CPU.register () -> (%1646:tensor<[128], Float32, CPU>[@model.layers.20.self_attn.k_norm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=756), symbol:model.layers.20.self_attn.k_norm.weight])[symbol:model.layers.20.self_attn.k_norm.weight]
            tensor.CPU.register () -> (%3587:tensor<[2048, 2048], Float32, CPU>[@model.layers.20.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=769), symbol:model.layers.20.self_attn.o_proj.weight])[symbol:model.layers.20.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%2726:tensor<[2048], Float32, CPU>[@model.layers.20.post_attention_layernorm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=772), symbol:model.layers.20.post_attention_layernorm.weight])[symbol:model.layers.20.post_attention_layernorm.weight]
            tensor.CPU.register () -> (%3656:tensor<[6144, 2048], Float32, CPU>[@model.layers.20.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=773), symbol:model.layers.20.mlp.gate_proj.weight])[symbol:model.layers.20.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%802:tensor<[6144, 2048], Float32, CPU>[@model.layers.20.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=776), symbol:model.layers.20.mlp.up_proj.weight])[symbol:model.layers.20.mlp.up_proj.weight]
            tensor.CPU.register () -> (%62:tensor<[2048, 6144], Float32, CPU>[@model.layers.20.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=778), symbol:model.layers.20.mlp.down_proj.weight])[symbol:model.layers.20.mlp.down_proj.weight]
            tensor.CPU.register () -> (%1237:tensor<[2048], Float32, CPU>[@model.layers.21.input_layernorm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=781), symbol:model.layers.21.input_layernorm.weight])[symbol:model.layers.21.input_layernorm.weight]
            tensor.CPU.register () -> (%2397:tensor<[2048, 2048], Float32, CPU>[@model.layers.21.self_attn.q_proj.weight][symbol:model.layers.21.self_attn.q_proj.weight])[symbol:model.layers.21.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%7562:tensor<[1024, 2048], Float32, CPU>[@model.layers.21.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=782), symbol:model.layers.21.self_attn.k_proj.weight])[symbol:model.layers.21.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%4665:tensor<[1024, 2048], Float32, CPU>[@model.layers.21.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=784), symbol:model.layers.21.self_attn.v_proj.weight])[symbol:model.layers.21.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%6195:tensor<[128], Float32, CPU>[@model.layers.21.self_attn.q_norm.weight][quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=788), symbol:model.layers.21.self_attn.q_norm.weight])[symbol:model.layers.21.self_attn.q_norm.weight]
            tensor.CPU.register () -> (%701:tensor<[128], Float32, CPU>[@model.layers.21.self_attn.k_norm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=790), symbol:model.layers.21.self_attn.k_norm.weight])[symbol:model.layers.21.self_attn.k_norm.weight]
            tensor.CPU.register () -> (%5913:tensor<[2048, 2048], Float32, CPU>[@model.layers.21.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=803), symbol:model.layers.21.self_attn.o_proj.weight])[symbol:model.layers.21.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%4765:tensor<[2048], Float32, CPU>[@model.layers.21.post_attention_layernorm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=806), symbol:model.layers.21.post_attention_layernorm.weight])[symbol:model.layers.21.post_attention_layernorm.weight]
            tensor.CPU.register () -> (%864:tensor<[6144, 2048], Float32, CPU>[@model.layers.21.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=807), symbol:model.layers.21.mlp.gate_proj.weight])[symbol:model.layers.21.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%923:tensor<[6144, 2048], Float32, CPU>[@model.layers.21.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=810), symbol:model.layers.21.mlp.up_proj.weight])[symbol:model.layers.21.mlp.up_proj.weight]
            tensor.CPU.register () -> (%6934:tensor<[2048, 6144], Float32, CPU>[@model.layers.21.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=812), symbol:model.layers.21.mlp.down_proj.weight])[symbol:model.layers.21.mlp.down_proj.weight]
            tensor.CPU.register () -> (%425:tensor<[2048], Float32, CPU>[@model.layers.22.input_layernorm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=815), symbol:model.layers.22.input_layernorm.weight])[symbol:model.layers.22.input_layernorm.weight]
            tensor.CPU.register () -> (%1036:tensor<[2048, 2048], Float32, CPU>[@model.layers.22.self_attn.q_proj.weight][symbol:model.layers.22.self_attn.q_proj.weight])[symbol:model.layers.22.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%6990:tensor<[1024, 2048], Float32, CPU>[@model.layers.22.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=816), symbol:model.layers.22.self_attn.k_proj.weight])[symbol:model.layers.22.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%2703:tensor<[1024, 2048], Float32, CPU>[@model.layers.22.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=818), symbol:model.layers.22.self_attn.v_proj.weight])[symbol:model.layers.22.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%1995:tensor<[128], Float32, CPU>[@model.layers.22.self_attn.q_norm.weight][quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=822), symbol:model.layers.22.self_attn.q_norm.weight])[symbol:model.layers.22.self_attn.q_norm.weight]
            tensor.CPU.register () -> (%2702:tensor<[128], Float32, CPU>[@model.layers.22.self_attn.k_norm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=824), symbol:model.layers.22.self_attn.k_norm.weight])[symbol:model.layers.22.self_attn.k_norm.weight]
            tensor.CPU.register () -> (%2221:tensor<[2048, 2048], Float32, CPU>[@model.layers.22.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=837), symbol:model.layers.22.self_attn.o_proj.weight])[symbol:model.layers.22.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%5286:tensor<[2048], Float32, CPU>[@model.layers.22.post_attention_layernorm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=840), symbol:model.layers.22.post_attention_layernorm.weight])[symbol:model.layers.22.post_attention_layernorm.weight]
            tensor.CPU.register () -> (%7377:tensor<[6144, 2048], Float32, CPU>[@model.layers.22.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=841), symbol:model.layers.22.mlp.gate_proj.weight])[symbol:model.layers.22.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%694:tensor<[6144, 2048], Float32, CPU>[@model.layers.22.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=844), symbol:model.layers.22.mlp.up_proj.weight])[symbol:model.layers.22.mlp.up_proj.weight]
            tensor.CPU.register () -> (%1401:tensor<[2048, 6144], Float32, CPU>[@model.layers.22.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=846), symbol:model.layers.22.mlp.down_proj.weight])[symbol:model.layers.22.mlp.down_proj.weight]
            tensor.CPU.register () -> (%809:tensor<[2048], Float32, CPU>[@model.layers.23.input_layernorm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=849), symbol:model.layers.23.input_layernorm.weight])[symbol:model.layers.23.input_layernorm.weight]
            tensor.CPU.register () -> (%2936:tensor<[2048, 2048], Float32, CPU>[@model.layers.23.self_attn.q_proj.weight][symbol:model.layers.23.self_attn.q_proj.weight])[symbol:model.layers.23.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%577:tensor<[1024, 2048], Float32, CPU>[@model.layers.23.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=850), symbol:model.layers.23.self_attn.k_proj.weight])[symbol:model.layers.23.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%5308:tensor<[1024, 2048], Float32, CPU>[@model.layers.23.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=852), symbol:model.layers.23.self_attn.v_proj.weight])[symbol:model.layers.23.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%5454:tensor<[128], Float32, CPU>[@model.layers.23.self_attn.q_norm.weight][quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=856), symbol:model.layers.23.self_attn.q_norm.weight])[symbol:model.layers.23.self_attn.q_norm.weight]
            tensor.CPU.register () -> (%1089:tensor<[128], Float32, CPU>[@model.layers.23.self_attn.k_norm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=858), symbol:model.layers.23.self_attn.k_norm.weight])[symbol:model.layers.23.self_attn.k_norm.weight]
            tensor.CPU.register () -> (%4076:tensor<[2048, 2048], Float32, CPU>[@model.layers.23.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=871), symbol:model.layers.23.self_attn.o_proj.weight])[symbol:model.layers.23.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%4535:tensor<[2048], Float32, CPU>[@model.layers.23.post_attention_layernorm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=874), symbol:model.layers.23.post_attention_layernorm.weight])[symbol:model.layers.23.post_attention_layernorm.weight]
            tensor.CPU.register () -> (%7750:tensor<[6144, 2048], Float32, CPU>[@model.layers.23.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=875), symbol:model.layers.23.mlp.gate_proj.weight])[symbol:model.layers.23.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%4744:tensor<[6144, 2048], Float32, CPU>[@model.layers.23.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=878), symbol:model.layers.23.mlp.up_proj.weight])[symbol:model.layers.23.mlp.up_proj.weight]
            tensor.CPU.register () -> (%2933:tensor<[2048, 6144], Float32, CPU>[@model.layers.23.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=880), symbol:model.layers.23.mlp.down_proj.weight])[symbol:model.layers.23.mlp.down_proj.weight]
            tensor.CPU.register () -> (%1154:tensor<[2048], Float32, CPU>[@model.layers.24.input_layernorm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=883), symbol:model.layers.24.input_layernorm.weight])[symbol:model.layers.24.input_layernorm.weight]
            tensor.CPU.register () -> (%2384:tensor<[2048, 2048], Float32, CPU>[@model.layers.24.self_attn.q_proj.weight][symbol:model.layers.24.self_attn.q_proj.weight])[symbol:model.layers.24.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%2620:tensor<[1024, 2048], Float32, CPU>[@model.layers.24.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=884), symbol:model.layers.24.self_attn.k_proj.weight])[symbol:model.layers.24.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%3265:tensor<[1024, 2048], Float32, CPU>[@model.layers.24.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=886), symbol:model.layers.24.self_attn.v_proj.weight])[symbol:model.layers.24.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%2985:tensor<[128], Float32, CPU>[@model.layers.24.self_attn.q_norm.weight][quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=890), symbol:model.layers.24.self_attn.q_norm.weight])[symbol:model.layers.24.self_attn.q_norm.weight]
            tensor.CPU.register () -> (%3894:tensor<[128], Float32, CPU>[@model.layers.24.self_attn.k_norm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=892), symbol:model.layers.24.self_attn.k_norm.weight])[symbol:model.layers.24.self_attn.k_norm.weight]
            tensor.CPU.register () -> (%7488:tensor<[2048, 2048], Float32, CPU>[@model.layers.24.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=905), symbol:model.layers.24.self_attn.o_proj.weight])[symbol:model.layers.24.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%6713:tensor<[2048], Float32, CPU>[@model.layers.24.post_attention_layernorm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=908), symbol:model.layers.24.post_attention_layernorm.weight])[symbol:model.layers.24.post_attention_layernorm.weight]
            tensor.CPU.register () -> (%1336:tensor<[6144, 2048], Float32, CPU>[@model.layers.24.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=909), symbol:model.layers.24.mlp.gate_proj.weight])[symbol:model.layers.24.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%7035:tensor<[6144, 2048], Float32, CPU>[@model.layers.24.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=912), symbol:model.layers.24.mlp.up_proj.weight])[symbol:model.layers.24.mlp.up_proj.weight]
            tensor.CPU.register () -> (%7069:tensor<[2048, 6144], Float32, CPU>[@model.layers.24.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=914), symbol:model.layers.24.mlp.down_proj.weight])[symbol:model.layers.24.mlp.down_proj.weight]
            tensor.CPU.register () -> (%6496:tensor<[2048], Float32, CPU>[@model.layers.25.input_layernorm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=917), symbol:model.layers.25.input_layernorm.weight])[symbol:model.layers.25.input_layernorm.weight]
            tensor.CPU.register () -> (%1852:tensor<[2048, 2048], Float32, CPU>[@model.layers.25.self_attn.q_proj.weight][symbol:model.layers.25.self_attn.q_proj.weight])[symbol:model.layers.25.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%3615:tensor<[1024, 2048], Float32, CPU>[@model.layers.25.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=918), symbol:model.layers.25.self_attn.k_proj.weight])[symbol:model.layers.25.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%2014:tensor<[1024, 2048], Float32, CPU>[@model.layers.25.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=920), symbol:model.layers.25.self_attn.v_proj.weight])[symbol:model.layers.25.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%2021:tensor<[128], Float32, CPU>[@model.layers.25.self_attn.q_norm.weight][quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=924), symbol:model.layers.25.self_attn.q_norm.weight])[symbol:model.layers.25.self_attn.q_norm.weight]
            tensor.CPU.register () -> (%1413:tensor<[128], Float32, CPU>[@model.layers.25.self_attn.k_norm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=926), symbol:model.layers.25.self_attn.k_norm.weight])[symbol:model.layers.25.self_attn.k_norm.weight]
            tensor.CPU.register () -> (%7074:tensor<[2048, 2048], Float32, CPU>[@model.layers.25.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=939), symbol:model.layers.25.self_attn.o_proj.weight])[symbol:model.layers.25.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%6424:tensor<[2048], Float32, CPU>[@model.layers.25.post_attention_layernorm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=942), symbol:model.layers.25.post_attention_layernorm.weight])[symbol:model.layers.25.post_attention_layernorm.weight]
            tensor.CPU.register () -> (%1860:tensor<[6144, 2048], Float32, CPU>[@model.layers.25.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=943), symbol:model.layers.25.mlp.gate_proj.weight])[symbol:model.layers.25.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%5840:tensor<[6144, 2048], Float32, CPU>[@model.layers.25.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=946), symbol:model.layers.25.mlp.up_proj.weight])[symbol:model.layers.25.mlp.up_proj.weight]
            tensor.CPU.register () -> (%6869:tensor<[2048, 6144], Float32, CPU>[@model.layers.25.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=948), symbol:model.layers.25.mlp.down_proj.weight])[symbol:model.layers.25.mlp.down_proj.weight]
            tensor.CPU.register () -> (%611:tensor<[2048], Float32, CPU>[@model.layers.26.input_layernorm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=951), symbol:model.layers.26.input_layernorm.weight])[symbol:model.layers.26.input_layernorm.weight]
            tensor.CPU.register () -> (%1040:tensor<[2048, 2048], Float32, CPU>[@model.layers.26.self_attn.q_proj.weight][symbol:model.layers.26.self_attn.q_proj.weight])[symbol:model.layers.26.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%2312:tensor<[1024, 2048], Float32, CPU>[@model.layers.26.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=952), symbol:model.layers.26.self_attn.k_proj.weight])[symbol:model.layers.26.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%174:tensor<[1024, 2048], Float32, CPU>[@model.layers.26.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=954), symbol:model.layers.26.self_attn.v_proj.weight])[symbol:model.layers.26.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%2799:tensor<[128], Float32, CPU>[@model.layers.26.self_attn.q_norm.weight][quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=958), symbol:model.layers.26.self_attn.q_norm.weight])[symbol:model.layers.26.self_attn.q_norm.weight]
            tensor.CPU.register () -> (%6479:tensor<[128], Float32, CPU>[@model.layers.26.self_attn.k_norm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=960), symbol:model.layers.26.self_attn.k_norm.weight])[symbol:model.layers.26.self_attn.k_norm.weight]
            tensor.CPU.register () -> (%504:tensor<[2048, 2048], Float32, CPU>[@model.layers.26.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=973), symbol:model.layers.26.self_attn.o_proj.weight])[symbol:model.layers.26.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%5096:tensor<[2048], Float32, CPU>[@model.layers.26.post_attention_layernorm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=976), symbol:model.layers.26.post_attention_layernorm.weight])[symbol:model.layers.26.post_attention_layernorm.weight]
            tensor.CPU.register () -> (%4867:tensor<[6144, 2048], Float32, CPU>[@model.layers.26.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=977), symbol:model.layers.26.mlp.gate_proj.weight])[symbol:model.layers.26.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%2619:tensor<[6144, 2048], Float32, CPU>[@model.layers.26.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=980), symbol:model.layers.26.mlp.up_proj.weight])[symbol:model.layers.26.mlp.up_proj.weight]
            tensor.CPU.register () -> (%1355:tensor<[2048, 6144], Float32, CPU>[@model.layers.26.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=982), symbol:model.layers.26.mlp.down_proj.weight])[symbol:model.layers.26.mlp.down_proj.weight]
            tensor.CPU.register () -> (%6381:tensor<[2048], Float32, CPU>[@model.layers.27.input_layernorm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=985), symbol:model.layers.27.input_layernorm.weight])[symbol:model.layers.27.input_layernorm.weight]
            tensor.CPU.register () -> (%5946:tensor<[2048, 2048], Float32, CPU>[@model.layers.27.self_attn.q_proj.weight][symbol:model.layers.27.self_attn.q_proj.weight])[symbol:model.layers.27.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%1802:tensor<[1024, 2048], Float32, CPU>[@model.layers.27.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=986), symbol:model.layers.27.self_attn.k_proj.weight])[symbol:model.layers.27.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%6652:tensor<[1024, 2048], Float32, CPU>[@model.layers.27.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=988), symbol:model.layers.27.self_attn.v_proj.weight])[symbol:model.layers.27.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%6206:tensor<[128], Float32, CPU>[@model.layers.27.self_attn.q_norm.weight][quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=992), symbol:model.layers.27.self_attn.q_norm.weight])[symbol:model.layers.27.self_attn.q_norm.weight]
            tensor.CPU.register () -> (%1743:tensor<[128], Float32, CPU>[@model.layers.27.self_attn.k_norm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=994), symbol:model.layers.27.self_attn.k_norm.weight])[symbol:model.layers.27.self_attn.k_norm.weight]
            tensor.CPU.register () -> (%5189:tensor<[2048, 2048], Float32, CPU>[@model.layers.27.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=1007), symbol:model.layers.27.self_attn.o_proj.weight])[symbol:model.layers.27.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%3001:tensor<[2048], Float32, CPU>[@model.layers.27.post_attention_layernorm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1010), symbol:model.layers.27.post_attention_layernorm.weight])[symbol:model.layers.27.post_attention_layernorm.weight]
            tensor.CPU.register () -> (%5561:tensor<[6144, 2048], Float32, CPU>[@model.layers.27.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=1011), symbol:model.layers.27.mlp.gate_proj.weight])[symbol:model.layers.27.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%2731:tensor<[6144, 2048], Float32, CPU>[@model.layers.27.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=1014), symbol:model.layers.27.mlp.up_proj.weight])[symbol:model.layers.27.mlp.up_proj.weight]
            tensor.CPU.register () -> (%3783:tensor<[2048, 6144], Float32, CPU>[@model.layers.27.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=1016), symbol:model.layers.27.mlp.down_proj.weight])[symbol:model.layers.27.mlp.down_proj.weight]
            tensor.CPU.register () -> (%5765:tensor<[2048], Float32, CPU>[@model.norm.weight][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1019), symbol:model.norm.weight])[symbol:model.norm.weight]
            tensor.CPU.register () -> (%6130:tensor<[151936, 2048], Float32, CPU>[@lm_head.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=1020), symbol:lm_head.weight])[symbol:lm_head.weight]
        }
    }
    graph.SubGraphOp @deinit <notype> [symbol:deinit] {
        () -> () {
            
        }
    }
    graph.CallGraphOp @model (%8013:tensor<[1, 32], Int32, CPU>[quant_recipe:QuantSpec(Raw(type: Int32), uuid=0)], %8071:tensor<[1, 32], Int64, CPU>[quant_recipe:QuantSpec(Raw(type: Int64), uuid=1)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8015:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=3)], %8017:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=4)], %8019:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=5)], %8021:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=6)], %8023:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=7)], %8025:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=8)], %8027:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=9)], %8029:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=10)], %8031:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=11)], %8033:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=12)], %8035:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=13)], %8037:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=14)], %8039:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=15)], %8041:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=16)], %8043:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=17)], %8045:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=18)], %8047:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=19)], %8049:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=20)], %8051:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=21)], %8053:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=22)], %8055:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=23)], %8057:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=24)], %8059:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=25)], %8061:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=26)], %8063:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=27)], %8065:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=28)], %8067:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=29)], %8069:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=30)], %8016:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=31)], %8018:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=32)], %8020:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=33)], %8022:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=34)], %8024:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=35)], %8026:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=36)], %8028:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=37)], %8030:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=38)], %8032:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=39)], %8034:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=40)], %8036:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=41)], %8038:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=42)], %8040:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=43)], %8042:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=44)], %8044:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=45)], %8046:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=46)], %8048:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=47)], %8050:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=48)], %8052:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=49)], %8054:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=50)], %8056:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=51)], %8058:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=52)], %8060:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=53)], %8062:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=54)], %8064:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=55)], %8066:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=56)], %8068:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=57)], %8070:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=58)]) -> (%9225:tensor<[1, 32, 151936], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1021)], %8089:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=78)], %8130:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=112)], %8171:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=146)], %8212:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=180)], %8253:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=214)], %8294:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=248)], %8335:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=282)], %8376:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=316)], %8417:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=350)], %8458:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=384)], %8499:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=418)], %8540:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=452)], %8581:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=486)], %8622:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=520)], %8663:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=554)], %8704:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=588)], %8745:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=622)], %8786:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=656)], %8827:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=690)], %8868:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=724)], %8909:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=758)], %8950:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=792)], %8991:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=826)], %9032:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=860)], %9073:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=894)], %9114:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=928)], %9155:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=962)], %9196:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=996)], %8091:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=80)], %8132:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=114)], %8173:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=148)], %8214:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=182)], %8255:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=216)], %8296:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=250)], %8337:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=284)], %8378:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=318)], %8419:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=352)], %8460:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=386)], %8501:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=420)], %8542:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=454)], %8583:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=488)], %8624:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=522)], %8665:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=556)], %8706:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=590)], %8747:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=624)], %8788:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=658)], %8829:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=692)], %8870:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=726)], %8911:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=760)], %8952:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=794)], %8993:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=828)], %9034:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=862)], %9075:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=896)], %9116:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=930)], %9157:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=964)], %9198:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=998)])
    graph.SubGraphOp @model <CPU> [using_qnn:true, symbol:model] {
        (%8013:tensor<[1, 32], Int32, CPU>[quant_recipe:QuantSpec(Raw(type: Int32), uuid=0)], %8071:tensor<[1, 32], Int64, CPU>[quant_recipe:QuantSpec(Raw(type: Int64), uuid=1)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8015:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=3)], %8017:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=4)], %8019:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=5)], %8021:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=6)], %8023:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=7)], %8025:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=8)], %8027:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=9)], %8029:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=10)], %8031:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=11)], %8033:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=12)], %8035:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=13)], %8037:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=14)], %8039:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=15)], %8041:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=16)], %8043:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=17)], %8045:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=18)], %8047:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=19)], %8049:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=20)], %8051:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=21)], %8053:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=22)], %8055:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=23)], %8057:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=24)], %8059:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=25)], %8061:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=26)], %8063:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=27)], %8065:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=28)], %8067:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=29)], %8069:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=30)], %8016:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=31)], %8018:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=32)], %8020:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=33)], %8022:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=34)], %8024:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=35)], %8026:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=36)], %8028:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=37)], %8030:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=38)], %8032:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=39)], %8034:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=40)], %8036:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=41)], %8038:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=42)], %8040:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=43)], %8042:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=44)], %8044:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=45)], %8046:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=46)], %8048:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=47)], %8050:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=48)], %8052:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=49)], %8054:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=50)], %8056:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=51)], %8058:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=52)], %8060:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=53)], %8062:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=54)], %8064:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=55)], %8066:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=56)], %8068:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=57)], %8070:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=58)]) -> (%9225:tensor<[1, 32, 151936], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1021)], %8089:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=78)], %8130:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=112)], %8171:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=146)], %8212:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=180)], %8253:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=214)], %8294:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=248)], %8335:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=282)], %8376:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=316)], %8417:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=350)], %8458:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=384)], %8499:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=418)], %8540:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=452)], %8581:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=486)], %8622:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=520)], %8663:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=554)], %8704:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=588)], %8745:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=622)], %8786:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=656)], %8827:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=690)], %8868:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=724)], %8909:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=758)], %8950:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=792)], %8991:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=826)], %9032:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=860)], %9073:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=894)], %9114:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=928)], %9155:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=962)], %9196:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=996)], %8091:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=80)], %8132:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=114)], %8173:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=148)], %8214:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=182)], %8255:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=216)], %8296:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=250)], %8337:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=284)], %8378:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=318)], %8419:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=352)], %8460:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=386)], %8501:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=420)], %8542:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=454)], %8583:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=488)], %8624:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=522)], %8665:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=556)], %8706:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=590)], %8747:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=624)], %8788:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=658)], %8829:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=692)], %8870:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=726)], %8911:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=760)], %8952:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=794)], %8993:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=828)], %9034:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=862)], %9075:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=896)], %9116:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=930)], %9157:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=964)], %9198:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=998)]) {
            linalg.CPU.EmbeddingOp <name="model.embed_tokens">(%8013:tensor<[1, 32], Int32, CPU>[quant_recipe:QuantSpec(Raw(type: Int32), uuid=0)]) -> (%8072:tensor<[1, 32, 2048], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=59)])
            linalg.CPU.CastTypeOp <name="model.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float32), uuid=59), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=60), weight_weight:QuantSpec(Raw(type: Float32), uuid=61))] (%8072:tensor<[1, 32, 2048], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=59)]) -> (%8073:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=60)])
            linalg.CPU.ViewOp <name="model.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int64), uuid=1), outputs_0:QuantSpec(Raw(type: Int64), uuid=1), )] (%8071:tensor<[1, 32], Int64, CPU>[quant_recipe:QuantSpec(Raw(type: Int64), uuid=1)]) -> (%8071:tensor<[32], Int64, CPU>[quant_recipe:QuantSpec(Raw(type: Int64), uuid=1)])
            linalg.CPU.IndexOp <name="model.Index.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63), )] (%8011:tensor<[1, 1024, 128], Int16PerTensor, CPU>[@rope_sin][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), symbol:rope_sin]) -> (%8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)])
            linalg.CPU.IndexOp <name="model.Index.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), )] (%8012:tensor<[1, 1024, 128], Int16PerTensor, CPU>[@rope_cos][quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), symbol:rope_cos]) -> (%8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)])
            graph.CallGraphOp @model.layers.0 (%8073:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=60)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8015:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=3)], %8016:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=31)]) -> (%8116:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=99)], %8089:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=78)], %8091:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=80)])
            graph.CallGraphOp @model.layers.1 (%8116:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=99)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8017:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=4)], %8018:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=32)]) -> (%8157:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=133)], %8130:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=112)], %8132:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=114)])
            graph.CallGraphOp @model.layers.2 (%8157:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=133)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8019:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=5)], %8020:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=33)]) -> (%8198:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=167)], %8171:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=146)], %8173:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=148)])
            graph.CallGraphOp @model.layers.3 (%8198:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=167)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8021:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=6)], %8022:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=34)]) -> (%8239:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=201)], %8212:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=180)], %8214:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=182)])
            graph.CallGraphOp @model.layers.4 (%8239:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=201)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8023:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=7)], %8024:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=35)]) -> (%8280:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=235)], %8253:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=214)], %8255:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=216)])
            graph.CallGraphOp @model.layers.5 (%8280:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=235)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8025:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=8)], %8026:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=36)]) -> (%8321:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=269)], %8294:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=248)], %8296:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=250)])
            graph.CallGraphOp @model.layers.6 (%8321:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=269)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8027:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=9)], %8028:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=37)]) -> (%8362:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=303)], %8335:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=282)], %8337:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=284)])
            graph.CallGraphOp @model.layers.7 (%8362:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=303)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8029:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=10)], %8030:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=38)]) -> (%8403:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=337)], %8376:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=316)], %8378:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=318)])
            graph.CallGraphOp @model.layers.8 (%8403:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=337)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8031:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=11)], %8032:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=39)]) -> (%8444:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=371)], %8417:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=350)], %8419:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=352)])
            graph.CallGraphOp @model.layers.9 (%8444:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=371)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8033:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=12)], %8034:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=40)]) -> (%8485:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=405)], %8458:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=384)], %8460:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=386)])
            graph.CallGraphOp @model.layers.10 (%8485:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=405)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8035:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=13)], %8036:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=41)]) -> (%8526:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=439)], %8499:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=418)], %8501:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=420)])
            graph.CallGraphOp @model.layers.11 (%8526:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=439)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8037:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=14)], %8038:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=42)]) -> (%8567:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=473)], %8540:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=452)], %8542:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=454)])
            graph.CallGraphOp @model.layers.12 (%8567:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=473)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8039:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=15)], %8040:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=43)]) -> (%8608:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=507)], %8581:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=486)], %8583:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=488)])
            graph.CallGraphOp @model.layers.13 (%8608:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=507)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8041:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=16)], %8042:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=44)]) -> (%8649:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=541)], %8622:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=520)], %8624:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=522)])
            graph.CallGraphOp @model.layers.14 (%8649:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=541)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8043:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=17)], %8044:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=45)]) -> (%8690:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=575)], %8663:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=554)], %8665:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=556)])
            graph.CallGraphOp @model.layers.15 (%8690:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=575)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8045:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=18)], %8046:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=46)]) -> (%8731:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=609)], %8704:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=588)], %8706:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=590)])
            graph.CallGraphOp @model.layers.16 (%8731:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=609)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8047:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=19)], %8048:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=47)]) -> (%8772:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=643)], %8745:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=622)], %8747:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=624)])
            graph.CallGraphOp @model.layers.17 (%8772:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=643)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8049:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=20)], %8050:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=48)]) -> (%8813:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=677)], %8786:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=656)], %8788:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=658)])
            graph.CallGraphOp @model.layers.18 (%8813:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=677)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8051:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=21)], %8052:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=49)]) -> (%8854:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=711)], %8827:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=690)], %8829:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=692)])
            graph.CallGraphOp @model.layers.19 (%8854:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=711)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8053:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=22)], %8054:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=50)]) -> (%8895:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=745)], %8868:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=724)], %8870:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=726)])
            graph.CallGraphOp @model.layers.20 (%8895:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=745)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8055:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=23)], %8056:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=51)]) -> (%8936:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=779)], %8909:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=758)], %8911:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=760)])
            graph.CallGraphOp @model.layers.21 (%8936:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=779)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8057:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=24)], %8058:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=52)]) -> (%8977:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=813)], %8950:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=792)], %8952:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=794)])
            graph.CallGraphOp @model.layers.22 (%8977:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=813)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8059:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=25)], %8060:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=53)]) -> (%9018:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=847)], %8991:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=826)], %8993:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=828)])
            graph.CallGraphOp @model.layers.23 (%9018:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=847)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8061:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=26)], %8062:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=54)]) -> (%9059:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=881)], %9032:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=860)], %9034:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=862)])
            graph.CallGraphOp @model.layers.24 (%9059:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=881)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8063:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=27)], %8064:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=55)]) -> (%9100:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=915)], %9073:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=894)], %9075:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=896)])
            graph.CallGraphOp @model.layers.25 (%9100:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=915)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8065:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=28)], %8066:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=56)]) -> (%9141:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=949)], %9114:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=928)], %9116:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=930)])
            graph.CallGraphOp @model.layers.26 (%9141:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=949)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8067:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=29)], %8068:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=57)]) -> (%9182:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=983)], %9155:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=962)], %9157:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=964)])
            graph.CallGraphOp @model.layers.27 (%9182:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=983)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8069:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=30)], %8070:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=58)]) -> (%9223:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1017)], %9196:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=996)], %9198:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=998)])
            linalg.CPU.RMSNormOp <name="model.norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1017), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1018), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1019))] (%9223:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1017)]) -> (%9224:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1018)])
            linalg.CPU.LinearOp <name="lm_head"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1018), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1021), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=1020)), using_qnn:true] (%9224:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1018)]) -> (%9225:tensor<[1, 32, 151936], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1021)])
            cf.ReturnOp (%9225:tensor<[1, 32, 151936], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1021)], %8089:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=78)], %8130:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=112)], %8171:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=146)], %8212:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=180)], %8253:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=214)], %8294:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=248)], %8335:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=282)], %8376:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=316)], %8417:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=350)], %8458:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=384)], %8499:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=418)], %8540:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=452)], %8581:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=486)], %8622:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=520)], %8663:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=554)], %8704:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=588)], %8745:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=622)], %8786:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=656)], %8827:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=690)], %8868:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=724)], %8909:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=758)], %8950:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=792)], %8991:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=826)], %9032:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=860)], %9073:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=894)], %9114:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=928)], %9155:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=962)], %9196:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=996)], %8091:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=80)], %8132:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=114)], %8173:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=148)], %8214:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=182)], %8255:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=216)], %8296:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=250)], %8337:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=284)], %8378:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=318)], %8419:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=352)], %8460:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=386)], %8501:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=420)], %8542:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=454)], %8583:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=488)], %8624:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=522)], %8665:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=556)], %8706:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=590)], %8747:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=624)], %8788:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=658)], %8829:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=692)], %8870:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=726)], %8911:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=760)], %8952:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=794)], %8993:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=828)], %9034:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=862)], %9075:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=896)], %9116:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=930)], %9157:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=964)], %9198:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=998)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.0 <CPU> [using_qnn:true, symbol:model.layers.0] {
        (%8073:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=60)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8015:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=3)], %8016:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=31)]) -> (%8116:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=99)], %8089:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=78)], %8091:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=80)]) {
            linalg.CPU.RMSNormOp <name="model.layers.0.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=60), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=66), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=67))] (%8073:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=60)]) -> (%8076:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=66)])
            graph.CallGraphOp @model.layers.0.self_attn (%8076:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=66)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8015:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=3)], %8016:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=31)]) -> (%8108:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=90)], %8089:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=78)], %8091:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=80)])
            linalg.CPU.AddOp <name="model.layers.0.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=90), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=60), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=90), )] (%8108:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=90)], %8073:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=60)]) -> (%8109:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=90)])
            linalg.CPU.RMSNormOp <name="model.layers.0.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=90), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=91), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=92))] (%8109:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=90)]) -> (%8110:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=91)])
            graph.CallGraphOp @model.layers.0.mlp (%8110:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=91)]) -> (%8115:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=99)])
            linalg.CPU.AddOp <name="model.layers.0.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=99), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=90), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=99), )] (%8115:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=99)], %8109:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=90)]) -> (%8116:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=99)])
            cf.ReturnOp (%8116:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=99)], %8089:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=78)], %8091:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=80)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.0.self_attn <CPU> [using_qnn:true, symbol:model.layers.0.self_attn] {
        (%8076:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=66)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8015:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=3)], %8016:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=31)]) -> (%8108:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=90)], %8089:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=78)], %8091:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=80)]) {
            linalg.CPU.LinearOp <name="model.layers.0.self_attn.q_proj">(%8076:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=66)]) -> (%8077:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=72)])
            linalg.CPU.LinearOp <name="model.layers.0.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=66), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=69), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=68))] (%8076:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=66)]) -> (%8078:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=69)])
            linalg.CPU.LinearOp <name="model.layers.0.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=66), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=71), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=70))] (%8076:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=66)]) -> (%8079:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=71)])
            linalg.CPU.ViewOp <name="model.layers.0.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=72), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=72), )] (%8077:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=72)]) -> (%8077:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=72)])
            linalg.CPU.TransposeOp <name="model.layers.0.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=72), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=72), )] (%8077:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=72)]) -> (%8080:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=72)])
            linalg.CPU.ViewOp <name="model.layers.0.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=69), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=69), )] (%8078:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=69)]) -> (%8078:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=69)])
            linalg.CPU.TransposeOp <name="model.layers.0.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=69), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=69), )] (%8078:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=69)]) -> (%8081:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=69)])
            linalg.CPU.ViewOp <name="model.layers.0.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=71), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=71), )] (%8079:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=71)]) -> (%8079:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=71)])
            linalg.CPU.TransposeOp <name="model.layers.0.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=71), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=71), )] (%8079:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=71)]) -> (%8082:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=71)])
            linalg.CPU.RMSNormOp <name="model.layers.0.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=72), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=73), weight_weight:QuantSpec(Raw(type: Int16PerTensor), uuid=74))] (%8080:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=72)]) -> (%8083:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=73)])
            linalg.CPU.RMSNormOp <name="model.layers.0.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=69), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=75), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=76))] (%8081:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=69)]) -> (%8084:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=75)])
            linalg.CPU.RoPEOp <name="model.layers.0.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=73), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=73), )] (%8083:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=73)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%8085:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=73)])
            linalg.CPU.RoPEOp <name="model.layers.0.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=75), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=75), )] (%8084:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=75)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%8086:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=75)])
            linalg.CPU.CastTypeOp <name="model.layers.0.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=75), outputs_0:QuantSpec(Raw(type: Float16), uuid=77), )] (%8086:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=75)]) -> (%8087:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=77)])
            linalg.CPU.CastTypeOp <name="model.layers.0.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=77), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=78), )] (%8087:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=77)]) -> (%8088:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=78)])
            linalg.CPU.TransposeOp <name="model.layers.0.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=78), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=78), )] (%8088:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=78)]) -> (%8089:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=78)])
            linalg.CPU.CastTypeOp <name="model.layers.0.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=71), outputs_0:QuantSpec(Raw(type: Float16), uuid=79), )] (%8082:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=71)]) -> (%8090:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=79)])
            linalg.CPU.CastTypeOp <name="model.layers.0.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=79), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=80), )] (%8090:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=79)]) -> (%8091:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=80)])
            linalg.CPU.ConcatOp <name="model.layers.0.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=3), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=78), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=3), )] (%8015:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=3)], %8089:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=78)]) -> (%8092:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=3)])
            linalg.CPU.ConcatOp <name="model.layers.0.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=31), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=80), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=31), )] (%8016:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=31)], %8091:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=80)]) -> (%8093:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=31)])
            linalg.CPU.RepeatOp <name="model.layers.0.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=3), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=3), )] (%8092:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=3)]) -> (%8094:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=3)])
            linalg.CPU.RepeatOp <name="model.layers.0.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=31), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=31), )] (%8093:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=31)]) -> (%8095:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=31)])
            linalg.CPU.MatMulOp <name="model.layers.0.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=73), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=3), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=81), )] (%8085:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=73)], %8094:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=3)]) -> (%8096:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=81)])
            linalg.CPU.MulOp <name="model.layers.0.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=81), inputs_1:QuantSpec(Raw(type: Float32), uuid=82), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=81), )] (%8096:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=81)], %8097:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=82), constant:[0.088388346]]) -> (%8098:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=81)])
            linalg.CPU.ReduceMinOp <name="model.layers.0.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=81), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=83), )] (%8098:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=81)]) -> (%8099:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=83)])
            linalg.CPU.AddOp <name="model.layers.0.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=83), inputs_1:QuantSpec(Raw(type: Int16), uuid=84), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=83), )] (%8099:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=83)], %8100:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=84), constant:[-20]]) -> (%8101:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=83)])
            linalg.CPU.EqualOp <name="model.layers.0.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=85), outputs_0:QuantSpec(Raw(type: UInt8), uuid=86), )] (%8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8102:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=85), constant:[0]]) -> (%8103:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=86)])
            linalg.CPU.WhereOp <name="model.layers.0.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=86), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=81), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=83), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=83), )] (%8103:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=86)], %8098:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=81)], %8101:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=83)]) -> (%8104:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=83)])
            linalg.CPU.SoftmaxOp <name="model.layers.0.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=83), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=87), )] (%8104:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=83)]) -> (%8105:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=87)])
            linalg.CPU.MatMulOp <name="model.layers.0.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=87), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=31), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=88), )] (%8105:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=87)], %8095:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=31)]) -> (%8106:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=88)])
            linalg.CPU.TransposeOp <name="model.layers.0.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=88), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=88), )] (%8106:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=88)]) -> (%8107:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=88)])
            linalg.CPU.ViewOp <name="model.layers.0.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=88), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=88), )] (%8107:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=88)]) -> (%8107:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=88)])
            linalg.CPU.LinearOp <name="model.layers.0.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=88), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=90), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=89))] (%8107:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=88)]) -> (%8108:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=90)])
            cf.ReturnOp (%8108:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=90)], %8089:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=78)], %8091:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=80)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.0.mlp <CPU> [using_qnn:true, symbol:model.layers.0.mlp] {
        (%8110:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=91)]) -> (%8115:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=99)]) {
            linalg.CPU.LinearOp <name="model.layers.0.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=91), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=94), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=93))] (%8110:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=91)]) -> (%8111:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=94)])
            linalg.CPU.SiLUOp <name="model.layers.0.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=94), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=95), )] (%8111:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=94)]) -> (%8112:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=95)])
            linalg.CPU.LinearOp <name="model.layers.0.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=91), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=97), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=96))] (%8110:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=91)]) -> (%8113:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=97)])
            linalg.CPU.MulOp <name="model.layers.0.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=95), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=97), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=95), )] (%8112:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=95)], %8113:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=97)]) -> (%8114:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=95)])
            linalg.CPU.LinearOp <name="model.layers.0.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=95), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=99), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=98))] (%8114:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=95)]) -> (%8115:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=99)])
            cf.ReturnOp (%8115:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=99)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.1 <CPU> [using_qnn:true, symbol:model.layers.1] {
        (%8116:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=99)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8017:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=4)], %8018:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=32)]) -> (%8157:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=133)], %8130:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=112)], %8132:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=114)]) {
            linalg.CPU.RMSNormOp <name="model.layers.1.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=99), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=100), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=101))] (%8116:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=99)]) -> (%8117:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=100)])
            graph.CallGraphOp @model.layers.1.self_attn (%8117:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=100)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8017:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=4)], %8018:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=32)]) -> (%8149:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=124)], %8130:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=112)], %8132:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=114)])
            linalg.CPU.AddOp <name="model.layers.1.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=124), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=99), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=124), )] (%8149:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=124)], %8116:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=99)]) -> (%8150:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=124)])
            linalg.CPU.RMSNormOp <name="model.layers.1.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=124), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=125), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=126))] (%8150:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=124)]) -> (%8151:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=125)])
            graph.CallGraphOp @model.layers.1.mlp (%8151:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=125)]) -> (%8156:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=133)])
            linalg.CPU.AddOp <name="model.layers.1.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=133), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=124), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=133), )] (%8156:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=133)], %8150:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=124)]) -> (%8157:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=133)])
            cf.ReturnOp (%8157:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=133)], %8130:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=112)], %8132:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=114)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.1.self_attn <CPU> [using_qnn:true, symbol:model.layers.1.self_attn] {
        (%8117:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=100)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8017:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=4)], %8018:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=32)]) -> (%8149:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=124)], %8130:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=112)], %8132:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=114)]) {
            linalg.CPU.LinearOp <name="model.layers.1.self_attn.q_proj">(%8117:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=100)]) -> (%8118:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=106)])
            linalg.CPU.LinearOp <name="model.layers.1.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=100), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=103), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=102))] (%8117:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=100)]) -> (%8119:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=103)])
            linalg.CPU.LinearOp <name="model.layers.1.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=100), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=105), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=104))] (%8117:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=100)]) -> (%8120:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=105)])
            linalg.CPU.ViewOp <name="model.layers.1.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=106), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=106), )] (%8118:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=106)]) -> (%8118:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=106)])
            linalg.CPU.TransposeOp <name="model.layers.1.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=106), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=106), )] (%8118:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=106)]) -> (%8121:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=106)])
            linalg.CPU.ViewOp <name="model.layers.1.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=103), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=103), )] (%8119:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=103)]) -> (%8119:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=103)])
            linalg.CPU.TransposeOp <name="model.layers.1.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=103), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=103), )] (%8119:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=103)]) -> (%8122:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=103)])
            linalg.CPU.ViewOp <name="model.layers.1.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=105), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=105), )] (%8120:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=105)]) -> (%8120:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=105)])
            linalg.CPU.TransposeOp <name="model.layers.1.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=105), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=105), )] (%8120:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=105)]) -> (%8123:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=105)])
            linalg.CPU.RMSNormOp <name="model.layers.1.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=106), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=107), weight_weight:QuantSpec(Raw(type: Int16PerTensor), uuid=108))] (%8121:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=106)]) -> (%8124:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=107)])
            linalg.CPU.RMSNormOp <name="model.layers.1.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=103), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=109), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=110))] (%8122:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=103)]) -> (%8125:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=109)])
            linalg.CPU.RoPEOp <name="model.layers.1.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=107), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=107), )] (%8124:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=107)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%8126:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=107)])
            linalg.CPU.RoPEOp <name="model.layers.1.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=109), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=109), )] (%8125:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=109)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%8127:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=109)])
            linalg.CPU.CastTypeOp <name="model.layers.1.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=109), outputs_0:QuantSpec(Raw(type: Float16), uuid=111), )] (%8127:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=109)]) -> (%8128:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=111)])
            linalg.CPU.CastTypeOp <name="model.layers.1.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=111), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=112), )] (%8128:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=111)]) -> (%8129:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=112)])
            linalg.CPU.TransposeOp <name="model.layers.1.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=112), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=112), )] (%8129:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=112)]) -> (%8130:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=112)])
            linalg.CPU.CastTypeOp <name="model.layers.1.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=105), outputs_0:QuantSpec(Raw(type: Float16), uuid=113), )] (%8123:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=105)]) -> (%8131:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=113)])
            linalg.CPU.CastTypeOp <name="model.layers.1.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=113), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=114), )] (%8131:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=113)]) -> (%8132:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=114)])
            linalg.CPU.ConcatOp <name="model.layers.1.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=4), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=112), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=4), )] (%8017:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=4)], %8130:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=112)]) -> (%8133:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=4)])
            linalg.CPU.ConcatOp <name="model.layers.1.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=32), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=114), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=32), )] (%8018:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=32)], %8132:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=114)]) -> (%8134:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=32)])
            linalg.CPU.RepeatOp <name="model.layers.1.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=4), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=4), )] (%8133:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=4)]) -> (%8135:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=4)])
            linalg.CPU.RepeatOp <name="model.layers.1.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=32), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=32), )] (%8134:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=32)]) -> (%8136:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=32)])
            linalg.CPU.MatMulOp <name="model.layers.1.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=107), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=4), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=115), )] (%8126:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=107)], %8135:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=4)]) -> (%8137:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=115)])
            linalg.CPU.MulOp <name="model.layers.1.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=115), inputs_1:QuantSpec(Raw(type: Float32), uuid=116), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=115), )] (%8137:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=115)], %8138:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=116), constant:[0.088388346]]) -> (%8139:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=115)])
            linalg.CPU.ReduceMinOp <name="model.layers.1.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=115), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=117), )] (%8139:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=115)]) -> (%8140:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=117)])
            linalg.CPU.AddOp <name="model.layers.1.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=117), inputs_1:QuantSpec(Raw(type: Int16), uuid=118), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=117), )] (%8140:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=117)], %8141:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=118), constant:[-20]]) -> (%8142:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=117)])
            linalg.CPU.EqualOp <name="model.layers.1.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=119), outputs_0:QuantSpec(Raw(type: UInt8), uuid=120), )] (%8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8143:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=119), constant:[0]]) -> (%8144:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=120)])
            linalg.CPU.WhereOp <name="model.layers.1.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=120), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=115), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=117), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=117), )] (%8144:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=120)], %8139:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=115)], %8142:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=117)]) -> (%8145:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=117)])
            linalg.CPU.SoftmaxOp <name="model.layers.1.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=117), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=121), )] (%8145:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=117)]) -> (%8146:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=121)])
            linalg.CPU.MatMulOp <name="model.layers.1.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=121), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=32), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=122), )] (%8146:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=121)], %8136:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=32)]) -> (%8147:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=122)])
            linalg.CPU.TransposeOp <name="model.layers.1.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=122), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=122), )] (%8147:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=122)]) -> (%8148:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=122)])
            linalg.CPU.ViewOp <name="model.layers.1.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=122), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=122), )] (%8148:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=122)]) -> (%8148:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=122)])
            linalg.CPU.LinearOp <name="model.layers.1.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=122), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=124), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=123))] (%8148:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=122)]) -> (%8149:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=124)])
            cf.ReturnOp (%8149:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=124)], %8130:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=112)], %8132:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=114)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.1.mlp <CPU> [using_qnn:true, symbol:model.layers.1.mlp] {
        (%8151:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=125)]) -> (%8156:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=133)]) {
            linalg.CPU.LinearOp <name="model.layers.1.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=125), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=128), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=127))] (%8151:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=125)]) -> (%8152:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=128)])
            linalg.CPU.SiLUOp <name="model.layers.1.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=128), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=129), )] (%8152:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=128)]) -> (%8153:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=129)])
            linalg.CPU.LinearOp <name="model.layers.1.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=125), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=131), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=130))] (%8151:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=125)]) -> (%8154:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=131)])
            linalg.CPU.MulOp <name="model.layers.1.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=129), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=131), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=129), )] (%8153:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=129)], %8154:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=131)]) -> (%8155:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=129)])
            linalg.CPU.LinearOp <name="model.layers.1.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=129), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=133), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=132))] (%8155:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=129)]) -> (%8156:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=133)])
            cf.ReturnOp (%8156:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=133)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.2 <CPU> [using_qnn:true, symbol:model.layers.2] {
        (%8157:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=133)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8019:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=5)], %8020:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=33)]) -> (%8198:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=167)], %8171:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=146)], %8173:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=148)]) {
            linalg.CPU.RMSNormOp <name="model.layers.2.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=133), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=134), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=135))] (%8157:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=133)]) -> (%8158:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=134)])
            graph.CallGraphOp @model.layers.2.self_attn (%8158:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=134)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8019:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=5)], %8020:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=33)]) -> (%8190:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=158)], %8171:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=146)], %8173:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=148)])
            linalg.CPU.AddOp <name="model.layers.2.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=158), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=133), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=158), )] (%8190:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=158)], %8157:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=133)]) -> (%8191:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=158)])
            linalg.CPU.RMSNormOp <name="model.layers.2.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=158), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=159), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=160))] (%8191:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=158)]) -> (%8192:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=159)])
            graph.CallGraphOp @model.layers.2.mlp (%8192:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=159)]) -> (%8197:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=167)])
            linalg.CPU.AddOp <name="model.layers.2.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=167), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=158), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=167), )] (%8197:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=167)], %8191:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=158)]) -> (%8198:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=167)])
            cf.ReturnOp (%8198:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=167)], %8171:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=146)], %8173:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=148)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.2.self_attn <CPU> [using_qnn:true, symbol:model.layers.2.self_attn] {
        (%8158:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=134)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8019:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=5)], %8020:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=33)]) -> (%8190:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=158)], %8171:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=146)], %8173:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=148)]) {
            linalg.CPU.LinearOp <name="model.layers.2.self_attn.q_proj">(%8158:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=134)]) -> (%8159:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=140)])
            linalg.CPU.LinearOp <name="model.layers.2.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=134), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=137), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=136))] (%8158:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=134)]) -> (%8160:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=137)])
            linalg.CPU.LinearOp <name="model.layers.2.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=134), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=139), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=138))] (%8158:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=134)]) -> (%8161:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=139)])
            linalg.CPU.ViewOp <name="model.layers.2.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=140), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=140), )] (%8159:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=140)]) -> (%8159:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=140)])
            linalg.CPU.TransposeOp <name="model.layers.2.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=140), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=140), )] (%8159:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=140)]) -> (%8162:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=140)])
            linalg.CPU.ViewOp <name="model.layers.2.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=137), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=137), )] (%8160:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=137)]) -> (%8160:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=137)])
            linalg.CPU.TransposeOp <name="model.layers.2.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=137), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=137), )] (%8160:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=137)]) -> (%8163:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=137)])
            linalg.CPU.ViewOp <name="model.layers.2.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=139), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=139), )] (%8161:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=139)]) -> (%8161:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=139)])
            linalg.CPU.TransposeOp <name="model.layers.2.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=139), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=139), )] (%8161:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=139)]) -> (%8164:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=139)])
            linalg.CPU.RMSNormOp <name="model.layers.2.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=140), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=141), weight_weight:QuantSpec(Raw(type: Int16PerTensor), uuid=142))] (%8162:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=140)]) -> (%8165:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=141)])
            linalg.CPU.RMSNormOp <name="model.layers.2.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=137), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=143), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=144))] (%8163:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=137)]) -> (%8166:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=143)])
            linalg.CPU.RoPEOp <name="model.layers.2.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=141), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=141), )] (%8165:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=141)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%8167:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=141)])
            linalg.CPU.RoPEOp <name="model.layers.2.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=143), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=143), )] (%8166:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=143)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%8168:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=143)])
            linalg.CPU.CastTypeOp <name="model.layers.2.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=143), outputs_0:QuantSpec(Raw(type: Float16), uuid=145), )] (%8168:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=143)]) -> (%8169:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=145)])
            linalg.CPU.CastTypeOp <name="model.layers.2.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=145), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=146), )] (%8169:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=145)]) -> (%8170:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=146)])
            linalg.CPU.TransposeOp <name="model.layers.2.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=146), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=146), )] (%8170:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=146)]) -> (%8171:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=146)])
            linalg.CPU.CastTypeOp <name="model.layers.2.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=139), outputs_0:QuantSpec(Raw(type: Float16), uuid=147), )] (%8164:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=139)]) -> (%8172:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=147)])
            linalg.CPU.CastTypeOp <name="model.layers.2.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=147), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=148), )] (%8172:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=147)]) -> (%8173:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=148)])
            linalg.CPU.ConcatOp <name="model.layers.2.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=5), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=146), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=5), )] (%8019:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=5)], %8171:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=146)]) -> (%8174:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=5)])
            linalg.CPU.ConcatOp <name="model.layers.2.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=33), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=148), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=33), )] (%8020:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=33)], %8173:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=148)]) -> (%8175:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=33)])
            linalg.CPU.RepeatOp <name="model.layers.2.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=5), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=5), )] (%8174:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=5)]) -> (%8176:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=5)])
            linalg.CPU.RepeatOp <name="model.layers.2.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=33), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=33), )] (%8175:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=33)]) -> (%8177:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=33)])
            linalg.CPU.MatMulOp <name="model.layers.2.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=141), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=5), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=149), )] (%8167:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=141)], %8176:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=5)]) -> (%8178:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=149)])
            linalg.CPU.MulOp <name="model.layers.2.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=149), inputs_1:QuantSpec(Raw(type: Float32), uuid=150), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=149), )] (%8178:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=149)], %8179:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=150), constant:[0.088388346]]) -> (%8180:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=149)])
            linalg.CPU.ReduceMinOp <name="model.layers.2.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=149), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=151), )] (%8180:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=149)]) -> (%8181:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=151)])
            linalg.CPU.AddOp <name="model.layers.2.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=151), inputs_1:QuantSpec(Raw(type: Int16), uuid=152), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=151), )] (%8181:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=151)], %8182:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=152), constant:[-20]]) -> (%8183:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=151)])
            linalg.CPU.EqualOp <name="model.layers.2.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=153), outputs_0:QuantSpec(Raw(type: UInt8), uuid=154), )] (%8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8184:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=153), constant:[0]]) -> (%8185:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=154)])
            linalg.CPU.WhereOp <name="model.layers.2.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=154), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=149), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=151), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=151), )] (%8185:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=154)], %8180:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=149)], %8183:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=151)]) -> (%8186:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=151)])
            linalg.CPU.SoftmaxOp <name="model.layers.2.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=151), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=155), )] (%8186:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=151)]) -> (%8187:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=155)])
            linalg.CPU.MatMulOp <name="model.layers.2.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=155), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=33), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=156), )] (%8187:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=155)], %8177:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=33)]) -> (%8188:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=156)])
            linalg.CPU.TransposeOp <name="model.layers.2.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=156), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=156), )] (%8188:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=156)]) -> (%8189:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=156)])
            linalg.CPU.ViewOp <name="model.layers.2.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=156), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=156), )] (%8189:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=156)]) -> (%8189:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=156)])
            linalg.CPU.LinearOp <name="model.layers.2.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=156), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=158), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=157))] (%8189:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=156)]) -> (%8190:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=158)])
            cf.ReturnOp (%8190:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=158)], %8171:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=146)], %8173:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=148)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.2.mlp <CPU> [using_qnn:true, symbol:model.layers.2.mlp] {
        (%8192:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=159)]) -> (%8197:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=167)]) {
            linalg.CPU.LinearOp <name="model.layers.2.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=159), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=162), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=161))] (%8192:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=159)]) -> (%8193:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=162)])
            linalg.CPU.SiLUOp <name="model.layers.2.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=162), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=163), )] (%8193:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=162)]) -> (%8194:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=163)])
            linalg.CPU.LinearOp <name="model.layers.2.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=159), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=165), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=164))] (%8192:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=159)]) -> (%8195:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=165)])
            linalg.CPU.MulOp <name="model.layers.2.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=163), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=165), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=163), )] (%8194:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=163)], %8195:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=165)]) -> (%8196:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=163)])
            linalg.CPU.LinearOp <name="model.layers.2.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=163), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=167), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=166))] (%8196:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=163)]) -> (%8197:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=167)])
            cf.ReturnOp (%8197:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=167)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.3 <CPU> [using_qnn:true, symbol:model.layers.3] {
        (%8198:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=167)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8021:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=6)], %8022:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=34)]) -> (%8239:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=201)], %8212:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=180)], %8214:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=182)]) {
            linalg.CPU.RMSNormOp <name="model.layers.3.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=167), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=168), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=169))] (%8198:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=167)]) -> (%8199:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=168)])
            graph.CallGraphOp @model.layers.3.self_attn (%8199:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=168)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8021:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=6)], %8022:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=34)]) -> (%8231:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=192)], %8212:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=180)], %8214:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=182)])
            linalg.CPU.AddOp <name="model.layers.3.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=192), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=167), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=192), )] (%8231:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=192)], %8198:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=167)]) -> (%8232:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=192)])
            linalg.CPU.RMSNormOp <name="model.layers.3.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=192), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=193), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=194))] (%8232:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=192)]) -> (%8233:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=193)])
            graph.CallGraphOp @model.layers.3.mlp (%8233:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=193)]) -> (%8238:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=201)])
            linalg.CPU.AddOp <name="model.layers.3.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=201), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=192), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=201), )] (%8238:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=201)], %8232:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=192)]) -> (%8239:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=201)])
            cf.ReturnOp (%8239:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=201)], %8212:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=180)], %8214:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=182)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.3.self_attn <CPU> [using_qnn:true, symbol:model.layers.3.self_attn] {
        (%8199:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=168)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8021:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=6)], %8022:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=34)]) -> (%8231:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=192)], %8212:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=180)], %8214:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=182)]) {
            linalg.CPU.LinearOp <name="model.layers.3.self_attn.q_proj">(%8199:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=168)]) -> (%8200:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=174)])
            linalg.CPU.LinearOp <name="model.layers.3.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=168), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=171), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=170))] (%8199:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=168)]) -> (%8201:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=171)])
            linalg.CPU.LinearOp <name="model.layers.3.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=168), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=173), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=172))] (%8199:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=168)]) -> (%8202:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=173)])
            linalg.CPU.ViewOp <name="model.layers.3.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=174), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=174), )] (%8200:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=174)]) -> (%8200:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=174)])
            linalg.CPU.TransposeOp <name="model.layers.3.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=174), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=174), )] (%8200:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=174)]) -> (%8203:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=174)])
            linalg.CPU.ViewOp <name="model.layers.3.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=171), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=171), )] (%8201:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=171)]) -> (%8201:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=171)])
            linalg.CPU.TransposeOp <name="model.layers.3.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=171), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=171), )] (%8201:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=171)]) -> (%8204:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=171)])
            linalg.CPU.ViewOp <name="model.layers.3.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=173), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=173), )] (%8202:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=173)]) -> (%8202:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=173)])
            linalg.CPU.TransposeOp <name="model.layers.3.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=173), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=173), )] (%8202:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=173)]) -> (%8205:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=173)])
            linalg.CPU.RMSNormOp <name="model.layers.3.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=174), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=175), weight_weight:QuantSpec(Raw(type: Int16PerTensor), uuid=176))] (%8203:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=174)]) -> (%8206:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=175)])
            linalg.CPU.RMSNormOp <name="model.layers.3.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=171), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=177), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=178))] (%8204:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=171)]) -> (%8207:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=177)])
            linalg.CPU.RoPEOp <name="model.layers.3.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=175), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=175), )] (%8206:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=175)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%8208:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=175)])
            linalg.CPU.RoPEOp <name="model.layers.3.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=177), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=177), )] (%8207:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=177)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%8209:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=177)])
            linalg.CPU.CastTypeOp <name="model.layers.3.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=177), outputs_0:QuantSpec(Raw(type: Float16), uuid=179), )] (%8209:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=177)]) -> (%8210:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=179)])
            linalg.CPU.CastTypeOp <name="model.layers.3.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=179), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=180), )] (%8210:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=179)]) -> (%8211:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=180)])
            linalg.CPU.TransposeOp <name="model.layers.3.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=180), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=180), )] (%8211:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=180)]) -> (%8212:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=180)])
            linalg.CPU.CastTypeOp <name="model.layers.3.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=173), outputs_0:QuantSpec(Raw(type: Float16), uuid=181), )] (%8205:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=173)]) -> (%8213:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=181)])
            linalg.CPU.CastTypeOp <name="model.layers.3.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=181), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=182), )] (%8213:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=181)]) -> (%8214:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=182)])
            linalg.CPU.ConcatOp <name="model.layers.3.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=6), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=180), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=6), )] (%8021:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=6)], %8212:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=180)]) -> (%8215:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=6)])
            linalg.CPU.ConcatOp <name="model.layers.3.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=34), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=182), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=34), )] (%8022:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=34)], %8214:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=182)]) -> (%8216:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=34)])
            linalg.CPU.RepeatOp <name="model.layers.3.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=6), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=6), )] (%8215:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=6)]) -> (%8217:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=6)])
            linalg.CPU.RepeatOp <name="model.layers.3.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=34), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=34), )] (%8216:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=34)]) -> (%8218:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=34)])
            linalg.CPU.MatMulOp <name="model.layers.3.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=175), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=6), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=183), )] (%8208:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=175)], %8217:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=6)]) -> (%8219:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=183)])
            linalg.CPU.MulOp <name="model.layers.3.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=183), inputs_1:QuantSpec(Raw(type: Float32), uuid=184), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=183), )] (%8219:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=183)], %8220:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=184), constant:[0.088388346]]) -> (%8221:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=183)])
            linalg.CPU.ReduceMinOp <name="model.layers.3.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=183), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=185), )] (%8221:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=183)]) -> (%8222:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=185)])
            linalg.CPU.AddOp <name="model.layers.3.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=185), inputs_1:QuantSpec(Raw(type: Int16), uuid=186), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=185), )] (%8222:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=185)], %8223:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=186), constant:[-20]]) -> (%8224:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=185)])
            linalg.CPU.EqualOp <name="model.layers.3.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=187), outputs_0:QuantSpec(Raw(type: UInt8), uuid=188), )] (%8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8225:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=187), constant:[0]]) -> (%8226:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=188)])
            linalg.CPU.WhereOp <name="model.layers.3.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=188), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=183), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=185), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=185), )] (%8226:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=188)], %8221:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=183)], %8224:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=185)]) -> (%8227:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=185)])
            linalg.CPU.SoftmaxOp <name="model.layers.3.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=185), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=189), )] (%8227:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=185)]) -> (%8228:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=189)])
            linalg.CPU.MatMulOp <name="model.layers.3.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=189), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=34), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=190), )] (%8228:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=189)], %8218:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=34)]) -> (%8229:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=190)])
            linalg.CPU.TransposeOp <name="model.layers.3.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=190), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=190), )] (%8229:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=190)]) -> (%8230:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=190)])
            linalg.CPU.ViewOp <name="model.layers.3.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=190), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=190), )] (%8230:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=190)]) -> (%8230:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=190)])
            linalg.CPU.LinearOp <name="model.layers.3.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=190), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=192), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=191))] (%8230:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=190)]) -> (%8231:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=192)])
            cf.ReturnOp (%8231:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=192)], %8212:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=180)], %8214:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=182)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.3.mlp <CPU> [using_qnn:true, symbol:model.layers.3.mlp] {
        (%8233:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=193)]) -> (%8238:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=201)]) {
            linalg.CPU.LinearOp <name="model.layers.3.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=193), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=196), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=195))] (%8233:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=193)]) -> (%8234:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=196)])
            linalg.CPU.SiLUOp <name="model.layers.3.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=196), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=197), )] (%8234:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=196)]) -> (%8235:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=197)])
            linalg.CPU.LinearOp <name="model.layers.3.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=193), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=199), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=198))] (%8233:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=193)]) -> (%8236:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=199)])
            linalg.CPU.MulOp <name="model.layers.3.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=197), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=199), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=197), )] (%8235:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=197)], %8236:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=199)]) -> (%8237:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=197)])
            linalg.CPU.LinearOp <name="model.layers.3.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=197), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=201), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=200))] (%8237:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=197)]) -> (%8238:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=201)])
            cf.ReturnOp (%8238:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=201)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.4 <CPU> [using_qnn:true, symbol:model.layers.4] {
        (%8239:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=201)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8023:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=7)], %8024:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=35)]) -> (%8280:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=235)], %8253:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=214)], %8255:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=216)]) {
            linalg.CPU.RMSNormOp <name="model.layers.4.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=201), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=202), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=203))] (%8239:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=201)]) -> (%8240:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=202)])
            graph.CallGraphOp @model.layers.4.self_attn (%8240:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=202)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8023:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=7)], %8024:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=35)]) -> (%8272:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=226)], %8253:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=214)], %8255:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=216)])
            linalg.CPU.AddOp <name="model.layers.4.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=226), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=201), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=226), )] (%8272:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=226)], %8239:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=201)]) -> (%8273:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=226)])
            linalg.CPU.RMSNormOp <name="model.layers.4.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=226), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=227), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=228))] (%8273:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=226)]) -> (%8274:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=227)])
            graph.CallGraphOp @model.layers.4.mlp (%8274:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=227)]) -> (%8279:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=235)])
            linalg.CPU.AddOp <name="model.layers.4.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=235), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=226), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=235), )] (%8279:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=235)], %8273:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=226)]) -> (%8280:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=235)])
            cf.ReturnOp (%8280:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=235)], %8253:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=214)], %8255:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=216)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.4.self_attn <CPU> [using_qnn:true, symbol:model.layers.4.self_attn] {
        (%8240:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=202)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8023:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=7)], %8024:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=35)]) -> (%8272:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=226)], %8253:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=214)], %8255:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=216)]) {
            linalg.CPU.LinearOp <name="model.layers.4.self_attn.q_proj">(%8240:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=202)]) -> (%8241:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=208)])
            linalg.CPU.LinearOp <name="model.layers.4.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=202), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=205), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=204))] (%8240:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=202)]) -> (%8242:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=205)])
            linalg.CPU.LinearOp <name="model.layers.4.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=202), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=207), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=206))] (%8240:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=202)]) -> (%8243:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=207)])
            linalg.CPU.ViewOp <name="model.layers.4.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=208), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=208), )] (%8241:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=208)]) -> (%8241:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=208)])
            linalg.CPU.TransposeOp <name="model.layers.4.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=208), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=208), )] (%8241:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=208)]) -> (%8244:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=208)])
            linalg.CPU.ViewOp <name="model.layers.4.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=205), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=205), )] (%8242:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=205)]) -> (%8242:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=205)])
            linalg.CPU.TransposeOp <name="model.layers.4.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=205), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=205), )] (%8242:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=205)]) -> (%8245:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=205)])
            linalg.CPU.ViewOp <name="model.layers.4.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=207), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=207), )] (%8243:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=207)]) -> (%8243:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=207)])
            linalg.CPU.TransposeOp <name="model.layers.4.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=207), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=207), )] (%8243:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=207)]) -> (%8246:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=207)])
            linalg.CPU.RMSNormOp <name="model.layers.4.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=208), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=209), weight_weight:QuantSpec(Raw(type: Int16PerTensor), uuid=210))] (%8244:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=208)]) -> (%8247:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=209)])
            linalg.CPU.RMSNormOp <name="model.layers.4.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=205), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=211), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=212))] (%8245:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=205)]) -> (%8248:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=211)])
            linalg.CPU.RoPEOp <name="model.layers.4.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=209), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=209), )] (%8247:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=209)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%8249:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=209)])
            linalg.CPU.RoPEOp <name="model.layers.4.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=211), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=211), )] (%8248:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=211)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%8250:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=211)])
            linalg.CPU.CastTypeOp <name="model.layers.4.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=211), outputs_0:QuantSpec(Raw(type: Float16), uuid=213), )] (%8250:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=211)]) -> (%8251:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=213)])
            linalg.CPU.CastTypeOp <name="model.layers.4.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=213), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=214), )] (%8251:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=213)]) -> (%8252:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=214)])
            linalg.CPU.TransposeOp <name="model.layers.4.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=214), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=214), )] (%8252:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=214)]) -> (%8253:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=214)])
            linalg.CPU.CastTypeOp <name="model.layers.4.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=207), outputs_0:QuantSpec(Raw(type: Float16), uuid=215), )] (%8246:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=207)]) -> (%8254:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=215)])
            linalg.CPU.CastTypeOp <name="model.layers.4.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=215), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=216), )] (%8254:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=215)]) -> (%8255:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=216)])
            linalg.CPU.ConcatOp <name="model.layers.4.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=7), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=214), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=7), )] (%8023:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=7)], %8253:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=214)]) -> (%8256:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=7)])
            linalg.CPU.ConcatOp <name="model.layers.4.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=35), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=216), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=35), )] (%8024:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=35)], %8255:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=216)]) -> (%8257:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=35)])
            linalg.CPU.RepeatOp <name="model.layers.4.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=7), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=7), )] (%8256:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=7)]) -> (%8258:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=7)])
            linalg.CPU.RepeatOp <name="model.layers.4.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=35), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=35), )] (%8257:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=35)]) -> (%8259:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=35)])
            linalg.CPU.MatMulOp <name="model.layers.4.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=209), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=7), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=217), )] (%8249:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=209)], %8258:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=7)]) -> (%8260:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=217)])
            linalg.CPU.MulOp <name="model.layers.4.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=217), inputs_1:QuantSpec(Raw(type: Float32), uuid=218), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=217), )] (%8260:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=217)], %8261:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=218), constant:[0.088388346]]) -> (%8262:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=217)])
            linalg.CPU.ReduceMinOp <name="model.layers.4.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=217), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=219), )] (%8262:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=217)]) -> (%8263:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=219)])
            linalg.CPU.AddOp <name="model.layers.4.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=219), inputs_1:QuantSpec(Raw(type: Int16), uuid=220), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=219), )] (%8263:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=219)], %8264:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=220), constant:[-20]]) -> (%8265:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=219)])
            linalg.CPU.EqualOp <name="model.layers.4.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=221), outputs_0:QuantSpec(Raw(type: UInt8), uuid=222), )] (%8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8266:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=221), constant:[0]]) -> (%8267:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=222)])
            linalg.CPU.WhereOp <name="model.layers.4.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=222), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=217), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=219), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=219), )] (%8267:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=222)], %8262:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=217)], %8265:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=219)]) -> (%8268:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=219)])
            linalg.CPU.SoftmaxOp <name="model.layers.4.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=219), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=223), )] (%8268:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=219)]) -> (%8269:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=223)])
            linalg.CPU.MatMulOp <name="model.layers.4.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=223), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=35), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=224), )] (%8269:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=223)], %8259:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=35)]) -> (%8270:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=224)])
            linalg.CPU.TransposeOp <name="model.layers.4.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=224), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=224), )] (%8270:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=224)]) -> (%8271:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=224)])
            linalg.CPU.ViewOp <name="model.layers.4.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=224), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=224), )] (%8271:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=224)]) -> (%8271:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=224)])
            linalg.CPU.LinearOp <name="model.layers.4.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=224), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=226), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=225))] (%8271:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=224)]) -> (%8272:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=226)])
            cf.ReturnOp (%8272:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=226)], %8253:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=214)], %8255:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=216)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.4.mlp <CPU> [using_qnn:true, symbol:model.layers.4.mlp] {
        (%8274:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=227)]) -> (%8279:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=235)]) {
            linalg.CPU.LinearOp <name="model.layers.4.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=227), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=230), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=229))] (%8274:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=227)]) -> (%8275:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=230)])
            linalg.CPU.SiLUOp <name="model.layers.4.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=230), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=231), )] (%8275:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=230)]) -> (%8276:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=231)])
            linalg.CPU.LinearOp <name="model.layers.4.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=227), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=233), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=232))] (%8274:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=227)]) -> (%8277:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=233)])
            linalg.CPU.MulOp <name="model.layers.4.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=231), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=233), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=231), )] (%8276:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=231)], %8277:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=233)]) -> (%8278:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=231)])
            linalg.CPU.LinearOp <name="model.layers.4.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=231), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=235), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=234))] (%8278:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=231)]) -> (%8279:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=235)])
            cf.ReturnOp (%8279:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=235)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.5 <CPU> [using_qnn:true, symbol:model.layers.5] {
        (%8280:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=235)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8025:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=8)], %8026:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=36)]) -> (%8321:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=269)], %8294:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=248)], %8296:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=250)]) {
            linalg.CPU.RMSNormOp <name="model.layers.5.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=235), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=236), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=237))] (%8280:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=235)]) -> (%8281:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=236)])
            graph.CallGraphOp @model.layers.5.self_attn (%8281:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=236)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8025:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=8)], %8026:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=36)]) -> (%8313:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=260)], %8294:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=248)], %8296:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=250)])
            linalg.CPU.AddOp <name="model.layers.5.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=260), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=235), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=260), )] (%8313:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=260)], %8280:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=235)]) -> (%8314:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=260)])
            linalg.CPU.RMSNormOp <name="model.layers.5.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=260), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=261), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=262))] (%8314:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=260)]) -> (%8315:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=261)])
            graph.CallGraphOp @model.layers.5.mlp (%8315:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=261)]) -> (%8320:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=269)])
            linalg.CPU.AddOp <name="model.layers.5.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=269), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=260), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=269), )] (%8320:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=269)], %8314:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=260)]) -> (%8321:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=269)])
            cf.ReturnOp (%8321:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=269)], %8294:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=248)], %8296:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=250)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.5.self_attn <CPU> [using_qnn:true, symbol:model.layers.5.self_attn] {
        (%8281:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=236)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8025:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=8)], %8026:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=36)]) -> (%8313:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=260)], %8294:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=248)], %8296:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=250)]) {
            linalg.CPU.LinearOp <name="model.layers.5.self_attn.q_proj">(%8281:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=236)]) -> (%8282:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=242)])
            linalg.CPU.LinearOp <name="model.layers.5.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=236), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=239), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=238))] (%8281:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=236)]) -> (%8283:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=239)])
            linalg.CPU.LinearOp <name="model.layers.5.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=236), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=241), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=240))] (%8281:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=236)]) -> (%8284:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=241)])
            linalg.CPU.ViewOp <name="model.layers.5.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=242), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=242), )] (%8282:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=242)]) -> (%8282:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=242)])
            linalg.CPU.TransposeOp <name="model.layers.5.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=242), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=242), )] (%8282:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=242)]) -> (%8285:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=242)])
            linalg.CPU.ViewOp <name="model.layers.5.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=239), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=239), )] (%8283:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=239)]) -> (%8283:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=239)])
            linalg.CPU.TransposeOp <name="model.layers.5.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=239), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=239), )] (%8283:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=239)]) -> (%8286:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=239)])
            linalg.CPU.ViewOp <name="model.layers.5.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=241), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=241), )] (%8284:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=241)]) -> (%8284:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=241)])
            linalg.CPU.TransposeOp <name="model.layers.5.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=241), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=241), )] (%8284:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=241)]) -> (%8287:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=241)])
            linalg.CPU.RMSNormOp <name="model.layers.5.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=242), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=243), weight_weight:QuantSpec(Raw(type: Int16PerTensor), uuid=244))] (%8285:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=242)]) -> (%8288:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=243)])
            linalg.CPU.RMSNormOp <name="model.layers.5.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=239), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=245), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=246))] (%8286:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=239)]) -> (%8289:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=245)])
            linalg.CPU.RoPEOp <name="model.layers.5.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=243), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=243), )] (%8288:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=243)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%8290:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=243)])
            linalg.CPU.RoPEOp <name="model.layers.5.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=245), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=245), )] (%8289:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=245)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%8291:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=245)])
            linalg.CPU.CastTypeOp <name="model.layers.5.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=245), outputs_0:QuantSpec(Raw(type: Float16), uuid=247), )] (%8291:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=245)]) -> (%8292:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=247)])
            linalg.CPU.CastTypeOp <name="model.layers.5.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=247), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=248), )] (%8292:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=247)]) -> (%8293:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=248)])
            linalg.CPU.TransposeOp <name="model.layers.5.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=248), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=248), )] (%8293:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=248)]) -> (%8294:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=248)])
            linalg.CPU.CastTypeOp <name="model.layers.5.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=241), outputs_0:QuantSpec(Raw(type: Float16), uuid=249), )] (%8287:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=241)]) -> (%8295:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=249)])
            linalg.CPU.CastTypeOp <name="model.layers.5.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=249), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=250), )] (%8295:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=249)]) -> (%8296:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=250)])
            linalg.CPU.ConcatOp <name="model.layers.5.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=8), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=248), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=8), )] (%8025:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=8)], %8294:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=248)]) -> (%8297:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=8)])
            linalg.CPU.ConcatOp <name="model.layers.5.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=36), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=250), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=36), )] (%8026:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=36)], %8296:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=250)]) -> (%8298:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=36)])
            linalg.CPU.RepeatOp <name="model.layers.5.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=8), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=8), )] (%8297:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=8)]) -> (%8299:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=8)])
            linalg.CPU.RepeatOp <name="model.layers.5.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=36), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=36), )] (%8298:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=36)]) -> (%8300:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=36)])
            linalg.CPU.MatMulOp <name="model.layers.5.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=243), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=8), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=251), )] (%8290:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=243)], %8299:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=8)]) -> (%8301:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=251)])
            linalg.CPU.MulOp <name="model.layers.5.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=251), inputs_1:QuantSpec(Raw(type: Float32), uuid=252), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=251), )] (%8301:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=251)], %8302:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=252), constant:[0.088388346]]) -> (%8303:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=251)])
            linalg.CPU.ReduceMinOp <name="model.layers.5.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=251), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=253), )] (%8303:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=251)]) -> (%8304:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=253)])
            linalg.CPU.AddOp <name="model.layers.5.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=253), inputs_1:QuantSpec(Raw(type: Int16), uuid=254), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=253), )] (%8304:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=253)], %8305:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=254), constant:[-20]]) -> (%8306:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=253)])
            linalg.CPU.EqualOp <name="model.layers.5.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=255), outputs_0:QuantSpec(Raw(type: UInt8), uuid=256), )] (%8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8307:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=255), constant:[0]]) -> (%8308:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=256)])
            linalg.CPU.WhereOp <name="model.layers.5.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=256), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=251), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=253), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=253), )] (%8308:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=256)], %8303:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=251)], %8306:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=253)]) -> (%8309:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=253)])
            linalg.CPU.SoftmaxOp <name="model.layers.5.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=253), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=257), )] (%8309:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=253)]) -> (%8310:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=257)])
            linalg.CPU.MatMulOp <name="model.layers.5.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=257), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=36), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=258), )] (%8310:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=257)], %8300:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=36)]) -> (%8311:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=258)])
            linalg.CPU.TransposeOp <name="model.layers.5.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=258), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=258), )] (%8311:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=258)]) -> (%8312:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=258)])
            linalg.CPU.ViewOp <name="model.layers.5.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=258), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=258), )] (%8312:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=258)]) -> (%8312:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=258)])
            linalg.CPU.LinearOp <name="model.layers.5.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=258), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=260), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=259))] (%8312:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=258)]) -> (%8313:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=260)])
            cf.ReturnOp (%8313:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=260)], %8294:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=248)], %8296:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=250)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.5.mlp <CPU> [using_qnn:true, symbol:model.layers.5.mlp] {
        (%8315:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=261)]) -> (%8320:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=269)]) {
            linalg.CPU.LinearOp <name="model.layers.5.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=261), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=264), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=263))] (%8315:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=261)]) -> (%8316:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=264)])
            linalg.CPU.SiLUOp <name="model.layers.5.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=264), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=265), )] (%8316:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=264)]) -> (%8317:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=265)])
            linalg.CPU.LinearOp <name="model.layers.5.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=261), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=267), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=266))] (%8315:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=261)]) -> (%8318:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=267)])
            linalg.CPU.MulOp <name="model.layers.5.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=265), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=267), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=265), )] (%8317:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=265)], %8318:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=267)]) -> (%8319:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=265)])
            linalg.CPU.LinearOp <name="model.layers.5.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=265), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=269), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=268))] (%8319:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=265)]) -> (%8320:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=269)])
            cf.ReturnOp (%8320:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=269)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.6 <CPU> [using_qnn:true, symbol:model.layers.6] {
        (%8321:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=269)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8027:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=9)], %8028:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=37)]) -> (%8362:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=303)], %8335:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=282)], %8337:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=284)]) {
            linalg.CPU.RMSNormOp <name="model.layers.6.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=269), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=270), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=271))] (%8321:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=269)]) -> (%8322:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=270)])
            graph.CallGraphOp @model.layers.6.self_attn (%8322:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=270)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8027:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=9)], %8028:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=37)]) -> (%8354:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=294)], %8335:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=282)], %8337:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=284)])
            linalg.CPU.AddOp <name="model.layers.6.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=294), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=269), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=294), )] (%8354:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=294)], %8321:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=269)]) -> (%8355:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=294)])
            linalg.CPU.RMSNormOp <name="model.layers.6.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=294), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=295), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=296))] (%8355:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=294)]) -> (%8356:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=295)])
            graph.CallGraphOp @model.layers.6.mlp (%8356:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=295)]) -> (%8361:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=303)])
            linalg.CPU.AddOp <name="model.layers.6.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=303), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=294), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=303), )] (%8361:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=303)], %8355:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=294)]) -> (%8362:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=303)])
            cf.ReturnOp (%8362:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=303)], %8335:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=282)], %8337:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=284)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.6.self_attn <CPU> [using_qnn:true, symbol:model.layers.6.self_attn] {
        (%8322:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=270)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8027:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=9)], %8028:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=37)]) -> (%8354:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=294)], %8335:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=282)], %8337:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=284)]) {
            linalg.CPU.LinearOp <name="model.layers.6.self_attn.q_proj">(%8322:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=270)]) -> (%8323:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=276)])
            linalg.CPU.LinearOp <name="model.layers.6.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=270), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=273), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=272))] (%8322:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=270)]) -> (%8324:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=273)])
            linalg.CPU.LinearOp <name="model.layers.6.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=270), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=275), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=274))] (%8322:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=270)]) -> (%8325:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=275)])
            linalg.CPU.ViewOp <name="model.layers.6.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=276), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=276), )] (%8323:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=276)]) -> (%8323:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=276)])
            linalg.CPU.TransposeOp <name="model.layers.6.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=276), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=276), )] (%8323:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=276)]) -> (%8326:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=276)])
            linalg.CPU.ViewOp <name="model.layers.6.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=273), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=273), )] (%8324:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=273)]) -> (%8324:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=273)])
            linalg.CPU.TransposeOp <name="model.layers.6.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=273), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=273), )] (%8324:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=273)]) -> (%8327:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=273)])
            linalg.CPU.ViewOp <name="model.layers.6.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=275), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=275), )] (%8325:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=275)]) -> (%8325:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=275)])
            linalg.CPU.TransposeOp <name="model.layers.6.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=275), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=275), )] (%8325:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=275)]) -> (%8328:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=275)])
            linalg.CPU.RMSNormOp <name="model.layers.6.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=276), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=277), weight_weight:QuantSpec(Raw(type: Int16PerTensor), uuid=278))] (%8326:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=276)]) -> (%8329:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=277)])
            linalg.CPU.RMSNormOp <name="model.layers.6.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=273), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=279), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=280))] (%8327:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=273)]) -> (%8330:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=279)])
            linalg.CPU.RoPEOp <name="model.layers.6.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=277), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=277), )] (%8329:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=277)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%8331:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=277)])
            linalg.CPU.RoPEOp <name="model.layers.6.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=279), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=279), )] (%8330:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=279)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%8332:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=279)])
            linalg.CPU.CastTypeOp <name="model.layers.6.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=279), outputs_0:QuantSpec(Raw(type: Float16), uuid=281), )] (%8332:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=279)]) -> (%8333:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=281)])
            linalg.CPU.CastTypeOp <name="model.layers.6.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=281), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=282), )] (%8333:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=281)]) -> (%8334:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=282)])
            linalg.CPU.TransposeOp <name="model.layers.6.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=282), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=282), )] (%8334:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=282)]) -> (%8335:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=282)])
            linalg.CPU.CastTypeOp <name="model.layers.6.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=275), outputs_0:QuantSpec(Raw(type: Float16), uuid=283), )] (%8328:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=275)]) -> (%8336:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=283)])
            linalg.CPU.CastTypeOp <name="model.layers.6.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=283), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=284), )] (%8336:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=283)]) -> (%8337:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=284)])
            linalg.CPU.ConcatOp <name="model.layers.6.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=9), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=282), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=9), )] (%8027:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=9)], %8335:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=282)]) -> (%8338:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=9)])
            linalg.CPU.ConcatOp <name="model.layers.6.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=37), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=284), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=37), )] (%8028:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=37)], %8337:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=284)]) -> (%8339:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=37)])
            linalg.CPU.RepeatOp <name="model.layers.6.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=9), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=9), )] (%8338:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=9)]) -> (%8340:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=9)])
            linalg.CPU.RepeatOp <name="model.layers.6.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=37), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=37), )] (%8339:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=37)]) -> (%8341:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=37)])
            linalg.CPU.MatMulOp <name="model.layers.6.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=277), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=9), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=285), )] (%8331:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=277)], %8340:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=9)]) -> (%8342:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=285)])
            linalg.CPU.MulOp <name="model.layers.6.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=285), inputs_1:QuantSpec(Raw(type: Float32), uuid=286), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=285), )] (%8342:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=285)], %8343:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=286), constant:[0.088388346]]) -> (%8344:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=285)])
            linalg.CPU.ReduceMinOp <name="model.layers.6.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=285), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=287), )] (%8344:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=285)]) -> (%8345:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=287)])
            linalg.CPU.AddOp <name="model.layers.6.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=287), inputs_1:QuantSpec(Raw(type: Int16), uuid=288), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=287), )] (%8345:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=287)], %8346:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=288), constant:[-20]]) -> (%8347:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=287)])
            linalg.CPU.EqualOp <name="model.layers.6.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=289), outputs_0:QuantSpec(Raw(type: UInt8), uuid=290), )] (%8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8348:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=289), constant:[0]]) -> (%8349:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=290)])
            linalg.CPU.WhereOp <name="model.layers.6.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=290), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=285), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=287), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=287), )] (%8349:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=290)], %8344:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=285)], %8347:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=287)]) -> (%8350:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=287)])
            linalg.CPU.SoftmaxOp <name="model.layers.6.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=287), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=291), )] (%8350:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=287)]) -> (%8351:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=291)])
            linalg.CPU.MatMulOp <name="model.layers.6.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=291), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=37), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=292), )] (%8351:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=291)], %8341:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=37)]) -> (%8352:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=292)])
            linalg.CPU.TransposeOp <name="model.layers.6.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=292), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=292), )] (%8352:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=292)]) -> (%8353:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=292)])
            linalg.CPU.ViewOp <name="model.layers.6.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=292), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=292), )] (%8353:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=292)]) -> (%8353:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=292)])
            linalg.CPU.LinearOp <name="model.layers.6.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=292), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=294), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=293))] (%8353:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=292)]) -> (%8354:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=294)])
            cf.ReturnOp (%8354:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=294)], %8335:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=282)], %8337:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=284)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.6.mlp <CPU> [using_qnn:true, symbol:model.layers.6.mlp] {
        (%8356:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=295)]) -> (%8361:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=303)]) {
            linalg.CPU.LinearOp <name="model.layers.6.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=295), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=298), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=297))] (%8356:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=295)]) -> (%8357:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=298)])
            linalg.CPU.SiLUOp <name="model.layers.6.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=298), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=299), )] (%8357:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=298)]) -> (%8358:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=299)])
            linalg.CPU.LinearOp <name="model.layers.6.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=295), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=301), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=300))] (%8356:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=295)]) -> (%8359:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=301)])
            linalg.CPU.MulOp <name="model.layers.6.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=299), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=301), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=299), )] (%8358:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=299)], %8359:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=301)]) -> (%8360:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=299)])
            linalg.CPU.LinearOp <name="model.layers.6.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=299), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=303), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=302))] (%8360:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=299)]) -> (%8361:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=303)])
            cf.ReturnOp (%8361:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=303)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.7 <CPU> [using_qnn:true, symbol:model.layers.7] {
        (%8362:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=303)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8029:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=10)], %8030:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=38)]) -> (%8403:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=337)], %8376:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=316)], %8378:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=318)]) {
            linalg.CPU.RMSNormOp <name="model.layers.7.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=303), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=304), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=305))] (%8362:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=303)]) -> (%8363:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=304)])
            graph.CallGraphOp @model.layers.7.self_attn (%8363:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=304)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8029:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=10)], %8030:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=38)]) -> (%8395:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=328)], %8376:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=316)], %8378:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=318)])
            linalg.CPU.AddOp <name="model.layers.7.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=328), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=303), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=328), )] (%8395:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=328)], %8362:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=303)]) -> (%8396:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=328)])
            linalg.CPU.RMSNormOp <name="model.layers.7.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=328), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=329), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=330))] (%8396:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=328)]) -> (%8397:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=329)])
            graph.CallGraphOp @model.layers.7.mlp (%8397:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=329)]) -> (%8402:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=337)])
            linalg.CPU.AddOp <name="model.layers.7.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=337), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=328), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=337), )] (%8402:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=337)], %8396:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=328)]) -> (%8403:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=337)])
            cf.ReturnOp (%8403:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=337)], %8376:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=316)], %8378:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=318)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.7.self_attn <CPU> [using_qnn:true, symbol:model.layers.7.self_attn] {
        (%8363:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=304)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8029:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=10)], %8030:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=38)]) -> (%8395:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=328)], %8376:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=316)], %8378:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=318)]) {
            linalg.CPU.LinearOp <name="model.layers.7.self_attn.q_proj">(%8363:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=304)]) -> (%8364:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=310)])
            linalg.CPU.LinearOp <name="model.layers.7.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=304), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=307), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=306))] (%8363:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=304)]) -> (%8365:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=307)])
            linalg.CPU.LinearOp <name="model.layers.7.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=304), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=309), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=308))] (%8363:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=304)]) -> (%8366:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=309)])
            linalg.CPU.ViewOp <name="model.layers.7.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=310), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=310), )] (%8364:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=310)]) -> (%8364:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=310)])
            linalg.CPU.TransposeOp <name="model.layers.7.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=310), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=310), )] (%8364:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=310)]) -> (%8367:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=310)])
            linalg.CPU.ViewOp <name="model.layers.7.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=307), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=307), )] (%8365:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=307)]) -> (%8365:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=307)])
            linalg.CPU.TransposeOp <name="model.layers.7.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=307), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=307), )] (%8365:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=307)]) -> (%8368:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=307)])
            linalg.CPU.ViewOp <name="model.layers.7.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=309), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=309), )] (%8366:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=309)]) -> (%8366:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=309)])
            linalg.CPU.TransposeOp <name="model.layers.7.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=309), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=309), )] (%8366:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=309)]) -> (%8369:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=309)])
            linalg.CPU.RMSNormOp <name="model.layers.7.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=310), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=311), weight_weight:QuantSpec(Raw(type: Int16PerTensor), uuid=312))] (%8367:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=310)]) -> (%8370:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=311)])
            linalg.CPU.RMSNormOp <name="model.layers.7.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=307), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=313), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=314))] (%8368:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=307)]) -> (%8371:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=313)])
            linalg.CPU.RoPEOp <name="model.layers.7.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=311), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=311), )] (%8370:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=311)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%8372:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=311)])
            linalg.CPU.RoPEOp <name="model.layers.7.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=313), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=313), )] (%8371:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=313)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%8373:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=313)])
            linalg.CPU.CastTypeOp <name="model.layers.7.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=313), outputs_0:QuantSpec(Raw(type: Float16), uuid=315), )] (%8373:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=313)]) -> (%8374:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=315)])
            linalg.CPU.CastTypeOp <name="model.layers.7.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=315), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=316), )] (%8374:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=315)]) -> (%8375:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=316)])
            linalg.CPU.TransposeOp <name="model.layers.7.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=316), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=316), )] (%8375:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=316)]) -> (%8376:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=316)])
            linalg.CPU.CastTypeOp <name="model.layers.7.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=309), outputs_0:QuantSpec(Raw(type: Float16), uuid=317), )] (%8369:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=309)]) -> (%8377:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=317)])
            linalg.CPU.CastTypeOp <name="model.layers.7.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=317), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=318), )] (%8377:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=317)]) -> (%8378:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=318)])
            linalg.CPU.ConcatOp <name="model.layers.7.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=10), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=316), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=10), )] (%8029:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=10)], %8376:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=316)]) -> (%8379:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=10)])
            linalg.CPU.ConcatOp <name="model.layers.7.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=38), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=318), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=38), )] (%8030:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=38)], %8378:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=318)]) -> (%8380:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=38)])
            linalg.CPU.RepeatOp <name="model.layers.7.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=10), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=10), )] (%8379:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=10)]) -> (%8381:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=10)])
            linalg.CPU.RepeatOp <name="model.layers.7.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=38), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=38), )] (%8380:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=38)]) -> (%8382:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=38)])
            linalg.CPU.MatMulOp <name="model.layers.7.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=311), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=10), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=319), )] (%8372:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=311)], %8381:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=10)]) -> (%8383:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=319)])
            linalg.CPU.MulOp <name="model.layers.7.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=319), inputs_1:QuantSpec(Raw(type: Float32), uuid=320), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=319), )] (%8383:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=319)], %8384:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=320), constant:[0.088388346]]) -> (%8385:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=319)])
            linalg.CPU.ReduceMinOp <name="model.layers.7.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=319), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=321), )] (%8385:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=319)]) -> (%8386:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=321)])
            linalg.CPU.AddOp <name="model.layers.7.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=321), inputs_1:QuantSpec(Raw(type: Int16), uuid=322), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=321), )] (%8386:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=321)], %8387:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=322), constant:[-20]]) -> (%8388:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=321)])
            linalg.CPU.EqualOp <name="model.layers.7.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=323), outputs_0:QuantSpec(Raw(type: UInt8), uuid=324), )] (%8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8389:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=323), constant:[0]]) -> (%8390:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=324)])
            linalg.CPU.WhereOp <name="model.layers.7.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=324), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=319), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=321), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=321), )] (%8390:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=324)], %8385:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=319)], %8388:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=321)]) -> (%8391:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=321)])
            linalg.CPU.SoftmaxOp <name="model.layers.7.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=321), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=325), )] (%8391:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=321)]) -> (%8392:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=325)])
            linalg.CPU.MatMulOp <name="model.layers.7.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=325), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=38), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=326), )] (%8392:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=325)], %8382:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=38)]) -> (%8393:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=326)])
            linalg.CPU.TransposeOp <name="model.layers.7.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=326), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=326), )] (%8393:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=326)]) -> (%8394:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=326)])
            linalg.CPU.ViewOp <name="model.layers.7.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=326), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=326), )] (%8394:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=326)]) -> (%8394:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=326)])
            linalg.CPU.LinearOp <name="model.layers.7.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=326), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=328), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=327))] (%8394:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=326)]) -> (%8395:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=328)])
            cf.ReturnOp (%8395:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=328)], %8376:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=316)], %8378:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=318)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.7.mlp <CPU> [using_qnn:true, symbol:model.layers.7.mlp] {
        (%8397:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=329)]) -> (%8402:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=337)]) {
            linalg.CPU.LinearOp <name="model.layers.7.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=329), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=332), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=331))] (%8397:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=329)]) -> (%8398:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=332)])
            linalg.CPU.SiLUOp <name="model.layers.7.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=332), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=333), )] (%8398:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=332)]) -> (%8399:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=333)])
            linalg.CPU.LinearOp <name="model.layers.7.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=329), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=335), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=334))] (%8397:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=329)]) -> (%8400:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=335)])
            linalg.CPU.MulOp <name="model.layers.7.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=333), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=335), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=333), )] (%8399:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=333)], %8400:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=335)]) -> (%8401:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=333)])
            linalg.CPU.LinearOp <name="model.layers.7.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=333), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=337), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=336))] (%8401:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=333)]) -> (%8402:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=337)])
            cf.ReturnOp (%8402:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=337)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.8 <CPU> [using_qnn:true, symbol:model.layers.8] {
        (%8403:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=337)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8031:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=11)], %8032:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=39)]) -> (%8444:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=371)], %8417:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=350)], %8419:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=352)]) {
            linalg.CPU.RMSNormOp <name="model.layers.8.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=337), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=338), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=339))] (%8403:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=337)]) -> (%8404:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=338)])
            graph.CallGraphOp @model.layers.8.self_attn (%8404:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=338)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8031:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=11)], %8032:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=39)]) -> (%8436:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=362)], %8417:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=350)], %8419:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=352)])
            linalg.CPU.AddOp <name="model.layers.8.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=362), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=337), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=362), )] (%8436:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=362)], %8403:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=337)]) -> (%8437:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=362)])
            linalg.CPU.RMSNormOp <name="model.layers.8.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=362), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=363), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=364))] (%8437:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=362)]) -> (%8438:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=363)])
            graph.CallGraphOp @model.layers.8.mlp (%8438:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=363)]) -> (%8443:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=371)])
            linalg.CPU.AddOp <name="model.layers.8.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=371), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=362), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=371), )] (%8443:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=371)], %8437:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=362)]) -> (%8444:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=371)])
            cf.ReturnOp (%8444:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=371)], %8417:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=350)], %8419:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=352)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.8.self_attn <CPU> [using_qnn:true, symbol:model.layers.8.self_attn] {
        (%8404:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=338)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8031:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=11)], %8032:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=39)]) -> (%8436:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=362)], %8417:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=350)], %8419:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=352)]) {
            linalg.CPU.LinearOp <name="model.layers.8.self_attn.q_proj">(%8404:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=338)]) -> (%8405:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=344)])
            linalg.CPU.LinearOp <name="model.layers.8.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=338), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=341), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=340))] (%8404:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=338)]) -> (%8406:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=341)])
            linalg.CPU.LinearOp <name="model.layers.8.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=338), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=343), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=342))] (%8404:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=338)]) -> (%8407:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=343)])
            linalg.CPU.ViewOp <name="model.layers.8.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=344), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=344), )] (%8405:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=344)]) -> (%8405:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=344)])
            linalg.CPU.TransposeOp <name="model.layers.8.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=344), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=344), )] (%8405:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=344)]) -> (%8408:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=344)])
            linalg.CPU.ViewOp <name="model.layers.8.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=341), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=341), )] (%8406:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=341)]) -> (%8406:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=341)])
            linalg.CPU.TransposeOp <name="model.layers.8.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=341), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=341), )] (%8406:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=341)]) -> (%8409:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=341)])
            linalg.CPU.ViewOp <name="model.layers.8.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=343), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=343), )] (%8407:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=343)]) -> (%8407:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=343)])
            linalg.CPU.TransposeOp <name="model.layers.8.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=343), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=343), )] (%8407:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=343)]) -> (%8410:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=343)])
            linalg.CPU.RMSNormOp <name="model.layers.8.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=344), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=345), weight_weight:QuantSpec(Raw(type: Int16PerTensor), uuid=346))] (%8408:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=344)]) -> (%8411:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=345)])
            linalg.CPU.RMSNormOp <name="model.layers.8.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=341), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=347), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=348))] (%8409:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=341)]) -> (%8412:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=347)])
            linalg.CPU.RoPEOp <name="model.layers.8.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=345), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=345), )] (%8411:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=345)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%8413:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=345)])
            linalg.CPU.RoPEOp <name="model.layers.8.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=347), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=347), )] (%8412:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=347)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%8414:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=347)])
            linalg.CPU.CastTypeOp <name="model.layers.8.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=347), outputs_0:QuantSpec(Raw(type: Float16), uuid=349), )] (%8414:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=347)]) -> (%8415:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=349)])
            linalg.CPU.CastTypeOp <name="model.layers.8.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=349), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=350), )] (%8415:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=349)]) -> (%8416:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=350)])
            linalg.CPU.TransposeOp <name="model.layers.8.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=350), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=350), )] (%8416:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=350)]) -> (%8417:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=350)])
            linalg.CPU.CastTypeOp <name="model.layers.8.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=343), outputs_0:QuantSpec(Raw(type: Float16), uuid=351), )] (%8410:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=343)]) -> (%8418:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=351)])
            linalg.CPU.CastTypeOp <name="model.layers.8.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=351), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=352), )] (%8418:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=351)]) -> (%8419:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=352)])
            linalg.CPU.ConcatOp <name="model.layers.8.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=11), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=350), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=11), )] (%8031:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=11)], %8417:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=350)]) -> (%8420:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=11)])
            linalg.CPU.ConcatOp <name="model.layers.8.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=39), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=352), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=39), )] (%8032:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=39)], %8419:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=352)]) -> (%8421:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=39)])
            linalg.CPU.RepeatOp <name="model.layers.8.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=11), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=11), )] (%8420:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=11)]) -> (%8422:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=11)])
            linalg.CPU.RepeatOp <name="model.layers.8.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=39), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=39), )] (%8421:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=39)]) -> (%8423:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=39)])
            linalg.CPU.MatMulOp <name="model.layers.8.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=345), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=11), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=353), )] (%8413:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=345)], %8422:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=11)]) -> (%8424:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=353)])
            linalg.CPU.MulOp <name="model.layers.8.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=353), inputs_1:QuantSpec(Raw(type: Float32), uuid=354), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=353), )] (%8424:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=353)], %8425:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=354), constant:[0.088388346]]) -> (%8426:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=353)])
            linalg.CPU.ReduceMinOp <name="model.layers.8.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=353), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=355), )] (%8426:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=353)]) -> (%8427:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=355)])
            linalg.CPU.AddOp <name="model.layers.8.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=355), inputs_1:QuantSpec(Raw(type: Int16), uuid=356), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=355), )] (%8427:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=355)], %8428:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=356), constant:[-20]]) -> (%8429:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=355)])
            linalg.CPU.EqualOp <name="model.layers.8.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=357), outputs_0:QuantSpec(Raw(type: UInt8), uuid=358), )] (%8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8430:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=357), constant:[1]]) -> (%8431:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=358)])
            linalg.CPU.WhereOp <name="model.layers.8.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=358), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=353), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=355), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=355), )] (%8431:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=358)], %8426:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=353)], %8429:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=355)]) -> (%8432:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=355)])
            linalg.CPU.SoftmaxOp <name="model.layers.8.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=355), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=359), )] (%8432:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=355)]) -> (%8433:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=359)])
            linalg.CPU.MatMulOp <name="model.layers.8.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=359), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=39), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=360), )] (%8433:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=359)], %8423:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=39)]) -> (%8434:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=360)])
            linalg.CPU.TransposeOp <name="model.layers.8.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=360), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=360), )] (%8434:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=360)]) -> (%8435:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=360)])
            linalg.CPU.ViewOp <name="model.layers.8.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=360), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=360), )] (%8435:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=360)]) -> (%8435:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=360)])
            linalg.CPU.LinearOp <name="model.layers.8.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=360), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=362), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=361))] (%8435:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=360)]) -> (%8436:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=362)])
            cf.ReturnOp (%8436:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=362)], %8417:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=350)], %8419:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=352)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.8.mlp <CPU> [using_qnn:true, symbol:model.layers.8.mlp] {
        (%8438:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=363)]) -> (%8443:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=371)]) {
            linalg.CPU.LinearOp <name="model.layers.8.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=363), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=366), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=365))] (%8438:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=363)]) -> (%8439:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=366)])
            linalg.CPU.SiLUOp <name="model.layers.8.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=366), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=367), )] (%8439:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=366)]) -> (%8440:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=367)])
            linalg.CPU.LinearOp <name="model.layers.8.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=363), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=369), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=368))] (%8438:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=363)]) -> (%8441:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=369)])
            linalg.CPU.MulOp <name="model.layers.8.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=367), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=369), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=367), )] (%8440:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=367)], %8441:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=369)]) -> (%8442:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=367)])
            linalg.CPU.LinearOp <name="model.layers.8.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=367), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=371), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=370))] (%8442:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=367)]) -> (%8443:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=371)])
            cf.ReturnOp (%8443:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=371)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.9 <CPU> [using_qnn:true, symbol:model.layers.9] {
        (%8444:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=371)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8033:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=12)], %8034:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=40)]) -> (%8485:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=405)], %8458:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=384)], %8460:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=386)]) {
            linalg.CPU.RMSNormOp <name="model.layers.9.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=371), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=372), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=373))] (%8444:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=371)]) -> (%8445:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=372)])
            graph.CallGraphOp @model.layers.9.self_attn (%8445:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=372)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8033:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=12)], %8034:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=40)]) -> (%8477:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=396)], %8458:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=384)], %8460:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=386)])
            linalg.CPU.AddOp <name="model.layers.9.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=396), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=371), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=396), )] (%8477:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=396)], %8444:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=371)]) -> (%8478:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=396)])
            linalg.CPU.RMSNormOp <name="model.layers.9.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=396), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=397), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=398))] (%8478:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=396)]) -> (%8479:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=397)])
            graph.CallGraphOp @model.layers.9.mlp (%8479:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=397)]) -> (%8484:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=405)])
            linalg.CPU.AddOp <name="model.layers.9.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=405), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=396), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=405), )] (%8484:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=405)], %8478:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=396)]) -> (%8485:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=405)])
            cf.ReturnOp (%8485:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=405)], %8458:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=384)], %8460:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=386)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.9.self_attn <CPU> [using_qnn:true, symbol:model.layers.9.self_attn] {
        (%8445:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=372)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8033:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=12)], %8034:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=40)]) -> (%8477:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=396)], %8458:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=384)], %8460:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=386)]) {
            linalg.CPU.LinearOp <name="model.layers.9.self_attn.q_proj">(%8445:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=372)]) -> (%8446:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=378)])
            linalg.CPU.LinearOp <name="model.layers.9.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=372), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=375), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=374))] (%8445:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=372)]) -> (%8447:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=375)])
            linalg.CPU.LinearOp <name="model.layers.9.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=372), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=377), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=376))] (%8445:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=372)]) -> (%8448:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=377)])
            linalg.CPU.ViewOp <name="model.layers.9.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=378), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=378), )] (%8446:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=378)]) -> (%8446:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=378)])
            linalg.CPU.TransposeOp <name="model.layers.9.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=378), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=378), )] (%8446:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=378)]) -> (%8449:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=378)])
            linalg.CPU.ViewOp <name="model.layers.9.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=375), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=375), )] (%8447:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=375)]) -> (%8447:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=375)])
            linalg.CPU.TransposeOp <name="model.layers.9.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=375), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=375), )] (%8447:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=375)]) -> (%8450:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=375)])
            linalg.CPU.ViewOp <name="model.layers.9.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=377), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=377), )] (%8448:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=377)]) -> (%8448:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=377)])
            linalg.CPU.TransposeOp <name="model.layers.9.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=377), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=377), )] (%8448:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=377)]) -> (%8451:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=377)])
            linalg.CPU.RMSNormOp <name="model.layers.9.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=378), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=379), weight_weight:QuantSpec(Raw(type: Int16PerTensor), uuid=380))] (%8449:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=378)]) -> (%8452:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=379)])
            linalg.CPU.RMSNormOp <name="model.layers.9.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=375), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=381), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=382))] (%8450:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=375)]) -> (%8453:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=381)])
            linalg.CPU.RoPEOp <name="model.layers.9.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=379), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=379), )] (%8452:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=379)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%8454:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=379)])
            linalg.CPU.RoPEOp <name="model.layers.9.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=381), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=381), )] (%8453:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=381)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%8455:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=381)])
            linalg.CPU.CastTypeOp <name="model.layers.9.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=381), outputs_0:QuantSpec(Raw(type: Float16), uuid=383), )] (%8455:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=381)]) -> (%8456:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=383)])
            linalg.CPU.CastTypeOp <name="model.layers.9.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=383), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=384), )] (%8456:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=383)]) -> (%8457:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=384)])
            linalg.CPU.TransposeOp <name="model.layers.9.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=384), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=384), )] (%8457:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=384)]) -> (%8458:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=384)])
            linalg.CPU.CastTypeOp <name="model.layers.9.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=377), outputs_0:QuantSpec(Raw(type: Float16), uuid=385), )] (%8451:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=377)]) -> (%8459:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=385)])
            linalg.CPU.CastTypeOp <name="model.layers.9.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=385), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=386), )] (%8459:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=385)]) -> (%8460:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=386)])
            linalg.CPU.ConcatOp <name="model.layers.9.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=12), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=384), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=12), )] (%8033:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=12)], %8458:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=384)]) -> (%8461:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=12)])
            linalg.CPU.ConcatOp <name="model.layers.9.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=40), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=386), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=40), )] (%8034:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=40)], %8460:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=386)]) -> (%8462:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=40)])
            linalg.CPU.RepeatOp <name="model.layers.9.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=12), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=12), )] (%8461:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=12)]) -> (%8463:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=12)])
            linalg.CPU.RepeatOp <name="model.layers.9.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=40), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=40), )] (%8462:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=40)]) -> (%8464:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=40)])
            linalg.CPU.MatMulOp <name="model.layers.9.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=379), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=12), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=387), )] (%8454:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=379)], %8463:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=12)]) -> (%8465:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=387)])
            linalg.CPU.MulOp <name="model.layers.9.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=387), inputs_1:QuantSpec(Raw(type: Float32), uuid=388), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=387), )] (%8465:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=387)], %8466:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=388), constant:[0.088388346]]) -> (%8467:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=387)])
            linalg.CPU.ReduceMinOp <name="model.layers.9.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=387), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=389), )] (%8467:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=387)]) -> (%8468:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=389)])
            linalg.CPU.AddOp <name="model.layers.9.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=389), inputs_1:QuantSpec(Raw(type: Int16), uuid=390), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=389), )] (%8468:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=389)], %8469:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=390), constant:[-20]]) -> (%8470:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=389)])
            linalg.CPU.EqualOp <name="model.layers.9.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=391), outputs_0:QuantSpec(Raw(type: UInt8), uuid=392), )] (%8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8471:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=391), constant:[-0.1796875]]) -> (%8472:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=392)])
            linalg.CPU.WhereOp <name="model.layers.9.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=392), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=387), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=389), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=389), )] (%8472:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=392)], %8467:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=387)], %8470:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=389)]) -> (%8473:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=389)])
            linalg.CPU.SoftmaxOp <name="model.layers.9.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=389), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=393), )] (%8473:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=389)]) -> (%8474:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=393)])
            linalg.CPU.MatMulOp <name="model.layers.9.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=393), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=40), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=394), )] (%8474:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=393)], %8464:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=40)]) -> (%8475:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=394)])
            linalg.CPU.TransposeOp <name="model.layers.9.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=394), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=394), )] (%8475:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=394)]) -> (%8476:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=394)])
            linalg.CPU.ViewOp <name="model.layers.9.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=394), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=394), )] (%8476:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=394)]) -> (%8476:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=394)])
            linalg.CPU.LinearOp <name="model.layers.9.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=394), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=396), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=395))] (%8476:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=394)]) -> (%8477:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=396)])
            cf.ReturnOp (%8477:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=396)], %8458:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=384)], %8460:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=386)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.9.mlp <CPU> [using_qnn:true, symbol:model.layers.9.mlp] {
        (%8479:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=397)]) -> (%8484:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=405)]) {
            linalg.CPU.LinearOp <name="model.layers.9.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=397), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=400), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=399))] (%8479:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=397)]) -> (%8480:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=400)])
            linalg.CPU.SiLUOp <name="model.layers.9.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=400), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=401), )] (%8480:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=400)]) -> (%8481:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=401)])
            linalg.CPU.LinearOp <name="model.layers.9.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=397), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=403), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=402))] (%8479:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=397)]) -> (%8482:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=403)])
            linalg.CPU.MulOp <name="model.layers.9.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=401), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=403), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=401), )] (%8481:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=401)], %8482:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=403)]) -> (%8483:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=401)])
            linalg.CPU.LinearOp <name="model.layers.9.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=401), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=405), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=404))] (%8483:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=401)]) -> (%8484:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=405)])
            cf.ReturnOp (%8484:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=405)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.10 <CPU> [using_qnn:true, symbol:model.layers.10] {
        (%8485:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=405)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8035:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=13)], %8036:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=41)]) -> (%8526:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=439)], %8499:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=418)], %8501:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=420)]) {
            linalg.CPU.RMSNormOp <name="model.layers.10.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=405), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=406), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=407))] (%8485:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=405)]) -> (%8486:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=406)])
            graph.CallGraphOp @model.layers.10.self_attn (%8486:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=406)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8035:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=13)], %8036:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=41)]) -> (%8518:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=430)], %8499:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=418)], %8501:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=420)])
            linalg.CPU.AddOp <name="model.layers.10.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=430), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=405), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=430), )] (%8518:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=430)], %8485:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=405)]) -> (%8519:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=430)])
            linalg.CPU.RMSNormOp <name="model.layers.10.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=430), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=431), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=432))] (%8519:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=430)]) -> (%8520:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=431)])
            graph.CallGraphOp @model.layers.10.mlp (%8520:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=431)]) -> (%8525:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=439)])
            linalg.CPU.AddOp <name="model.layers.10.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=439), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=430), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=439), )] (%8525:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=439)], %8519:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=430)]) -> (%8526:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=439)])
            cf.ReturnOp (%8526:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=439)], %8499:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=418)], %8501:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=420)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.10.self_attn <CPU> [using_qnn:true, symbol:model.layers.10.self_attn] {
        (%8486:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=406)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8035:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=13)], %8036:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=41)]) -> (%8518:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=430)], %8499:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=418)], %8501:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=420)]) {
            linalg.CPU.LinearOp <name="model.layers.10.self_attn.q_proj">(%8486:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=406)]) -> (%8487:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=412)])
            linalg.CPU.LinearOp <name="model.layers.10.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=406), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=409), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=408))] (%8486:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=406)]) -> (%8488:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=409)])
            linalg.CPU.LinearOp <name="model.layers.10.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=406), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=411), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=410))] (%8486:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=406)]) -> (%8489:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=411)])
            linalg.CPU.ViewOp <name="model.layers.10.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=412), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=412), )] (%8487:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=412)]) -> (%8487:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=412)])
            linalg.CPU.TransposeOp <name="model.layers.10.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=412), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=412), )] (%8487:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=412)]) -> (%8490:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=412)])
            linalg.CPU.ViewOp <name="model.layers.10.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=409), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=409), )] (%8488:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=409)]) -> (%8488:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=409)])
            linalg.CPU.TransposeOp <name="model.layers.10.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=409), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=409), )] (%8488:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=409)]) -> (%8491:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=409)])
            linalg.CPU.ViewOp <name="model.layers.10.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=411), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=411), )] (%8489:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=411)]) -> (%8489:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=411)])
            linalg.CPU.TransposeOp <name="model.layers.10.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=411), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=411), )] (%8489:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=411)]) -> (%8492:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=411)])
            linalg.CPU.RMSNormOp <name="model.layers.10.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=412), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=413), weight_weight:QuantSpec(Raw(type: Int16PerTensor), uuid=414))] (%8490:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=412)]) -> (%8493:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=413)])
            linalg.CPU.RMSNormOp <name="model.layers.10.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=409), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=415), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=416))] (%8491:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=409)]) -> (%8494:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=415)])
            linalg.CPU.RoPEOp <name="model.layers.10.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=413), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=413), )] (%8493:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=413)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%8495:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=413)])
            linalg.CPU.RoPEOp <name="model.layers.10.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=415), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=415), )] (%8494:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=415)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%8496:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=415)])
            linalg.CPU.CastTypeOp <name="model.layers.10.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=415), outputs_0:QuantSpec(Raw(type: Float16), uuid=417), )] (%8496:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=415)]) -> (%8497:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=417)])
            linalg.CPU.CastTypeOp <name="model.layers.10.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=417), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=418), )] (%8497:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=417)]) -> (%8498:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=418)])
            linalg.CPU.TransposeOp <name="model.layers.10.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=418), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=418), )] (%8498:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=418)]) -> (%8499:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=418)])
            linalg.CPU.CastTypeOp <name="model.layers.10.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=411), outputs_0:QuantSpec(Raw(type: Float16), uuid=419), )] (%8492:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=411)]) -> (%8500:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=419)])
            linalg.CPU.CastTypeOp <name="model.layers.10.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=419), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=420), )] (%8500:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=419)]) -> (%8501:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=420)])
            linalg.CPU.ConcatOp <name="model.layers.10.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=13), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=418), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=13), )] (%8035:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=13)], %8499:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=418)]) -> (%8502:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=13)])
            linalg.CPU.ConcatOp <name="model.layers.10.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=41), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=420), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=41), )] (%8036:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=41)], %8501:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=420)]) -> (%8503:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=41)])
            linalg.CPU.RepeatOp <name="model.layers.10.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=13), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=13), )] (%8502:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=13)]) -> (%8504:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=13)])
            linalg.CPU.RepeatOp <name="model.layers.10.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=41), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=41), )] (%8503:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=41)]) -> (%8505:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=41)])
            linalg.CPU.MatMulOp <name="model.layers.10.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=413), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=13), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=421), )] (%8495:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=413)], %8504:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=13)]) -> (%8506:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=421)])
            linalg.CPU.MulOp <name="model.layers.10.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=421), inputs_1:QuantSpec(Raw(type: Float32), uuid=422), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=421), )] (%8506:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=421)], %8507:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=422), constant:[0.088388346]]) -> (%8508:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=421)])
            linalg.CPU.ReduceMinOp <name="model.layers.10.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=421), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=423), )] (%8508:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=421)]) -> (%8509:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=423)])
            linalg.CPU.AddOp <name="model.layers.10.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=423), inputs_1:QuantSpec(Raw(type: Int16), uuid=424), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=423), )] (%8509:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=423)], %8510:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=424), constant:[-20]]) -> (%8511:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=423)])
            linalg.CPU.EqualOp <name="model.layers.10.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=425), outputs_0:QuantSpec(Raw(type: UInt8), uuid=426), )] (%8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8512:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=425), constant:[-0.93359375]]) -> (%8513:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=426)])
            linalg.CPU.WhereOp <name="model.layers.10.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=426), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=421), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=423), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=423), )] (%8513:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=426)], %8508:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=421)], %8511:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=423)]) -> (%8514:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=423)])
            linalg.CPU.SoftmaxOp <name="model.layers.10.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=423), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=427), )] (%8514:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=423)]) -> (%8515:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=427)])
            linalg.CPU.MatMulOp <name="model.layers.10.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=427), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=41), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=428), )] (%8515:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=427)], %8505:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=41)]) -> (%8516:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=428)])
            linalg.CPU.TransposeOp <name="model.layers.10.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=428), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=428), )] (%8516:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=428)]) -> (%8517:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=428)])
            linalg.CPU.ViewOp <name="model.layers.10.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=428), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=428), )] (%8517:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=428)]) -> (%8517:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=428)])
            linalg.CPU.LinearOp <name="model.layers.10.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=428), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=430), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=429))] (%8517:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=428)]) -> (%8518:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=430)])
            cf.ReturnOp (%8518:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=430)], %8499:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=418)], %8501:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=420)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.10.mlp <CPU> [using_qnn:true, symbol:model.layers.10.mlp] {
        (%8520:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=431)]) -> (%8525:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=439)]) {
            linalg.CPU.LinearOp <name="model.layers.10.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=431), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=434), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=433))] (%8520:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=431)]) -> (%8521:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=434)])
            linalg.CPU.SiLUOp <name="model.layers.10.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=434), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=435), )] (%8521:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=434)]) -> (%8522:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=435)])
            linalg.CPU.LinearOp <name="model.layers.10.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=431), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=437), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=436))] (%8520:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=431)]) -> (%8523:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=437)])
            linalg.CPU.MulOp <name="model.layers.10.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=435), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=437), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=435), )] (%8522:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=435)], %8523:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=437)]) -> (%8524:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=435)])
            linalg.CPU.LinearOp <name="model.layers.10.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=435), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=439), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=438))] (%8524:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=435)]) -> (%8525:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=439)])
            cf.ReturnOp (%8525:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=439)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.11 <CPU> [using_qnn:true, symbol:model.layers.11] {
        (%8526:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=439)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8037:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=14)], %8038:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=42)]) -> (%8567:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=473)], %8540:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=452)], %8542:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=454)]) {
            linalg.CPU.RMSNormOp <name="model.layers.11.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=439), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=440), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=441))] (%8526:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=439)]) -> (%8527:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=440)])
            graph.CallGraphOp @model.layers.11.self_attn (%8527:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=440)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8037:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=14)], %8038:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=42)]) -> (%8559:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=464)], %8540:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=452)], %8542:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=454)])
            linalg.CPU.AddOp <name="model.layers.11.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=464), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=439), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=464), )] (%8559:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=464)], %8526:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=439)]) -> (%8560:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=464)])
            linalg.CPU.RMSNormOp <name="model.layers.11.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=464), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=465), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=466))] (%8560:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=464)]) -> (%8561:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=465)])
            graph.CallGraphOp @model.layers.11.mlp (%8561:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=465)]) -> (%8566:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=473)])
            linalg.CPU.AddOp <name="model.layers.11.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=473), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=464), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=473), )] (%8566:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=473)], %8560:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=464)]) -> (%8567:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=473)])
            cf.ReturnOp (%8567:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=473)], %8540:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=452)], %8542:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=454)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.11.self_attn <CPU> [using_qnn:true, symbol:model.layers.11.self_attn] {
        (%8527:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=440)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8037:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=14)], %8038:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=42)]) -> (%8559:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=464)], %8540:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=452)], %8542:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=454)]) {
            linalg.CPU.LinearOp <name="model.layers.11.self_attn.q_proj">(%8527:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=440)]) -> (%8528:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=446)])
            linalg.CPU.LinearOp <name="model.layers.11.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=440), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=443), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=442))] (%8527:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=440)]) -> (%8529:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=443)])
            linalg.CPU.LinearOp <name="model.layers.11.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=440), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=445), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=444))] (%8527:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=440)]) -> (%8530:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=445)])
            linalg.CPU.ViewOp <name="model.layers.11.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=446), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=446), )] (%8528:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=446)]) -> (%8528:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=446)])
            linalg.CPU.TransposeOp <name="model.layers.11.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=446), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=446), )] (%8528:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=446)]) -> (%8531:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=446)])
            linalg.CPU.ViewOp <name="model.layers.11.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=443), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=443), )] (%8529:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=443)]) -> (%8529:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=443)])
            linalg.CPU.TransposeOp <name="model.layers.11.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=443), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=443), )] (%8529:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=443)]) -> (%8532:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=443)])
            linalg.CPU.ViewOp <name="model.layers.11.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=445), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=445), )] (%8530:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=445)]) -> (%8530:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=445)])
            linalg.CPU.TransposeOp <name="model.layers.11.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=445), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=445), )] (%8530:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=445)]) -> (%8533:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=445)])
            linalg.CPU.RMSNormOp <name="model.layers.11.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=446), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=447), weight_weight:QuantSpec(Raw(type: Int16PerTensor), uuid=448))] (%8531:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=446)]) -> (%8534:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=447)])
            linalg.CPU.RMSNormOp <name="model.layers.11.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=443), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=449), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=450))] (%8532:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=443)]) -> (%8535:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=449)])
            linalg.CPU.RoPEOp <name="model.layers.11.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=447), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=447), )] (%8534:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=447)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%8536:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=447)])
            linalg.CPU.RoPEOp <name="model.layers.11.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=449), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=449), )] (%8535:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=449)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%8537:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=449)])
            linalg.CPU.CastTypeOp <name="model.layers.11.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=449), outputs_0:QuantSpec(Raw(type: Float16), uuid=451), )] (%8537:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=449)]) -> (%8538:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=451)])
            linalg.CPU.CastTypeOp <name="model.layers.11.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=451), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=452), )] (%8538:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=451)]) -> (%8539:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=452)])
            linalg.CPU.TransposeOp <name="model.layers.11.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=452), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=452), )] (%8539:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=452)]) -> (%8540:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=452)])
            linalg.CPU.CastTypeOp <name="model.layers.11.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=445), outputs_0:QuantSpec(Raw(type: Float16), uuid=453), )] (%8533:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=445)]) -> (%8541:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=453)])
            linalg.CPU.CastTypeOp <name="model.layers.11.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=453), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=454), )] (%8541:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=453)]) -> (%8542:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=454)])
            linalg.CPU.ConcatOp <name="model.layers.11.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=14), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=452), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=14), )] (%8037:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=14)], %8540:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=452)]) -> (%8543:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=14)])
            linalg.CPU.ConcatOp <name="model.layers.11.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=42), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=454), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=42), )] (%8038:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=42)], %8542:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=454)]) -> (%8544:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=42)])
            linalg.CPU.RepeatOp <name="model.layers.11.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=14), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=14), )] (%8543:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=14)]) -> (%8545:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=14)])
            linalg.CPU.RepeatOp <name="model.layers.11.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=42), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=42), )] (%8544:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=42)]) -> (%8546:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=42)])
            linalg.CPU.MatMulOp <name="model.layers.11.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=447), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=14), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=455), )] (%8536:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=447)], %8545:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=14)]) -> (%8547:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=455)])
            linalg.CPU.MulOp <name="model.layers.11.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=455), inputs_1:QuantSpec(Raw(type: Float32), uuid=456), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=455), )] (%8547:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=455)], %8548:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=456), constant:[0.088388346]]) -> (%8549:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=455)])
            linalg.CPU.ReduceMinOp <name="model.layers.11.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=455), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=457), )] (%8549:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=455)]) -> (%8550:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=457)])
            linalg.CPU.AddOp <name="model.layers.11.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=457), inputs_1:QuantSpec(Raw(type: Int16), uuid=458), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=457), )] (%8550:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=457)], %8551:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=458), constant:[-20]]) -> (%8552:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=457)])
            linalg.CPU.EqualOp <name="model.layers.11.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=459), outputs_0:QuantSpec(Raw(type: UInt8), uuid=460), )] (%8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8553:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=459), constant:[0.515625]]) -> (%8554:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=460)])
            linalg.CPU.WhereOp <name="model.layers.11.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=460), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=455), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=457), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=457), )] (%8554:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=460)], %8549:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=455)], %8552:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=457)]) -> (%8555:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=457)])
            linalg.CPU.SoftmaxOp <name="model.layers.11.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=457), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=461), )] (%8555:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=457)]) -> (%8556:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=461)])
            linalg.CPU.MatMulOp <name="model.layers.11.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=461), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=42), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=462), )] (%8556:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=461)], %8546:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=42)]) -> (%8557:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=462)])
            linalg.CPU.TransposeOp <name="model.layers.11.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=462), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=462), )] (%8557:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=462)]) -> (%8558:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=462)])
            linalg.CPU.ViewOp <name="model.layers.11.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=462), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=462), )] (%8558:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=462)]) -> (%8558:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=462)])
            linalg.CPU.LinearOp <name="model.layers.11.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=462), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=464), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=463))] (%8558:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=462)]) -> (%8559:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=464)])
            cf.ReturnOp (%8559:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=464)], %8540:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=452)], %8542:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=454)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.11.mlp <CPU> [using_qnn:true, symbol:model.layers.11.mlp] {
        (%8561:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=465)]) -> (%8566:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=473)]) {
            linalg.CPU.LinearOp <name="model.layers.11.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=465), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=468), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=467))] (%8561:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=465)]) -> (%8562:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=468)])
            linalg.CPU.SiLUOp <name="model.layers.11.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=468), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=469), )] (%8562:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=468)]) -> (%8563:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=469)])
            linalg.CPU.LinearOp <name="model.layers.11.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=465), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=471), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=470))] (%8561:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=465)]) -> (%8564:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=471)])
            linalg.CPU.MulOp <name="model.layers.11.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=469), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=471), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=469), )] (%8563:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=469)], %8564:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=471)]) -> (%8565:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=469)])
            linalg.CPU.LinearOp <name="model.layers.11.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=469), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=473), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=472))] (%8565:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=469)]) -> (%8566:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=473)])
            cf.ReturnOp (%8566:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=473)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.12 <CPU> [using_qnn:true, symbol:model.layers.12] {
        (%8567:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=473)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8039:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=15)], %8040:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=43)]) -> (%8608:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=507)], %8581:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=486)], %8583:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=488)]) {
            linalg.CPU.RMSNormOp <name="model.layers.12.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=473), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=474), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=475))] (%8567:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=473)]) -> (%8568:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=474)])
            graph.CallGraphOp @model.layers.12.self_attn (%8568:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=474)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8039:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=15)], %8040:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=43)]) -> (%8600:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=498)], %8581:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=486)], %8583:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=488)])
            linalg.CPU.AddOp <name="model.layers.12.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=498), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=473), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=498), )] (%8600:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=498)], %8567:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=473)]) -> (%8601:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=498)])
            linalg.CPU.RMSNormOp <name="model.layers.12.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=498), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=499), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=500))] (%8601:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=498)]) -> (%8602:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=499)])
            graph.CallGraphOp @model.layers.12.mlp (%8602:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=499)]) -> (%8607:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=507)])
            linalg.CPU.AddOp <name="model.layers.12.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=507), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=498), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=507), )] (%8607:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=507)], %8601:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=498)]) -> (%8608:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=507)])
            cf.ReturnOp (%8608:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=507)], %8581:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=486)], %8583:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=488)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.12.self_attn <CPU> [using_qnn:true, symbol:model.layers.12.self_attn] {
        (%8568:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=474)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8039:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=15)], %8040:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=43)]) -> (%8600:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=498)], %8581:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=486)], %8583:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=488)]) {
            linalg.CPU.LinearOp <name="model.layers.12.self_attn.q_proj">(%8568:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=474)]) -> (%8569:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=480)])
            linalg.CPU.LinearOp <name="model.layers.12.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=474), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=477), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=476))] (%8568:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=474)]) -> (%8570:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=477)])
            linalg.CPU.LinearOp <name="model.layers.12.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=474), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=479), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=478))] (%8568:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=474)]) -> (%8571:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=479)])
            linalg.CPU.ViewOp <name="model.layers.12.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=480), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=480), )] (%8569:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=480)]) -> (%8569:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=480)])
            linalg.CPU.TransposeOp <name="model.layers.12.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=480), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=480), )] (%8569:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=480)]) -> (%8572:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=480)])
            linalg.CPU.ViewOp <name="model.layers.12.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=477), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=477), )] (%8570:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=477)]) -> (%8570:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=477)])
            linalg.CPU.TransposeOp <name="model.layers.12.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=477), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=477), )] (%8570:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=477)]) -> (%8573:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=477)])
            linalg.CPU.ViewOp <name="model.layers.12.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=479), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=479), )] (%8571:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=479)]) -> (%8571:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=479)])
            linalg.CPU.TransposeOp <name="model.layers.12.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=479), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=479), )] (%8571:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=479)]) -> (%8574:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=479)])
            linalg.CPU.RMSNormOp <name="model.layers.12.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=480), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=481), weight_weight:QuantSpec(Raw(type: Int16PerTensor), uuid=482))] (%8572:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=480)]) -> (%8575:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=481)])
            linalg.CPU.RMSNormOp <name="model.layers.12.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=477), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=483), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=484))] (%8573:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=477)]) -> (%8576:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=483)])
            linalg.CPU.RoPEOp <name="model.layers.12.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=481), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=481), )] (%8575:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=481)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%8577:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=481)])
            linalg.CPU.RoPEOp <name="model.layers.12.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=483), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=483), )] (%8576:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=483)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%8578:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=483)])
            linalg.CPU.CastTypeOp <name="model.layers.12.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=483), outputs_0:QuantSpec(Raw(type: Float16), uuid=485), )] (%8578:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=483)]) -> (%8579:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=485)])
            linalg.CPU.CastTypeOp <name="model.layers.12.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=485), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=486), )] (%8579:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=485)]) -> (%8580:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=486)])
            linalg.CPU.TransposeOp <name="model.layers.12.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=486), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=486), )] (%8580:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=486)]) -> (%8581:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=486)])
            linalg.CPU.CastTypeOp <name="model.layers.12.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=479), outputs_0:QuantSpec(Raw(type: Float16), uuid=487), )] (%8574:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=479)]) -> (%8582:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=487)])
            linalg.CPU.CastTypeOp <name="model.layers.12.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=487), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=488), )] (%8582:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=487)]) -> (%8583:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=488)])
            linalg.CPU.ConcatOp <name="model.layers.12.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=15), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=486), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=15), )] (%8039:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=15)], %8581:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=486)]) -> (%8584:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=15)])
            linalg.CPU.ConcatOp <name="model.layers.12.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=43), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=488), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=43), )] (%8040:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=43)], %8583:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=488)]) -> (%8585:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=43)])
            linalg.CPU.RepeatOp <name="model.layers.12.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=15), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=15), )] (%8584:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=15)]) -> (%8586:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=15)])
            linalg.CPU.RepeatOp <name="model.layers.12.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=43), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=43), )] (%8585:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=43)]) -> (%8587:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=43)])
            linalg.CPU.MatMulOp <name="model.layers.12.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=481), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=15), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=489), )] (%8577:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=481)], %8586:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=15)]) -> (%8588:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=489)])
            linalg.CPU.MulOp <name="model.layers.12.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=489), inputs_1:QuantSpec(Raw(type: Float32), uuid=490), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=489), )] (%8588:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=489)], %8589:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=490), constant:[0.088388346]]) -> (%8590:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=489)])
            linalg.CPU.ReduceMinOp <name="model.layers.12.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=489), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=491), )] (%8590:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=489)]) -> (%8591:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=491)])
            linalg.CPU.AddOp <name="model.layers.12.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=491), inputs_1:QuantSpec(Raw(type: Int16), uuid=492), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=491), )] (%8591:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=491)], %8592:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=492), constant:[-20]]) -> (%8593:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=491)])
            linalg.CPU.EqualOp <name="model.layers.12.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=493), outputs_0:QuantSpec(Raw(type: UInt8), uuid=494), )] (%8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8594:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=493), constant:[0.74609375]]) -> (%8595:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=494)])
            linalg.CPU.WhereOp <name="model.layers.12.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=494), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=489), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=491), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=491), )] (%8595:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=494)], %8590:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=489)], %8593:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=491)]) -> (%8596:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=491)])
            linalg.CPU.SoftmaxOp <name="model.layers.12.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=491), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=495), )] (%8596:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=491)]) -> (%8597:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=495)])
            linalg.CPU.MatMulOp <name="model.layers.12.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=495), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=43), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=496), )] (%8597:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=495)], %8587:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=43)]) -> (%8598:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=496)])
            linalg.CPU.TransposeOp <name="model.layers.12.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=496), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=496), )] (%8598:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=496)]) -> (%8599:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=496)])
            linalg.CPU.ViewOp <name="model.layers.12.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=496), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=496), )] (%8599:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=496)]) -> (%8599:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=496)])
            linalg.CPU.LinearOp <name="model.layers.12.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=496), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=498), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=497))] (%8599:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=496)]) -> (%8600:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=498)])
            cf.ReturnOp (%8600:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=498)], %8581:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=486)], %8583:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=488)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.12.mlp <CPU> [using_qnn:true, symbol:model.layers.12.mlp] {
        (%8602:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=499)]) -> (%8607:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=507)]) {
            linalg.CPU.LinearOp <name="model.layers.12.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=499), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=502), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=501))] (%8602:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=499)]) -> (%8603:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=502)])
            linalg.CPU.SiLUOp <name="model.layers.12.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=502), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=503), )] (%8603:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=502)]) -> (%8604:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=503)])
            linalg.CPU.LinearOp <name="model.layers.12.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=499), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=505), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=504))] (%8602:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=499)]) -> (%8605:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=505)])
            linalg.CPU.MulOp <name="model.layers.12.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=503), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=505), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=503), )] (%8604:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=503)], %8605:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=505)]) -> (%8606:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=503)])
            linalg.CPU.LinearOp <name="model.layers.12.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=503), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=507), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=506))] (%8606:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=503)]) -> (%8607:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=507)])
            cf.ReturnOp (%8607:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=507)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.13 <CPU> [using_qnn:true, symbol:model.layers.13] {
        (%8608:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=507)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8041:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=16)], %8042:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=44)]) -> (%8649:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=541)], %8622:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=520)], %8624:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=522)]) {
            linalg.CPU.RMSNormOp <name="model.layers.13.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=507), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=508), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=509))] (%8608:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=507)]) -> (%8609:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=508)])
            graph.CallGraphOp @model.layers.13.self_attn (%8609:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=508)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8041:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=16)], %8042:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=44)]) -> (%8641:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=532)], %8622:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=520)], %8624:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=522)])
            linalg.CPU.AddOp <name="model.layers.13.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=532), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=507), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=532), )] (%8641:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=532)], %8608:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=507)]) -> (%8642:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=532)])
            linalg.CPU.RMSNormOp <name="model.layers.13.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=532), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=533), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=534))] (%8642:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=532)]) -> (%8643:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=533)])
            graph.CallGraphOp @model.layers.13.mlp (%8643:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=533)]) -> (%8648:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=541)])
            linalg.CPU.AddOp <name="model.layers.13.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=541), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=532), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=541), )] (%8648:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=541)], %8642:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=532)]) -> (%8649:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=541)])
            cf.ReturnOp (%8649:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=541)], %8622:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=520)], %8624:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=522)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.13.self_attn <CPU> [using_qnn:true, symbol:model.layers.13.self_attn] {
        (%8609:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=508)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8041:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=16)], %8042:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=44)]) -> (%8641:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=532)], %8622:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=520)], %8624:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=522)]) {
            linalg.CPU.LinearOp <name="model.layers.13.self_attn.q_proj">(%8609:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=508)]) -> (%8610:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=514)])
            linalg.CPU.LinearOp <name="model.layers.13.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=508), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=511), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=510))] (%8609:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=508)]) -> (%8611:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=511)])
            linalg.CPU.LinearOp <name="model.layers.13.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=508), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=513), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=512))] (%8609:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=508)]) -> (%8612:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=513)])
            linalg.CPU.ViewOp <name="model.layers.13.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=514), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=514), )] (%8610:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=514)]) -> (%8610:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=514)])
            linalg.CPU.TransposeOp <name="model.layers.13.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=514), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=514), )] (%8610:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=514)]) -> (%8613:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=514)])
            linalg.CPU.ViewOp <name="model.layers.13.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=511), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=511), )] (%8611:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=511)]) -> (%8611:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=511)])
            linalg.CPU.TransposeOp <name="model.layers.13.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=511), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=511), )] (%8611:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=511)]) -> (%8614:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=511)])
            linalg.CPU.ViewOp <name="model.layers.13.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=513), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=513), )] (%8612:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=513)]) -> (%8612:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=513)])
            linalg.CPU.TransposeOp <name="model.layers.13.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=513), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=513), )] (%8612:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=513)]) -> (%8615:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=513)])
            linalg.CPU.RMSNormOp <name="model.layers.13.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=514), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=515), weight_weight:QuantSpec(Raw(type: Int16PerTensor), uuid=516))] (%8613:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=514)]) -> (%8616:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=515)])
            linalg.CPU.RMSNormOp <name="model.layers.13.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=511), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=517), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=518))] (%8614:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=511)]) -> (%8617:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=517)])
            linalg.CPU.RoPEOp <name="model.layers.13.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=515), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=515), )] (%8616:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=515)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%8618:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=515)])
            linalg.CPU.RoPEOp <name="model.layers.13.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=517), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=517), )] (%8617:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=517)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%8619:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=517)])
            linalg.CPU.CastTypeOp <name="model.layers.13.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=517), outputs_0:QuantSpec(Raw(type: Float16), uuid=519), )] (%8619:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=517)]) -> (%8620:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=519)])
            linalg.CPU.CastTypeOp <name="model.layers.13.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=519), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=520), )] (%8620:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=519)]) -> (%8621:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=520)])
            linalg.CPU.TransposeOp <name="model.layers.13.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=520), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=520), )] (%8621:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=520)]) -> (%8622:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=520)])
            linalg.CPU.CastTypeOp <name="model.layers.13.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=513), outputs_0:QuantSpec(Raw(type: Float16), uuid=521), )] (%8615:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=513)]) -> (%8623:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=521)])
            linalg.CPU.CastTypeOp <name="model.layers.13.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=521), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=522), )] (%8623:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=521)]) -> (%8624:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=522)])
            linalg.CPU.ConcatOp <name="model.layers.13.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=16), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=520), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=16), )] (%8041:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=16)], %8622:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=520)]) -> (%8625:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=16)])
            linalg.CPU.ConcatOp <name="model.layers.13.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=44), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=522), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=44), )] (%8042:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=44)], %8624:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=522)]) -> (%8626:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=44)])
            linalg.CPU.RepeatOp <name="model.layers.13.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=16), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=16), )] (%8625:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=16)]) -> (%8627:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=16)])
            linalg.CPU.RepeatOp <name="model.layers.13.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=44), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=44), )] (%8626:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=44)]) -> (%8628:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=44)])
            linalg.CPU.MatMulOp <name="model.layers.13.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=515), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=16), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=523), )] (%8618:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=515)], %8627:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=16)]) -> (%8629:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=523)])
            linalg.CPU.MulOp <name="model.layers.13.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=523), inputs_1:QuantSpec(Raw(type: Float32), uuid=524), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=523), )] (%8629:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=523)], %8630:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=524), constant:[0.088388346]]) -> (%8631:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=523)])
            linalg.CPU.ReduceMinOp <name="model.layers.13.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=523), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=525), )] (%8631:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=523)]) -> (%8632:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=525)])
            linalg.CPU.AddOp <name="model.layers.13.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=525), inputs_1:QuantSpec(Raw(type: Int16), uuid=526), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=525), )] (%8632:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=525)], %8633:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=526), constant:[-20]]) -> (%8634:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=525)])
            linalg.CPU.EqualOp <name="model.layers.13.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=527), outputs_0:QuantSpec(Raw(type: UInt8), uuid=528), )] (%8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8635:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=527), constant:[-0.78515625]]) -> (%8636:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=528)])
            linalg.CPU.WhereOp <name="model.layers.13.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=528), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=523), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=525), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=525), )] (%8636:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=528)], %8631:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=523)], %8634:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=525)]) -> (%8637:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=525)])
            linalg.CPU.SoftmaxOp <name="model.layers.13.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=525), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=529), )] (%8637:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=525)]) -> (%8638:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=529)])
            linalg.CPU.MatMulOp <name="model.layers.13.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=529), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=44), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=530), )] (%8638:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=529)], %8628:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=44)]) -> (%8639:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=530)])
            linalg.CPU.TransposeOp <name="model.layers.13.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=530), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=530), )] (%8639:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=530)]) -> (%8640:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=530)])
            linalg.CPU.ViewOp <name="model.layers.13.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=530), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=530), )] (%8640:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=530)]) -> (%8640:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=530)])
            linalg.CPU.LinearOp <name="model.layers.13.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=530), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=532), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=531))] (%8640:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=530)]) -> (%8641:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=532)])
            cf.ReturnOp (%8641:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=532)], %8622:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=520)], %8624:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=522)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.13.mlp <CPU> [using_qnn:true, symbol:model.layers.13.mlp] {
        (%8643:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=533)]) -> (%8648:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=541)]) {
            linalg.CPU.LinearOp <name="model.layers.13.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=533), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=536), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=535))] (%8643:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=533)]) -> (%8644:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=536)])
            linalg.CPU.SiLUOp <name="model.layers.13.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=536), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=537), )] (%8644:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=536)]) -> (%8645:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=537)])
            linalg.CPU.LinearOp <name="model.layers.13.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=533), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=539), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=538))] (%8643:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=533)]) -> (%8646:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=539)])
            linalg.CPU.MulOp <name="model.layers.13.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=537), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=539), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=537), )] (%8645:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=537)], %8646:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=539)]) -> (%8647:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=537)])
            linalg.CPU.LinearOp <name="model.layers.13.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=537), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=541), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=540))] (%8647:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=537)]) -> (%8648:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=541)])
            cf.ReturnOp (%8648:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=541)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.14 <CPU> [using_qnn:true, symbol:model.layers.14] {
        (%8649:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=541)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8043:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=17)], %8044:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=45)]) -> (%8690:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=575)], %8663:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=554)], %8665:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=556)]) {
            linalg.CPU.RMSNormOp <name="model.layers.14.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=541), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=542), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=543))] (%8649:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=541)]) -> (%8650:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=542)])
            graph.CallGraphOp @model.layers.14.self_attn (%8650:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=542)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8043:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=17)], %8044:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=45)]) -> (%8682:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=566)], %8663:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=554)], %8665:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=556)])
            linalg.CPU.AddOp <name="model.layers.14.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=566), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=541), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=566), )] (%8682:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=566)], %8649:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=541)]) -> (%8683:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=566)])
            linalg.CPU.RMSNormOp <name="model.layers.14.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=566), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=567), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=568))] (%8683:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=566)]) -> (%8684:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=567)])
            graph.CallGraphOp @model.layers.14.mlp (%8684:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=567)]) -> (%8689:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=575)])
            linalg.CPU.AddOp <name="model.layers.14.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=575), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=566), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=575), )] (%8689:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=575)], %8683:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=566)]) -> (%8690:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=575)])
            cf.ReturnOp (%8690:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=575)], %8663:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=554)], %8665:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=556)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.14.self_attn <CPU> [using_qnn:true, symbol:model.layers.14.self_attn] {
        (%8650:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=542)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8043:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=17)], %8044:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=45)]) -> (%8682:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=566)], %8663:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=554)], %8665:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=556)]) {
            linalg.CPU.LinearOp <name="model.layers.14.self_attn.q_proj">(%8650:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=542)]) -> (%8651:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=548)])
            linalg.CPU.LinearOp <name="model.layers.14.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=542), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=545), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=544))] (%8650:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=542)]) -> (%8652:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=545)])
            linalg.CPU.LinearOp <name="model.layers.14.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=542), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=547), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=546))] (%8650:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=542)]) -> (%8653:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=547)])
            linalg.CPU.ViewOp <name="model.layers.14.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=548), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=548), )] (%8651:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=548)]) -> (%8651:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=548)])
            linalg.CPU.TransposeOp <name="model.layers.14.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=548), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=548), )] (%8651:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=548)]) -> (%8654:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=548)])
            linalg.CPU.ViewOp <name="model.layers.14.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=545), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=545), )] (%8652:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=545)]) -> (%8652:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=545)])
            linalg.CPU.TransposeOp <name="model.layers.14.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=545), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=545), )] (%8652:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=545)]) -> (%8655:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=545)])
            linalg.CPU.ViewOp <name="model.layers.14.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=547), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=547), )] (%8653:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=547)]) -> (%8653:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=547)])
            linalg.CPU.TransposeOp <name="model.layers.14.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=547), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=547), )] (%8653:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=547)]) -> (%8656:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=547)])
            linalg.CPU.RMSNormOp <name="model.layers.14.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=548), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=549), weight_weight:QuantSpec(Raw(type: Int16PerTensor), uuid=550))] (%8654:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=548)]) -> (%8657:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=549)])
            linalg.CPU.RMSNormOp <name="model.layers.14.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=545), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=551), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=552))] (%8655:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=545)]) -> (%8658:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=551)])
            linalg.CPU.RoPEOp <name="model.layers.14.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=549), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=549), )] (%8657:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=549)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%8659:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=549)])
            linalg.CPU.RoPEOp <name="model.layers.14.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=551), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=551), )] (%8658:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=551)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%8660:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=551)])
            linalg.CPU.CastTypeOp <name="model.layers.14.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=551), outputs_0:QuantSpec(Raw(type: Float16), uuid=553), )] (%8660:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=551)]) -> (%8661:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=553)])
            linalg.CPU.CastTypeOp <name="model.layers.14.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=553), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=554), )] (%8661:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=553)]) -> (%8662:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=554)])
            linalg.CPU.TransposeOp <name="model.layers.14.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=554), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=554), )] (%8662:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=554)]) -> (%8663:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=554)])
            linalg.CPU.CastTypeOp <name="model.layers.14.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=547), outputs_0:QuantSpec(Raw(type: Float16), uuid=555), )] (%8656:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=547)]) -> (%8664:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=555)])
            linalg.CPU.CastTypeOp <name="model.layers.14.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=555), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=556), )] (%8664:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=555)]) -> (%8665:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=556)])
            linalg.CPU.ConcatOp <name="model.layers.14.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=17), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=554), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=17), )] (%8043:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=17)], %8663:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=554)]) -> (%8666:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=17)])
            linalg.CPU.ConcatOp <name="model.layers.14.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=45), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=556), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=45), )] (%8044:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=45)], %8665:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=556)]) -> (%8667:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=45)])
            linalg.CPU.RepeatOp <name="model.layers.14.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=17), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=17), )] (%8666:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=17)]) -> (%8668:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=17)])
            linalg.CPU.RepeatOp <name="model.layers.14.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=45), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=45), )] (%8667:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=45)]) -> (%8669:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=45)])
            linalg.CPU.MatMulOp <name="model.layers.14.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=549), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=17), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=557), )] (%8659:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=549)], %8668:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=17)]) -> (%8670:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=557)])
            linalg.CPU.MulOp <name="model.layers.14.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=557), inputs_1:QuantSpec(Raw(type: Float32), uuid=558), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=557), )] (%8670:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=557)], %8671:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=558), constant:[0.088388346]]) -> (%8672:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=557)])
            linalg.CPU.ReduceMinOp <name="model.layers.14.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=557), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=559), )] (%8672:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=557)]) -> (%8673:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=559)])
            linalg.CPU.AddOp <name="model.layers.14.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=559), inputs_1:QuantSpec(Raw(type: Int16), uuid=560), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=559), )] (%8673:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=559)], %8674:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=560), constant:[-20]]) -> (%8675:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=559)])
            linalg.CPU.EqualOp <name="model.layers.14.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=561), outputs_0:QuantSpec(Raw(type: UInt8), uuid=562), )] (%8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8676:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=561), constant:[-0.46289062]]) -> (%8677:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=562)])
            linalg.CPU.WhereOp <name="model.layers.14.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=562), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=557), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=559), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=559), )] (%8677:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=562)], %8672:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=557)], %8675:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=559)]) -> (%8678:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=559)])
            linalg.CPU.SoftmaxOp <name="model.layers.14.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=559), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=563), )] (%8678:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=559)]) -> (%8679:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=563)])
            linalg.CPU.MatMulOp <name="model.layers.14.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=563), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=45), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=564), )] (%8679:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=563)], %8669:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=45)]) -> (%8680:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=564)])
            linalg.CPU.TransposeOp <name="model.layers.14.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=564), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=564), )] (%8680:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=564)]) -> (%8681:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=564)])
            linalg.CPU.ViewOp <name="model.layers.14.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=564), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=564), )] (%8681:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=564)]) -> (%8681:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=564)])
            linalg.CPU.LinearOp <name="model.layers.14.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=564), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=566), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=565))] (%8681:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=564)]) -> (%8682:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=566)])
            cf.ReturnOp (%8682:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=566)], %8663:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=554)], %8665:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=556)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.14.mlp <CPU> [using_qnn:true, symbol:model.layers.14.mlp] {
        (%8684:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=567)]) -> (%8689:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=575)]) {
            linalg.CPU.LinearOp <name="model.layers.14.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=567), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=570), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=569))] (%8684:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=567)]) -> (%8685:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=570)])
            linalg.CPU.SiLUOp <name="model.layers.14.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=570), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=571), )] (%8685:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=570)]) -> (%8686:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=571)])
            linalg.CPU.LinearOp <name="model.layers.14.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=567), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=573), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=572))] (%8684:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=567)]) -> (%8687:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=573)])
            linalg.CPU.MulOp <name="model.layers.14.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=571), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=573), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=571), )] (%8686:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=571)], %8687:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=573)]) -> (%8688:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=571)])
            linalg.CPU.LinearOp <name="model.layers.14.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=571), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=575), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=574))] (%8688:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=571)]) -> (%8689:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=575)])
            cf.ReturnOp (%8689:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=575)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.15 <CPU> [using_qnn:true, symbol:model.layers.15] {
        (%8690:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=575)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8045:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=18)], %8046:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=46)]) -> (%8731:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=609)], %8704:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=588)], %8706:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=590)]) {
            linalg.CPU.RMSNormOp <name="model.layers.15.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=575), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=576), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=577))] (%8690:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=575)]) -> (%8691:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=576)])
            graph.CallGraphOp @model.layers.15.self_attn (%8691:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=576)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8045:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=18)], %8046:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=46)]) -> (%8723:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=600)], %8704:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=588)], %8706:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=590)])
            linalg.CPU.AddOp <name="model.layers.15.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=600), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=575), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=600), )] (%8723:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=600)], %8690:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=575)]) -> (%8724:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=600)])
            linalg.CPU.RMSNormOp <name="model.layers.15.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=600), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=601), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=602))] (%8724:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=600)]) -> (%8725:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=601)])
            graph.CallGraphOp @model.layers.15.mlp (%8725:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=601)]) -> (%8730:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=609)])
            linalg.CPU.AddOp <name="model.layers.15.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=609), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=600), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=609), )] (%8730:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=609)], %8724:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=600)]) -> (%8731:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=609)])
            cf.ReturnOp (%8731:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=609)], %8704:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=588)], %8706:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=590)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.15.self_attn <CPU> [using_qnn:true, symbol:model.layers.15.self_attn] {
        (%8691:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=576)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8045:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=18)], %8046:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=46)]) -> (%8723:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=600)], %8704:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=588)], %8706:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=590)]) {
            linalg.CPU.LinearOp <name="model.layers.15.self_attn.q_proj">(%8691:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=576)]) -> (%8692:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=582)])
            linalg.CPU.LinearOp <name="model.layers.15.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=576), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=579), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=578))] (%8691:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=576)]) -> (%8693:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=579)])
            linalg.CPU.LinearOp <name="model.layers.15.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=576), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=581), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=580))] (%8691:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=576)]) -> (%8694:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=581)])
            linalg.CPU.ViewOp <name="model.layers.15.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=582), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=582), )] (%8692:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=582)]) -> (%8692:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=582)])
            linalg.CPU.TransposeOp <name="model.layers.15.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=582), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=582), )] (%8692:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=582)]) -> (%8695:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=582)])
            linalg.CPU.ViewOp <name="model.layers.15.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=579), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=579), )] (%8693:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=579)]) -> (%8693:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=579)])
            linalg.CPU.TransposeOp <name="model.layers.15.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=579), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=579), )] (%8693:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=579)]) -> (%8696:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=579)])
            linalg.CPU.ViewOp <name="model.layers.15.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=581), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=581), )] (%8694:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=581)]) -> (%8694:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=581)])
            linalg.CPU.TransposeOp <name="model.layers.15.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=581), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=581), )] (%8694:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=581)]) -> (%8697:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=581)])
            linalg.CPU.RMSNormOp <name="model.layers.15.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=582), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=583), weight_weight:QuantSpec(Raw(type: Int16PerTensor), uuid=584))] (%8695:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=582)]) -> (%8698:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=583)])
            linalg.CPU.RMSNormOp <name="model.layers.15.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=579), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=585), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=586))] (%8696:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=579)]) -> (%8699:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=585)])
            linalg.CPU.RoPEOp <name="model.layers.15.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=583), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=583), )] (%8698:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=583)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%8700:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=583)])
            linalg.CPU.RoPEOp <name="model.layers.15.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=585), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=585), )] (%8699:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=585)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%8701:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=585)])
            linalg.CPU.CastTypeOp <name="model.layers.15.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=585), outputs_0:QuantSpec(Raw(type: Float16), uuid=587), )] (%8701:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=585)]) -> (%8702:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=587)])
            linalg.CPU.CastTypeOp <name="model.layers.15.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=587), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=588), )] (%8702:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=587)]) -> (%8703:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=588)])
            linalg.CPU.TransposeOp <name="model.layers.15.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=588), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=588), )] (%8703:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=588)]) -> (%8704:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=588)])
            linalg.CPU.CastTypeOp <name="model.layers.15.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=581), outputs_0:QuantSpec(Raw(type: Float16), uuid=589), )] (%8697:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=581)]) -> (%8705:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=589)])
            linalg.CPU.CastTypeOp <name="model.layers.15.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=589), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=590), )] (%8705:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=589)]) -> (%8706:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=590)])
            linalg.CPU.ConcatOp <name="model.layers.15.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=18), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=588), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=18), )] (%8045:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=18)], %8704:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=588)]) -> (%8707:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=18)])
            linalg.CPU.ConcatOp <name="model.layers.15.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=46), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=590), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=46), )] (%8046:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=46)], %8706:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=590)]) -> (%8708:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=46)])
            linalg.CPU.RepeatOp <name="model.layers.15.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=18), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=18), )] (%8707:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=18)]) -> (%8709:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=18)])
            linalg.CPU.RepeatOp <name="model.layers.15.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=46), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=46), )] (%8708:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=46)]) -> (%8710:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=46)])
            linalg.CPU.MatMulOp <name="model.layers.15.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=583), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=18), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=591), )] (%8700:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=583)], %8709:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=18)]) -> (%8711:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=591)])
            linalg.CPU.MulOp <name="model.layers.15.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=591), inputs_1:QuantSpec(Raw(type: Float32), uuid=592), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=591), )] (%8711:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=591)], %8712:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=592), constant:[0.088388346]]) -> (%8713:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=591)])
            linalg.CPU.ReduceMinOp <name="model.layers.15.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=591), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=593), )] (%8713:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=591)]) -> (%8714:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=593)])
            linalg.CPU.AddOp <name="model.layers.15.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=593), inputs_1:QuantSpec(Raw(type: Int16), uuid=594), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=593), )] (%8714:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=593)], %8715:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=594), constant:[-20]]) -> (%8716:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=593)])
            linalg.CPU.EqualOp <name="model.layers.15.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=595), outputs_0:QuantSpec(Raw(type: UInt8), uuid=596), )] (%8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8717:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=595), constant:[0.953125]]) -> (%8718:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=596)])
            linalg.CPU.WhereOp <name="model.layers.15.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=596), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=591), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=593), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=593), )] (%8718:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=596)], %8713:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=591)], %8716:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=593)]) -> (%8719:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=593)])
            linalg.CPU.SoftmaxOp <name="model.layers.15.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=593), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=597), )] (%8719:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=593)]) -> (%8720:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=597)])
            linalg.CPU.MatMulOp <name="model.layers.15.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=597), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=46), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=598), )] (%8720:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=597)], %8710:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=46)]) -> (%8721:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=598)])
            linalg.CPU.TransposeOp <name="model.layers.15.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=598), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=598), )] (%8721:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=598)]) -> (%8722:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=598)])
            linalg.CPU.ViewOp <name="model.layers.15.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=598), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=598), )] (%8722:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=598)]) -> (%8722:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=598)])
            linalg.CPU.LinearOp <name="model.layers.15.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=598), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=600), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=599))] (%8722:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=598)]) -> (%8723:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=600)])
            cf.ReturnOp (%8723:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=600)], %8704:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=588)], %8706:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=590)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.15.mlp <CPU> [using_qnn:true, symbol:model.layers.15.mlp] {
        (%8725:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=601)]) -> (%8730:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=609)]) {
            linalg.CPU.LinearOp <name="model.layers.15.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=601), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=604), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=603))] (%8725:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=601)]) -> (%8726:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=604)])
            linalg.CPU.SiLUOp <name="model.layers.15.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=604), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=605), )] (%8726:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=604)]) -> (%8727:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=605)])
            linalg.CPU.LinearOp <name="model.layers.15.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=601), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=607), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=606))] (%8725:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=601)]) -> (%8728:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=607)])
            linalg.CPU.MulOp <name="model.layers.15.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=605), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=607), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=605), )] (%8727:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=605)], %8728:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=607)]) -> (%8729:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=605)])
            linalg.CPU.LinearOp <name="model.layers.15.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=605), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=609), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=608))] (%8729:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=605)]) -> (%8730:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=609)])
            cf.ReturnOp (%8730:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=609)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.16 <CPU> [using_qnn:true, symbol:model.layers.16] {
        (%8731:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=609)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8047:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=19)], %8048:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=47)]) -> (%8772:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=643)], %8745:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=622)], %8747:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=624)]) {
            linalg.CPU.RMSNormOp <name="model.layers.16.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=609), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=610), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=611))] (%8731:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=609)]) -> (%8732:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=610)])
            graph.CallGraphOp @model.layers.16.self_attn (%8732:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=610)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8047:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=19)], %8048:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=47)]) -> (%8764:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=634)], %8745:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=622)], %8747:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=624)])
            linalg.CPU.AddOp <name="model.layers.16.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=634), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=609), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=634), )] (%8764:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=634)], %8731:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=609)]) -> (%8765:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=634)])
            linalg.CPU.RMSNormOp <name="model.layers.16.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=634), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=635), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=636))] (%8765:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=634)]) -> (%8766:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=635)])
            graph.CallGraphOp @model.layers.16.mlp (%8766:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=635)]) -> (%8771:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=643)])
            linalg.CPU.AddOp <name="model.layers.16.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=643), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=634), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=643), )] (%8771:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=643)], %8765:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=634)]) -> (%8772:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=643)])
            cf.ReturnOp (%8772:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=643)], %8745:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=622)], %8747:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=624)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.16.self_attn <CPU> [using_qnn:true, symbol:model.layers.16.self_attn] {
        (%8732:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=610)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8047:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=19)], %8048:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=47)]) -> (%8764:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=634)], %8745:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=622)], %8747:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=624)]) {
            linalg.CPU.LinearOp <name="model.layers.16.self_attn.q_proj">(%8732:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=610)]) -> (%8733:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=616)])
            linalg.CPU.LinearOp <name="model.layers.16.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=610), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=613), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=612))] (%8732:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=610)]) -> (%8734:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=613)])
            linalg.CPU.LinearOp <name="model.layers.16.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=610), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=615), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=614))] (%8732:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=610)]) -> (%8735:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=615)])
            linalg.CPU.ViewOp <name="model.layers.16.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=616), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=616), )] (%8733:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=616)]) -> (%8733:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=616)])
            linalg.CPU.TransposeOp <name="model.layers.16.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=616), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=616), )] (%8733:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=616)]) -> (%8736:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=616)])
            linalg.CPU.ViewOp <name="model.layers.16.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=613), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=613), )] (%8734:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=613)]) -> (%8734:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=613)])
            linalg.CPU.TransposeOp <name="model.layers.16.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=613), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=613), )] (%8734:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=613)]) -> (%8737:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=613)])
            linalg.CPU.ViewOp <name="model.layers.16.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=615), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=615), )] (%8735:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=615)]) -> (%8735:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=615)])
            linalg.CPU.TransposeOp <name="model.layers.16.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=615), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=615), )] (%8735:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=615)]) -> (%8738:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=615)])
            linalg.CPU.RMSNormOp <name="model.layers.16.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=616), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=617), weight_weight:QuantSpec(Raw(type: Int16PerTensor), uuid=618))] (%8736:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=616)]) -> (%8739:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=617)])
            linalg.CPU.RMSNormOp <name="model.layers.16.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=613), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=619), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=620))] (%8737:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=613)]) -> (%8740:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=619)])
            linalg.CPU.RoPEOp <name="model.layers.16.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=617), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=617), )] (%8739:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=617)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%8741:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=617)])
            linalg.CPU.RoPEOp <name="model.layers.16.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=619), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=619), )] (%8740:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=619)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%8742:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=619)])
            linalg.CPU.CastTypeOp <name="model.layers.16.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=619), outputs_0:QuantSpec(Raw(type: Float16), uuid=621), )] (%8742:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=619)]) -> (%8743:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=621)])
            linalg.CPU.CastTypeOp <name="model.layers.16.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=621), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=622), )] (%8743:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=621)]) -> (%8744:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=622)])
            linalg.CPU.TransposeOp <name="model.layers.16.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=622), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=622), )] (%8744:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=622)]) -> (%8745:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=622)])
            linalg.CPU.CastTypeOp <name="model.layers.16.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=615), outputs_0:QuantSpec(Raw(type: Float16), uuid=623), )] (%8738:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=615)]) -> (%8746:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=623)])
            linalg.CPU.CastTypeOp <name="model.layers.16.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=623), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=624), )] (%8746:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=623)]) -> (%8747:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=624)])
            linalg.CPU.ConcatOp <name="model.layers.16.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=19), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=622), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=19), )] (%8047:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=19)], %8745:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=622)]) -> (%8748:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=19)])
            linalg.CPU.ConcatOp <name="model.layers.16.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=47), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=624), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=47), )] (%8048:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=47)], %8747:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=624)]) -> (%8749:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=47)])
            linalg.CPU.RepeatOp <name="model.layers.16.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=19), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=19), )] (%8748:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=19)]) -> (%8750:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=19)])
            linalg.CPU.RepeatOp <name="model.layers.16.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=47), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=47), )] (%8749:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=47)]) -> (%8751:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=47)])
            linalg.CPU.MatMulOp <name="model.layers.16.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=617), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=19), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=625), )] (%8741:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=617)], %8750:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=19)]) -> (%8752:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=625)])
            linalg.CPU.MulOp <name="model.layers.16.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=625), inputs_1:QuantSpec(Raw(type: Float32), uuid=626), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=625), )] (%8752:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=625)], %8753:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=626), constant:[0.088388346]]) -> (%8754:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=625)])
            linalg.CPU.ReduceMinOp <name="model.layers.16.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=625), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=627), )] (%8754:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=625)]) -> (%8755:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=627)])
            linalg.CPU.AddOp <name="model.layers.16.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=627), inputs_1:QuantSpec(Raw(type: Int16), uuid=628), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=627), )] (%8755:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=627)], %8756:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=628), constant:[-20]]) -> (%8757:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=627)])
            linalg.CPU.EqualOp <name="model.layers.16.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=629), outputs_0:QuantSpec(Raw(type: UInt8), uuid=630), )] (%8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8758:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=629), constant:[0.118652344]]) -> (%8759:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=630)])
            linalg.CPU.WhereOp <name="model.layers.16.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=630), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=625), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=627), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=627), )] (%8759:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=630)], %8754:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=625)], %8757:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=627)]) -> (%8760:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=627)])
            linalg.CPU.SoftmaxOp <name="model.layers.16.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=627), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=631), )] (%8760:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=627)]) -> (%8761:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=631)])
            linalg.CPU.MatMulOp <name="model.layers.16.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=631), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=47), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=632), )] (%8761:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=631)], %8751:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=47)]) -> (%8762:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=632)])
            linalg.CPU.TransposeOp <name="model.layers.16.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=632), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=632), )] (%8762:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=632)]) -> (%8763:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=632)])
            linalg.CPU.ViewOp <name="model.layers.16.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=632), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=632), )] (%8763:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=632)]) -> (%8763:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=632)])
            linalg.CPU.LinearOp <name="model.layers.16.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=632), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=634), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=633))] (%8763:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=632)]) -> (%8764:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=634)])
            cf.ReturnOp (%8764:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=634)], %8745:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=622)], %8747:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=624)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.16.mlp <CPU> [using_qnn:true, symbol:model.layers.16.mlp] {
        (%8766:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=635)]) -> (%8771:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=643)]) {
            linalg.CPU.LinearOp <name="model.layers.16.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=635), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=638), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=637))] (%8766:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=635)]) -> (%8767:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=638)])
            linalg.CPU.SiLUOp <name="model.layers.16.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=638), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=639), )] (%8767:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=638)]) -> (%8768:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=639)])
            linalg.CPU.LinearOp <name="model.layers.16.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=635), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=641), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=640))] (%8766:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=635)]) -> (%8769:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=641)])
            linalg.CPU.MulOp <name="model.layers.16.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=639), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=641), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=639), )] (%8768:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=639)], %8769:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=641)]) -> (%8770:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=639)])
            linalg.CPU.LinearOp <name="model.layers.16.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=639), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=643), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=642))] (%8770:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=639)]) -> (%8771:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=643)])
            cf.ReturnOp (%8771:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=643)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.17 <CPU> [using_qnn:true, symbol:model.layers.17] {
        (%8772:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=643)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8049:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=20)], %8050:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=48)]) -> (%8813:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=677)], %8786:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=656)], %8788:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=658)]) {
            linalg.CPU.RMSNormOp <name="model.layers.17.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=643), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=644), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=645))] (%8772:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=643)]) -> (%8773:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=644)])
            graph.CallGraphOp @model.layers.17.self_attn (%8773:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=644)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8049:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=20)], %8050:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=48)]) -> (%8805:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=668)], %8786:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=656)], %8788:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=658)])
            linalg.CPU.AddOp <name="model.layers.17.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=668), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=643), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=668), )] (%8805:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=668)], %8772:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=643)]) -> (%8806:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=668)])
            linalg.CPU.RMSNormOp <name="model.layers.17.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=668), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=669), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=670))] (%8806:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=668)]) -> (%8807:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=669)])
            graph.CallGraphOp @model.layers.17.mlp (%8807:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=669)]) -> (%8812:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=677)])
            linalg.CPU.AddOp <name="model.layers.17.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=677), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=668), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=677), )] (%8812:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=677)], %8806:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=668)]) -> (%8813:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=677)])
            cf.ReturnOp (%8813:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=677)], %8786:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=656)], %8788:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=658)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.17.self_attn <CPU> [using_qnn:true, symbol:model.layers.17.self_attn] {
        (%8773:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=644)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8049:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=20)], %8050:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=48)]) -> (%8805:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=668)], %8786:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=656)], %8788:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=658)]) {
            linalg.CPU.LinearOp <name="model.layers.17.self_attn.q_proj">(%8773:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=644)]) -> (%8774:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=650)])
            linalg.CPU.LinearOp <name="model.layers.17.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=644), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=647), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=646))] (%8773:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=644)]) -> (%8775:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=647)])
            linalg.CPU.LinearOp <name="model.layers.17.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=644), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=649), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=648))] (%8773:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=644)]) -> (%8776:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=649)])
            linalg.CPU.ViewOp <name="model.layers.17.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=650), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=650), )] (%8774:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=650)]) -> (%8774:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=650)])
            linalg.CPU.TransposeOp <name="model.layers.17.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=650), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=650), )] (%8774:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=650)]) -> (%8777:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=650)])
            linalg.CPU.ViewOp <name="model.layers.17.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=647), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=647), )] (%8775:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=647)]) -> (%8775:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=647)])
            linalg.CPU.TransposeOp <name="model.layers.17.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=647), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=647), )] (%8775:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=647)]) -> (%8778:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=647)])
            linalg.CPU.ViewOp <name="model.layers.17.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=649), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=649), )] (%8776:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=649)]) -> (%8776:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=649)])
            linalg.CPU.TransposeOp <name="model.layers.17.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=649), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=649), )] (%8776:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=649)]) -> (%8779:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=649)])
            linalg.CPU.RMSNormOp <name="model.layers.17.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=650), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=651), weight_weight:QuantSpec(Raw(type: Int16PerTensor), uuid=652))] (%8777:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=650)]) -> (%8780:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=651)])
            linalg.CPU.RMSNormOp <name="model.layers.17.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=647), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=653), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=654))] (%8778:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=647)]) -> (%8781:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=653)])
            linalg.CPU.RoPEOp <name="model.layers.17.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=651), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=651), )] (%8780:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=651)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%8782:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=651)])
            linalg.CPU.RoPEOp <name="model.layers.17.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=653), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=653), )] (%8781:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=653)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%8783:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=653)])
            linalg.CPU.CastTypeOp <name="model.layers.17.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=653), outputs_0:QuantSpec(Raw(type: Float16), uuid=655), )] (%8783:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=653)]) -> (%8784:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=655)])
            linalg.CPU.CastTypeOp <name="model.layers.17.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=655), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=656), )] (%8784:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=655)]) -> (%8785:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=656)])
            linalg.CPU.TransposeOp <name="model.layers.17.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=656), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=656), )] (%8785:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=656)]) -> (%8786:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=656)])
            linalg.CPU.CastTypeOp <name="model.layers.17.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=649), outputs_0:QuantSpec(Raw(type: Float16), uuid=657), )] (%8779:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=649)]) -> (%8787:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=657)])
            linalg.CPU.CastTypeOp <name="model.layers.17.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=657), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=658), )] (%8787:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=657)]) -> (%8788:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=658)])
            linalg.CPU.ConcatOp <name="model.layers.17.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=20), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=656), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=20), )] (%8049:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=20)], %8786:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=656)]) -> (%8789:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=20)])
            linalg.CPU.ConcatOp <name="model.layers.17.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=48), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=658), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=48), )] (%8050:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=48)], %8788:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=658)]) -> (%8790:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=48)])
            linalg.CPU.RepeatOp <name="model.layers.17.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=20), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=20), )] (%8789:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=20)]) -> (%8791:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=20)])
            linalg.CPU.RepeatOp <name="model.layers.17.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=48), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=48), )] (%8790:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=48)]) -> (%8792:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=48)])
            linalg.CPU.MatMulOp <name="model.layers.17.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=651), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=20), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=659), )] (%8782:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=651)], %8791:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=20)]) -> (%8793:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=659)])
            linalg.CPU.MulOp <name="model.layers.17.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=659), inputs_1:QuantSpec(Raw(type: Float32), uuid=660), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=659), )] (%8793:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=659)], %8794:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=660), constant:[0.088388346]]) -> (%8795:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=659)])
            linalg.CPU.ReduceMinOp <name="model.layers.17.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=659), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=661), )] (%8795:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=659)]) -> (%8796:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=661)])
            linalg.CPU.AddOp <name="model.layers.17.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=661), inputs_1:QuantSpec(Raw(type: Int16), uuid=662), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=661), )] (%8796:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=661)], %8797:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=662), constant:[-20]]) -> (%8798:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=661)])
            linalg.CPU.EqualOp <name="model.layers.17.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=663), outputs_0:QuantSpec(Raw(type: UInt8), uuid=664), )] (%8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8799:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=663), constant:[-0.99609375]]) -> (%8800:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=664)])
            linalg.CPU.WhereOp <name="model.layers.17.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=664), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=659), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=661), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=661), )] (%8800:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=664)], %8795:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=659)], %8798:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=661)]) -> (%8801:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=661)])
            linalg.CPU.SoftmaxOp <name="model.layers.17.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=661), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=665), )] (%8801:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=661)]) -> (%8802:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=665)])
            linalg.CPU.MatMulOp <name="model.layers.17.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=665), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=48), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=666), )] (%8802:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=665)], %8792:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=48)]) -> (%8803:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=666)])
            linalg.CPU.TransposeOp <name="model.layers.17.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=666), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=666), )] (%8803:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=666)]) -> (%8804:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=666)])
            linalg.CPU.ViewOp <name="model.layers.17.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=666), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=666), )] (%8804:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=666)]) -> (%8804:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=666)])
            linalg.CPU.LinearOp <name="model.layers.17.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=666), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=668), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=667))] (%8804:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=666)]) -> (%8805:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=668)])
            cf.ReturnOp (%8805:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=668)], %8786:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=656)], %8788:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=658)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.17.mlp <CPU> [using_qnn:true, symbol:model.layers.17.mlp] {
        (%8807:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=669)]) -> (%8812:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=677)]) {
            linalg.CPU.LinearOp <name="model.layers.17.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=669), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=672), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=671))] (%8807:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=669)]) -> (%8808:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=672)])
            linalg.CPU.SiLUOp <name="model.layers.17.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=672), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=673), )] (%8808:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=672)]) -> (%8809:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=673)])
            linalg.CPU.LinearOp <name="model.layers.17.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=669), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=675), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=674))] (%8807:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=669)]) -> (%8810:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=675)])
            linalg.CPU.MulOp <name="model.layers.17.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=673), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=675), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=673), )] (%8809:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=673)], %8810:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=675)]) -> (%8811:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=673)])
            linalg.CPU.LinearOp <name="model.layers.17.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=673), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=677), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=676))] (%8811:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=673)]) -> (%8812:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=677)])
            cf.ReturnOp (%8812:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=677)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.18 <CPU> [using_qnn:true, symbol:model.layers.18] {
        (%8813:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=677)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8051:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=21)], %8052:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=49)]) -> (%8854:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=711)], %8827:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=690)], %8829:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=692)]) {
            linalg.CPU.RMSNormOp <name="model.layers.18.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=677), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=678), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=679))] (%8813:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=677)]) -> (%8814:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=678)])
            graph.CallGraphOp @model.layers.18.self_attn (%8814:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=678)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8051:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=21)], %8052:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=49)]) -> (%8846:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=702)], %8827:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=690)], %8829:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=692)])
            linalg.CPU.AddOp <name="model.layers.18.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=702), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=677), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=702), )] (%8846:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=702)], %8813:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=677)]) -> (%8847:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=702)])
            linalg.CPU.RMSNormOp <name="model.layers.18.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=702), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=703), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=704))] (%8847:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=702)]) -> (%8848:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=703)])
            graph.CallGraphOp @model.layers.18.mlp (%8848:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=703)]) -> (%8853:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=711)])
            linalg.CPU.AddOp <name="model.layers.18.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=711), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=702), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=711), )] (%8853:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=711)], %8847:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=702)]) -> (%8854:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=711)])
            cf.ReturnOp (%8854:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=711)], %8827:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=690)], %8829:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=692)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.18.self_attn <CPU> [using_qnn:true, symbol:model.layers.18.self_attn] {
        (%8814:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=678)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8051:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=21)], %8052:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=49)]) -> (%8846:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=702)], %8827:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=690)], %8829:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=692)]) {
            linalg.CPU.LinearOp <name="model.layers.18.self_attn.q_proj">(%8814:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=678)]) -> (%8815:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=684)])
            linalg.CPU.LinearOp <name="model.layers.18.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=678), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=681), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=680))] (%8814:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=678)]) -> (%8816:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=681)])
            linalg.CPU.LinearOp <name="model.layers.18.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=678), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=683), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=682))] (%8814:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=678)]) -> (%8817:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=683)])
            linalg.CPU.ViewOp <name="model.layers.18.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=684), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=684), )] (%8815:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=684)]) -> (%8815:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=684)])
            linalg.CPU.TransposeOp <name="model.layers.18.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=684), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=684), )] (%8815:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=684)]) -> (%8818:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=684)])
            linalg.CPU.ViewOp <name="model.layers.18.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=681), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=681), )] (%8816:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=681)]) -> (%8816:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=681)])
            linalg.CPU.TransposeOp <name="model.layers.18.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=681), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=681), )] (%8816:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=681)]) -> (%8819:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=681)])
            linalg.CPU.ViewOp <name="model.layers.18.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=683), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=683), )] (%8817:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=683)]) -> (%8817:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=683)])
            linalg.CPU.TransposeOp <name="model.layers.18.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=683), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=683), )] (%8817:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=683)]) -> (%8820:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=683)])
            linalg.CPU.RMSNormOp <name="model.layers.18.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=684), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=685), weight_weight:QuantSpec(Raw(type: Int16PerTensor), uuid=686))] (%8818:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=684)]) -> (%8821:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=685)])
            linalg.CPU.RMSNormOp <name="model.layers.18.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=681), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=687), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=688))] (%8819:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=681)]) -> (%8822:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=687)])
            linalg.CPU.RoPEOp <name="model.layers.18.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=685), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=685), )] (%8821:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=685)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%8823:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=685)])
            linalg.CPU.RoPEOp <name="model.layers.18.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=687), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=687), )] (%8822:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=687)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%8824:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=687)])
            linalg.CPU.CastTypeOp <name="model.layers.18.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=687), outputs_0:QuantSpec(Raw(type: Float16), uuid=689), )] (%8824:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=687)]) -> (%8825:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=689)])
            linalg.CPU.CastTypeOp <name="model.layers.18.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=689), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=690), )] (%8825:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=689)]) -> (%8826:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=690)])
            linalg.CPU.TransposeOp <name="model.layers.18.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=690), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=690), )] (%8826:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=690)]) -> (%8827:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=690)])
            linalg.CPU.CastTypeOp <name="model.layers.18.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=683), outputs_0:QuantSpec(Raw(type: Float16), uuid=691), )] (%8820:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=683)]) -> (%8828:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=691)])
            linalg.CPU.CastTypeOp <name="model.layers.18.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=691), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=692), )] (%8828:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=691)]) -> (%8829:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=692)])
            linalg.CPU.ConcatOp <name="model.layers.18.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=21), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=690), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=21), )] (%8051:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=21)], %8827:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=690)]) -> (%8830:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=21)])
            linalg.CPU.ConcatOp <name="model.layers.18.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=49), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=692), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=49), )] (%8052:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=49)], %8829:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=692)]) -> (%8831:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=49)])
            linalg.CPU.RepeatOp <name="model.layers.18.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=21), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=21), )] (%8830:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=21)]) -> (%8832:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=21)])
            linalg.CPU.RepeatOp <name="model.layers.18.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=49), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=49), )] (%8831:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=49)]) -> (%8833:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=49)])
            linalg.CPU.MatMulOp <name="model.layers.18.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=685), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=21), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=693), )] (%8823:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=685)], %8832:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=21)]) -> (%8834:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=693)])
            linalg.CPU.MulOp <name="model.layers.18.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=693), inputs_1:QuantSpec(Raw(type: Float32), uuid=694), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=693), )] (%8834:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=693)], %8835:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=694), constant:[0.088388346]]) -> (%8836:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=693)])
            linalg.CPU.ReduceMinOp <name="model.layers.18.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=693), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=695), )] (%8836:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=693)]) -> (%8837:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=695)])
            linalg.CPU.AddOp <name="model.layers.18.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=695), inputs_1:QuantSpec(Raw(type: Int16), uuid=696), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=695), )] (%8837:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=695)], %8838:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=696), constant:[-20]]) -> (%8839:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=695)])
            linalg.CPU.EqualOp <name="model.layers.18.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=697), outputs_0:QuantSpec(Raw(type: UInt8), uuid=698), )] (%8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8840:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=697), constant:[0.24023438]]) -> (%8841:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=698)])
            linalg.CPU.WhereOp <name="model.layers.18.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=698), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=693), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=695), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=695), )] (%8841:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=698)], %8836:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=693)], %8839:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=695)]) -> (%8842:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=695)])
            linalg.CPU.SoftmaxOp <name="model.layers.18.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=695), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=699), )] (%8842:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=695)]) -> (%8843:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=699)])
            linalg.CPU.MatMulOp <name="model.layers.18.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=699), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=49), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=700), )] (%8843:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=699)], %8833:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=49)]) -> (%8844:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=700)])
            linalg.CPU.TransposeOp <name="model.layers.18.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=700), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=700), )] (%8844:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=700)]) -> (%8845:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=700)])
            linalg.CPU.ViewOp <name="model.layers.18.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=700), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=700), )] (%8845:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=700)]) -> (%8845:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=700)])
            linalg.CPU.LinearOp <name="model.layers.18.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=700), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=702), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=701))] (%8845:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=700)]) -> (%8846:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=702)])
            cf.ReturnOp (%8846:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=702)], %8827:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=690)], %8829:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=692)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.18.mlp <CPU> [using_qnn:true, symbol:model.layers.18.mlp] {
        (%8848:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=703)]) -> (%8853:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=711)]) {
            linalg.CPU.LinearOp <name="model.layers.18.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=703), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=706), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=705))] (%8848:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=703)]) -> (%8849:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=706)])
            linalg.CPU.SiLUOp <name="model.layers.18.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=706), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=707), )] (%8849:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=706)]) -> (%8850:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=707)])
            linalg.CPU.LinearOp <name="model.layers.18.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=703), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=709), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=708))] (%8848:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=703)]) -> (%8851:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=709)])
            linalg.CPU.MulOp <name="model.layers.18.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=707), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=709), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=707), )] (%8850:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=707)], %8851:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=709)]) -> (%8852:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=707)])
            linalg.CPU.LinearOp <name="model.layers.18.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=707), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=711), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=710))] (%8852:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=707)]) -> (%8853:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=711)])
            cf.ReturnOp (%8853:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=711)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.19 <CPU> [using_qnn:true, symbol:model.layers.19] {
        (%8854:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=711)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8053:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=22)], %8054:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=50)]) -> (%8895:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=745)], %8868:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=724)], %8870:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=726)]) {
            linalg.CPU.RMSNormOp <name="model.layers.19.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=711), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=712), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=713))] (%8854:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=711)]) -> (%8855:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=712)])
            graph.CallGraphOp @model.layers.19.self_attn (%8855:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=712)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8053:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=22)], %8054:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=50)]) -> (%8887:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=736)], %8868:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=724)], %8870:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=726)])
            linalg.CPU.AddOp <name="model.layers.19.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=736), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=711), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=736), )] (%8887:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=736)], %8854:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=711)]) -> (%8888:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=736)])
            linalg.CPU.RMSNormOp <name="model.layers.19.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=736), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=737), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=738))] (%8888:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=736)]) -> (%8889:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=737)])
            graph.CallGraphOp @model.layers.19.mlp (%8889:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=737)]) -> (%8894:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=745)])
            linalg.CPU.AddOp <name="model.layers.19.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=745), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=736), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=745), )] (%8894:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=745)], %8888:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=736)]) -> (%8895:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=745)])
            cf.ReturnOp (%8895:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=745)], %8868:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=724)], %8870:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=726)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.19.self_attn <CPU> [using_qnn:true, symbol:model.layers.19.self_attn] {
        (%8855:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=712)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8053:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=22)], %8054:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=50)]) -> (%8887:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=736)], %8868:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=724)], %8870:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=726)]) {
            linalg.CPU.LinearOp <name="model.layers.19.self_attn.q_proj">(%8855:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=712)]) -> (%8856:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=718)])
            linalg.CPU.LinearOp <name="model.layers.19.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=712), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=715), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=714))] (%8855:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=712)]) -> (%8857:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=715)])
            linalg.CPU.LinearOp <name="model.layers.19.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=712), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=717), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=716))] (%8855:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=712)]) -> (%8858:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=717)])
            linalg.CPU.ViewOp <name="model.layers.19.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=718), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=718), )] (%8856:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=718)]) -> (%8856:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=718)])
            linalg.CPU.TransposeOp <name="model.layers.19.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=718), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=718), )] (%8856:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=718)]) -> (%8859:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=718)])
            linalg.CPU.ViewOp <name="model.layers.19.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=715), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=715), )] (%8857:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=715)]) -> (%8857:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=715)])
            linalg.CPU.TransposeOp <name="model.layers.19.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=715), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=715), )] (%8857:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=715)]) -> (%8860:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=715)])
            linalg.CPU.ViewOp <name="model.layers.19.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=717), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=717), )] (%8858:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=717)]) -> (%8858:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=717)])
            linalg.CPU.TransposeOp <name="model.layers.19.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=717), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=717), )] (%8858:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=717)]) -> (%8861:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=717)])
            linalg.CPU.RMSNormOp <name="model.layers.19.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=718), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=719), weight_weight:QuantSpec(Raw(type: Int16PerTensor), uuid=720))] (%8859:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=718)]) -> (%8862:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=719)])
            linalg.CPU.RMSNormOp <name="model.layers.19.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=715), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=721), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=722))] (%8860:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=715)]) -> (%8863:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=721)])
            linalg.CPU.RoPEOp <name="model.layers.19.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=719), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=719), )] (%8862:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=719)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%8864:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=719)])
            linalg.CPU.RoPEOp <name="model.layers.19.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=721), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=721), )] (%8863:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=721)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%8865:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=721)])
            linalg.CPU.CastTypeOp <name="model.layers.19.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=721), outputs_0:QuantSpec(Raw(type: Float16), uuid=723), )] (%8865:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=721)]) -> (%8866:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=723)])
            linalg.CPU.CastTypeOp <name="model.layers.19.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=723), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=724), )] (%8866:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=723)]) -> (%8867:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=724)])
            linalg.CPU.TransposeOp <name="model.layers.19.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=724), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=724), )] (%8867:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=724)]) -> (%8868:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=724)])
            linalg.CPU.CastTypeOp <name="model.layers.19.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=717), outputs_0:QuantSpec(Raw(type: Float16), uuid=725), )] (%8861:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=717)]) -> (%8869:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=725)])
            linalg.CPU.CastTypeOp <name="model.layers.19.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=725), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=726), )] (%8869:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=725)]) -> (%8870:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=726)])
            linalg.CPU.ConcatOp <name="model.layers.19.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=22), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=724), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=22), )] (%8053:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=22)], %8868:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=724)]) -> (%8871:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=22)])
            linalg.CPU.ConcatOp <name="model.layers.19.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=50), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=726), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=50), )] (%8054:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=50)], %8870:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=726)]) -> (%8872:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=50)])
            linalg.CPU.RepeatOp <name="model.layers.19.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=22), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=22), )] (%8871:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=22)]) -> (%8873:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=22)])
            linalg.CPU.RepeatOp <name="model.layers.19.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=50), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=50), )] (%8872:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=50)]) -> (%8874:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=50)])
            linalg.CPU.MatMulOp <name="model.layers.19.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=719), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=22), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=727), )] (%8864:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=719)], %8873:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=22)]) -> (%8875:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=727)])
            linalg.CPU.MulOp <name="model.layers.19.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=727), inputs_1:QuantSpec(Raw(type: Float32), uuid=728), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=727), )] (%8875:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=727)], %8876:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=728), constant:[0.088388346]]) -> (%8877:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=727)])
            linalg.CPU.ReduceMinOp <name="model.layers.19.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=727), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=729), )] (%8877:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=727)]) -> (%8878:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=729)])
            linalg.CPU.AddOp <name="model.layers.19.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=729), inputs_1:QuantSpec(Raw(type: Int16), uuid=730), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=729), )] (%8878:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=729)], %8879:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=730), constant:[-20]]) -> (%8880:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=729)])
            linalg.CPU.EqualOp <name="model.layers.19.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=731), outputs_0:QuantSpec(Raw(type: UInt8), uuid=732), )] (%8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8881:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=731), constant:[0.55078125]]) -> (%8882:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=732)])
            linalg.CPU.WhereOp <name="model.layers.19.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=732), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=727), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=729), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=729), )] (%8882:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=732)], %8877:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=727)], %8880:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=729)]) -> (%8883:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=729)])
            linalg.CPU.SoftmaxOp <name="model.layers.19.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=729), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=733), )] (%8883:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=729)]) -> (%8884:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=733)])
            linalg.CPU.MatMulOp <name="model.layers.19.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=733), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=50), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=734), )] (%8884:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=733)], %8874:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=50)]) -> (%8885:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=734)])
            linalg.CPU.TransposeOp <name="model.layers.19.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=734), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=734), )] (%8885:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=734)]) -> (%8886:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=734)])
            linalg.CPU.ViewOp <name="model.layers.19.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=734), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=734), )] (%8886:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=734)]) -> (%8886:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=734)])
            linalg.CPU.LinearOp <name="model.layers.19.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=734), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=736), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=735))] (%8886:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=734)]) -> (%8887:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=736)])
            cf.ReturnOp (%8887:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=736)], %8868:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=724)], %8870:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=726)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.19.mlp <CPU> [using_qnn:true, symbol:model.layers.19.mlp] {
        (%8889:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=737)]) -> (%8894:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=745)]) {
            linalg.CPU.LinearOp <name="model.layers.19.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=737), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=740), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=739))] (%8889:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=737)]) -> (%8890:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=740)])
            linalg.CPU.SiLUOp <name="model.layers.19.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=740), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=741), )] (%8890:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=740)]) -> (%8891:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=741)])
            linalg.CPU.LinearOp <name="model.layers.19.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=737), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=743), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=742))] (%8889:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=737)]) -> (%8892:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=743)])
            linalg.CPU.MulOp <name="model.layers.19.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=741), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=743), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=741), )] (%8891:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=741)], %8892:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=743)]) -> (%8893:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=741)])
            linalg.CPU.LinearOp <name="model.layers.19.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=741), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=745), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=744))] (%8893:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=741)]) -> (%8894:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=745)])
            cf.ReturnOp (%8894:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=745)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.20 <CPU> [using_qnn:true, symbol:model.layers.20] {
        (%8895:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=745)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8055:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=23)], %8056:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=51)]) -> (%8936:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=779)], %8909:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=758)], %8911:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=760)]) {
            linalg.CPU.RMSNormOp <name="model.layers.20.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=745), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=746), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=747))] (%8895:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=745)]) -> (%8896:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=746)])
            graph.CallGraphOp @model.layers.20.self_attn (%8896:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=746)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8055:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=23)], %8056:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=51)]) -> (%8928:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=770)], %8909:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=758)], %8911:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=760)])
            linalg.CPU.AddOp <name="model.layers.20.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=770), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=745), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=770), )] (%8928:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=770)], %8895:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=745)]) -> (%8929:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=770)])
            linalg.CPU.RMSNormOp <name="model.layers.20.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=770), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=771), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=772))] (%8929:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=770)]) -> (%8930:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=771)])
            graph.CallGraphOp @model.layers.20.mlp (%8930:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=771)]) -> (%8935:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=779)])
            linalg.CPU.AddOp <name="model.layers.20.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=779), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=770), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=779), )] (%8935:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=779)], %8929:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=770)]) -> (%8936:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=779)])
            cf.ReturnOp (%8936:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=779)], %8909:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=758)], %8911:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=760)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.20.self_attn <CPU> [using_qnn:true, symbol:model.layers.20.self_attn] {
        (%8896:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=746)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8055:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=23)], %8056:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=51)]) -> (%8928:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=770)], %8909:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=758)], %8911:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=760)]) {
            linalg.CPU.LinearOp <name="model.layers.20.self_attn.q_proj">(%8896:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=746)]) -> (%8897:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=752)])
            linalg.CPU.LinearOp <name="model.layers.20.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=746), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=749), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=748))] (%8896:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=746)]) -> (%8898:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=749)])
            linalg.CPU.LinearOp <name="model.layers.20.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=746), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=751), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=750))] (%8896:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=746)]) -> (%8899:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=751)])
            linalg.CPU.ViewOp <name="model.layers.20.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=752), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=752), )] (%8897:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=752)]) -> (%8897:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=752)])
            linalg.CPU.TransposeOp <name="model.layers.20.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=752), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=752), )] (%8897:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=752)]) -> (%8900:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=752)])
            linalg.CPU.ViewOp <name="model.layers.20.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=749), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=749), )] (%8898:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=749)]) -> (%8898:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=749)])
            linalg.CPU.TransposeOp <name="model.layers.20.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=749), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=749), )] (%8898:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=749)]) -> (%8901:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=749)])
            linalg.CPU.ViewOp <name="model.layers.20.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=751), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=751), )] (%8899:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=751)]) -> (%8899:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=751)])
            linalg.CPU.TransposeOp <name="model.layers.20.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=751), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=751), )] (%8899:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=751)]) -> (%8902:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=751)])
            linalg.CPU.RMSNormOp <name="model.layers.20.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=752), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=753), weight_weight:QuantSpec(Raw(type: Int16PerTensor), uuid=754))] (%8900:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=752)]) -> (%8903:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=753)])
            linalg.CPU.RMSNormOp <name="model.layers.20.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=749), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=755), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=756))] (%8901:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=749)]) -> (%8904:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=755)])
            linalg.CPU.RoPEOp <name="model.layers.20.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=753), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=753), )] (%8903:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=753)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%8905:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=753)])
            linalg.CPU.RoPEOp <name="model.layers.20.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=755), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=755), )] (%8904:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=755)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%8906:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=755)])
            linalg.CPU.CastTypeOp <name="model.layers.20.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=755), outputs_0:QuantSpec(Raw(type: Float16), uuid=757), )] (%8906:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=755)]) -> (%8907:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=757)])
            linalg.CPU.CastTypeOp <name="model.layers.20.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=757), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=758), )] (%8907:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=757)]) -> (%8908:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=758)])
            linalg.CPU.TransposeOp <name="model.layers.20.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=758), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=758), )] (%8908:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=758)]) -> (%8909:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=758)])
            linalg.CPU.CastTypeOp <name="model.layers.20.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=751), outputs_0:QuantSpec(Raw(type: Float16), uuid=759), )] (%8902:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=751)]) -> (%8910:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=759)])
            linalg.CPU.CastTypeOp <name="model.layers.20.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=759), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=760), )] (%8910:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=759)]) -> (%8911:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=760)])
            linalg.CPU.ConcatOp <name="model.layers.20.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=23), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=758), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=23), )] (%8055:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=23)], %8909:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=758)]) -> (%8912:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=23)])
            linalg.CPU.ConcatOp <name="model.layers.20.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=51), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=760), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=51), )] (%8056:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=51)], %8911:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=760)]) -> (%8913:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=51)])
            linalg.CPU.RepeatOp <name="model.layers.20.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=23), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=23), )] (%8912:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=23)]) -> (%8914:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=23)])
            linalg.CPU.RepeatOp <name="model.layers.20.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=51), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=51), )] (%8913:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=51)]) -> (%8915:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=51)])
            linalg.CPU.MatMulOp <name="model.layers.20.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=753), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=23), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=761), )] (%8905:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=753)], %8914:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=23)]) -> (%8916:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=761)])
            linalg.CPU.MulOp <name="model.layers.20.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=761), inputs_1:QuantSpec(Raw(type: Float32), uuid=762), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=761), )] (%8916:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=761)], %8917:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=762), constant:[0.088388346]]) -> (%8918:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=761)])
            linalg.CPU.ReduceMinOp <name="model.layers.20.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=761), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=763), )] (%8918:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=761)]) -> (%8919:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=763)])
            linalg.CPU.AddOp <name="model.layers.20.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=763), inputs_1:QuantSpec(Raw(type: Int16), uuid=764), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=763), )] (%8919:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=763)], %8920:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=764), constant:[-20]]) -> (%8921:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=763)])
            linalg.CPU.EqualOp <name="model.layers.20.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=765), outputs_0:QuantSpec(Raw(type: UInt8), uuid=766), )] (%8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8922:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=765), constant:[0.71875]]) -> (%8923:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=766)])
            linalg.CPU.WhereOp <name="model.layers.20.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=766), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=761), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=763), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=763), )] (%8923:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=766)], %8918:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=761)], %8921:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=763)]) -> (%8924:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=763)])
            linalg.CPU.SoftmaxOp <name="model.layers.20.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=763), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=767), )] (%8924:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=763)]) -> (%8925:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=767)])
            linalg.CPU.MatMulOp <name="model.layers.20.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=767), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=51), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=768), )] (%8925:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=767)], %8915:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=51)]) -> (%8926:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=768)])
            linalg.CPU.TransposeOp <name="model.layers.20.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=768), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=768), )] (%8926:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=768)]) -> (%8927:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=768)])
            linalg.CPU.ViewOp <name="model.layers.20.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=768), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=768), )] (%8927:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=768)]) -> (%8927:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=768)])
            linalg.CPU.LinearOp <name="model.layers.20.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=768), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=770), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=769))] (%8927:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=768)]) -> (%8928:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=770)])
            cf.ReturnOp (%8928:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=770)], %8909:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=758)], %8911:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=760)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.20.mlp <CPU> [using_qnn:true, symbol:model.layers.20.mlp] {
        (%8930:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=771)]) -> (%8935:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=779)]) {
            linalg.CPU.LinearOp <name="model.layers.20.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=771), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=774), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=773))] (%8930:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=771)]) -> (%8931:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=774)])
            linalg.CPU.SiLUOp <name="model.layers.20.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=774), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=775), )] (%8931:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=774)]) -> (%8932:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=775)])
            linalg.CPU.LinearOp <name="model.layers.20.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=771), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=777), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=776))] (%8930:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=771)]) -> (%8933:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=777)])
            linalg.CPU.MulOp <name="model.layers.20.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=775), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=777), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=775), )] (%8932:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=775)], %8933:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=777)]) -> (%8934:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=775)])
            linalg.CPU.LinearOp <name="model.layers.20.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=775), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=779), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=778))] (%8934:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=775)]) -> (%8935:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=779)])
            cf.ReturnOp (%8935:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=779)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.21 <CPU> [using_qnn:true, symbol:model.layers.21] {
        (%8936:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=779)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8057:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=24)], %8058:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=52)]) -> (%8977:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=813)], %8950:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=792)], %8952:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=794)]) {
            linalg.CPU.RMSNormOp <name="model.layers.21.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=779), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=780), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=781))] (%8936:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=779)]) -> (%8937:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=780)])
            graph.CallGraphOp @model.layers.21.self_attn (%8937:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=780)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8057:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=24)], %8058:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=52)]) -> (%8969:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=804)], %8950:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=792)], %8952:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=794)])
            linalg.CPU.AddOp <name="model.layers.21.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=804), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=779), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=804), )] (%8969:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=804)], %8936:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=779)]) -> (%8970:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=804)])
            linalg.CPU.RMSNormOp <name="model.layers.21.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=804), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=805), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=806))] (%8970:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=804)]) -> (%8971:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=805)])
            graph.CallGraphOp @model.layers.21.mlp (%8971:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=805)]) -> (%8976:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=813)])
            linalg.CPU.AddOp <name="model.layers.21.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=813), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=804), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=813), )] (%8976:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=813)], %8970:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=804)]) -> (%8977:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=813)])
            cf.ReturnOp (%8977:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=813)], %8950:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=792)], %8952:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=794)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.21.self_attn <CPU> [using_qnn:true, symbol:model.layers.21.self_attn] {
        (%8937:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=780)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8057:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=24)], %8058:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=52)]) -> (%8969:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=804)], %8950:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=792)], %8952:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=794)]) {
            linalg.CPU.LinearOp <name="model.layers.21.self_attn.q_proj">(%8937:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=780)]) -> (%8938:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=786)])
            linalg.CPU.LinearOp <name="model.layers.21.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=780), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=783), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=782))] (%8937:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=780)]) -> (%8939:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=783)])
            linalg.CPU.LinearOp <name="model.layers.21.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=780), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=785), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=784))] (%8937:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=780)]) -> (%8940:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=785)])
            linalg.CPU.ViewOp <name="model.layers.21.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=786), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=786), )] (%8938:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=786)]) -> (%8938:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=786)])
            linalg.CPU.TransposeOp <name="model.layers.21.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=786), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=786), )] (%8938:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=786)]) -> (%8941:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=786)])
            linalg.CPU.ViewOp <name="model.layers.21.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=783), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=783), )] (%8939:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=783)]) -> (%8939:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=783)])
            linalg.CPU.TransposeOp <name="model.layers.21.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=783), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=783), )] (%8939:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=783)]) -> (%8942:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=783)])
            linalg.CPU.ViewOp <name="model.layers.21.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=785), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=785), )] (%8940:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=785)]) -> (%8940:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=785)])
            linalg.CPU.TransposeOp <name="model.layers.21.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=785), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=785), )] (%8940:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=785)]) -> (%8943:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=785)])
            linalg.CPU.RMSNormOp <name="model.layers.21.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=786), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=787), weight_weight:QuantSpec(Raw(type: Int16PerTensor), uuid=788))] (%8941:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=786)]) -> (%8944:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=787)])
            linalg.CPU.RMSNormOp <name="model.layers.21.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=783), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=789), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=790))] (%8942:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=783)]) -> (%8945:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=789)])
            linalg.CPU.RoPEOp <name="model.layers.21.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=787), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=787), )] (%8944:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=787)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%8946:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=787)])
            linalg.CPU.RoPEOp <name="model.layers.21.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=789), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=789), )] (%8945:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=789)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%8947:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=789)])
            linalg.CPU.CastTypeOp <name="model.layers.21.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=789), outputs_0:QuantSpec(Raw(type: Float16), uuid=791), )] (%8947:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=789)]) -> (%8948:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=791)])
            linalg.CPU.CastTypeOp <name="model.layers.21.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=791), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=792), )] (%8948:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=791)]) -> (%8949:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=792)])
            linalg.CPU.TransposeOp <name="model.layers.21.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=792), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=792), )] (%8949:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=792)]) -> (%8950:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=792)])
            linalg.CPU.CastTypeOp <name="model.layers.21.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=785), outputs_0:QuantSpec(Raw(type: Float16), uuid=793), )] (%8943:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=785)]) -> (%8951:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=793)])
            linalg.CPU.CastTypeOp <name="model.layers.21.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=793), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=794), )] (%8951:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=793)]) -> (%8952:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=794)])
            linalg.CPU.ConcatOp <name="model.layers.21.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=24), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=792), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=24), )] (%8057:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=24)], %8950:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=792)]) -> (%8953:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=24)])
            linalg.CPU.ConcatOp <name="model.layers.21.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=52), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=794), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=52), )] (%8058:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=52)], %8952:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=794)]) -> (%8954:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=52)])
            linalg.CPU.RepeatOp <name="model.layers.21.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=24), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=24), )] (%8953:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=24)]) -> (%8955:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=24)])
            linalg.CPU.RepeatOp <name="model.layers.21.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=52), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=52), )] (%8954:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=52)]) -> (%8956:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=52)])
            linalg.CPU.MatMulOp <name="model.layers.21.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=787), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=24), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=795), )] (%8946:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=787)], %8955:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=24)]) -> (%8957:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=795)])
            linalg.CPU.MulOp <name="model.layers.21.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=795), inputs_1:QuantSpec(Raw(type: Float32), uuid=796), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=795), )] (%8957:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=795)], %8958:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=796), constant:[0.088388346]]) -> (%8959:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=795)])
            linalg.CPU.ReduceMinOp <name="model.layers.21.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=795), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=797), )] (%8959:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=795)]) -> (%8960:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=797)])
            linalg.CPU.AddOp <name="model.layers.21.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=797), inputs_1:QuantSpec(Raw(type: Int16), uuid=798), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=797), )] (%8960:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=797)], %8961:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=798), constant:[-20]]) -> (%8962:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=797)])
            linalg.CPU.EqualOp <name="model.layers.21.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=799), outputs_0:QuantSpec(Raw(type: UInt8), uuid=800), )] (%8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8963:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=799), constant:[-0.80859375]]) -> (%8964:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=800)])
            linalg.CPU.WhereOp <name="model.layers.21.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=800), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=795), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=797), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=797), )] (%8964:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=800)], %8959:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=795)], %8962:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=797)]) -> (%8965:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=797)])
            linalg.CPU.SoftmaxOp <name="model.layers.21.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=797), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=801), )] (%8965:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=797)]) -> (%8966:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=801)])
            linalg.CPU.MatMulOp <name="model.layers.21.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=801), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=52), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=802), )] (%8966:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=801)], %8956:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=52)]) -> (%8967:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=802)])
            linalg.CPU.TransposeOp <name="model.layers.21.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=802), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=802), )] (%8967:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=802)]) -> (%8968:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=802)])
            linalg.CPU.ViewOp <name="model.layers.21.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=802), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=802), )] (%8968:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=802)]) -> (%8968:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=802)])
            linalg.CPU.LinearOp <name="model.layers.21.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=802), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=804), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=803))] (%8968:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=802)]) -> (%8969:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=804)])
            cf.ReturnOp (%8969:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=804)], %8950:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=792)], %8952:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=794)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.21.mlp <CPU> [using_qnn:true, symbol:model.layers.21.mlp] {
        (%8971:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=805)]) -> (%8976:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=813)]) {
            linalg.CPU.LinearOp <name="model.layers.21.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=805), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=808), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=807))] (%8971:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=805)]) -> (%8972:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=808)])
            linalg.CPU.SiLUOp <name="model.layers.21.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=808), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=809), )] (%8972:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=808)]) -> (%8973:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=809)])
            linalg.CPU.LinearOp <name="model.layers.21.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=805), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=811), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=810))] (%8971:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=805)]) -> (%8974:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=811)])
            linalg.CPU.MulOp <name="model.layers.21.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=809), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=811), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=809), )] (%8973:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=809)], %8974:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=811)]) -> (%8975:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=809)])
            linalg.CPU.LinearOp <name="model.layers.21.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=809), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=813), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=812))] (%8975:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=809)]) -> (%8976:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=813)])
            cf.ReturnOp (%8976:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=813)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.22 <CPU> [using_qnn:true, symbol:model.layers.22] {
        (%8977:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=813)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8059:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=25)], %8060:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=53)]) -> (%9018:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=847)], %8991:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=826)], %8993:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=828)]) {
            linalg.CPU.RMSNormOp <name="model.layers.22.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=813), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=814), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=815))] (%8977:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=813)]) -> (%8978:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=814)])
            graph.CallGraphOp @model.layers.22.self_attn (%8978:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=814)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8059:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=25)], %8060:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=53)]) -> (%9010:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=838)], %8991:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=826)], %8993:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=828)])
            linalg.CPU.AddOp <name="model.layers.22.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=838), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=813), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=838), )] (%9010:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=838)], %8977:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=813)]) -> (%9011:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=838)])
            linalg.CPU.RMSNormOp <name="model.layers.22.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=838), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=839), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=840))] (%9011:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=838)]) -> (%9012:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=839)])
            graph.CallGraphOp @model.layers.22.mlp (%9012:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=839)]) -> (%9017:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=847)])
            linalg.CPU.AddOp <name="model.layers.22.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=847), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=838), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=847), )] (%9017:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=847)], %9011:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=838)]) -> (%9018:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=847)])
            cf.ReturnOp (%9018:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=847)], %8991:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=826)], %8993:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=828)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.22.self_attn <CPU> [using_qnn:true, symbol:model.layers.22.self_attn] {
        (%8978:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=814)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8059:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=25)], %8060:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=53)]) -> (%9010:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=838)], %8991:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=826)], %8993:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=828)]) {
            linalg.CPU.LinearOp <name="model.layers.22.self_attn.q_proj">(%8978:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=814)]) -> (%8979:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=820)])
            linalg.CPU.LinearOp <name="model.layers.22.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=814), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=817), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=816))] (%8978:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=814)]) -> (%8980:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=817)])
            linalg.CPU.LinearOp <name="model.layers.22.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=814), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=819), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=818))] (%8978:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=814)]) -> (%8981:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=819)])
            linalg.CPU.ViewOp <name="model.layers.22.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=820), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=820), )] (%8979:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=820)]) -> (%8979:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=820)])
            linalg.CPU.TransposeOp <name="model.layers.22.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=820), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=820), )] (%8979:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=820)]) -> (%8982:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=820)])
            linalg.CPU.ViewOp <name="model.layers.22.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=817), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=817), )] (%8980:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=817)]) -> (%8980:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=817)])
            linalg.CPU.TransposeOp <name="model.layers.22.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=817), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=817), )] (%8980:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=817)]) -> (%8983:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=817)])
            linalg.CPU.ViewOp <name="model.layers.22.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=819), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=819), )] (%8981:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=819)]) -> (%8981:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=819)])
            linalg.CPU.TransposeOp <name="model.layers.22.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=819), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=819), )] (%8981:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=819)]) -> (%8984:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=819)])
            linalg.CPU.RMSNormOp <name="model.layers.22.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=820), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=821), weight_weight:QuantSpec(Raw(type: Int16PerTensor), uuid=822))] (%8982:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=820)]) -> (%8985:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=821)])
            linalg.CPU.RMSNormOp <name="model.layers.22.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=817), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=823), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=824))] (%8983:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=817)]) -> (%8986:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=823)])
            linalg.CPU.RoPEOp <name="model.layers.22.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=821), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=821), )] (%8985:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=821)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%8987:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=821)])
            linalg.CPU.RoPEOp <name="model.layers.22.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=823), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=823), )] (%8986:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=823)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%8988:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=823)])
            linalg.CPU.CastTypeOp <name="model.layers.22.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=823), outputs_0:QuantSpec(Raw(type: Float16), uuid=825), )] (%8988:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=823)]) -> (%8989:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=825)])
            linalg.CPU.CastTypeOp <name="model.layers.22.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=825), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=826), )] (%8989:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=825)]) -> (%8990:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=826)])
            linalg.CPU.TransposeOp <name="model.layers.22.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=826), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=826), )] (%8990:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=826)]) -> (%8991:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=826)])
            linalg.CPU.CastTypeOp <name="model.layers.22.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=819), outputs_0:QuantSpec(Raw(type: Float16), uuid=827), )] (%8984:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=819)]) -> (%8992:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=827)])
            linalg.CPU.CastTypeOp <name="model.layers.22.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=827), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=828), )] (%8992:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=827)]) -> (%8993:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=828)])
            linalg.CPU.ConcatOp <name="model.layers.22.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=25), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=826), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=25), )] (%8059:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=25)], %8991:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=826)]) -> (%8994:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=25)])
            linalg.CPU.ConcatOp <name="model.layers.22.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=53), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=828), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=53), )] (%8060:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=53)], %8993:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=828)]) -> (%8995:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=53)])
            linalg.CPU.RepeatOp <name="model.layers.22.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=25), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=25), )] (%8994:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=25)]) -> (%8996:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=25)])
            linalg.CPU.RepeatOp <name="model.layers.22.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=53), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=53), )] (%8995:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=53)]) -> (%8997:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=53)])
            linalg.CPU.MatMulOp <name="model.layers.22.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=821), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=25), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=829), )] (%8987:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=821)], %8996:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=25)]) -> (%8998:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=829)])
            linalg.CPU.MulOp <name="model.layers.22.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=829), inputs_1:QuantSpec(Raw(type: Float32), uuid=830), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=829), )] (%8998:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=829)], %8999:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=830), constant:[0.088388346]]) -> (%9000:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=829)])
            linalg.CPU.ReduceMinOp <name="model.layers.22.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=829), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=831), )] (%9000:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=829)]) -> (%9001:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=831)])
            linalg.CPU.AddOp <name="model.layers.22.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=831), inputs_1:QuantSpec(Raw(type: Int16), uuid=832), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=831), )] (%9001:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=831)], %9002:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=832), constant:[-20]]) -> (%9003:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=831)])
            linalg.CPU.EqualOp <name="model.layers.22.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=833), outputs_0:QuantSpec(Raw(type: UInt8), uuid=834), )] (%8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %9004:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=833), constant:[-0.42773438]]) -> (%9005:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=834)])
            linalg.CPU.WhereOp <name="model.layers.22.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=834), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=829), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=831), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=831), )] (%9005:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=834)], %9000:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=829)], %9003:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=831)]) -> (%9006:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=831)])
            linalg.CPU.SoftmaxOp <name="model.layers.22.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=831), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=835), )] (%9006:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=831)]) -> (%9007:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=835)])
            linalg.CPU.MatMulOp <name="model.layers.22.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=835), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=53), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=836), )] (%9007:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=835)], %8997:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=53)]) -> (%9008:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=836)])
            linalg.CPU.TransposeOp <name="model.layers.22.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=836), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=836), )] (%9008:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=836)]) -> (%9009:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=836)])
            linalg.CPU.ViewOp <name="model.layers.22.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=836), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=836), )] (%9009:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=836)]) -> (%9009:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=836)])
            linalg.CPU.LinearOp <name="model.layers.22.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=836), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=838), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=837))] (%9009:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=836)]) -> (%9010:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=838)])
            cf.ReturnOp (%9010:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=838)], %8991:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=826)], %8993:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=828)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.22.mlp <CPU> [using_qnn:true, symbol:model.layers.22.mlp] {
        (%9012:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=839)]) -> (%9017:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=847)]) {
            linalg.CPU.LinearOp <name="model.layers.22.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=839), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=842), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=841))] (%9012:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=839)]) -> (%9013:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=842)])
            linalg.CPU.SiLUOp <name="model.layers.22.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=842), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=843), )] (%9013:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=842)]) -> (%9014:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=843)])
            linalg.CPU.LinearOp <name="model.layers.22.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=839), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=845), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=844))] (%9012:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=839)]) -> (%9015:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=845)])
            linalg.CPU.MulOp <name="model.layers.22.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=843), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=845), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=843), )] (%9014:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=843)], %9015:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=845)]) -> (%9016:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=843)])
            linalg.CPU.LinearOp <name="model.layers.22.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=843), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=847), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=846))] (%9016:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=843)]) -> (%9017:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=847)])
            cf.ReturnOp (%9017:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=847)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.23 <CPU> [using_qnn:true, symbol:model.layers.23] {
        (%9018:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=847)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8061:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=26)], %8062:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=54)]) -> (%9059:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=881)], %9032:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=860)], %9034:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=862)]) {
            linalg.CPU.RMSNormOp <name="model.layers.23.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=847), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=848), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=849))] (%9018:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=847)]) -> (%9019:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=848)])
            graph.CallGraphOp @model.layers.23.self_attn (%9019:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=848)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8061:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=26)], %8062:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=54)]) -> (%9051:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=872)], %9032:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=860)], %9034:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=862)])
            linalg.CPU.AddOp <name="model.layers.23.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=872), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=847), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=872), )] (%9051:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=872)], %9018:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=847)]) -> (%9052:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=872)])
            linalg.CPU.RMSNormOp <name="model.layers.23.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=872), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=873), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=874))] (%9052:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=872)]) -> (%9053:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=873)])
            graph.CallGraphOp @model.layers.23.mlp (%9053:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=873)]) -> (%9058:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=881)])
            linalg.CPU.AddOp <name="model.layers.23.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=881), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=872), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=881), )] (%9058:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=881)], %9052:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=872)]) -> (%9059:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=881)])
            cf.ReturnOp (%9059:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=881)], %9032:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=860)], %9034:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=862)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.23.self_attn <CPU> [using_qnn:true, symbol:model.layers.23.self_attn] {
        (%9019:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=848)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8061:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=26)], %8062:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=54)]) -> (%9051:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=872)], %9032:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=860)], %9034:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=862)]) {
            linalg.CPU.LinearOp <name="model.layers.23.self_attn.q_proj">(%9019:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=848)]) -> (%9020:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=854)])
            linalg.CPU.LinearOp <name="model.layers.23.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=848), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=851), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=850))] (%9019:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=848)]) -> (%9021:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=851)])
            linalg.CPU.LinearOp <name="model.layers.23.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=848), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=853), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=852))] (%9019:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=848)]) -> (%9022:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=853)])
            linalg.CPU.ViewOp <name="model.layers.23.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=854), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=854), )] (%9020:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=854)]) -> (%9020:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=854)])
            linalg.CPU.TransposeOp <name="model.layers.23.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=854), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=854), )] (%9020:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=854)]) -> (%9023:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=854)])
            linalg.CPU.ViewOp <name="model.layers.23.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=851), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=851), )] (%9021:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=851)]) -> (%9021:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=851)])
            linalg.CPU.TransposeOp <name="model.layers.23.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=851), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=851), )] (%9021:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=851)]) -> (%9024:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=851)])
            linalg.CPU.ViewOp <name="model.layers.23.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=853), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=853), )] (%9022:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=853)]) -> (%9022:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=853)])
            linalg.CPU.TransposeOp <name="model.layers.23.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=853), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=853), )] (%9022:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=853)]) -> (%9025:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=853)])
            linalg.CPU.RMSNormOp <name="model.layers.23.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=854), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=855), weight_weight:QuantSpec(Raw(type: Int16PerTensor), uuid=856))] (%9023:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=854)]) -> (%9026:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=855)])
            linalg.CPU.RMSNormOp <name="model.layers.23.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=851), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=857), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=858))] (%9024:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=851)]) -> (%9027:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=857)])
            linalg.CPU.RoPEOp <name="model.layers.23.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=855), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=855), )] (%9026:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=855)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%9028:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=855)])
            linalg.CPU.RoPEOp <name="model.layers.23.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=857), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=857), )] (%9027:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=857)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%9029:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=857)])
            linalg.CPU.CastTypeOp <name="model.layers.23.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=857), outputs_0:QuantSpec(Raw(type: Float16), uuid=859), )] (%9029:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=857)]) -> (%9030:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=859)])
            linalg.CPU.CastTypeOp <name="model.layers.23.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=859), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=860), )] (%9030:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=859)]) -> (%9031:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=860)])
            linalg.CPU.TransposeOp <name="model.layers.23.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=860), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=860), )] (%9031:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=860)]) -> (%9032:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=860)])
            linalg.CPU.CastTypeOp <name="model.layers.23.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=853), outputs_0:QuantSpec(Raw(type: Float16), uuid=861), )] (%9025:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=853)]) -> (%9033:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=861)])
            linalg.CPU.CastTypeOp <name="model.layers.23.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=861), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=862), )] (%9033:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=861)]) -> (%9034:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=862)])
            linalg.CPU.ConcatOp <name="model.layers.23.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=26), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=860), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=26), )] (%8061:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=26)], %9032:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=860)]) -> (%9035:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=26)])
            linalg.CPU.ConcatOp <name="model.layers.23.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=54), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=862), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=54), )] (%8062:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=54)], %9034:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=862)]) -> (%9036:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=54)])
            linalg.CPU.RepeatOp <name="model.layers.23.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=26), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=26), )] (%9035:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=26)]) -> (%9037:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=26)])
            linalg.CPU.RepeatOp <name="model.layers.23.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=54), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=54), )] (%9036:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=54)]) -> (%9038:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=54)])
            linalg.CPU.MatMulOp <name="model.layers.23.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=855), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=26), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=863), )] (%9028:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=855)], %9037:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=26)]) -> (%9039:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=863)])
            linalg.CPU.MulOp <name="model.layers.23.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=863), inputs_1:QuantSpec(Raw(type: Float32), uuid=864), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=863), )] (%9039:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=863)], %9040:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=864), constant:[0.088388346]]) -> (%9041:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=863)])
            linalg.CPU.ReduceMinOp <name="model.layers.23.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=863), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=865), )] (%9041:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=863)]) -> (%9042:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=865)])
            linalg.CPU.AddOp <name="model.layers.23.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=865), inputs_1:QuantSpec(Raw(type: Int16), uuid=866), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=865), )] (%9042:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=865)], %9043:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=866), constant:[-20]]) -> (%9044:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=865)])
            linalg.CPU.EqualOp <name="model.layers.23.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=867), outputs_0:QuantSpec(Raw(type: UInt8), uuid=868), )] (%8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %9045:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=867), constant:[0.96484375]]) -> (%9046:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=868)])
            linalg.CPU.WhereOp <name="model.layers.23.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=868), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=863), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=865), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=865), )] (%9046:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=868)], %9041:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=863)], %9044:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=865)]) -> (%9047:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=865)])
            linalg.CPU.SoftmaxOp <name="model.layers.23.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=865), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=869), )] (%9047:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=865)]) -> (%9048:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=869)])
            linalg.CPU.MatMulOp <name="model.layers.23.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=869), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=54), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=870), )] (%9048:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=869)], %9038:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=54)]) -> (%9049:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=870)])
            linalg.CPU.TransposeOp <name="model.layers.23.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=870), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=870), )] (%9049:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=870)]) -> (%9050:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=870)])
            linalg.CPU.ViewOp <name="model.layers.23.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=870), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=870), )] (%9050:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=870)]) -> (%9050:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=870)])
            linalg.CPU.LinearOp <name="model.layers.23.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=870), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=872), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=871))] (%9050:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=870)]) -> (%9051:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=872)])
            cf.ReturnOp (%9051:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=872)], %9032:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=860)], %9034:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=862)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.23.mlp <CPU> [using_qnn:true, symbol:model.layers.23.mlp] {
        (%9053:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=873)]) -> (%9058:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=881)]) {
            linalg.CPU.LinearOp <name="model.layers.23.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=873), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=876), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=875))] (%9053:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=873)]) -> (%9054:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=876)])
            linalg.CPU.SiLUOp <name="model.layers.23.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=876), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=877), )] (%9054:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=876)]) -> (%9055:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=877)])
            linalg.CPU.LinearOp <name="model.layers.23.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=873), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=879), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=878))] (%9053:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=873)]) -> (%9056:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=879)])
            linalg.CPU.MulOp <name="model.layers.23.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=877), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=879), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=877), )] (%9055:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=877)], %9056:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=879)]) -> (%9057:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=877)])
            linalg.CPU.LinearOp <name="model.layers.23.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=877), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=881), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=880))] (%9057:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=877)]) -> (%9058:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=881)])
            cf.ReturnOp (%9058:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=881)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.24 <CPU> [using_qnn:true, symbol:model.layers.24] {
        (%9059:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=881)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8063:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=27)], %8064:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=55)]) -> (%9100:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=915)], %9073:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=894)], %9075:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=896)]) {
            linalg.CPU.RMSNormOp <name="model.layers.24.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=881), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=882), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=883))] (%9059:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=881)]) -> (%9060:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=882)])
            graph.CallGraphOp @model.layers.24.self_attn (%9060:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=882)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8063:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=27)], %8064:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=55)]) -> (%9092:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=906)], %9073:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=894)], %9075:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=896)])
            linalg.CPU.AddOp <name="model.layers.24.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=906), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=881), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=906), )] (%9092:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=906)], %9059:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=881)]) -> (%9093:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=906)])
            linalg.CPU.RMSNormOp <name="model.layers.24.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=906), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=907), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=908))] (%9093:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=906)]) -> (%9094:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=907)])
            graph.CallGraphOp @model.layers.24.mlp (%9094:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=907)]) -> (%9099:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=915)])
            linalg.CPU.AddOp <name="model.layers.24.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=915), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=906), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=915), )] (%9099:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=915)], %9093:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=906)]) -> (%9100:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=915)])
            cf.ReturnOp (%9100:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=915)], %9073:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=894)], %9075:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=896)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.24.self_attn <CPU> [using_qnn:true, symbol:model.layers.24.self_attn] {
        (%9060:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=882)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8063:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=27)], %8064:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=55)]) -> (%9092:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=906)], %9073:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=894)], %9075:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=896)]) {
            linalg.CPU.LinearOp <name="model.layers.24.self_attn.q_proj">(%9060:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=882)]) -> (%9061:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=888)])
            linalg.CPU.LinearOp <name="model.layers.24.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=882), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=885), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=884))] (%9060:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=882)]) -> (%9062:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=885)])
            linalg.CPU.LinearOp <name="model.layers.24.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=882), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=887), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=886))] (%9060:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=882)]) -> (%9063:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=887)])
            linalg.CPU.ViewOp <name="model.layers.24.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=888), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=888), )] (%9061:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=888)]) -> (%9061:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=888)])
            linalg.CPU.TransposeOp <name="model.layers.24.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=888), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=888), )] (%9061:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=888)]) -> (%9064:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=888)])
            linalg.CPU.ViewOp <name="model.layers.24.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=885), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=885), )] (%9062:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=885)]) -> (%9062:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=885)])
            linalg.CPU.TransposeOp <name="model.layers.24.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=885), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=885), )] (%9062:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=885)]) -> (%9065:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=885)])
            linalg.CPU.ViewOp <name="model.layers.24.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=887), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=887), )] (%9063:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=887)]) -> (%9063:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=887)])
            linalg.CPU.TransposeOp <name="model.layers.24.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=887), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=887), )] (%9063:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=887)]) -> (%9066:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=887)])
            linalg.CPU.RMSNormOp <name="model.layers.24.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=888), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=889), weight_weight:QuantSpec(Raw(type: Int16PerTensor), uuid=890))] (%9064:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=888)]) -> (%9067:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=889)])
            linalg.CPU.RMSNormOp <name="model.layers.24.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=885), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=891), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=892))] (%9065:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=885)]) -> (%9068:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=891)])
            linalg.CPU.RoPEOp <name="model.layers.24.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=889), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=889), )] (%9067:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=889)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%9069:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=889)])
            linalg.CPU.RoPEOp <name="model.layers.24.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=891), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=891), )] (%9068:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=891)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%9070:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=891)])
            linalg.CPU.CastTypeOp <name="model.layers.24.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=891), outputs_0:QuantSpec(Raw(type: Float16), uuid=893), )] (%9070:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=891)]) -> (%9071:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=893)])
            linalg.CPU.CastTypeOp <name="model.layers.24.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=893), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=894), )] (%9071:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=893)]) -> (%9072:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=894)])
            linalg.CPU.TransposeOp <name="model.layers.24.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=894), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=894), )] (%9072:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=894)]) -> (%9073:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=894)])
            linalg.CPU.CastTypeOp <name="model.layers.24.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=887), outputs_0:QuantSpec(Raw(type: Float16), uuid=895), )] (%9066:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=887)]) -> (%9074:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=895)])
            linalg.CPU.CastTypeOp <name="model.layers.24.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=895), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=896), )] (%9074:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=895)]) -> (%9075:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=896)])
            linalg.CPU.ConcatOp <name="model.layers.24.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=27), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=894), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=27), )] (%8063:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=27)], %9073:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=894)]) -> (%9076:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=27)])
            linalg.CPU.ConcatOp <name="model.layers.24.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=55), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=896), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=55), )] (%8064:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=55)], %9075:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=896)]) -> (%9077:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=55)])
            linalg.CPU.RepeatOp <name="model.layers.24.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=27), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=27), )] (%9076:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=27)]) -> (%9078:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=27)])
            linalg.CPU.RepeatOp <name="model.layers.24.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=55), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=55), )] (%9077:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=55)]) -> (%9079:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=55)])
            linalg.CPU.MatMulOp <name="model.layers.24.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=889), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=27), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=897), )] (%9069:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=889)], %9078:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=27)]) -> (%9080:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=897)])
            linalg.CPU.MulOp <name="model.layers.24.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=897), inputs_1:QuantSpec(Raw(type: Float32), uuid=898), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=897), )] (%9080:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=897)], %9081:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=898), constant:[0.088388346]]) -> (%9082:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=897)])
            linalg.CPU.ReduceMinOp <name="model.layers.24.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=897), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=899), )] (%9082:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=897)]) -> (%9083:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=899)])
            linalg.CPU.AddOp <name="model.layers.24.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=899), inputs_1:QuantSpec(Raw(type: Int16), uuid=900), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=899), )] (%9083:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=899)], %9084:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=900), constant:[-20]]) -> (%9085:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=899)])
            linalg.CPU.EqualOp <name="model.layers.24.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=901), outputs_0:QuantSpec(Raw(type: UInt8), uuid=902), )] (%8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %9086:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=901), constant:[0.07910156]]) -> (%9087:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=902)])
            linalg.CPU.WhereOp <name="model.layers.24.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=902), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=897), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=899), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=899), )] (%9087:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=902)], %9082:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=897)], %9085:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=899)]) -> (%9088:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=899)])
            linalg.CPU.SoftmaxOp <name="model.layers.24.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=899), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=903), )] (%9088:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=899)]) -> (%9089:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=903)])
            linalg.CPU.MatMulOp <name="model.layers.24.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=903), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=55), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=904), )] (%9089:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=903)], %9079:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=55)]) -> (%9090:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=904)])
            linalg.CPU.TransposeOp <name="model.layers.24.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=904), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=904), )] (%9090:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=904)]) -> (%9091:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=904)])
            linalg.CPU.ViewOp <name="model.layers.24.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=904), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=904), )] (%9091:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=904)]) -> (%9091:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=904)])
            linalg.CPU.LinearOp <name="model.layers.24.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=904), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=906), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=905))] (%9091:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=904)]) -> (%9092:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=906)])
            cf.ReturnOp (%9092:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=906)], %9073:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=894)], %9075:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=896)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.24.mlp <CPU> [using_qnn:true, symbol:model.layers.24.mlp] {
        (%9094:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=907)]) -> (%9099:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=915)]) {
            linalg.CPU.LinearOp <name="model.layers.24.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=907), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=910), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=909))] (%9094:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=907)]) -> (%9095:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=910)])
            linalg.CPU.SiLUOp <name="model.layers.24.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=910), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=911), )] (%9095:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=910)]) -> (%9096:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=911)])
            linalg.CPU.LinearOp <name="model.layers.24.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=907), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=913), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=912))] (%9094:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=907)]) -> (%9097:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=913)])
            linalg.CPU.MulOp <name="model.layers.24.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=911), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=913), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=911), )] (%9096:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=911)], %9097:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=913)]) -> (%9098:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=911)])
            linalg.CPU.LinearOp <name="model.layers.24.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=911), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=915), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=914))] (%9098:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=911)]) -> (%9099:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=915)])
            cf.ReturnOp (%9099:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=915)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.25 <CPU> [using_qnn:true, symbol:model.layers.25] {
        (%9100:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=915)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8065:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=28)], %8066:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=56)]) -> (%9141:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=949)], %9114:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=928)], %9116:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=930)]) {
            linalg.CPU.RMSNormOp <name="model.layers.25.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=915), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=916), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=917))] (%9100:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=915)]) -> (%9101:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=916)])
            graph.CallGraphOp @model.layers.25.self_attn (%9101:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=916)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8065:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=28)], %8066:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=56)]) -> (%9133:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=940)], %9114:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=928)], %9116:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=930)])
            linalg.CPU.AddOp <name="model.layers.25.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=940), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=915), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=940), )] (%9133:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=940)], %9100:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=915)]) -> (%9134:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=940)])
            linalg.CPU.RMSNormOp <name="model.layers.25.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=940), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=941), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=942))] (%9134:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=940)]) -> (%9135:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=941)])
            graph.CallGraphOp @model.layers.25.mlp (%9135:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=941)]) -> (%9140:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=949)])
            linalg.CPU.AddOp <name="model.layers.25.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=949), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=940), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=949), )] (%9140:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=949)], %9134:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=940)]) -> (%9141:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=949)])
            cf.ReturnOp (%9141:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=949)], %9114:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=928)], %9116:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=930)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.25.self_attn <CPU> [using_qnn:true, symbol:model.layers.25.self_attn] {
        (%9101:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=916)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8065:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=28)], %8066:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=56)]) -> (%9133:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=940)], %9114:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=928)], %9116:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=930)]) {
            linalg.CPU.LinearOp <name="model.layers.25.self_attn.q_proj">(%9101:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=916)]) -> (%9102:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=922)])
            linalg.CPU.LinearOp <name="model.layers.25.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=916), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=919), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=918))] (%9101:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=916)]) -> (%9103:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=919)])
            linalg.CPU.LinearOp <name="model.layers.25.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=916), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=921), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=920))] (%9101:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=916)]) -> (%9104:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=921)])
            linalg.CPU.ViewOp <name="model.layers.25.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=922), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=922), )] (%9102:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=922)]) -> (%9102:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=922)])
            linalg.CPU.TransposeOp <name="model.layers.25.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=922), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=922), )] (%9102:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=922)]) -> (%9105:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=922)])
            linalg.CPU.ViewOp <name="model.layers.25.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=919), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=919), )] (%9103:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=919)]) -> (%9103:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=919)])
            linalg.CPU.TransposeOp <name="model.layers.25.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=919), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=919), )] (%9103:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=919)]) -> (%9106:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=919)])
            linalg.CPU.ViewOp <name="model.layers.25.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=921), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=921), )] (%9104:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=921)]) -> (%9104:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=921)])
            linalg.CPU.TransposeOp <name="model.layers.25.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=921), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=921), )] (%9104:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=921)]) -> (%9107:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=921)])
            linalg.CPU.RMSNormOp <name="model.layers.25.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=922), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=923), weight_weight:QuantSpec(Raw(type: Int16PerTensor), uuid=924))] (%9105:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=922)]) -> (%9108:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=923)])
            linalg.CPU.RMSNormOp <name="model.layers.25.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=919), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=925), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=926))] (%9106:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=919)]) -> (%9109:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=925)])
            linalg.CPU.RoPEOp <name="model.layers.25.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=923), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=923), )] (%9108:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=923)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%9110:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=923)])
            linalg.CPU.RoPEOp <name="model.layers.25.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=925), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=925), )] (%9109:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=925)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%9111:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=925)])
            linalg.CPU.CastTypeOp <name="model.layers.25.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=925), outputs_0:QuantSpec(Raw(type: Float16), uuid=927), )] (%9111:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=925)]) -> (%9112:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=927)])
            linalg.CPU.CastTypeOp <name="model.layers.25.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=927), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=928), )] (%9112:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=927)]) -> (%9113:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=928)])
            linalg.CPU.TransposeOp <name="model.layers.25.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=928), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=928), )] (%9113:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=928)]) -> (%9114:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=928)])
            linalg.CPU.CastTypeOp <name="model.layers.25.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=921), outputs_0:QuantSpec(Raw(type: Float16), uuid=929), )] (%9107:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=921)]) -> (%9115:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=929)])
            linalg.CPU.CastTypeOp <name="model.layers.25.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=929), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=930), )] (%9115:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=929)]) -> (%9116:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=930)])
            linalg.CPU.ConcatOp <name="model.layers.25.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=28), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=928), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=28), )] (%8065:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=28)], %9114:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=928)]) -> (%9117:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=28)])
            linalg.CPU.ConcatOp <name="model.layers.25.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=56), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=930), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=56), )] (%8066:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=56)], %9116:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=930)]) -> (%9118:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=56)])
            linalg.CPU.RepeatOp <name="model.layers.25.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=28), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=28), )] (%9117:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=28)]) -> (%9119:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=28)])
            linalg.CPU.RepeatOp <name="model.layers.25.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=56), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=56), )] (%9118:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=56)]) -> (%9120:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=56)])
            linalg.CPU.MatMulOp <name="model.layers.25.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=923), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=28), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=931), )] (%9110:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=923)], %9119:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=28)]) -> (%9121:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=931)])
            linalg.CPU.MulOp <name="model.layers.25.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=931), inputs_1:QuantSpec(Raw(type: Float32), uuid=932), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=931), )] (%9121:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=931)], %9122:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=932), constant:[0.088388346]]) -> (%9123:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=931)])
            linalg.CPU.ReduceMinOp <name="model.layers.25.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=931), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=933), )] (%9123:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=931)]) -> (%9124:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=933)])
            linalg.CPU.AddOp <name="model.layers.25.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=933), inputs_1:QuantSpec(Raw(type: Int16), uuid=934), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=933), )] (%9124:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=933)], %9125:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=934), constant:[-20]]) -> (%9126:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=933)])
            linalg.CPU.EqualOp <name="model.layers.25.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=935), outputs_0:QuantSpec(Raw(type: UInt8), uuid=936), )] (%8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %9127:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=935), constant:[-0.9921875]]) -> (%9128:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=936)])
            linalg.CPU.WhereOp <name="model.layers.25.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=936), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=931), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=933), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=933), )] (%9128:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=936)], %9123:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=931)], %9126:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=933)]) -> (%9129:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=933)])
            linalg.CPU.SoftmaxOp <name="model.layers.25.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=933), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=937), )] (%9129:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=933)]) -> (%9130:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=937)])
            linalg.CPU.MatMulOp <name="model.layers.25.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=937), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=56), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=938), )] (%9130:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=937)], %9120:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=56)]) -> (%9131:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=938)])
            linalg.CPU.TransposeOp <name="model.layers.25.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=938), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=938), )] (%9131:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=938)]) -> (%9132:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=938)])
            linalg.CPU.ViewOp <name="model.layers.25.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=938), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=938), )] (%9132:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=938)]) -> (%9132:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=938)])
            linalg.CPU.LinearOp <name="model.layers.25.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=938), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=940), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=939))] (%9132:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=938)]) -> (%9133:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=940)])
            cf.ReturnOp (%9133:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=940)], %9114:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=928)], %9116:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=930)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.25.mlp <CPU> [using_qnn:true, symbol:model.layers.25.mlp] {
        (%9135:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=941)]) -> (%9140:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=949)]) {
            linalg.CPU.LinearOp <name="model.layers.25.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=941), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=944), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=943))] (%9135:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=941)]) -> (%9136:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=944)])
            linalg.CPU.SiLUOp <name="model.layers.25.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=944), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=945), )] (%9136:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=944)]) -> (%9137:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=945)])
            linalg.CPU.LinearOp <name="model.layers.25.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=941), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=947), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=946))] (%9135:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=941)]) -> (%9138:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=947)])
            linalg.CPU.MulOp <name="model.layers.25.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=945), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=947), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=945), )] (%9137:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=945)], %9138:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=947)]) -> (%9139:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=945)])
            linalg.CPU.LinearOp <name="model.layers.25.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=945), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=949), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=948))] (%9139:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=945)]) -> (%9140:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=949)])
            cf.ReturnOp (%9140:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=949)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.26 <CPU> [using_qnn:true, symbol:model.layers.26] {
        (%9141:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=949)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8067:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=29)], %8068:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=57)]) -> (%9182:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=983)], %9155:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=962)], %9157:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=964)]) {
            linalg.CPU.RMSNormOp <name="model.layers.26.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=949), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=950), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=951))] (%9141:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=949)]) -> (%9142:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=950)])
            graph.CallGraphOp @model.layers.26.self_attn (%9142:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=950)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8067:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=29)], %8068:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=57)]) -> (%9174:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=974)], %9155:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=962)], %9157:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=964)])
            linalg.CPU.AddOp <name="model.layers.26.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=974), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=949), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=974), )] (%9174:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=974)], %9141:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=949)]) -> (%9175:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=974)])
            linalg.CPU.RMSNormOp <name="model.layers.26.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=974), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=975), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=976))] (%9175:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=974)]) -> (%9176:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=975)])
            graph.CallGraphOp @model.layers.26.mlp (%9176:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=975)]) -> (%9181:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=983)])
            linalg.CPU.AddOp <name="model.layers.26.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=983), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=974), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=983), )] (%9181:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=983)], %9175:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=974)]) -> (%9182:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=983)])
            cf.ReturnOp (%9182:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=983)], %9155:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=962)], %9157:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=964)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.26.self_attn <CPU> [using_qnn:true, symbol:model.layers.26.self_attn] {
        (%9142:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=950)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8067:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=29)], %8068:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=57)]) -> (%9174:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=974)], %9155:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=962)], %9157:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=964)]) {
            linalg.CPU.LinearOp <name="model.layers.26.self_attn.q_proj">(%9142:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=950)]) -> (%9143:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=956)])
            linalg.CPU.LinearOp <name="model.layers.26.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=950), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=953), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=952))] (%9142:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=950)]) -> (%9144:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=953)])
            linalg.CPU.LinearOp <name="model.layers.26.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=950), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=955), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=954))] (%9142:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=950)]) -> (%9145:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=955)])
            linalg.CPU.ViewOp <name="model.layers.26.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=956), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=956), )] (%9143:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=956)]) -> (%9143:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=956)])
            linalg.CPU.TransposeOp <name="model.layers.26.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=956), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=956), )] (%9143:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=956)]) -> (%9146:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=956)])
            linalg.CPU.ViewOp <name="model.layers.26.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=953), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=953), )] (%9144:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=953)]) -> (%9144:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=953)])
            linalg.CPU.TransposeOp <name="model.layers.26.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=953), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=953), )] (%9144:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=953)]) -> (%9147:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=953)])
            linalg.CPU.ViewOp <name="model.layers.26.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=955), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=955), )] (%9145:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=955)]) -> (%9145:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=955)])
            linalg.CPU.TransposeOp <name="model.layers.26.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=955), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=955), )] (%9145:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=955)]) -> (%9148:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=955)])
            linalg.CPU.RMSNormOp <name="model.layers.26.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=956), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=957), weight_weight:QuantSpec(Raw(type: Int16PerTensor), uuid=958))] (%9146:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=956)]) -> (%9149:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=957)])
            linalg.CPU.RMSNormOp <name="model.layers.26.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=953), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=959), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=960))] (%9147:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=953)]) -> (%9150:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=959)])
            linalg.CPU.RoPEOp <name="model.layers.26.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=957), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=957), )] (%9149:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=957)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%9151:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=957)])
            linalg.CPU.RoPEOp <name="model.layers.26.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=959), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=959), )] (%9150:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=959)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%9152:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=959)])
            linalg.CPU.CastTypeOp <name="model.layers.26.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=959), outputs_0:QuantSpec(Raw(type: Float16), uuid=961), )] (%9152:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=959)]) -> (%9153:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=961)])
            linalg.CPU.CastTypeOp <name="model.layers.26.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=961), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=962), )] (%9153:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=961)]) -> (%9154:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=962)])
            linalg.CPU.TransposeOp <name="model.layers.26.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=962), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=962), )] (%9154:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=962)]) -> (%9155:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=962)])
            linalg.CPU.CastTypeOp <name="model.layers.26.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=955), outputs_0:QuantSpec(Raw(type: Float16), uuid=963), )] (%9148:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=955)]) -> (%9156:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=963)])
            linalg.CPU.CastTypeOp <name="model.layers.26.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=963), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=964), )] (%9156:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=963)]) -> (%9157:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=964)])
            linalg.CPU.ConcatOp <name="model.layers.26.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=29), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=962), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=29), )] (%8067:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=29)], %9155:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=962)]) -> (%9158:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=29)])
            linalg.CPU.ConcatOp <name="model.layers.26.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=57), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=964), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=57), )] (%8068:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=57)], %9157:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=964)]) -> (%9159:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=57)])
            linalg.CPU.RepeatOp <name="model.layers.26.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=29), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=29), )] (%9158:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=29)]) -> (%9160:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=29)])
            linalg.CPU.RepeatOp <name="model.layers.26.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=57), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=57), )] (%9159:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=57)]) -> (%9161:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=57)])
            linalg.CPU.MatMulOp <name="model.layers.26.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=957), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=29), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=965), )] (%9151:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=957)], %9160:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=29)]) -> (%9162:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=965)])
            linalg.CPU.MulOp <name="model.layers.26.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=965), inputs_1:QuantSpec(Raw(type: Float32), uuid=966), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=965), )] (%9162:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=965)], %9163:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=966), constant:[0.088388346]]) -> (%9164:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=965)])
            linalg.CPU.ReduceMinOp <name="model.layers.26.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=965), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=967), )] (%9164:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=965)]) -> (%9165:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=967)])
            linalg.CPU.AddOp <name="model.layers.26.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=967), inputs_1:QuantSpec(Raw(type: Int16), uuid=968), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=967), )] (%9165:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=967)], %9166:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=968), constant:[-20]]) -> (%9167:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=967)])
            linalg.CPU.EqualOp <name="model.layers.26.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=969), outputs_0:QuantSpec(Raw(type: UInt8), uuid=970), )] (%8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %9168:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=969), constant:[0.27929688]]) -> (%9169:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=970)])
            linalg.CPU.WhereOp <name="model.layers.26.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=970), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=965), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=967), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=967), )] (%9169:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=970)], %9164:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=965)], %9167:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=967)]) -> (%9170:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=967)])
            linalg.CPU.SoftmaxOp <name="model.layers.26.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=967), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=971), )] (%9170:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=967)]) -> (%9171:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=971)])
            linalg.CPU.MatMulOp <name="model.layers.26.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=971), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=57), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=972), )] (%9171:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=971)], %9161:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=57)]) -> (%9172:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=972)])
            linalg.CPU.TransposeOp <name="model.layers.26.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=972), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=972), )] (%9172:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=972)]) -> (%9173:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=972)])
            linalg.CPU.ViewOp <name="model.layers.26.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=972), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=972), )] (%9173:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=972)]) -> (%9173:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=972)])
            linalg.CPU.LinearOp <name="model.layers.26.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=972), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=974), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=973))] (%9173:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=972)]) -> (%9174:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=974)])
            cf.ReturnOp (%9174:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=974)], %9155:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=962)], %9157:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=964)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.26.mlp <CPU> [using_qnn:true, symbol:model.layers.26.mlp] {
        (%9176:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=975)]) -> (%9181:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=983)]) {
            linalg.CPU.LinearOp <name="model.layers.26.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=975), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=978), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=977))] (%9176:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=975)]) -> (%9177:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=978)])
            linalg.CPU.SiLUOp <name="model.layers.26.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=978), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=979), )] (%9177:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=978)]) -> (%9178:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=979)])
            linalg.CPU.LinearOp <name="model.layers.26.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=975), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=981), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=980))] (%9176:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=975)]) -> (%9179:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=981)])
            linalg.CPU.MulOp <name="model.layers.26.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=979), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=981), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=979), )] (%9178:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=979)], %9179:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=981)]) -> (%9180:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=979)])
            linalg.CPU.LinearOp <name="model.layers.26.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=979), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=983), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=982))] (%9180:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=979)]) -> (%9181:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=983)])
            cf.ReturnOp (%9181:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=983)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.27 <CPU> [using_qnn:true, symbol:model.layers.27] {
        (%9182:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=983)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8069:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=30)], %8070:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=58)]) -> (%9223:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1017)], %9196:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=996)], %9198:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=998)]) {
            linalg.CPU.RMSNormOp <name="model.layers.27.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=983), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=984), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=985))] (%9182:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=983)]) -> (%9183:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=984)])
            graph.CallGraphOp @model.layers.27.self_attn (%9183:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=984)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8069:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=30)], %8070:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=58)]) -> (%9215:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1008)], %9196:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=996)], %9198:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=998)])
            linalg.CPU.AddOp <name="model.layers.27.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1008), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=983), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1008), )] (%9215:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1008)], %9182:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=983)]) -> (%9216:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1008)])
            linalg.CPU.RMSNormOp <name="model.layers.27.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1008), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1009), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1010))] (%9216:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1008)]) -> (%9217:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1009)])
            graph.CallGraphOp @model.layers.27.mlp (%9217:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1009)]) -> (%9222:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1017)])
            linalg.CPU.AddOp <name="model.layers.27.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1017), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1008), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1017), )] (%9222:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1017)], %9216:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1008)]) -> (%9223:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1017)])
            cf.ReturnOp (%9223:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1017)], %9196:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=996)], %9198:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=998)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.27.self_attn <CPU> [using_qnn:true, symbol:model.layers.27.self_attn] {
        (%9183:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=984)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %8069:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=30)], %8070:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=58)]) -> (%9215:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1008)], %9196:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=996)], %9198:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=998)]) {
            linalg.CPU.LinearOp <name="model.layers.27.self_attn.q_proj">(%9183:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=984)]) -> (%9184:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=990)])
            linalg.CPU.LinearOp <name="model.layers.27.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=984), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=987), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=986))] (%9183:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=984)]) -> (%9185:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=987)])
            linalg.CPU.LinearOp <name="model.layers.27.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=984), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=989), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=988))] (%9183:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=984)]) -> (%9186:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=989)])
            linalg.CPU.ViewOp <name="model.layers.27.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=990), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=990), )] (%9184:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=990)]) -> (%9184:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=990)])
            linalg.CPU.TransposeOp <name="model.layers.27.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=990), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=990), )] (%9184:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=990)]) -> (%9187:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=990)])
            linalg.CPU.ViewOp <name="model.layers.27.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=987), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=987), )] (%9185:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=987)]) -> (%9185:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=987)])
            linalg.CPU.TransposeOp <name="model.layers.27.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=987), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=987), )] (%9185:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=987)]) -> (%9188:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=987)])
            linalg.CPU.ViewOp <name="model.layers.27.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=989), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=989), )] (%9186:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=989)]) -> (%9186:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=989)])
            linalg.CPU.TransposeOp <name="model.layers.27.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=989), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=989), )] (%9186:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=989)]) -> (%9189:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=989)])
            linalg.CPU.RMSNormOp <name="model.layers.27.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=990), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=991), weight_weight:QuantSpec(Raw(type: Int16PerTensor), uuid=992))] (%9187:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=990)]) -> (%9190:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=991)])
            linalg.CPU.RMSNormOp <name="model.layers.27.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=987), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=993), weight_weight:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=994))] (%9188:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=987)]) -> (%9191:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=993)])
            linalg.CPU.RoPEOp <name="model.layers.27.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=991), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=991), )] (%9190:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=991)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%9192:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=991)])
            linalg.CPU.RoPEOp <name="model.layers.27.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=993), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=993), )] (%9191:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=993)], %8074:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)], %8075:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%9193:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=993)])
            linalg.CPU.CastTypeOp <name="model.layers.27.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=993), outputs_0:QuantSpec(Raw(type: Float16), uuid=995), )] (%9193:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=993)]) -> (%9194:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=995)])
            linalg.CPU.CastTypeOp <name="model.layers.27.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=995), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=996), )] (%9194:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=995)]) -> (%9195:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=996)])
            linalg.CPU.TransposeOp <name="model.layers.27.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=996), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=996), )] (%9195:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=996)]) -> (%9196:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=996)])
            linalg.CPU.CastTypeOp <name="model.layers.27.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=989), outputs_0:QuantSpec(Raw(type: Float16), uuid=997), )] (%9189:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=989)]) -> (%9197:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=997)])
            linalg.CPU.CastTypeOp <name="model.layers.27.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=997), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=998), )] (%9197:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=997)]) -> (%9198:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=998)])
            linalg.CPU.ConcatOp <name="model.layers.27.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=30), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=996), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=30), )] (%8069:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=30)], %9196:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=996)]) -> (%9199:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=30)])
            linalg.CPU.ConcatOp <name="model.layers.27.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=58), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=998), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=58), )] (%8070:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=58)], %9198:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=998)]) -> (%9200:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=58)])
            linalg.CPU.RepeatOp <name="model.layers.27.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=30), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=30), )] (%9199:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=30)]) -> (%9201:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=30)])
            linalg.CPU.RepeatOp <name="model.layers.27.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=58), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=58), )] (%9200:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=58)]) -> (%9202:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=58)])
            linalg.CPU.MatMulOp <name="model.layers.27.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=991), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=30), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=999), )] (%9192:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=991)], %9201:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=30)]) -> (%9203:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=999)])
            linalg.CPU.MulOp <name="model.layers.27.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=999), inputs_1:QuantSpec(Raw(type: Float32), uuid=1000), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=999), )] (%9203:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=999)], %9204:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=1000), constant:[0.088388346]]) -> (%9205:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=999)])
            linalg.CPU.ReduceMinOp <name="model.layers.27.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=999), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1001), )] (%9205:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=999)]) -> (%9206:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1001)])
            linalg.CPU.AddOp <name="model.layers.27.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1001), inputs_1:QuantSpec(Raw(type: Int16), uuid=1002), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1001), )] (%9206:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1001)], %9207:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=1002), constant:[-20]]) -> (%9208:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1001)])
            linalg.CPU.EqualOp <name="model.layers.27.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=1003), outputs_0:QuantSpec(Raw(type: UInt8), uuid=1004), )] (%8014:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %9209:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=1003), constant:[0.890625]]) -> (%9210:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=1004)])
            linalg.CPU.WhereOp <name="model.layers.27.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=1004), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=999), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1001), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1001), )] (%9210:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=1004)], %9205:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=999)], %9208:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1001)]) -> (%9211:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1001)])
            linalg.CPU.SoftmaxOp <name="model.layers.27.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1001), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1005), )] (%9211:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1001)]) -> (%9212:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1005)])
            linalg.CPU.MatMulOp <name="model.layers.27.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1005), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=58), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1006), )] (%9212:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1005)], %9202:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=58)]) -> (%9213:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1006)])
            linalg.CPU.TransposeOp <name="model.layers.27.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1006), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1006), )] (%9213:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1006)]) -> (%9214:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1006)])
            linalg.CPU.ViewOp <name="model.layers.27.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1006), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1006), )] (%9214:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1006)]) -> (%9214:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1006)])
            linalg.CPU.LinearOp <name="model.layers.27.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1006), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1008), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=1007))] (%9214:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1006)]) -> (%9215:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1008)])
            cf.ReturnOp (%9215:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1008)], %9196:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=996)], %9198:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=998)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.27.mlp <CPU> [using_qnn:true, symbol:model.layers.27.mlp] {
        (%9217:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1009)]) -> (%9222:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1017)]) {
            linalg.CPU.LinearOp <name="model.layers.27.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1009), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1012), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=1011))] (%9217:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1009)]) -> (%9218:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1012)])
            linalg.CPU.SiLUOp <name="model.layers.27.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1012), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1013), )] (%9218:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1012)]) -> (%9219:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1013)])
            linalg.CPU.LinearOp <name="model.layers.27.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1009), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1015), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=1014))] (%9217:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1009)]) -> (%9220:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1015)])
            linalg.CPU.MulOp <name="model.layers.27.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1013), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1015), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1013), )] (%9219:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1013)], %9220:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1015)]) -> (%9221:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1013)])
            linalg.CPU.LinearOp <name="model.layers.27.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1013), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1017), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=1016))] (%9221:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1013)]) -> (%9222:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1017)])
            cf.ReturnOp (%9222:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1017)]) -> ()
        }
    }
    //        
    //      o o    
    //            
    //       
    //             
    //        
}
 
