@main () -> () {
    graph.SubGraphOp @init <notype> [symbol:init] {
        () -> () {
            tensor.CPU.register () -> (%105:tensor<[151936, 2048], Float32, CPU>[@model.embed_tokens.weight][symbol:model.embed_tokens.weight])[symbol:model.embed_tokens.weight]
            tensor.CPU.register () -> (%76:tensor<[2048, 2048], Float32, CPU>[@model.layers.0.self_attn.q_proj.weight][symbol:model.layers.0.self_attn.q_proj.weight])[symbol:model.layers.0.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%133:tensor<[1024, 2048], Float32, CPU>[@model.layers.0.self_attn.k_proj.weight][symbol:model.layers.0.self_attn.k_proj.weight])[symbol:model.layers.0.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%179:tensor<[1024, 2048], Float32, CPU>[@model.layers.0.self_attn.v_proj.weight][symbol:model.layers.0.self_attn.v_proj.weight])[symbol:model.layers.0.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%269:tensor<[2048, 2048], Float32, CPU>[@model.layers.0.self_attn.o_proj.weight][symbol:model.layers.0.self_attn.o_proj.weight])[symbol:model.layers.0.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%9:tensor<[6144, 2048], Float32, CPU>[@model.layers.0.mlp.gate_proj.weight][symbol:model.layers.0.mlp.gate_proj.weight])[symbol:model.layers.0.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%111:tensor<[6144, 2048], Float32, CPU>[@model.layers.0.mlp.up_proj.weight][symbol:model.layers.0.mlp.up_proj.weight])[symbol:model.layers.0.mlp.up_proj.weight]
            tensor.CPU.register () -> (%184:tensor<[2048, 6144], Float32, CPU>[@model.layers.0.mlp.down_proj.weight][symbol:model.layers.0.mlp.down_proj.weight])[symbol:model.layers.0.mlp.down_proj.weight]
            tensor.CPU.register () -> (%285:tensor<[2048, 2048], Float32, CPU>[@model.layers.1.self_attn.q_proj.weight][symbol:model.layers.1.self_attn.q_proj.weight])[symbol:model.layers.1.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%32:tensor<[1024, 2048], Float32, CPU>[@model.layers.1.self_attn.k_proj.weight][symbol:model.layers.1.self_attn.k_proj.weight])[symbol:model.layers.1.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%154:tensor<[1024, 2048], Float32, CPU>[@model.layers.1.self_attn.v_proj.weight][symbol:model.layers.1.self_attn.v_proj.weight])[symbol:model.layers.1.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%20:tensor<[2048, 2048], Float32, CPU>[@model.layers.1.self_attn.o_proj.weight][symbol:model.layers.1.self_attn.o_proj.weight])[symbol:model.layers.1.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%245:tensor<[6144, 2048], Float32, CPU>[@model.layers.1.mlp.gate_proj.weight][symbol:model.layers.1.mlp.gate_proj.weight])[symbol:model.layers.1.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%230:tensor<[6144, 2048], Float32, CPU>[@model.layers.1.mlp.up_proj.weight][symbol:model.layers.1.mlp.up_proj.weight])[symbol:model.layers.1.mlp.up_proj.weight]
            tensor.CPU.register () -> (%43:tensor<[2048, 6144], Float32, CPU>[@model.layers.1.mlp.down_proj.weight][symbol:model.layers.1.mlp.down_proj.weight])[symbol:model.layers.1.mlp.down_proj.weight]
            tensor.CPU.register () -> (%221:tensor<[2048, 2048], Float32, CPU>[@model.layers.2.self_attn.q_proj.weight][symbol:model.layers.2.self_attn.q_proj.weight])[symbol:model.layers.2.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%103:tensor<[1024, 2048], Float32, CPU>[@model.layers.2.self_attn.k_proj.weight][symbol:model.layers.2.self_attn.k_proj.weight])[symbol:model.layers.2.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%47:tensor<[1024, 2048], Float32, CPU>[@model.layers.2.self_attn.v_proj.weight][symbol:model.layers.2.self_attn.v_proj.weight])[symbol:model.layers.2.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%85:tensor<[2048, 2048], Float32, CPU>[@model.layers.2.self_attn.o_proj.weight][symbol:model.layers.2.self_attn.o_proj.weight])[symbol:model.layers.2.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%252:tensor<[6144, 2048], Float32, CPU>[@model.layers.2.mlp.gate_proj.weight][symbol:model.layers.2.mlp.gate_proj.weight])[symbol:model.layers.2.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%24:tensor<[6144, 2048], Float32, CPU>[@model.layers.2.mlp.up_proj.weight][symbol:model.layers.2.mlp.up_proj.weight])[symbol:model.layers.2.mlp.up_proj.weight]
            tensor.CPU.register () -> (%28:tensor<[2048, 6144], Float32, CPU>[@model.layers.2.mlp.down_proj.weight][symbol:model.layers.2.mlp.down_proj.weight])[symbol:model.layers.2.mlp.down_proj.weight]
            tensor.CPU.register () -> (%283:tensor<[2048, 2048], Float32, CPU>[@model.layers.3.self_attn.q_proj.weight][symbol:model.layers.3.self_attn.q_proj.weight])[symbol:model.layers.3.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%48:tensor<[1024, 2048], Float32, CPU>[@model.layers.3.self_attn.k_proj.weight][symbol:model.layers.3.self_attn.k_proj.weight])[symbol:model.layers.3.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%244:tensor<[1024, 2048], Float32, CPU>[@model.layers.3.self_attn.v_proj.weight][symbol:model.layers.3.self_attn.v_proj.weight])[symbol:model.layers.3.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%301:tensor<[2048, 2048], Float32, CPU>[@model.layers.3.self_attn.o_proj.weight][symbol:model.layers.3.self_attn.o_proj.weight])[symbol:model.layers.3.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%129:tensor<[6144, 2048], Float32, CPU>[@model.layers.3.mlp.gate_proj.weight][symbol:model.layers.3.mlp.gate_proj.weight])[symbol:model.layers.3.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%188:tensor<[6144, 2048], Float32, CPU>[@model.layers.3.mlp.up_proj.weight][symbol:model.layers.3.mlp.up_proj.weight])[symbol:model.layers.3.mlp.up_proj.weight]
            tensor.CPU.register () -> (%97:tensor<[2048, 6144], Float32, CPU>[@model.layers.3.mlp.down_proj.weight][symbol:model.layers.3.mlp.down_proj.weight])[symbol:model.layers.3.mlp.down_proj.weight]
            tensor.CPU.register () -> (%164:tensor<[2048, 2048], Float32, CPU>[@model.layers.4.self_attn.q_proj.weight][symbol:model.layers.4.self_attn.q_proj.weight])[symbol:model.layers.4.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%148:tensor<[1024, 2048], Float32, CPU>[@model.layers.4.self_attn.k_proj.weight][symbol:model.layers.4.self_attn.k_proj.weight])[symbol:model.layers.4.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%279:tensor<[1024, 2048], Float32, CPU>[@model.layers.4.self_attn.v_proj.weight][symbol:model.layers.4.self_attn.v_proj.weight])[symbol:model.layers.4.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%91:tensor<[2048, 2048], Float32, CPU>[@model.layers.4.self_attn.o_proj.weight][symbol:model.layers.4.self_attn.o_proj.weight])[symbol:model.layers.4.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%189:tensor<[6144, 2048], Float32, CPU>[@model.layers.4.mlp.gate_proj.weight][symbol:model.layers.4.mlp.gate_proj.weight])[symbol:model.layers.4.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%156:tensor<[6144, 2048], Float32, CPU>[@model.layers.4.mlp.up_proj.weight][symbol:model.layers.4.mlp.up_proj.weight])[symbol:model.layers.4.mlp.up_proj.weight]
            tensor.CPU.register () -> (%153:tensor<[2048, 6144], Float32, CPU>[@model.layers.4.mlp.down_proj.weight][symbol:model.layers.4.mlp.down_proj.weight])[symbol:model.layers.4.mlp.down_proj.weight]
            tensor.CPU.register () -> (%78:tensor<[2048, 2048], Float32, CPU>[@model.layers.5.self_attn.q_proj.weight][symbol:model.layers.5.self_attn.q_proj.weight])[symbol:model.layers.5.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%72:tensor<[1024, 2048], Float32, CPU>[@model.layers.5.self_attn.k_proj.weight][symbol:model.layers.5.self_attn.k_proj.weight])[symbol:model.layers.5.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%289:tensor<[1024, 2048], Float32, CPU>[@model.layers.5.self_attn.v_proj.weight][symbol:model.layers.5.self_attn.v_proj.weight])[symbol:model.layers.5.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%264:tensor<[2048, 2048], Float32, CPU>[@model.layers.5.self_attn.o_proj.weight][symbol:model.layers.5.self_attn.o_proj.weight])[symbol:model.layers.5.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%4:tensor<[6144, 2048], Float32, CPU>[@model.layers.5.mlp.gate_proj.weight][symbol:model.layers.5.mlp.gate_proj.weight])[symbol:model.layers.5.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%308:tensor<[6144, 2048], Float32, CPU>[@model.layers.5.mlp.up_proj.weight][symbol:model.layers.5.mlp.up_proj.weight])[symbol:model.layers.5.mlp.up_proj.weight]
            tensor.CPU.register () -> (%74:tensor<[2048, 6144], Float32, CPU>[@model.layers.5.mlp.down_proj.weight][symbol:model.layers.5.mlp.down_proj.weight])[symbol:model.layers.5.mlp.down_proj.weight]
            tensor.CPU.register () -> (%59:tensor<[2048, 2048], Float32, CPU>[@model.layers.6.self_attn.q_proj.weight][symbol:model.layers.6.self_attn.q_proj.weight])[symbol:model.layers.6.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%208:tensor<[1024, 2048], Float32, CPU>[@model.layers.6.self_attn.k_proj.weight][symbol:model.layers.6.self_attn.k_proj.weight])[symbol:model.layers.6.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%238:tensor<[1024, 2048], Float32, CPU>[@model.layers.6.self_attn.v_proj.weight][symbol:model.layers.6.self_attn.v_proj.weight])[symbol:model.layers.6.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%52:tensor<[2048, 2048], Float32, CPU>[@model.layers.6.self_attn.o_proj.weight][symbol:model.layers.6.self_attn.o_proj.weight])[symbol:model.layers.6.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%80:tensor<[6144, 2048], Float32, CPU>[@model.layers.6.mlp.gate_proj.weight][symbol:model.layers.6.mlp.gate_proj.weight])[symbol:model.layers.6.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%276:tensor<[6144, 2048], Float32, CPU>[@model.layers.6.mlp.up_proj.weight][symbol:model.layers.6.mlp.up_proj.weight])[symbol:model.layers.6.mlp.up_proj.weight]
            tensor.CPU.register () -> (%227:tensor<[2048, 6144], Float32, CPU>[@model.layers.6.mlp.down_proj.weight][symbol:model.layers.6.mlp.down_proj.weight])[symbol:model.layers.6.mlp.down_proj.weight]
            tensor.CPU.register () -> (%287:tensor<[2048, 2048], Float32, CPU>[@model.layers.7.self_attn.q_proj.weight][symbol:model.layers.7.self_attn.q_proj.weight])[symbol:model.layers.7.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%135:tensor<[1024, 2048], Float32, CPU>[@model.layers.7.self_attn.k_proj.weight][symbol:model.layers.7.self_attn.k_proj.weight])[symbol:model.layers.7.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%300:tensor<[1024, 2048], Float32, CPU>[@model.layers.7.self_attn.v_proj.weight][symbol:model.layers.7.self_attn.v_proj.weight])[symbol:model.layers.7.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%251:tensor<[2048, 2048], Float32, CPU>[@model.layers.7.self_attn.o_proj.weight][symbol:model.layers.7.self_attn.o_proj.weight])[symbol:model.layers.7.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%155:tensor<[6144, 2048], Float32, CPU>[@model.layers.7.mlp.gate_proj.weight][symbol:model.layers.7.mlp.gate_proj.weight])[symbol:model.layers.7.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%218:tensor<[6144, 2048], Float32, CPU>[@model.layers.7.mlp.up_proj.weight][symbol:model.layers.7.mlp.up_proj.weight])[symbol:model.layers.7.mlp.up_proj.weight]
            tensor.CPU.register () -> (%275:tensor<[2048, 6144], Float32, CPU>[@model.layers.7.mlp.down_proj.weight][symbol:model.layers.7.mlp.down_proj.weight])[symbol:model.layers.7.mlp.down_proj.weight]
            tensor.CPU.register () -> (%165:tensor<[2048, 2048], Float32, CPU>[@model.layers.8.self_attn.q_proj.weight][symbol:model.layers.8.self_attn.q_proj.weight])[symbol:model.layers.8.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%194:tensor<[1024, 2048], Float32, CPU>[@model.layers.8.self_attn.k_proj.weight][symbol:model.layers.8.self_attn.k_proj.weight])[symbol:model.layers.8.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%181:tensor<[1024, 2048], Float32, CPU>[@model.layers.8.self_attn.v_proj.weight][symbol:model.layers.8.self_attn.v_proj.weight])[symbol:model.layers.8.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%197:tensor<[2048, 2048], Float32, CPU>[@model.layers.8.self_attn.o_proj.weight][symbol:model.layers.8.self_attn.o_proj.weight])[symbol:model.layers.8.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%110:tensor<[6144, 2048], Float32, CPU>[@model.layers.8.mlp.gate_proj.weight][symbol:model.layers.8.mlp.gate_proj.weight])[symbol:model.layers.8.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%236:tensor<[6144, 2048], Float32, CPU>[@model.layers.8.mlp.up_proj.weight][symbol:model.layers.8.mlp.up_proj.weight])[symbol:model.layers.8.mlp.up_proj.weight]
            tensor.CPU.register () -> (%106:tensor<[2048, 6144], Float32, CPU>[@model.layers.8.mlp.down_proj.weight][symbol:model.layers.8.mlp.down_proj.weight])[symbol:model.layers.8.mlp.down_proj.weight]
            tensor.CPU.register () -> (%235:tensor<[2048, 2048], Float32, CPU>[@model.layers.9.self_attn.q_proj.weight][symbol:model.layers.9.self_attn.q_proj.weight])[symbol:model.layers.9.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%69:tensor<[1024, 2048], Float32, CPU>[@model.layers.9.self_attn.k_proj.weight][symbol:model.layers.9.self_attn.k_proj.weight])[symbol:model.layers.9.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%120:tensor<[1024, 2048], Float32, CPU>[@model.layers.9.self_attn.v_proj.weight][symbol:model.layers.9.self_attn.v_proj.weight])[symbol:model.layers.9.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%205:tensor<[2048, 2048], Float32, CPU>[@model.layers.9.self_attn.o_proj.weight][symbol:model.layers.9.self_attn.o_proj.weight])[symbol:model.layers.9.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%263:tensor<[6144, 2048], Float32, CPU>[@model.layers.9.mlp.gate_proj.weight][symbol:model.layers.9.mlp.gate_proj.weight])[symbol:model.layers.9.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%102:tensor<[6144, 2048], Float32, CPU>[@model.layers.9.mlp.up_proj.weight][symbol:model.layers.9.mlp.up_proj.weight])[symbol:model.layers.9.mlp.up_proj.weight]
            tensor.CPU.register () -> (%136:tensor<[2048, 6144], Float32, CPU>[@model.layers.9.mlp.down_proj.weight][symbol:model.layers.9.mlp.down_proj.weight])[symbol:model.layers.9.mlp.down_proj.weight]
            tensor.CPU.register () -> (%278:tensor<[2048, 2048], Float32, CPU>[@model.layers.10.self_attn.q_proj.weight][symbol:model.layers.10.self_attn.q_proj.weight])[symbol:model.layers.10.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%182:tensor<[1024, 2048], Float32, CPU>[@model.layers.10.self_attn.k_proj.weight][symbol:model.layers.10.self_attn.k_proj.weight])[symbol:model.layers.10.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%138:tensor<[1024, 2048], Float32, CPU>[@model.layers.10.self_attn.v_proj.weight][symbol:model.layers.10.self_attn.v_proj.weight])[symbol:model.layers.10.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%233:tensor<[2048, 2048], Float32, CPU>[@model.layers.10.self_attn.o_proj.weight][symbol:model.layers.10.self_attn.o_proj.weight])[symbol:model.layers.10.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%124:tensor<[6144, 2048], Float32, CPU>[@model.layers.10.mlp.gate_proj.weight][symbol:model.layers.10.mlp.gate_proj.weight])[symbol:model.layers.10.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%261:tensor<[6144, 2048], Float32, CPU>[@model.layers.10.mlp.up_proj.weight][symbol:model.layers.10.mlp.up_proj.weight])[symbol:model.layers.10.mlp.up_proj.weight]
            tensor.CPU.register () -> (%45:tensor<[2048, 6144], Float32, CPU>[@model.layers.10.mlp.down_proj.weight][symbol:model.layers.10.mlp.down_proj.weight])[symbol:model.layers.10.mlp.down_proj.weight]
            tensor.CPU.register () -> (%274:tensor<[2048, 2048], Float32, CPU>[@model.layers.11.self_attn.q_proj.weight][symbol:model.layers.11.self_attn.q_proj.weight])[symbol:model.layers.11.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%157:tensor<[1024, 2048], Float32, CPU>[@model.layers.11.self_attn.k_proj.weight][symbol:model.layers.11.self_attn.k_proj.weight])[symbol:model.layers.11.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%63:tensor<[1024, 2048], Float32, CPU>[@model.layers.11.self_attn.v_proj.weight][symbol:model.layers.11.self_attn.v_proj.weight])[symbol:model.layers.11.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%118:tensor<[2048, 2048], Float32, CPU>[@model.layers.11.self_attn.o_proj.weight][symbol:model.layers.11.self_attn.o_proj.weight])[symbol:model.layers.11.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%207:tensor<[6144, 2048], Float32, CPU>[@model.layers.11.mlp.gate_proj.weight][symbol:model.layers.11.mlp.gate_proj.weight])[symbol:model.layers.11.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%226:tensor<[6144, 2048], Float32, CPU>[@model.layers.11.mlp.up_proj.weight][symbol:model.layers.11.mlp.up_proj.weight])[symbol:model.layers.11.mlp.up_proj.weight]
            tensor.CPU.register () -> (%224:tensor<[2048, 6144], Float32, CPU>[@model.layers.11.mlp.down_proj.weight][symbol:model.layers.11.mlp.down_proj.weight])[symbol:model.layers.11.mlp.down_proj.weight]
            tensor.CPU.register () -> (%217:tensor<[2048, 2048], Float32, CPU>[@model.layers.12.self_attn.q_proj.weight][symbol:model.layers.12.self_attn.q_proj.weight])[symbol:model.layers.12.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%297:tensor<[1024, 2048], Float32, CPU>[@model.layers.12.self_attn.k_proj.weight][symbol:model.layers.12.self_attn.k_proj.weight])[symbol:model.layers.12.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%94:tensor<[1024, 2048], Float32, CPU>[@model.layers.12.self_attn.v_proj.weight][symbol:model.layers.12.self_attn.v_proj.weight])[symbol:model.layers.12.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%49:tensor<[2048, 2048], Float32, CPU>[@model.layers.12.self_attn.o_proj.weight][symbol:model.layers.12.self_attn.o_proj.weight])[symbol:model.layers.12.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%262:tensor<[6144, 2048], Float32, CPU>[@model.layers.12.mlp.gate_proj.weight][symbol:model.layers.12.mlp.gate_proj.weight])[symbol:model.layers.12.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%255:tensor<[6144, 2048], Float32, CPU>[@model.layers.12.mlp.up_proj.weight][symbol:model.layers.12.mlp.up_proj.weight])[symbol:model.layers.12.mlp.up_proj.weight]
            tensor.CPU.register () -> (%22:tensor<[2048, 6144], Float32, CPU>[@model.layers.12.mlp.down_proj.weight][symbol:model.layers.12.mlp.down_proj.weight])[symbol:model.layers.12.mlp.down_proj.weight]
            tensor.CPU.register () -> (%114:tensor<[2048, 2048], Float32, CPU>[@model.layers.13.self_attn.q_proj.weight][symbol:model.layers.13.self_attn.q_proj.weight])[symbol:model.layers.13.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%152:tensor<[1024, 2048], Float32, CPU>[@model.layers.13.self_attn.k_proj.weight][symbol:model.layers.13.self_attn.k_proj.weight])[symbol:model.layers.13.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%15:tensor<[1024, 2048], Float32, CPU>[@model.layers.13.self_attn.v_proj.weight][symbol:model.layers.13.self_attn.v_proj.weight])[symbol:model.layers.13.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%250:tensor<[2048, 2048], Float32, CPU>[@model.layers.13.self_attn.o_proj.weight][symbol:model.layers.13.self_attn.o_proj.weight])[symbol:model.layers.13.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%247:tensor<[6144, 2048], Float32, CPU>[@model.layers.13.mlp.gate_proj.weight][symbol:model.layers.13.mlp.gate_proj.weight])[symbol:model.layers.13.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%98:tensor<[6144, 2048], Float32, CPU>[@model.layers.13.mlp.up_proj.weight][symbol:model.layers.13.mlp.up_proj.weight])[symbol:model.layers.13.mlp.up_proj.weight]
            tensor.CPU.register () -> (%193:tensor<[2048, 6144], Float32, CPU>[@model.layers.13.mlp.down_proj.weight][symbol:model.layers.13.mlp.down_proj.weight])[symbol:model.layers.13.mlp.down_proj.weight]
            tensor.CPU.register () -> (%209:tensor<[2048, 2048], Float32, CPU>[@model.layers.14.self_attn.q_proj.weight][symbol:model.layers.14.self_attn.q_proj.weight])[symbol:model.layers.14.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%38:tensor<[1024, 2048], Float32, CPU>[@model.layers.14.self_attn.k_proj.weight][symbol:model.layers.14.self_attn.k_proj.weight])[symbol:model.layers.14.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%232:tensor<[1024, 2048], Float32, CPU>[@model.layers.14.self_attn.v_proj.weight][symbol:model.layers.14.self_attn.v_proj.weight])[symbol:model.layers.14.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%168:tensor<[2048, 2048], Float32, CPU>[@model.layers.14.self_attn.o_proj.weight][symbol:model.layers.14.self_attn.o_proj.weight])[symbol:model.layers.14.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%37:tensor<[6144, 2048], Float32, CPU>[@model.layers.14.mlp.gate_proj.weight][symbol:model.layers.14.mlp.gate_proj.weight])[symbol:model.layers.14.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%147:tensor<[6144, 2048], Float32, CPU>[@model.layers.14.mlp.up_proj.weight][symbol:model.layers.14.mlp.up_proj.weight])[symbol:model.layers.14.mlp.up_proj.weight]
            tensor.CPU.register () -> (%163:tensor<[2048, 6144], Float32, CPU>[@model.layers.14.mlp.down_proj.weight][symbol:model.layers.14.mlp.down_proj.weight])[symbol:model.layers.14.mlp.down_proj.weight]
            tensor.CPU.register () -> (%46:tensor<[2048, 2048], Float32, CPU>[@model.layers.15.self_attn.q_proj.weight][symbol:model.layers.15.self_attn.q_proj.weight])[symbol:model.layers.15.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%268:tensor<[1024, 2048], Float32, CPU>[@model.layers.15.self_attn.k_proj.weight][symbol:model.layers.15.self_attn.k_proj.weight])[symbol:model.layers.15.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%117:tensor<[1024, 2048], Float32, CPU>[@model.layers.15.self_attn.v_proj.weight][symbol:model.layers.15.self_attn.v_proj.weight])[symbol:model.layers.15.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%303:tensor<[2048, 2048], Float32, CPU>[@model.layers.15.self_attn.o_proj.weight][symbol:model.layers.15.self_attn.o_proj.weight])[symbol:model.layers.15.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%260:tensor<[6144, 2048], Float32, CPU>[@model.layers.15.mlp.gate_proj.weight][symbol:model.layers.15.mlp.gate_proj.weight])[symbol:model.layers.15.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%42:tensor<[6144, 2048], Float32, CPU>[@model.layers.15.mlp.up_proj.weight][symbol:model.layers.15.mlp.up_proj.weight])[symbol:model.layers.15.mlp.up_proj.weight]
            tensor.CPU.register () -> (%290:tensor<[2048, 6144], Float32, CPU>[@model.layers.15.mlp.down_proj.weight][symbol:model.layers.15.mlp.down_proj.weight])[symbol:model.layers.15.mlp.down_proj.weight]
            tensor.CPU.register () -> (%17:tensor<[2048, 2048], Float32, CPU>[@model.layers.16.self_attn.q_proj.weight][symbol:model.layers.16.self_attn.q_proj.weight])[symbol:model.layers.16.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%228:tensor<[1024, 2048], Float32, CPU>[@model.layers.16.self_attn.k_proj.weight][symbol:model.layers.16.self_attn.k_proj.weight])[symbol:model.layers.16.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%66:tensor<[1024, 2048], Float32, CPU>[@model.layers.16.self_attn.v_proj.weight][symbol:model.layers.16.self_attn.v_proj.weight])[symbol:model.layers.16.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%211:tensor<[2048, 2048], Float32, CPU>[@model.layers.16.self_attn.o_proj.weight][symbol:model.layers.16.self_attn.o_proj.weight])[symbol:model.layers.16.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%130:tensor<[6144, 2048], Float32, CPU>[@model.layers.16.mlp.gate_proj.weight][symbol:model.layers.16.mlp.gate_proj.weight])[symbol:model.layers.16.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%79:tensor<[6144, 2048], Float32, CPU>[@model.layers.16.mlp.up_proj.weight][symbol:model.layers.16.mlp.up_proj.weight])[symbol:model.layers.16.mlp.up_proj.weight]
            tensor.CPU.register () -> (%248:tensor<[2048, 6144], Float32, CPU>[@model.layers.16.mlp.down_proj.weight][symbol:model.layers.16.mlp.down_proj.weight])[symbol:model.layers.16.mlp.down_proj.weight]
            tensor.CPU.register () -> (%64:tensor<[2048, 2048], Float32, CPU>[@model.layers.17.self_attn.q_proj.weight][symbol:model.layers.17.self_attn.q_proj.weight])[symbol:model.layers.17.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%237:tensor<[1024, 2048], Float32, CPU>[@model.layers.17.self_attn.k_proj.weight][symbol:model.layers.17.self_attn.k_proj.weight])[symbol:model.layers.17.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%6:tensor<[1024, 2048], Float32, CPU>[@model.layers.17.self_attn.v_proj.weight][symbol:model.layers.17.self_attn.v_proj.weight])[symbol:model.layers.17.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%125:tensor<[2048, 2048], Float32, CPU>[@model.layers.17.self_attn.o_proj.weight][symbol:model.layers.17.self_attn.o_proj.weight])[symbol:model.layers.17.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%177:tensor<[6144, 2048], Float32, CPU>[@model.layers.17.mlp.gate_proj.weight][symbol:model.layers.17.mlp.gate_proj.weight])[symbol:model.layers.17.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%26:tensor<[6144, 2048], Float32, CPU>[@model.layers.17.mlp.up_proj.weight][symbol:model.layers.17.mlp.up_proj.weight])[symbol:model.layers.17.mlp.up_proj.weight]
            tensor.CPU.register () -> (%25:tensor<[2048, 6144], Float32, CPU>[@model.layers.17.mlp.down_proj.weight][symbol:model.layers.17.mlp.down_proj.weight])[symbol:model.layers.17.mlp.down_proj.weight]
            tensor.CPU.register () -> (%273:tensor<[2048, 2048], Float32, CPU>[@model.layers.18.self_attn.q_proj.weight][symbol:model.layers.18.self_attn.q_proj.weight])[symbol:model.layers.18.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%284:tensor<[1024, 2048], Float32, CPU>[@model.layers.18.self_attn.k_proj.weight][symbol:model.layers.18.self_attn.k_proj.weight])[symbol:model.layers.18.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%18:tensor<[1024, 2048], Float32, CPU>[@model.layers.18.self_attn.v_proj.weight][symbol:model.layers.18.self_attn.v_proj.weight])[symbol:model.layers.18.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%2:tensor<[2048, 2048], Float32, CPU>[@model.layers.18.self_attn.o_proj.weight][symbol:model.layers.18.self_attn.o_proj.weight])[symbol:model.layers.18.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%166:tensor<[6144, 2048], Float32, CPU>[@model.layers.18.mlp.gate_proj.weight][symbol:model.layers.18.mlp.gate_proj.weight])[symbol:model.layers.18.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%271:tensor<[6144, 2048], Float32, CPU>[@model.layers.18.mlp.up_proj.weight][symbol:model.layers.18.mlp.up_proj.weight])[symbol:model.layers.18.mlp.up_proj.weight]
            tensor.CPU.register () -> (%112:tensor<[2048, 6144], Float32, CPU>[@model.layers.18.mlp.down_proj.weight][symbol:model.layers.18.mlp.down_proj.weight])[symbol:model.layers.18.mlp.down_proj.weight]
            tensor.CPU.register () -> (%8:tensor<[2048, 2048], Float32, CPU>[@model.layers.19.self_attn.q_proj.weight][symbol:model.layers.19.self_attn.q_proj.weight])[symbol:model.layers.19.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%286:tensor<[1024, 2048], Float32, CPU>[@model.layers.19.self_attn.k_proj.weight][symbol:model.layers.19.self_attn.k_proj.weight])[symbol:model.layers.19.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%50:tensor<[1024, 2048], Float32, CPU>[@model.layers.19.self_attn.v_proj.weight][symbol:model.layers.19.self_attn.v_proj.weight])[symbol:model.layers.19.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%58:tensor<[2048, 2048], Float32, CPU>[@model.layers.19.self_attn.o_proj.weight][symbol:model.layers.19.self_attn.o_proj.weight])[symbol:model.layers.19.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%281:tensor<[6144, 2048], Float32, CPU>[@model.layers.19.mlp.gate_proj.weight][symbol:model.layers.19.mlp.gate_proj.weight])[symbol:model.layers.19.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%82:tensor<[6144, 2048], Float32, CPU>[@model.layers.19.mlp.up_proj.weight][symbol:model.layers.19.mlp.up_proj.weight])[symbol:model.layers.19.mlp.up_proj.weight]
            tensor.CPU.register () -> (%173:tensor<[2048, 6144], Float32, CPU>[@model.layers.19.mlp.down_proj.weight][symbol:model.layers.19.mlp.down_proj.weight])[symbol:model.layers.19.mlp.down_proj.weight]
            tensor.CPU.register () -> (%280:tensor<[2048, 2048], Float32, CPU>[@model.layers.20.self_attn.q_proj.weight][symbol:model.layers.20.self_attn.q_proj.weight])[symbol:model.layers.20.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%253:tensor<[1024, 2048], Float32, CPU>[@model.layers.20.self_attn.k_proj.weight][symbol:model.layers.20.self_attn.k_proj.weight])[symbol:model.layers.20.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%239:tensor<[1024, 2048], Float32, CPU>[@model.layers.20.self_attn.v_proj.weight][symbol:model.layers.20.self_attn.v_proj.weight])[symbol:model.layers.20.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%41:tensor<[2048, 2048], Float32, CPU>[@model.layers.20.self_attn.o_proj.weight][symbol:model.layers.20.self_attn.o_proj.weight])[symbol:model.layers.20.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%172:tensor<[6144, 2048], Float32, CPU>[@model.layers.20.mlp.gate_proj.weight][symbol:model.layers.20.mlp.gate_proj.weight])[symbol:model.layers.20.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%299:tensor<[6144, 2048], Float32, CPU>[@model.layers.20.mlp.up_proj.weight][symbol:model.layers.20.mlp.up_proj.weight])[symbol:model.layers.20.mlp.up_proj.weight]
            tensor.CPU.register () -> (%123:tensor<[2048, 6144], Float32, CPU>[@model.layers.20.mlp.down_proj.weight][symbol:model.layers.20.mlp.down_proj.weight])[symbol:model.layers.20.mlp.down_proj.weight]
            tensor.CPU.register () -> (%295:tensor<[2048, 2048], Float32, CPU>[@model.layers.21.self_attn.q_proj.weight][symbol:model.layers.21.self_attn.q_proj.weight])[symbol:model.layers.21.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%139:tensor<[1024, 2048], Float32, CPU>[@model.layers.21.self_attn.k_proj.weight][symbol:model.layers.21.self_attn.k_proj.weight])[symbol:model.layers.21.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%142:tensor<[1024, 2048], Float32, CPU>[@model.layers.21.self_attn.v_proj.weight][symbol:model.layers.21.self_attn.v_proj.weight])[symbol:model.layers.21.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%115:tensor<[2048, 2048], Float32, CPU>[@model.layers.21.self_attn.o_proj.weight][symbol:model.layers.21.self_attn.o_proj.weight])[symbol:model.layers.21.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%259:tensor<[6144, 2048], Float32, CPU>[@model.layers.21.mlp.gate_proj.weight][symbol:model.layers.21.mlp.gate_proj.weight])[symbol:model.layers.21.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%162:tensor<[6144, 2048], Float32, CPU>[@model.layers.21.mlp.up_proj.weight][symbol:model.layers.21.mlp.up_proj.weight])[symbol:model.layers.21.mlp.up_proj.weight]
            tensor.CPU.register () -> (%183:tensor<[2048, 6144], Float32, CPU>[@model.layers.21.mlp.down_proj.weight][symbol:model.layers.21.mlp.down_proj.weight])[symbol:model.layers.21.mlp.down_proj.weight]
            tensor.CPU.register () -> (%89:tensor<[2048, 2048], Float32, CPU>[@model.layers.22.self_attn.q_proj.weight][symbol:model.layers.22.self_attn.q_proj.weight])[symbol:model.layers.22.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%36:tensor<[1024, 2048], Float32, CPU>[@model.layers.22.self_attn.k_proj.weight][symbol:model.layers.22.self_attn.k_proj.weight])[symbol:model.layers.22.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%204:tensor<[1024, 2048], Float32, CPU>[@model.layers.22.self_attn.v_proj.weight][symbol:model.layers.22.self_attn.v_proj.weight])[symbol:model.layers.22.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%234:tensor<[2048, 2048], Float32, CPU>[@model.layers.22.self_attn.o_proj.weight][symbol:model.layers.22.self_attn.o_proj.weight])[symbol:model.layers.22.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%198:tensor<[6144, 2048], Float32, CPU>[@model.layers.22.mlp.gate_proj.weight][symbol:model.layers.22.mlp.gate_proj.weight])[symbol:model.layers.22.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%254:tensor<[6144, 2048], Float32, CPU>[@model.layers.22.mlp.up_proj.weight][symbol:model.layers.22.mlp.up_proj.weight])[symbol:model.layers.22.mlp.up_proj.weight]
            tensor.CPU.register () -> (%31:tensor<[2048, 6144], Float32, CPU>[@model.layers.22.mlp.down_proj.weight][symbol:model.layers.22.mlp.down_proj.weight])[symbol:model.layers.22.mlp.down_proj.weight]
            tensor.CPU.register () -> (%109:tensor<[2048, 2048], Float32, CPU>[@model.layers.23.self_attn.q_proj.weight][symbol:model.layers.23.self_attn.q_proj.weight])[symbol:model.layers.23.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%39:tensor<[1024, 2048], Float32, CPU>[@model.layers.23.self_attn.k_proj.weight][symbol:model.layers.23.self_attn.k_proj.weight])[symbol:model.layers.23.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%83:tensor<[1024, 2048], Float32, CPU>[@model.layers.23.self_attn.v_proj.weight][symbol:model.layers.23.self_attn.v_proj.weight])[symbol:model.layers.23.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%176:tensor<[2048, 2048], Float32, CPU>[@model.layers.23.self_attn.o_proj.weight][symbol:model.layers.23.self_attn.o_proj.weight])[symbol:model.layers.23.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%169:tensor<[6144, 2048], Float32, CPU>[@model.layers.23.mlp.gate_proj.weight][symbol:model.layers.23.mlp.gate_proj.weight])[symbol:model.layers.23.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%243:tensor<[6144, 2048], Float32, CPU>[@model.layers.23.mlp.up_proj.weight][symbol:model.layers.23.mlp.up_proj.weight])[symbol:model.layers.23.mlp.up_proj.weight]
            tensor.CPU.register () -> (%149:tensor<[2048, 6144], Float32, CPU>[@model.layers.23.mlp.down_proj.weight][symbol:model.layers.23.mlp.down_proj.weight])[symbol:model.layers.23.mlp.down_proj.weight]
            tensor.CPU.register () -> (%11:tensor<[2048, 2048], Float32, CPU>[@model.layers.24.self_attn.q_proj.weight][symbol:model.layers.24.self_attn.q_proj.weight])[symbol:model.layers.24.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%61:tensor<[1024, 2048], Float32, CPU>[@model.layers.24.self_attn.k_proj.weight][symbol:model.layers.24.self_attn.k_proj.weight])[symbol:model.layers.24.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%81:tensor<[1024, 2048], Float32, CPU>[@model.layers.24.self_attn.v_proj.weight][symbol:model.layers.24.self_attn.v_proj.weight])[symbol:model.layers.24.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%127:tensor<[2048, 2048], Float32, CPU>[@model.layers.24.self_attn.o_proj.weight][symbol:model.layers.24.self_attn.o_proj.weight])[symbol:model.layers.24.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%141:tensor<[6144, 2048], Float32, CPU>[@model.layers.24.mlp.gate_proj.weight][symbol:model.layers.24.mlp.gate_proj.weight])[symbol:model.layers.24.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%126:tensor<[6144, 2048], Float32, CPU>[@model.layers.24.mlp.up_proj.weight][symbol:model.layers.24.mlp.up_proj.weight])[symbol:model.layers.24.mlp.up_proj.weight]
            tensor.CPU.register () -> (%34:tensor<[2048, 6144], Float32, CPU>[@model.layers.24.mlp.down_proj.weight][symbol:model.layers.24.mlp.down_proj.weight])[symbol:model.layers.24.mlp.down_proj.weight]
            tensor.CPU.register () -> (%206:tensor<[2048, 2048], Float32, CPU>[@model.layers.25.self_attn.q_proj.weight][symbol:model.layers.25.self_attn.q_proj.weight])[symbol:model.layers.25.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%27:tensor<[1024, 2048], Float32, CPU>[@model.layers.25.self_attn.k_proj.weight][symbol:model.layers.25.self_attn.k_proj.weight])[symbol:model.layers.25.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%121:tensor<[1024, 2048], Float32, CPU>[@model.layers.25.self_attn.v_proj.weight][symbol:model.layers.25.self_attn.v_proj.weight])[symbol:model.layers.25.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%150:tensor<[2048, 2048], Float32, CPU>[@model.layers.25.self_attn.o_proj.weight][symbol:model.layers.25.self_attn.o_proj.weight])[symbol:model.layers.25.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%249:tensor<[6144, 2048], Float32, CPU>[@model.layers.25.mlp.gate_proj.weight][symbol:model.layers.25.mlp.gate_proj.weight])[symbol:model.layers.25.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%159:tensor<[6144, 2048], Float32, CPU>[@model.layers.25.mlp.up_proj.weight][symbol:model.layers.25.mlp.up_proj.weight])[symbol:model.layers.25.mlp.up_proj.weight]
            tensor.CPU.register () -> (%267:tensor<[2048, 6144], Float32, CPU>[@model.layers.25.mlp.down_proj.weight][symbol:model.layers.25.mlp.down_proj.weight])[symbol:model.layers.25.mlp.down_proj.weight]
            tensor.CPU.register () -> (%265:tensor<[2048, 2048], Float32, CPU>[@model.layers.26.self_attn.q_proj.weight][symbol:model.layers.26.self_attn.q_proj.weight])[symbol:model.layers.26.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%190:tensor<[1024, 2048], Float32, CPU>[@model.layers.26.self_attn.k_proj.weight][symbol:model.layers.26.self_attn.k_proj.weight])[symbol:model.layers.26.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%119:tensor<[1024, 2048], Float32, CPU>[@model.layers.26.self_attn.v_proj.weight][symbol:model.layers.26.self_attn.v_proj.weight])[symbol:model.layers.26.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%88:tensor<[2048, 2048], Float32, CPU>[@model.layers.26.self_attn.o_proj.weight][symbol:model.layers.26.self_attn.o_proj.weight])[symbol:model.layers.26.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%96:tensor<[6144, 2048], Float32, CPU>[@model.layers.26.mlp.gate_proj.weight][symbol:model.layers.26.mlp.gate_proj.weight])[symbol:model.layers.26.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%62:tensor<[6144, 2048], Float32, CPU>[@model.layers.26.mlp.up_proj.weight][symbol:model.layers.26.mlp.up_proj.weight])[symbol:model.layers.26.mlp.up_proj.weight]
            tensor.CPU.register () -> (%220:tensor<[2048, 6144], Float32, CPU>[@model.layers.26.mlp.down_proj.weight][symbol:model.layers.26.mlp.down_proj.weight])[symbol:model.layers.26.mlp.down_proj.weight]
            tensor.CPU.register () -> (%185:tensor<[2048, 2048], Float32, CPU>[@model.layers.27.self_attn.q_proj.weight][symbol:model.layers.27.self_attn.q_proj.weight])[symbol:model.layers.27.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%12:tensor<[1024, 2048], Float32, CPU>[@model.layers.27.self_attn.k_proj.weight][symbol:model.layers.27.self_attn.k_proj.weight])[symbol:model.layers.27.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%54:tensor<[1024, 2048], Float32, CPU>[@model.layers.27.self_attn.v_proj.weight][symbol:model.layers.27.self_attn.v_proj.weight])[symbol:model.layers.27.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%60:tensor<[2048, 2048], Float32, CPU>[@model.layers.27.self_attn.o_proj.weight][symbol:model.layers.27.self_attn.o_proj.weight])[symbol:model.layers.27.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%144:tensor<[6144, 2048], Float32, CPU>[@model.layers.27.mlp.gate_proj.weight][symbol:model.layers.27.mlp.gate_proj.weight])[symbol:model.layers.27.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%146:tensor<[6144, 2048], Float32, CPU>[@model.layers.27.mlp.up_proj.weight][symbol:model.layers.27.mlp.up_proj.weight])[symbol:model.layers.27.mlp.up_proj.weight]
            tensor.CPU.register () -> (%195:tensor<[2048, 6144], Float32, CPU>[@model.layers.27.mlp.down_proj.weight][symbol:model.layers.27.mlp.down_proj.weight])[symbol:model.layers.27.mlp.down_proj.weight]
            tensor.CPU.register () -> (%101:tensor<[151936, 2048], Float32, CPU>[@lm_head.weight][symbol:lm_head.weight])[symbol:lm_head.weight]
        }
    }
    graph.SubGraphOp @deinit <notype> [symbol:deinit] {
        () -> () {
            
        }
    }
    graph.CallGraphOp @model (%316:tensor<[1, 32], Float32, CPU>, %374:tensor<[1, 32], Int64, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %318:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %320:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %322:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %324:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %326:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %328:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %330:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %332:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %334:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %336:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %338:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %340:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %342:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %344:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %346:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %348:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %350:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %352:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %354:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %356:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %358:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %360:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %362:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %364:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %366:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %368:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %370:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %372:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %319:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %321:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %323:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %325:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %327:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %329:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %331:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %333:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %335:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %337:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %339:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %341:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %343:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %345:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %347:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %349:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %351:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %353:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %355:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %357:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %359:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %361:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %363:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %365:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %367:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %369:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %371:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %373:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1471:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %392:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %431:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %470:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %509:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %548:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %587:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %626:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %665:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %704:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %743:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %782:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %821:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %860:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %899:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %938:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %977:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1016:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1055:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1094:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1133:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1172:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1211:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1250:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1289:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1328:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1367:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1406:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1445:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %385:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %424:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %463:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %502:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %541:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %580:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %619:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %658:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %697:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %736:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %775:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %814:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %853:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %892:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %931:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %970:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1009:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1048:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1087:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1126:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1165:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1204:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1243:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1282:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1321:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1360:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1399:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1438:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
    graph.SubGraphOp @model <CPU> [using_qnn:true, symbol:model] {
        (%316:tensor<[1, 32], Float32, CPU>, %374:tensor<[1, 32], Int64, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %318:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %320:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %322:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %324:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %326:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %328:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %330:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %332:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %334:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %336:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %338:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %340:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %342:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %344:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %346:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %348:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %350:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %352:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %354:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %356:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %358:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %360:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %362:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %364:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %366:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %368:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %370:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %372:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %319:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %321:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %323:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %325:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %327:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %329:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %331:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %333:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %335:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %337:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %339:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %341:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %343:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %345:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %347:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %349:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %351:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %353:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %355:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %357:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %359:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %361:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %363:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %365:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %367:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %369:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %371:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %373:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1471:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %392:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %431:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %470:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %509:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %548:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %587:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %626:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %665:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %704:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %743:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %782:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %821:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %860:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %899:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %938:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %977:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1016:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1055:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1094:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1133:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1172:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1211:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1250:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1289:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1328:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1367:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1406:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1445:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %385:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %424:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %463:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %502:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %541:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %580:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %619:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %658:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %697:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %736:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %775:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %814:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %853:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %892:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %931:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %970:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1009:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1048:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1087:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1126:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1165:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1204:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1243:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1282:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1321:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1360:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1399:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1438:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.EmbeddingOp <name="model.embed_tokens">(%316:tensor<[1, 32], Float32, CPU>) -> (%375:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.CastTypeOp <name="model.CastType.0">(%375:tensor<[1, 32, 2048], Float32, CPU>) -> (%376:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.View.0">(%374:tensor<[1, 32], Int64, CPU>) -> (%374:tensor<[32], Int64, CPU>)
            linalg.CPU.IndexOp <name="model.Index.0">(%314:tensor<[1, 1024, 128], Float32, CPU>) -> (%377:tensor<[1, 32, 128], Float32, CPU>)
            linalg.CPU.ViewOp <name="model.View.1">(%374:tensor<[32], Int64, CPU>) -> (%374:tensor<[32], Int64, CPU>)
            linalg.CPU.IndexOp <name="model.Index.1">(%315:tensor<[1, 1024, 128], Float32, CPU>) -> (%378:tensor<[1, 32, 128], Float32, CPU>)
            graph.CallGraphOp @model.layers.0 (%376:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %318:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %319:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%417:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %392:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %385:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.1 (%417:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %320:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %321:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%456:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %431:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %424:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.2 (%456:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %322:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %323:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%495:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %470:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %463:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.3 (%495:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %324:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %325:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%534:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %509:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %502:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.4 (%534:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %326:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %327:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%573:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %548:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %541:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.5 (%573:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %328:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %329:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%612:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %587:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %580:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.6 (%612:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %330:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %331:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%651:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %626:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %619:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.7 (%651:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %332:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %333:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%690:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %665:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %658:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.8 (%690:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %334:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %335:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%729:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %704:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %697:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.9 (%729:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %336:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %337:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%768:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %743:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %736:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.10 (%768:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %338:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %339:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%807:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %782:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %775:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.11 (%807:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %340:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %341:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%846:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %821:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %814:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.12 (%846:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %342:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %343:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%885:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %860:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %853:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.13 (%885:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %344:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %345:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%924:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %899:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %892:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.14 (%924:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %346:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %347:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%963:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %938:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %931:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.15 (%963:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %348:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %349:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1002:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %977:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %970:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.16 (%1002:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %350:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %351:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1041:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1016:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1009:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.17 (%1041:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %352:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %353:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1080:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1055:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1048:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.18 (%1080:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %354:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %355:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1119:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1094:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1087:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.19 (%1119:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %356:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %357:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1158:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1133:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1126:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.20 (%1158:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %358:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %359:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1197:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1172:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1165:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.21 (%1197:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %360:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %361:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1236:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1211:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1204:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.22 (%1236:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %362:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %363:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1275:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1250:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1243:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.23 (%1275:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %364:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %365:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1314:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1289:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1282:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.24 (%1314:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %366:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %367:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1353:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1328:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1321:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.25 (%1353:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %368:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %369:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1392:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1367:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1360:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.26 (%1392:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %370:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %371:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1431:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1406:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1399:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.27 (%1431:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %372:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %373:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1470:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1445:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1438:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.norm">(%1470:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1471:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1471:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %392:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %431:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %470:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %509:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %548:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %587:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %626:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %665:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %704:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %743:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %782:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %821:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %860:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %899:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %938:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %977:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1016:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1055:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1094:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1133:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1172:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1211:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1250:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1289:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1328:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1367:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1406:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1445:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %385:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %424:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %463:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %502:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %541:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %580:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %619:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %658:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %697:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %736:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %775:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %814:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %853:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %892:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %931:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %970:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1009:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1048:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1087:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1126:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1165:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1204:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1243:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1282:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1321:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1360:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1399:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1438:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.0 <CPU> [using_qnn:true, symbol:model.layers.0] {
        (%376:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %318:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %319:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%417:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %392:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %385:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.0.input_layernorm">(%376:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%379:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.0.self_attn (%379:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %318:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %319:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%409:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %392:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %385:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.0.Add.0">(%409:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %376:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%410:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.0.post_attention_layernorm">(%410:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%411:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.0.mlp (%411:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%416:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.0.Add.1">(%416:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %410:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%417:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%417:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %392:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %385:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.0.self_attn <CPU> [using_qnn:true, symbol:model.layers.0.self_attn] {
        (%379:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %318:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %319:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%409:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %392:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %385:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.0.self_attn.q_proj">(%379:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%380:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.0.self_attn.k_proj">(%379:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%381:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.0.self_attn.v_proj">(%379:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%382:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.0.self_attn.View.0">(%380:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%380:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.0.self_attn.Transpose.0">(%380:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%383:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.0.self_attn.View.1">(%381:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%381:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.0.self_attn.Transpose.1">(%381:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%384:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.0.self_attn.View.2">(%382:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%382:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.0.self_attn.Transpose.2">(%382:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%385:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.0.self_attn.q_norm">(%383:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%386:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.0.self_attn.k_norm">(%384:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%387:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.0.self_attn.q_rope">(%386:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>) -> (%388:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.0.self_attn.k_rope">(%387:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>) -> (%389:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.0.self_attn.CastType.0">(%389:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%390:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.0.self_attn.CastType.1">(%390:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%391:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.0.self_attn.Transpose.3">(%391:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%392:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.0.self_attn.Concat.0">(%318:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %392:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%393:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.0.self_attn.Concat.1">(%319:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %385:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%394:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.0.self_attn.Repeat.0">(%393:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%395:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.0.self_attn.Repeat.1">(%394:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%396:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.0.self_attn.MatMul.0">(%388:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %395:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%397:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.0.self_attn.Mul.0">(%397:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %398:tensor<[1], Int16PerTensor, CPU>[constant: [0]][constant:[0]]) -> (%399:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.0.self_attn.ReduceMin.0">(%399:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%400:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.0.self_attn.Add.0">(%400:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %401:tensor<[1], Int16PerTensor, CPU>[constant: [9.1807e-41]][constant:[9.1807e-41]]) -> (%402:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.0.self_attn.Equal.0">(%317:tensor<[1, 1, 32, 1024], Float32, CPU>, %403:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%404:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.0.self_attn.Argsort.0">(%404:tensor<[1, 1, 32, 1024], UInt8, CPU>, %399:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %402:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%405:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.0.self_attn.Softmax.0">(%405:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%406:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.0.self_attn.MatMul.1">(%406:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %396:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%407:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.0.self_attn.Transpose.4">(%407:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%408:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.0.self_attn.View.3">(%408:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%408:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.0.self_attn.o_proj">(%408:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%409:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%409:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %392:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %385:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.0.mlp <CPU> [using_qnn:true, symbol:model.layers.0.mlp] {
        (%411:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%416:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.0.mlp.gate_proj">(%411:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%412:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.0.mlp.act">(%412:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%413:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.0.mlp.up_proj">(%411:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%414:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.0.mlp.Mul.0">(%413:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %414:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%415:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.0.mlp.down_proj">(%415:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%416:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%416:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.1 <CPU> [using_qnn:true, symbol:model.layers.1] {
        (%417:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %320:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %321:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%456:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %431:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %424:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.1.input_layernorm">(%417:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%418:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.1.self_attn (%418:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %320:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %321:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%448:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %431:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %424:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.1.Add.0">(%448:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %417:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%449:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.1.post_attention_layernorm">(%449:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%450:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.1.mlp (%450:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%455:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.1.Add.1">(%455:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %449:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%456:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%456:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %431:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %424:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.1.self_attn <CPU> [using_qnn:true, symbol:model.layers.1.self_attn] {
        (%418:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %320:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %321:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%448:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %431:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %424:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.1.self_attn.q_proj">(%418:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%419:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.1.self_attn.k_proj">(%418:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%420:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.1.self_attn.v_proj">(%418:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%421:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.1.self_attn.View.0">(%419:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%419:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.1.self_attn.Transpose.0">(%419:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%422:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.1.self_attn.View.1">(%420:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%420:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.1.self_attn.Transpose.1">(%420:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%423:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.1.self_attn.View.2">(%421:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%421:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.1.self_attn.Transpose.2">(%421:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%424:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.1.self_attn.q_norm">(%422:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%425:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.1.self_attn.k_norm">(%423:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%426:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.1.self_attn.q_rope">(%425:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>) -> (%427:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.1.self_attn.k_rope">(%426:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>) -> (%428:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.1.self_attn.CastType.0">(%428:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%429:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.1.self_attn.CastType.1">(%429:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%430:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.1.self_attn.Transpose.3">(%430:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%431:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.1.self_attn.Concat.0">(%320:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %431:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%432:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.1.self_attn.Concat.1">(%321:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %424:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%433:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.1.self_attn.Repeat.0">(%432:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%434:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.1.self_attn.Repeat.1">(%433:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%435:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.1.self_attn.MatMul.0">(%427:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %434:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%436:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.1.self_attn.Mul.0">(%436:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %437:tensor<[1], Int16PerTensor, CPU>[constant: [0]][constant:[0]]) -> (%438:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.1.self_attn.ReduceMin.0">(%438:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%439:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.1.self_attn.Add.0">(%439:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %440:tensor<[1], Int16PerTensor, CPU>[constant: [9.1807e-41]][constant:[9.1807e-41]]) -> (%441:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.1.self_attn.Equal.0">(%317:tensor<[1, 1, 32, 1024], Float32, CPU>, %442:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%443:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.1.self_attn.Argsort.0">(%443:tensor<[1, 1, 32, 1024], UInt8, CPU>, %438:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %441:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%444:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.1.self_attn.Softmax.0">(%444:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%445:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.1.self_attn.MatMul.1">(%445:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %435:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%446:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.1.self_attn.Transpose.4">(%446:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%447:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.1.self_attn.View.3">(%447:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%447:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.1.self_attn.o_proj">(%447:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%448:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%448:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %431:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %424:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.1.mlp <CPU> [using_qnn:true, symbol:model.layers.1.mlp] {
        (%450:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%455:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.1.mlp.gate_proj">(%450:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%451:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.1.mlp.act">(%451:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%452:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.1.mlp.up_proj">(%450:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%453:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.1.mlp.Mul.0">(%452:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %453:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%454:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.1.mlp.down_proj">(%454:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%455:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%455:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.2 <CPU> [using_qnn:true, symbol:model.layers.2] {
        (%456:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %322:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %323:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%495:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %470:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %463:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.2.input_layernorm">(%456:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%457:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.2.self_attn (%457:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %322:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %323:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%487:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %470:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %463:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.2.Add.0">(%487:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %456:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%488:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.2.post_attention_layernorm">(%488:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%489:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.2.mlp (%489:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%494:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.2.Add.1">(%494:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %488:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%495:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%495:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %470:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %463:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.2.self_attn <CPU> [using_qnn:true, symbol:model.layers.2.self_attn] {
        (%457:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %322:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %323:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%487:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %470:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %463:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.2.self_attn.q_proj">(%457:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%458:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.2.self_attn.k_proj">(%457:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%459:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.2.self_attn.v_proj">(%457:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%460:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.2.self_attn.View.0">(%458:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%458:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.2.self_attn.Transpose.0">(%458:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%461:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.2.self_attn.View.1">(%459:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%459:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.2.self_attn.Transpose.1">(%459:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%462:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.2.self_attn.View.2">(%460:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%460:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.2.self_attn.Transpose.2">(%460:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%463:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.2.self_attn.q_norm">(%461:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%464:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.2.self_attn.k_norm">(%462:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%465:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.2.self_attn.q_rope">(%464:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>) -> (%466:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.2.self_attn.k_rope">(%465:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>) -> (%467:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.2.self_attn.CastType.0">(%467:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%468:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.2.self_attn.CastType.1">(%468:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%469:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.2.self_attn.Transpose.3">(%469:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%470:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.2.self_attn.Concat.0">(%322:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %470:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%471:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.2.self_attn.Concat.1">(%323:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %463:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%472:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.2.self_attn.Repeat.0">(%471:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%473:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.2.self_attn.Repeat.1">(%472:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%474:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.2.self_attn.MatMul.0">(%466:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %473:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%475:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.2.self_attn.Mul.0">(%475:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %476:tensor<[1], Int16PerTensor, CPU>[constant: [0]][constant:[0]]) -> (%477:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.2.self_attn.ReduceMin.0">(%477:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%478:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.2.self_attn.Add.0">(%478:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %479:tensor<[1], Int16PerTensor, CPU>[constant: [9.1807e-41]][constant:[9.1807e-41]]) -> (%480:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.2.self_attn.Equal.0">(%317:tensor<[1, 1, 32, 1024], Float32, CPU>, %481:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%482:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.2.self_attn.Argsort.0">(%482:tensor<[1, 1, 32, 1024], UInt8, CPU>, %477:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %480:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%483:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.2.self_attn.Softmax.0">(%483:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%484:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.2.self_attn.MatMul.1">(%484:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %474:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%485:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.2.self_attn.Transpose.4">(%485:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%486:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.2.self_attn.View.3">(%486:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%486:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.2.self_attn.o_proj">(%486:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%487:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%487:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %470:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %463:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.2.mlp <CPU> [using_qnn:true, symbol:model.layers.2.mlp] {
        (%489:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%494:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.2.mlp.gate_proj">(%489:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%490:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.2.mlp.act">(%490:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%491:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.2.mlp.up_proj">(%489:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%492:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.2.mlp.Mul.0">(%491:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %492:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%493:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.2.mlp.down_proj">(%493:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%494:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%494:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.3 <CPU> [using_qnn:true, symbol:model.layers.3] {
        (%495:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %324:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %325:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%534:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %509:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %502:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.3.input_layernorm">(%495:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%496:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.3.self_attn (%496:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %324:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %325:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%526:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %509:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %502:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.3.Add.0">(%526:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %495:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%527:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.3.post_attention_layernorm">(%527:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%528:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.3.mlp (%528:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%533:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.3.Add.1">(%533:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %527:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%534:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%534:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %509:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %502:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.3.self_attn <CPU> [using_qnn:true, symbol:model.layers.3.self_attn] {
        (%496:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %324:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %325:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%526:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %509:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %502:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.3.self_attn.q_proj">(%496:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%497:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.3.self_attn.k_proj">(%496:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%498:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.3.self_attn.v_proj">(%496:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%499:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.3.self_attn.View.0">(%497:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%497:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.3.self_attn.Transpose.0">(%497:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%500:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.3.self_attn.View.1">(%498:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%498:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.3.self_attn.Transpose.1">(%498:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%501:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.3.self_attn.View.2">(%499:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%499:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.3.self_attn.Transpose.2">(%499:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%502:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.3.self_attn.q_norm">(%500:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%503:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.3.self_attn.k_norm">(%501:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%504:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.3.self_attn.q_rope">(%503:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>) -> (%505:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.3.self_attn.k_rope">(%504:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>) -> (%506:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.3.self_attn.CastType.0">(%506:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%507:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.3.self_attn.CastType.1">(%507:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%508:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.3.self_attn.Transpose.3">(%508:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%509:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.3.self_attn.Concat.0">(%324:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %509:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%510:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.3.self_attn.Concat.1">(%325:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %502:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%511:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.3.self_attn.Repeat.0">(%510:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%512:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.3.self_attn.Repeat.1">(%511:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%513:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.3.self_attn.MatMul.0">(%505:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %512:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%514:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.3.self_attn.Mul.0">(%514:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %515:tensor<[1], Int16PerTensor, CPU>[constant: [0]][constant:[0]]) -> (%516:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.3.self_attn.ReduceMin.0">(%516:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%517:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.3.self_attn.Add.0">(%517:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %518:tensor<[1], Int16PerTensor, CPU>[constant: [9.1807e-41]][constant:[9.1807e-41]]) -> (%519:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.3.self_attn.Equal.0">(%317:tensor<[1, 1, 32, 1024], Float32, CPU>, %520:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%521:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.3.self_attn.Argsort.0">(%521:tensor<[1, 1, 32, 1024], UInt8, CPU>, %516:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %519:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%522:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.3.self_attn.Softmax.0">(%522:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%523:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.3.self_attn.MatMul.1">(%523:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %513:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%524:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.3.self_attn.Transpose.4">(%524:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%525:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.3.self_attn.View.3">(%525:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%525:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.3.self_attn.o_proj">(%525:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%526:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%526:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %509:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %502:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.3.mlp <CPU> [using_qnn:true, symbol:model.layers.3.mlp] {
        (%528:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%533:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.3.mlp.gate_proj">(%528:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%529:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.3.mlp.act">(%529:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%530:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.3.mlp.up_proj">(%528:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%531:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.3.mlp.Mul.0">(%530:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %531:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%532:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.3.mlp.down_proj">(%532:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%533:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%533:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.4 <CPU> [using_qnn:true, symbol:model.layers.4] {
        (%534:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %326:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %327:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%573:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %548:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %541:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.4.input_layernorm">(%534:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%535:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.4.self_attn (%535:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %326:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %327:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%565:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %548:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %541:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.4.Add.0">(%565:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %534:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%566:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.4.post_attention_layernorm">(%566:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%567:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.4.mlp (%567:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%572:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.4.Add.1">(%572:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %566:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%573:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%573:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %548:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %541:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.4.self_attn <CPU> [using_qnn:true, symbol:model.layers.4.self_attn] {
        (%535:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %326:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %327:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%565:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %548:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %541:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.4.self_attn.q_proj">(%535:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%536:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.4.self_attn.k_proj">(%535:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%537:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.4.self_attn.v_proj">(%535:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%538:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.4.self_attn.View.0">(%536:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%536:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.4.self_attn.Transpose.0">(%536:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%539:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.4.self_attn.View.1">(%537:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%537:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.4.self_attn.Transpose.1">(%537:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%540:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.4.self_attn.View.2">(%538:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%538:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.4.self_attn.Transpose.2">(%538:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%541:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.4.self_attn.q_norm">(%539:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%542:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.4.self_attn.k_norm">(%540:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%543:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.4.self_attn.q_rope">(%542:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>) -> (%544:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.4.self_attn.k_rope">(%543:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>) -> (%545:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.4.self_attn.CastType.0">(%545:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%546:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.4.self_attn.CastType.1">(%546:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%547:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.4.self_attn.Transpose.3">(%547:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%548:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.4.self_attn.Concat.0">(%326:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %548:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%549:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.4.self_attn.Concat.1">(%327:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %541:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%550:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.4.self_attn.Repeat.0">(%549:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%551:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.4.self_attn.Repeat.1">(%550:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%552:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.4.self_attn.MatMul.0">(%544:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %551:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%553:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.4.self_attn.Mul.0">(%553:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %554:tensor<[1], Int16PerTensor, CPU>[constant: [0]][constant:[0]]) -> (%555:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.4.self_attn.ReduceMin.0">(%555:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%556:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.4.self_attn.Add.0">(%556:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %557:tensor<[1], Int16PerTensor, CPU>[constant: [9.1807e-41]][constant:[9.1807e-41]]) -> (%558:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.4.self_attn.Equal.0">(%317:tensor<[1, 1, 32, 1024], Float32, CPU>, %559:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%560:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.4.self_attn.Argsort.0">(%560:tensor<[1, 1, 32, 1024], UInt8, CPU>, %555:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %558:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%561:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.4.self_attn.Softmax.0">(%561:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%562:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.4.self_attn.MatMul.1">(%562:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %552:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%563:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.4.self_attn.Transpose.4">(%563:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%564:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.4.self_attn.View.3">(%564:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%564:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.4.self_attn.o_proj">(%564:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%565:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%565:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %548:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %541:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.4.mlp <CPU> [using_qnn:true, symbol:model.layers.4.mlp] {
        (%567:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%572:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.4.mlp.gate_proj">(%567:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%568:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.4.mlp.act">(%568:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%569:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.4.mlp.up_proj">(%567:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%570:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.4.mlp.Mul.0">(%569:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %570:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%571:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.4.mlp.down_proj">(%571:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%572:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%572:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.5 <CPU> [using_qnn:true, symbol:model.layers.5] {
        (%573:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %328:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %329:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%612:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %587:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %580:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.5.input_layernorm">(%573:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%574:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.5.self_attn (%574:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %328:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %329:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%604:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %587:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %580:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.5.Add.0">(%604:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %573:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%605:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.5.post_attention_layernorm">(%605:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%606:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.5.mlp (%606:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%611:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.5.Add.1">(%611:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %605:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%612:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%612:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %587:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %580:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.5.self_attn <CPU> [using_qnn:true, symbol:model.layers.5.self_attn] {
        (%574:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %328:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %329:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%604:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %587:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %580:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.5.self_attn.q_proj">(%574:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%575:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.5.self_attn.k_proj">(%574:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%576:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.5.self_attn.v_proj">(%574:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%577:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.5.self_attn.View.0">(%575:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%575:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.5.self_attn.Transpose.0">(%575:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%578:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.5.self_attn.View.1">(%576:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%576:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.5.self_attn.Transpose.1">(%576:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%579:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.5.self_attn.View.2">(%577:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%577:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.5.self_attn.Transpose.2">(%577:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%580:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.5.self_attn.q_norm">(%578:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%581:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.5.self_attn.k_norm">(%579:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%582:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.5.self_attn.q_rope">(%581:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>) -> (%583:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.5.self_attn.k_rope">(%582:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>) -> (%584:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.5.self_attn.CastType.0">(%584:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%585:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.5.self_attn.CastType.1">(%585:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%586:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.5.self_attn.Transpose.3">(%586:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%587:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.5.self_attn.Concat.0">(%328:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %587:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%588:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.5.self_attn.Concat.1">(%329:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %580:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%589:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.5.self_attn.Repeat.0">(%588:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%590:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.5.self_attn.Repeat.1">(%589:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%591:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.5.self_attn.MatMul.0">(%583:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %590:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%592:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.5.self_attn.Mul.0">(%592:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %593:tensor<[1], Int16PerTensor, CPU>[constant: [0]][constant:[0]]) -> (%594:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.5.self_attn.ReduceMin.0">(%594:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%595:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.5.self_attn.Add.0">(%595:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %596:tensor<[1], Int16PerTensor, CPU>[constant: [9.1807e-41]][constant:[9.1807e-41]]) -> (%597:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.5.self_attn.Equal.0">(%317:tensor<[1, 1, 32, 1024], Float32, CPU>, %598:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%599:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.5.self_attn.Argsort.0">(%599:tensor<[1, 1, 32, 1024], UInt8, CPU>, %594:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %597:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%600:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.5.self_attn.Softmax.0">(%600:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%601:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.5.self_attn.MatMul.1">(%601:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %591:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%602:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.5.self_attn.Transpose.4">(%602:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%603:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.5.self_attn.View.3">(%603:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%603:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.5.self_attn.o_proj">(%603:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%604:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%604:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %587:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %580:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.5.mlp <CPU> [using_qnn:true, symbol:model.layers.5.mlp] {
        (%606:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%611:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.5.mlp.gate_proj">(%606:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%607:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.5.mlp.act">(%607:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%608:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.5.mlp.up_proj">(%606:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%609:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.5.mlp.Mul.0">(%608:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %609:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%610:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.5.mlp.down_proj">(%610:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%611:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%611:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.6 <CPU> [using_qnn:true, symbol:model.layers.6] {
        (%612:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %330:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %331:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%651:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %626:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %619:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.6.input_layernorm">(%612:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%613:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.6.self_attn (%613:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %330:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %331:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%643:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %626:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %619:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.6.Add.0">(%643:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %612:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%644:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.6.post_attention_layernorm">(%644:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%645:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.6.mlp (%645:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%650:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.6.Add.1">(%650:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %644:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%651:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%651:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %626:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %619:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.6.self_attn <CPU> [using_qnn:true, symbol:model.layers.6.self_attn] {
        (%613:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %330:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %331:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%643:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %626:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %619:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.6.self_attn.q_proj">(%613:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%614:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.6.self_attn.k_proj">(%613:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%615:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.6.self_attn.v_proj">(%613:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%616:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.6.self_attn.View.0">(%614:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%614:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.6.self_attn.Transpose.0">(%614:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%617:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.6.self_attn.View.1">(%615:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%615:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.6.self_attn.Transpose.1">(%615:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%618:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.6.self_attn.View.2">(%616:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%616:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.6.self_attn.Transpose.2">(%616:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%619:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.6.self_attn.q_norm">(%617:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%620:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.6.self_attn.k_norm">(%618:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%621:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.6.self_attn.q_rope">(%620:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>) -> (%622:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.6.self_attn.k_rope">(%621:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>) -> (%623:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.6.self_attn.CastType.0">(%623:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%624:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.6.self_attn.CastType.1">(%624:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%625:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.6.self_attn.Transpose.3">(%625:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%626:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.6.self_attn.Concat.0">(%330:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %626:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%627:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.6.self_attn.Concat.1">(%331:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %619:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%628:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.6.self_attn.Repeat.0">(%627:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%629:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.6.self_attn.Repeat.1">(%628:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%630:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.6.self_attn.MatMul.0">(%622:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %629:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%631:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.6.self_attn.Mul.0">(%631:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %632:tensor<[1], Int16PerTensor, CPU>[constant: [0]][constant:[0]]) -> (%633:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.6.self_attn.ReduceMin.0">(%633:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%634:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.6.self_attn.Add.0">(%634:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %635:tensor<[1], Int16PerTensor, CPU>[constant: [9.1807e-41]][constant:[9.1807e-41]]) -> (%636:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.6.self_attn.Equal.0">(%317:tensor<[1, 1, 32, 1024], Float32, CPU>, %637:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%638:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.6.self_attn.Argsort.0">(%638:tensor<[1, 1, 32, 1024], UInt8, CPU>, %633:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %636:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%639:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.6.self_attn.Softmax.0">(%639:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%640:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.6.self_attn.MatMul.1">(%640:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %630:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%641:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.6.self_attn.Transpose.4">(%641:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%642:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.6.self_attn.View.3">(%642:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%642:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.6.self_attn.o_proj">(%642:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%643:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%643:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %626:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %619:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.6.mlp <CPU> [using_qnn:true, symbol:model.layers.6.mlp] {
        (%645:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%650:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.6.mlp.gate_proj">(%645:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%646:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.6.mlp.act">(%646:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%647:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.6.mlp.up_proj">(%645:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%648:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.6.mlp.Mul.0">(%647:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %648:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%649:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.6.mlp.down_proj">(%649:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%650:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%650:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.7 <CPU> [using_qnn:true, symbol:model.layers.7] {
        (%651:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %332:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %333:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%690:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %665:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %658:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.7.input_layernorm">(%651:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%652:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.7.self_attn (%652:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %332:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %333:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%682:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %665:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %658:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.7.Add.0">(%682:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %651:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%683:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.7.post_attention_layernorm">(%683:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%684:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.7.mlp (%684:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%689:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.7.Add.1">(%689:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %683:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%690:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%690:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %665:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %658:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.7.self_attn <CPU> [using_qnn:true, symbol:model.layers.7.self_attn] {
        (%652:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %332:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %333:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%682:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %665:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %658:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.7.self_attn.q_proj">(%652:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%653:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.7.self_attn.k_proj">(%652:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%654:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.7.self_attn.v_proj">(%652:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%655:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.7.self_attn.View.0">(%653:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%653:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.7.self_attn.Transpose.0">(%653:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%656:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.7.self_attn.View.1">(%654:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%654:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.7.self_attn.Transpose.1">(%654:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%657:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.7.self_attn.View.2">(%655:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%655:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.7.self_attn.Transpose.2">(%655:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%658:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.7.self_attn.q_norm">(%656:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%659:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.7.self_attn.k_norm">(%657:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%660:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.7.self_attn.q_rope">(%659:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>) -> (%661:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.7.self_attn.k_rope">(%660:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>) -> (%662:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.7.self_attn.CastType.0">(%662:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%663:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.7.self_attn.CastType.1">(%663:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%664:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.7.self_attn.Transpose.3">(%664:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%665:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.7.self_attn.Concat.0">(%332:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %665:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%666:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.7.self_attn.Concat.1">(%333:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %658:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%667:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.7.self_attn.Repeat.0">(%666:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%668:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.7.self_attn.Repeat.1">(%667:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%669:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.7.self_attn.MatMul.0">(%661:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %668:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%670:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.7.self_attn.Mul.0">(%670:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %671:tensor<[1], Int16PerTensor, CPU>[constant: [0]][constant:[0]]) -> (%672:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.7.self_attn.ReduceMin.0">(%672:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%673:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.7.self_attn.Add.0">(%673:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %674:tensor<[1], Int16PerTensor, CPU>[constant: [1.0078101]][constant:[1.0078101]]) -> (%675:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.7.self_attn.Equal.0">(%317:tensor<[1, 1, 32, 1024], Float32, CPU>, %676:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%677:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.7.self_attn.Argsort.0">(%677:tensor<[1, 1, 32, 1024], UInt8, CPU>, %672:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %675:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%678:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.7.self_attn.Softmax.0">(%678:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%679:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.7.self_attn.MatMul.1">(%679:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %669:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%680:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.7.self_attn.Transpose.4">(%680:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%681:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.7.self_attn.View.3">(%681:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%681:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.7.self_attn.o_proj">(%681:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%682:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%682:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %665:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %658:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.7.mlp <CPU> [using_qnn:true, symbol:model.layers.7.mlp] {
        (%684:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%689:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.7.mlp.gate_proj">(%684:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%685:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.7.mlp.act">(%685:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%686:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.7.mlp.up_proj">(%684:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%687:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.7.mlp.Mul.0">(%686:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %687:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%688:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.7.mlp.down_proj">(%688:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%689:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%689:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.8 <CPU> [using_qnn:true, symbol:model.layers.8] {
        (%690:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %334:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %335:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%729:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %704:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %697:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.8.input_layernorm">(%690:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%691:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.8.self_attn (%691:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %334:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %335:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%721:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %704:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %697:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.8.Add.0">(%721:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %690:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%722:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.8.post_attention_layernorm">(%722:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%723:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.8.mlp (%723:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%728:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.8.Add.1">(%728:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %722:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%729:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%729:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %704:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %697:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.8.self_attn <CPU> [using_qnn:true, symbol:model.layers.8.self_attn] {
        (%691:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %334:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %335:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%721:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %704:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %697:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.8.self_attn.q_proj">(%691:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%692:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.8.self_attn.k_proj">(%691:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%693:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.8.self_attn.v_proj">(%691:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%694:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.8.self_attn.View.0">(%692:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%692:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.8.self_attn.Transpose.0">(%692:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%695:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.8.self_attn.View.1">(%693:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%693:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.8.self_attn.Transpose.1">(%693:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%696:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.8.self_attn.View.2">(%694:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%694:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.8.self_attn.Transpose.2">(%694:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%697:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.8.self_attn.q_norm">(%695:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%698:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.8.self_attn.k_norm">(%696:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%699:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.8.self_attn.q_rope">(%698:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>) -> (%700:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.8.self_attn.k_rope">(%699:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>) -> (%701:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.8.self_attn.CastType.0">(%701:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%702:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.8.self_attn.CastType.1">(%702:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%703:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.8.self_attn.Transpose.3">(%703:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%704:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.8.self_attn.Concat.0">(%334:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %704:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%705:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.8.self_attn.Concat.1">(%335:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %697:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%706:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.8.self_attn.Repeat.0">(%705:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%707:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.8.self_attn.Repeat.1">(%706:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%708:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.8.self_attn.MatMul.0">(%700:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %707:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%709:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.8.self_attn.Mul.0">(%709:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %710:tensor<[1], Int16PerTensor, CPU>[constant: [0]][constant:[0]]) -> (%711:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.8.self_attn.ReduceMin.0">(%711:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%712:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.8.self_attn.Add.0">(%712:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %713:tensor<[1], Int16PerTensor, CPU>[constant: [9.1807e-41]][constant:[9.1807e-41]]) -> (%714:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.8.self_attn.Equal.0">(%317:tensor<[1, 1, 32, 1024], Float32, CPU>, %715:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%716:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.8.self_attn.Argsort.0">(%716:tensor<[1, 1, 32, 1024], UInt8, CPU>, %711:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %714:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%717:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.8.self_attn.Softmax.0">(%717:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%718:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.8.self_attn.MatMul.1">(%718:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %708:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%719:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.8.self_attn.Transpose.4">(%719:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%720:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.8.self_attn.View.3">(%720:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%720:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.8.self_attn.o_proj">(%720:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%721:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%721:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %704:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %697:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.8.mlp <CPU> [using_qnn:true, symbol:model.layers.8.mlp] {
        (%723:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%728:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.8.mlp.gate_proj">(%723:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%724:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.8.mlp.act">(%724:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%725:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.8.mlp.up_proj">(%723:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%726:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.8.mlp.Mul.0">(%725:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %726:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%727:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.8.mlp.down_proj">(%727:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%728:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%728:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.9 <CPU> [using_qnn:true, symbol:model.layers.9] {
        (%729:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %336:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %337:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%768:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %743:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %736:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.9.input_layernorm">(%729:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%730:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.9.self_attn (%730:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %336:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %337:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%760:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %743:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %736:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.9.Add.0">(%760:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %729:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%761:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.9.post_attention_layernorm">(%761:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%762:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.9.mlp (%762:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%767:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.9.Add.1">(%767:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %761:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%768:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%768:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %743:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %736:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.9.self_attn <CPU> [using_qnn:true, symbol:model.layers.9.self_attn] {
        (%730:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %336:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %337:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%760:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %743:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %736:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.9.self_attn.q_proj">(%730:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%731:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.9.self_attn.k_proj">(%730:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%732:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.9.self_attn.v_proj">(%730:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%733:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.9.self_attn.View.0">(%731:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%731:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.9.self_attn.Transpose.0">(%731:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%734:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.9.self_attn.View.1">(%732:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%732:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.9.self_attn.Transpose.1">(%732:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%735:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.9.self_attn.View.2">(%733:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%733:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.9.self_attn.Transpose.2">(%733:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%736:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.9.self_attn.q_norm">(%734:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%737:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.9.self_attn.k_norm">(%735:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%738:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.9.self_attn.q_rope">(%737:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>) -> (%739:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.9.self_attn.k_rope">(%738:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>) -> (%740:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.9.self_attn.CastType.0">(%740:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%741:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.9.self_attn.CastType.1">(%741:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%742:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.9.self_attn.Transpose.3">(%742:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%743:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.9.self_attn.Concat.0">(%336:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %743:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%744:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.9.self_attn.Concat.1">(%337:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %736:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%745:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.9.self_attn.Repeat.0">(%744:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%746:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.9.self_attn.Repeat.1">(%745:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%747:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.9.self_attn.MatMul.0">(%739:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %746:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%748:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.9.self_attn.Mul.0">(%748:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %749:tensor<[1], Int16PerTensor, CPU>[constant: [0]][constant:[0]]) -> (%750:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.9.self_attn.ReduceMin.0">(%750:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%751:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.9.self_attn.Add.0">(%751:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %752:tensor<[1], Int16PerTensor, CPU>[constant: [9.1807e-41]][constant:[9.1807e-41]]) -> (%753:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.9.self_attn.Equal.0">(%317:tensor<[1, 1, 32, 1024], Float32, CPU>, %754:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%755:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.9.self_attn.Argsort.0">(%755:tensor<[1, 1, 32, 1024], UInt8, CPU>, %750:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %753:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%756:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.9.self_attn.Softmax.0">(%756:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%757:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.9.self_attn.MatMul.1">(%757:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %747:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%758:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.9.self_attn.Transpose.4">(%758:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%759:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.9.self_attn.View.3">(%759:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%759:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.9.self_attn.o_proj">(%759:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%760:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%760:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %743:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %736:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.9.mlp <CPU> [using_qnn:true, symbol:model.layers.9.mlp] {
        (%762:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%767:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.9.mlp.gate_proj">(%762:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%763:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.9.mlp.act">(%763:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%764:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.9.mlp.up_proj">(%762:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%765:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.9.mlp.Mul.0">(%764:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %765:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%766:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.9.mlp.down_proj">(%766:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%767:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%767:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.10 <CPU> [using_qnn:true, symbol:model.layers.10] {
        (%768:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %338:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %339:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%807:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %782:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %775:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.10.input_layernorm">(%768:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%769:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.10.self_attn (%769:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %338:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %339:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%799:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %782:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %775:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.10.Add.0">(%799:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %768:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%800:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.10.post_attention_layernorm">(%800:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%801:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.10.mlp (%801:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%806:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.10.Add.1">(%806:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %800:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%807:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%807:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %782:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %775:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.10.self_attn <CPU> [using_qnn:true, symbol:model.layers.10.self_attn] {
        (%769:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %338:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %339:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%799:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %782:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %775:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.10.self_attn.q_proj">(%769:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%770:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.10.self_attn.k_proj">(%769:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%771:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.10.self_attn.v_proj">(%769:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%772:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.10.self_attn.View.0">(%770:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%770:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.10.self_attn.Transpose.0">(%770:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%773:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.10.self_attn.View.1">(%771:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%771:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.10.self_attn.Transpose.1">(%771:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%774:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.10.self_attn.View.2">(%772:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%772:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.10.self_attn.Transpose.2">(%772:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%775:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.10.self_attn.q_norm">(%773:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%776:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.10.self_attn.k_norm">(%774:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%777:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.10.self_attn.q_rope">(%776:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>) -> (%778:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.10.self_attn.k_rope">(%777:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>) -> (%779:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.10.self_attn.CastType.0">(%779:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%780:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.10.self_attn.CastType.1">(%780:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%781:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.10.self_attn.Transpose.3">(%781:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%782:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.10.self_attn.Concat.0">(%338:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %782:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%783:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.10.self_attn.Concat.1">(%339:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %775:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%784:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.10.self_attn.Repeat.0">(%783:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%785:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.10.self_attn.Repeat.1">(%784:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%786:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.10.self_attn.MatMul.0">(%778:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %785:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%787:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.10.self_attn.Mul.0">(%787:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %788:tensor<[1], Int16PerTensor, CPU>[constant: [0]][constant:[0]]) -> (%789:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.10.self_attn.ReduceMin.0">(%789:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%790:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.10.self_attn.Add.0">(%790:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %791:tensor<[1], Int16PerTensor, CPU>[constant: [9.1807e-41]][constant:[9.1807e-41]]) -> (%792:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.10.self_attn.Equal.0">(%317:tensor<[1, 1, 32, 1024], Float32, CPU>, %793:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%794:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.10.self_attn.Argsort.0">(%794:tensor<[1, 1, 32, 1024], UInt8, CPU>, %789:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %792:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%795:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.10.self_attn.Softmax.0">(%795:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%796:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.10.self_attn.MatMul.1">(%796:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %786:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%797:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.10.self_attn.Transpose.4">(%797:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%798:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.10.self_attn.View.3">(%798:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%798:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.10.self_attn.o_proj">(%798:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%799:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%799:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %782:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %775:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.10.mlp <CPU> [using_qnn:true, symbol:model.layers.10.mlp] {
        (%801:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%806:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.10.mlp.gate_proj">(%801:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%802:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.10.mlp.act">(%802:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%803:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.10.mlp.up_proj">(%801:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%804:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.10.mlp.Mul.0">(%803:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %804:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%805:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.10.mlp.down_proj">(%805:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%806:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%806:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.11 <CPU> [using_qnn:true, symbol:model.layers.11] {
        (%807:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %340:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %341:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%846:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %821:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %814:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.11.input_layernorm">(%807:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%808:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.11.self_attn (%808:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %340:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %341:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%838:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %821:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %814:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.11.Add.0">(%838:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %807:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%839:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.11.post_attention_layernorm">(%839:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%840:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.11.mlp (%840:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%845:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.11.Add.1">(%845:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %839:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%846:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%846:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %821:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %814:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.11.self_attn <CPU> [using_qnn:true, symbol:model.layers.11.self_attn] {
        (%808:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %340:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %341:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%838:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %821:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %814:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.11.self_attn.q_proj">(%808:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%809:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.11.self_attn.k_proj">(%808:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%810:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.11.self_attn.v_proj">(%808:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%811:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.11.self_attn.View.0">(%809:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%809:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.11.self_attn.Transpose.0">(%809:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%812:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.11.self_attn.View.1">(%810:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%810:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.11.self_attn.Transpose.1">(%810:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%813:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.11.self_attn.View.2">(%811:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%811:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.11.self_attn.Transpose.2">(%811:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%814:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.11.self_attn.q_norm">(%812:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%815:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.11.self_attn.k_norm">(%813:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%816:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.11.self_attn.q_rope">(%815:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>) -> (%817:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.11.self_attn.k_rope">(%816:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>) -> (%818:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.11.self_attn.CastType.0">(%818:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%819:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.11.self_attn.CastType.1">(%819:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%820:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.11.self_attn.Transpose.3">(%820:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%821:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.11.self_attn.Concat.0">(%340:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %821:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%822:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.11.self_attn.Concat.1">(%341:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %814:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%823:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.11.self_attn.Repeat.0">(%822:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%824:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.11.self_attn.Repeat.1">(%823:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%825:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.11.self_attn.MatMul.0">(%817:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %824:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%826:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.11.self_attn.Mul.0">(%826:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %827:tensor<[1], Int16PerTensor, CPU>[constant: [0]][constant:[0]]) -> (%828:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.11.self_attn.ReduceMin.0">(%828:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%829:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.11.self_attn.Add.0">(%829:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %830:tensor<[1], Int16PerTensor, CPU>[constant: [9.1807e-41]][constant:[9.1807e-41]]) -> (%831:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.11.self_attn.Equal.0">(%317:tensor<[1, 1, 32, 1024], Float32, CPU>, %832:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%833:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.11.self_attn.Argsort.0">(%833:tensor<[1, 1, 32, 1024], UInt8, CPU>, %828:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %831:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%834:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.11.self_attn.Softmax.0">(%834:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%835:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.11.self_attn.MatMul.1">(%835:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %825:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%836:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.11.self_attn.Transpose.4">(%836:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%837:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.11.self_attn.View.3">(%837:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%837:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.11.self_attn.o_proj">(%837:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%838:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%838:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %821:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %814:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.11.mlp <CPU> [using_qnn:true, symbol:model.layers.11.mlp] {
        (%840:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%845:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.11.mlp.gate_proj">(%840:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%841:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.11.mlp.act">(%841:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%842:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.11.mlp.up_proj">(%840:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%843:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.11.mlp.Mul.0">(%842:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %843:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%844:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.11.mlp.down_proj">(%844:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%845:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%845:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.12 <CPU> [using_qnn:true, symbol:model.layers.12] {
        (%846:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %342:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %343:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%885:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %860:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %853:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.12.input_layernorm">(%846:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%847:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.12.self_attn (%847:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %342:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %343:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%877:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %860:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %853:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.12.Add.0">(%877:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %846:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%878:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.12.post_attention_layernorm">(%878:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%879:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.12.mlp (%879:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%884:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.12.Add.1">(%884:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %878:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%885:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%885:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %860:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %853:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.12.self_attn <CPU> [using_qnn:true, symbol:model.layers.12.self_attn] {
        (%847:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %342:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %343:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%877:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %860:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %853:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.12.self_attn.q_proj">(%847:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%848:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.12.self_attn.k_proj">(%847:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%849:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.12.self_attn.v_proj">(%847:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%850:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.12.self_attn.View.0">(%848:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%848:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.12.self_attn.Transpose.0">(%848:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%851:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.12.self_attn.View.1">(%849:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%849:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.12.self_attn.Transpose.1">(%849:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%852:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.12.self_attn.View.2">(%850:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%850:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.12.self_attn.Transpose.2">(%850:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%853:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.12.self_attn.q_norm">(%851:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%854:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.12.self_attn.k_norm">(%852:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%855:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.12.self_attn.q_rope">(%854:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>) -> (%856:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.12.self_attn.k_rope">(%855:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>) -> (%857:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.12.self_attn.CastType.0">(%857:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%858:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.12.self_attn.CastType.1">(%858:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%859:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.12.self_attn.Transpose.3">(%859:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%860:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.12.self_attn.Concat.0">(%342:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %860:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%861:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.12.self_attn.Concat.1">(%343:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %853:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%862:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.12.self_attn.Repeat.0">(%861:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%863:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.12.self_attn.Repeat.1">(%862:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%864:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.12.self_attn.MatMul.0">(%856:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %863:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%865:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.12.self_attn.Mul.0">(%865:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %866:tensor<[1], Int16PerTensor, CPU>[constant: [0]][constant:[0]]) -> (%867:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.12.self_attn.ReduceMin.0">(%867:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%868:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.12.self_attn.Add.0">(%868:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %869:tensor<[1], Int16PerTensor, CPU>[constant: [9.1807e-41]][constant:[9.1807e-41]]) -> (%870:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.12.self_attn.Equal.0">(%317:tensor<[1, 1, 32, 1024], Float32, CPU>, %871:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%872:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.12.self_attn.Argsort.0">(%872:tensor<[1, 1, 32, 1024], UInt8, CPU>, %867:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %870:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%873:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.12.self_attn.Softmax.0">(%873:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%874:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.12.self_attn.MatMul.1">(%874:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %864:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%875:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.12.self_attn.Transpose.4">(%875:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%876:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.12.self_attn.View.3">(%876:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%876:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.12.self_attn.o_proj">(%876:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%877:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%877:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %860:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %853:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.12.mlp <CPU> [using_qnn:true, symbol:model.layers.12.mlp] {
        (%879:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%884:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.12.mlp.gate_proj">(%879:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%880:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.12.mlp.act">(%880:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%881:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.12.mlp.up_proj">(%879:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%882:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.12.mlp.Mul.0">(%881:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %882:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%883:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.12.mlp.down_proj">(%883:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%884:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%884:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.13 <CPU> [using_qnn:true, symbol:model.layers.13] {
        (%885:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %344:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %345:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%924:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %899:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %892:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.13.input_layernorm">(%885:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%886:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.13.self_attn (%886:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %344:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %345:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%916:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %899:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %892:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.13.Add.0">(%916:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %885:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%917:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.13.post_attention_layernorm">(%917:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%918:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.13.mlp (%918:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%923:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.13.Add.1">(%923:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %917:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%924:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%924:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %899:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %892:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.13.self_attn <CPU> [using_qnn:true, symbol:model.layers.13.self_attn] {
        (%886:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %344:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %345:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%916:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %899:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %892:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.13.self_attn.q_proj">(%886:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%887:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.13.self_attn.k_proj">(%886:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%888:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.13.self_attn.v_proj">(%886:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%889:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.13.self_attn.View.0">(%887:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%887:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.13.self_attn.Transpose.0">(%887:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%890:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.13.self_attn.View.1">(%888:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%888:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.13.self_attn.Transpose.1">(%888:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%891:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.13.self_attn.View.2">(%889:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%889:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.13.self_attn.Transpose.2">(%889:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%892:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.13.self_attn.q_norm">(%890:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%893:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.13.self_attn.k_norm">(%891:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%894:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.13.self_attn.q_rope">(%893:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>) -> (%895:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.13.self_attn.k_rope">(%894:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>) -> (%896:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.13.self_attn.CastType.0">(%896:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%897:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.13.self_attn.CastType.1">(%897:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%898:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.13.self_attn.Transpose.3">(%898:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%899:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.13.self_attn.Concat.0">(%344:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %899:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%900:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.13.self_attn.Concat.1">(%345:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %892:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%901:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.13.self_attn.Repeat.0">(%900:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%902:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.13.self_attn.Repeat.1">(%901:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%903:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.13.self_attn.MatMul.0">(%895:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %902:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%904:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.13.self_attn.Mul.0">(%904:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %905:tensor<[1], Int16PerTensor, CPU>[constant: [64]][constant:[64]]) -> (%906:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.13.self_attn.ReduceMin.0">(%906:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%907:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.13.self_attn.Add.0">(%907:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %908:tensor<[1], Int16PerTensor, CPU>[constant: [128.9997]][constant:[128.9997]]) -> (%909:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.13.self_attn.Equal.0">(%317:tensor<[1, 1, 32, 1024], Float32, CPU>, %910:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%911:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.13.self_attn.Argsort.0">(%911:tensor<[1, 1, 32, 1024], UInt8, CPU>, %906:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %909:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%912:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.13.self_attn.Softmax.0">(%912:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%913:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.13.self_attn.MatMul.1">(%913:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %903:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%914:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.13.self_attn.Transpose.4">(%914:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%915:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.13.self_attn.View.3">(%915:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%915:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.13.self_attn.o_proj">(%915:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%916:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%916:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %899:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %892:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.13.mlp <CPU> [using_qnn:true, symbol:model.layers.13.mlp] {
        (%918:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%923:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.13.mlp.gate_proj">(%918:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%919:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.13.mlp.act">(%919:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%920:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.13.mlp.up_proj">(%918:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%921:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.13.mlp.Mul.0">(%920:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %921:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%922:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.13.mlp.down_proj">(%922:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%923:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%923:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.14 <CPU> [using_qnn:true, symbol:model.layers.14] {
        (%924:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %346:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %347:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%963:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %938:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %931:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.14.input_layernorm">(%924:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%925:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.14.self_attn (%925:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %346:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %347:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%955:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %938:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %931:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.14.Add.0">(%955:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %924:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%956:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.14.post_attention_layernorm">(%956:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%957:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.14.mlp (%957:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%962:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.14.Add.1">(%962:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %956:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%963:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%963:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %938:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %931:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.14.self_attn <CPU> [using_qnn:true, symbol:model.layers.14.self_attn] {
        (%925:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %346:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %347:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%955:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %938:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %931:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.14.self_attn.q_proj">(%925:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%926:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.14.self_attn.k_proj">(%925:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%927:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.14.self_attn.v_proj">(%925:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%928:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.14.self_attn.View.0">(%926:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%926:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.14.self_attn.Transpose.0">(%926:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%929:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.14.self_attn.View.1">(%927:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%927:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.14.self_attn.Transpose.1">(%927:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%930:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.14.self_attn.View.2">(%928:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%928:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.14.self_attn.Transpose.2">(%928:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%931:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.14.self_attn.q_norm">(%929:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%932:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.14.self_attn.k_norm">(%930:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%933:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.14.self_attn.q_rope">(%932:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>) -> (%934:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.14.self_attn.k_rope">(%933:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>) -> (%935:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.14.self_attn.CastType.0">(%935:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%936:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.14.self_attn.CastType.1">(%936:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%937:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.14.self_attn.Transpose.3">(%937:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%938:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.14.self_attn.Concat.0">(%346:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %938:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%939:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.14.self_attn.Concat.1">(%347:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %931:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%940:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.14.self_attn.Repeat.0">(%939:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%941:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.14.self_attn.Repeat.1">(%940:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%942:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.14.self_attn.MatMul.0">(%934:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %941:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%943:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.14.self_attn.Mul.0">(%943:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %944:tensor<[1], Int16PerTensor, CPU>[constant: [256]][constant:[256]]) -> (%945:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.14.self_attn.ReduceMin.0">(%945:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%946:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.14.self_attn.Add.0">(%946:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %947:tensor<[1], Int16PerTensor, CPU>[constant: [321.9994]][constant:[321.9994]]) -> (%948:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.14.self_attn.Equal.0">(%317:tensor<[1, 1, 32, 1024], Float32, CPU>, %949:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%950:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.14.self_attn.Argsort.0">(%950:tensor<[1, 1, 32, 1024], UInt8, CPU>, %945:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %948:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%951:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.14.self_attn.Softmax.0">(%951:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%952:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.14.self_attn.MatMul.1">(%952:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %942:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%953:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.14.self_attn.Transpose.4">(%953:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%954:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.14.self_attn.View.3">(%954:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%954:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.14.self_attn.o_proj">(%954:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%955:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%955:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %938:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %931:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.14.mlp <CPU> [using_qnn:true, symbol:model.layers.14.mlp] {
        (%957:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%962:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.14.mlp.gate_proj">(%957:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%958:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.14.mlp.act">(%958:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%959:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.14.mlp.up_proj">(%957:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%960:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.14.mlp.Mul.0">(%959:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %960:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%961:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.14.mlp.down_proj">(%961:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%962:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%962:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.15 <CPU> [using_qnn:true, symbol:model.layers.15] {
        (%963:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %348:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %349:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1002:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %977:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %970:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.15.input_layernorm">(%963:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%964:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.15.self_attn (%964:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %348:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %349:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%994:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %977:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %970:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.15.Add.0">(%994:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %963:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%995:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.15.post_attention_layernorm">(%995:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%996:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.15.mlp (%996:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1001:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.15.Add.1">(%1001:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %995:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1002:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1002:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %977:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %970:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.15.self_attn <CPU> [using_qnn:true, symbol:model.layers.15.self_attn] {
        (%964:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %348:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %349:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%994:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %977:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %970:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.15.self_attn.q_proj">(%964:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%965:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.15.self_attn.k_proj">(%964:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%966:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.15.self_attn.v_proj">(%964:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%967:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.15.self_attn.View.0">(%965:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%965:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.15.self_attn.Transpose.0">(%965:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%968:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.15.self_attn.View.1">(%966:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%966:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.15.self_attn.Transpose.1">(%966:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%969:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.15.self_attn.View.2">(%967:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%967:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.15.self_attn.Transpose.2">(%967:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%970:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.15.self_attn.q_norm">(%968:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%971:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.15.self_attn.k_norm">(%969:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%972:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.15.self_attn.q_rope">(%971:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>) -> (%973:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.15.self_attn.k_rope">(%972:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>) -> (%974:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.15.self_attn.CastType.0">(%974:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%975:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.15.self_attn.CastType.1">(%975:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%976:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.15.self_attn.Transpose.3">(%976:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%977:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.15.self_attn.Concat.0">(%348:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %977:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%978:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.15.self_attn.Concat.1">(%349:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %970:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%979:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.15.self_attn.Repeat.0">(%978:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%980:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.15.self_attn.Repeat.1">(%979:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%981:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.15.self_attn.MatMul.0">(%973:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %980:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%982:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.15.self_attn.Mul.0">(%982:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %983:tensor<[1], Int16PerTensor, CPU>[constant: [448]][constant:[448]]) -> (%984:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.15.self_attn.ReduceMin.0">(%984:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%985:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.15.self_attn.Add.0">(%985:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %986:tensor<[1], Int16PerTensor, CPU>[constant: [515.9988]][constant:[515.9988]]) -> (%987:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.15.self_attn.Equal.0">(%317:tensor<[1, 1, 32, 1024], Float32, CPU>, %988:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%989:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.15.self_attn.Argsort.0">(%989:tensor<[1, 1, 32, 1024], UInt8, CPU>, %984:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %987:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%990:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.15.self_attn.Softmax.0">(%990:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%991:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.15.self_attn.MatMul.1">(%991:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %981:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%992:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.15.self_attn.Transpose.4">(%992:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%993:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.15.self_attn.View.3">(%993:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%993:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.15.self_attn.o_proj">(%993:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%994:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%994:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %977:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %970:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.15.mlp <CPU> [using_qnn:true, symbol:model.layers.15.mlp] {
        (%996:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1001:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.15.mlp.gate_proj">(%996:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%997:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.15.mlp.act">(%997:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%998:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.15.mlp.up_proj">(%996:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%999:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.15.mlp.Mul.0">(%998:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %999:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1000:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.15.mlp.down_proj">(%1000:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1001:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1001:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.16 <CPU> [using_qnn:true, symbol:model.layers.16] {
        (%1002:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %350:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %351:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1041:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1016:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1009:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.16.input_layernorm">(%1002:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1003:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.16.self_attn (%1003:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %350:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %351:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1033:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1016:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1009:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.16.Add.0">(%1033:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1002:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1034:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.16.post_attention_layernorm">(%1034:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1035:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.16.mlp (%1035:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1040:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.16.Add.1">(%1040:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1034:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1041:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1041:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1016:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1009:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.16.self_attn <CPU> [using_qnn:true, symbol:model.layers.16.self_attn] {
        (%1003:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %350:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %351:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1033:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1016:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1009:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.16.self_attn.q_proj">(%1003:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1004:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.16.self_attn.k_proj">(%1003:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1005:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.16.self_attn.v_proj">(%1003:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1006:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.16.self_attn.View.0">(%1004:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1004:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.16.self_attn.Transpose.0">(%1004:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1007:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.16.self_attn.View.1">(%1005:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1005:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.16.self_attn.Transpose.1">(%1005:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1008:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.16.self_attn.View.2">(%1006:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1006:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.16.self_attn.Transpose.2">(%1006:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1009:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.16.self_attn.q_norm">(%1007:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1010:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.16.self_attn.k_norm">(%1008:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1011:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.16.self_attn.q_rope">(%1010:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>) -> (%1012:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.16.self_attn.k_rope">(%1011:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>) -> (%1013:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.16.self_attn.CastType.0">(%1013:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1014:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.16.self_attn.CastType.1">(%1014:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1015:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.16.self_attn.Transpose.3">(%1015:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1016:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.16.self_attn.Concat.0">(%350:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %1016:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%1017:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.16.self_attn.Concat.1">(%351:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %1009:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1018:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.16.self_attn.Repeat.0">(%1017:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%1019:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.16.self_attn.Repeat.1">(%1018:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%1020:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.16.self_attn.MatMul.0">(%1012:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %1019:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%1021:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.16.self_attn.Mul.0">(%1021:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1022:tensor<[1], Int16PerTensor, CPU>[constant: [640]][constant:[640]]) -> (%1023:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.16.self_attn.ReduceMin.0">(%1023:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1024:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.16.self_attn.Add.0">(%1024:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %1025:tensor<[1], Int16PerTensor, CPU>[constant: [707.9988]][constant:[707.9988]]) -> (%1026:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.16.self_attn.Equal.0">(%317:tensor<[1, 1, 32, 1024], Float32, CPU>, %1027:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%1028:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.16.self_attn.Argsort.0">(%1028:tensor<[1, 1, 32, 1024], UInt8, CPU>, %1023:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1026:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%1029:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.16.self_attn.Softmax.0">(%1029:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1030:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.16.self_attn.MatMul.1">(%1030:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1020:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%1031:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.16.self_attn.Transpose.4">(%1031:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1032:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.16.self_attn.View.3">(%1032:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1032:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.16.self_attn.o_proj">(%1032:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1033:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1033:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1016:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1009:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.16.mlp <CPU> [using_qnn:true, symbol:model.layers.16.mlp] {
        (%1035:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1040:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.16.mlp.gate_proj">(%1035:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1036:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.16.mlp.act">(%1036:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1037:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.16.mlp.up_proj">(%1035:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1038:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.16.mlp.Mul.0">(%1037:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %1038:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1039:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.16.mlp.down_proj">(%1039:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1040:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1040:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.17 <CPU> [using_qnn:true, symbol:model.layers.17] {
        (%1041:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %352:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %353:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1080:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1055:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1048:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.17.input_layernorm">(%1041:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1042:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.17.self_attn (%1042:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %352:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %353:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1072:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1055:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1048:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.17.Add.0">(%1072:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1041:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1073:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.17.post_attention_layernorm">(%1073:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1074:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.17.mlp (%1074:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1079:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.17.Add.1">(%1079:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1073:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1080:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1080:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1055:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1048:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.17.self_attn <CPU> [using_qnn:true, symbol:model.layers.17.self_attn] {
        (%1042:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %352:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %353:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1072:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1055:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1048:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.17.self_attn.q_proj">(%1042:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1043:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.17.self_attn.k_proj">(%1042:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1044:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.17.self_attn.v_proj">(%1042:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1045:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.17.self_attn.View.0">(%1043:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1043:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.17.self_attn.Transpose.0">(%1043:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1046:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.17.self_attn.View.1">(%1044:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1044:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.17.self_attn.Transpose.1">(%1044:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1047:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.17.self_attn.View.2">(%1045:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1045:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.17.self_attn.Transpose.2">(%1045:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1048:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.17.self_attn.q_norm">(%1046:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1049:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.17.self_attn.k_norm">(%1047:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1050:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.17.self_attn.q_rope">(%1049:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>) -> (%1051:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.17.self_attn.k_rope">(%1050:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>) -> (%1052:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.17.self_attn.CastType.0">(%1052:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1053:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.17.self_attn.CastType.1">(%1053:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1054:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.17.self_attn.Transpose.3">(%1054:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1055:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.17.self_attn.Concat.0">(%352:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %1055:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%1056:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.17.self_attn.Concat.1">(%353:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %1048:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1057:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.17.self_attn.Repeat.0">(%1056:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%1058:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.17.self_attn.Repeat.1">(%1057:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%1059:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.17.self_attn.MatMul.0">(%1051:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %1058:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%1060:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.17.self_attn.Mul.0">(%1060:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1061:tensor<[1], Int16PerTensor, CPU>[constant: [832]][constant:[832]]) -> (%1062:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.17.self_attn.ReduceMin.0">(%1062:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1063:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.17.self_attn.Add.0">(%1063:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %1064:tensor<[1], Int16PerTensor, CPU>[constant: [899.9988]][constant:[899.9988]]) -> (%1065:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.17.self_attn.Equal.0">(%317:tensor<[1, 1, 32, 1024], Float32, CPU>, %1066:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%1067:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.17.self_attn.Argsort.0">(%1067:tensor<[1, 1, 32, 1024], UInt8, CPU>, %1062:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1065:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%1068:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.17.self_attn.Softmax.0">(%1068:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1069:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.17.self_attn.MatMul.1">(%1069:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1059:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%1070:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.17.self_attn.Transpose.4">(%1070:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1071:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.17.self_attn.View.3">(%1071:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1071:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.17.self_attn.o_proj">(%1071:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1072:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1072:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1055:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1048:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.17.mlp <CPU> [using_qnn:true, symbol:model.layers.17.mlp] {
        (%1074:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1079:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.17.mlp.gate_proj">(%1074:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1075:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.17.mlp.act">(%1075:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1076:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.17.mlp.up_proj">(%1074:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1077:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.17.mlp.Mul.0">(%1076:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %1077:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1078:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.17.mlp.down_proj">(%1078:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1079:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1079:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.18 <CPU> [using_qnn:true, symbol:model.layers.18] {
        (%1080:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %354:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %355:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1119:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1094:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1087:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.18.input_layernorm">(%1080:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1081:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.18.self_attn (%1081:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %354:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %355:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1111:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1094:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1087:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.18.Add.0">(%1111:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1080:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1112:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.18.post_attention_layernorm">(%1112:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1113:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.18.mlp (%1113:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1118:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.18.Add.1">(%1118:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1112:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1119:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1119:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1094:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1087:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.18.self_attn <CPU> [using_qnn:true, symbol:model.layers.18.self_attn] {
        (%1081:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %354:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %355:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1111:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1094:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1087:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.18.self_attn.q_proj">(%1081:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1082:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.18.self_attn.k_proj">(%1081:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1083:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.18.self_attn.v_proj">(%1081:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1084:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.18.self_attn.View.0">(%1082:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1082:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.18.self_attn.Transpose.0">(%1082:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1085:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.18.self_attn.View.1">(%1083:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1083:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.18.self_attn.Transpose.1">(%1083:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1086:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.18.self_attn.View.2">(%1084:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1084:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.18.self_attn.Transpose.2">(%1084:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1087:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.18.self_attn.q_norm">(%1085:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1088:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.18.self_attn.k_norm">(%1086:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1089:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.18.self_attn.q_rope">(%1088:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>) -> (%1090:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.18.self_attn.k_rope">(%1089:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>) -> (%1091:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.18.self_attn.CastType.0">(%1091:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1092:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.18.self_attn.CastType.1">(%1092:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1093:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.18.self_attn.Transpose.3">(%1093:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1094:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.18.self_attn.Concat.0">(%354:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %1094:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%1095:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.18.self_attn.Concat.1">(%355:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %1087:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1096:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.18.self_attn.Repeat.0">(%1095:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%1097:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.18.self_attn.Repeat.1">(%1096:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%1098:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.18.self_attn.MatMul.0">(%1090:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %1097:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%1099:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.18.self_attn.Mul.0">(%1099:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1100:tensor<[1], Int16PerTensor, CPU>[constant: [0]][constant:[0]]) -> (%1101:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.18.self_attn.ReduceMin.0">(%1101:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1102:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.18.self_attn.Add.0">(%1102:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %1103:tensor<[1], Int16PerTensor, CPU>[constant: [9.1807e-41]][constant:[9.1807e-41]]) -> (%1104:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.18.self_attn.Equal.0">(%317:tensor<[1, 1, 32, 1024], Float32, CPU>, %1105:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%1106:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.18.self_attn.Argsort.0">(%1106:tensor<[1, 1, 32, 1024], UInt8, CPU>, %1101:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1104:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%1107:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.18.self_attn.Softmax.0">(%1107:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1108:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.18.self_attn.MatMul.1">(%1108:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1098:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%1109:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.18.self_attn.Transpose.4">(%1109:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1110:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.18.self_attn.View.3">(%1110:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1110:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.18.self_attn.o_proj">(%1110:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1111:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1111:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1094:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1087:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.18.mlp <CPU> [using_qnn:true, symbol:model.layers.18.mlp] {
        (%1113:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1118:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.18.mlp.gate_proj">(%1113:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1114:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.18.mlp.act">(%1114:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1115:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.18.mlp.up_proj">(%1113:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1116:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.18.mlp.Mul.0">(%1115:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %1116:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1117:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.18.mlp.down_proj">(%1117:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1118:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1118:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.19 <CPU> [using_qnn:true, symbol:model.layers.19] {
        (%1119:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %356:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %357:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1158:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1133:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1126:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.19.input_layernorm">(%1119:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1120:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.19.self_attn (%1120:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %356:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %357:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1150:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1133:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1126:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.19.Add.0">(%1150:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1119:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1151:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.19.post_attention_layernorm">(%1151:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1152:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.19.mlp (%1152:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1157:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.19.Add.1">(%1157:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1151:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1158:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1158:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1133:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1126:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.19.self_attn <CPU> [using_qnn:true, symbol:model.layers.19.self_attn] {
        (%1120:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %356:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %357:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1150:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1133:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1126:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.19.self_attn.q_proj">(%1120:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1121:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.19.self_attn.k_proj">(%1120:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1122:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.19.self_attn.v_proj">(%1120:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1123:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.19.self_attn.View.0">(%1121:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1121:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.19.self_attn.Transpose.0">(%1121:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1124:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.19.self_attn.View.1">(%1122:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1122:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.19.self_attn.Transpose.1">(%1122:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1125:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.19.self_attn.View.2">(%1123:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1123:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.19.self_attn.Transpose.2">(%1123:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1126:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.19.self_attn.q_norm">(%1124:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1127:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.19.self_attn.k_norm">(%1125:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1128:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.19.self_attn.q_rope">(%1127:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>) -> (%1129:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.19.self_attn.k_rope">(%1128:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>) -> (%1130:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.19.self_attn.CastType.0">(%1130:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1131:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.19.self_attn.CastType.1">(%1131:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1132:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.19.self_attn.Transpose.3">(%1132:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1133:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.19.self_attn.Concat.0">(%356:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %1133:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%1134:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.19.self_attn.Concat.1">(%357:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %1126:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1135:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.19.self_attn.Repeat.0">(%1134:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%1136:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.19.self_attn.Repeat.1">(%1135:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%1137:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.19.self_attn.MatMul.0">(%1129:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %1136:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%1138:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.19.self_attn.Mul.0">(%1138:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1139:tensor<[1], Int16PerTensor, CPU>[constant: [0]][constant:[0]]) -> (%1140:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.19.self_attn.ReduceMin.0">(%1140:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1141:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.19.self_attn.Add.0">(%1141:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %1142:tensor<[1], Int16PerTensor, CPU>[constant: [9.1807e-41]][constant:[9.1807e-41]]) -> (%1143:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.19.self_attn.Equal.0">(%317:tensor<[1, 1, 32, 1024], Float32, CPU>, %1144:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%1145:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.19.self_attn.Argsort.0">(%1145:tensor<[1, 1, 32, 1024], UInt8, CPU>, %1140:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1143:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%1146:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.19.self_attn.Softmax.0">(%1146:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1147:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.19.self_attn.MatMul.1">(%1147:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1137:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%1148:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.19.self_attn.Transpose.4">(%1148:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1149:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.19.self_attn.View.3">(%1149:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1149:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.19.self_attn.o_proj">(%1149:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1150:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1150:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1133:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1126:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.19.mlp <CPU> [using_qnn:true, symbol:model.layers.19.mlp] {
        (%1152:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1157:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.19.mlp.gate_proj">(%1152:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1153:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.19.mlp.act">(%1153:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1154:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.19.mlp.up_proj">(%1152:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1155:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.19.mlp.Mul.0">(%1154:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %1155:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1156:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.19.mlp.down_proj">(%1156:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1157:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1157:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.20 <CPU> [using_qnn:true, symbol:model.layers.20] {
        (%1158:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %358:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %359:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1197:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1172:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1165:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.20.input_layernorm">(%1158:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1159:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.20.self_attn (%1159:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %358:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %359:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1189:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1172:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1165:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.20.Add.0">(%1189:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1158:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1190:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.20.post_attention_layernorm">(%1190:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1191:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.20.mlp (%1191:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1196:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.20.Add.1">(%1196:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1190:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1197:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1197:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1172:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1165:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.20.self_attn <CPU> [using_qnn:true, symbol:model.layers.20.self_attn] {
        (%1159:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %358:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %359:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1189:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1172:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1165:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.20.self_attn.q_proj">(%1159:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1160:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.20.self_attn.k_proj">(%1159:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1161:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.20.self_attn.v_proj">(%1159:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1162:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.20.self_attn.View.0">(%1160:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1160:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.20.self_attn.Transpose.0">(%1160:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1163:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.20.self_attn.View.1">(%1161:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1161:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.20.self_attn.Transpose.1">(%1161:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1164:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.20.self_attn.View.2">(%1162:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1162:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.20.self_attn.Transpose.2">(%1162:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1165:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.20.self_attn.q_norm">(%1163:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1166:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.20.self_attn.k_norm">(%1164:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1167:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.20.self_attn.q_rope">(%1166:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>) -> (%1168:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.20.self_attn.k_rope">(%1167:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>) -> (%1169:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.20.self_attn.CastType.0">(%1169:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1170:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.20.self_attn.CastType.1">(%1170:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1171:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.20.self_attn.Transpose.3">(%1171:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1172:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.20.self_attn.Concat.0">(%358:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %1172:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%1173:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.20.self_attn.Concat.1">(%359:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %1165:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1174:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.20.self_attn.Repeat.0">(%1173:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%1175:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.20.self_attn.Repeat.1">(%1174:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%1176:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.20.self_attn.MatMul.0">(%1168:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %1175:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%1177:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.20.self_attn.Mul.0">(%1177:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1178:tensor<[1], Int16PerTensor, CPU>[constant: [0]][constant:[0]]) -> (%1179:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.20.self_attn.ReduceMin.0">(%1179:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1180:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.20.self_attn.Add.0">(%1180:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %1181:tensor<[1], Int16PerTensor, CPU>[constant: [9.1807e-41]][constant:[9.1807e-41]]) -> (%1182:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.20.self_attn.Equal.0">(%317:tensor<[1, 1, 32, 1024], Float32, CPU>, %1183:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%1184:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.20.self_attn.Argsort.0">(%1184:tensor<[1, 1, 32, 1024], UInt8, CPU>, %1179:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1182:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%1185:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.20.self_attn.Softmax.0">(%1185:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1186:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.20.self_attn.MatMul.1">(%1186:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1176:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%1187:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.20.self_attn.Transpose.4">(%1187:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1188:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.20.self_attn.View.3">(%1188:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1188:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.20.self_attn.o_proj">(%1188:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1189:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1189:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1172:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1165:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.20.mlp <CPU> [using_qnn:true, symbol:model.layers.20.mlp] {
        (%1191:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1196:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.20.mlp.gate_proj">(%1191:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1192:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.20.mlp.act">(%1192:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1193:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.20.mlp.up_proj">(%1191:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1194:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.20.mlp.Mul.0">(%1193:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %1194:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1195:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.20.mlp.down_proj">(%1195:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1196:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1196:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.21 <CPU> [using_qnn:true, symbol:model.layers.21] {
        (%1197:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %360:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %361:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1236:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1211:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1204:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.21.input_layernorm">(%1197:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1198:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.21.self_attn (%1198:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %360:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %361:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1228:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1211:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1204:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.21.Add.0">(%1228:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1197:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1229:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.21.post_attention_layernorm">(%1229:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1230:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.21.mlp (%1230:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1235:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.21.Add.1">(%1235:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1229:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1236:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1236:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1211:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1204:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.21.self_attn <CPU> [using_qnn:true, symbol:model.layers.21.self_attn] {
        (%1198:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %360:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %361:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1228:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1211:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1204:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.21.self_attn.q_proj">(%1198:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1199:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.21.self_attn.k_proj">(%1198:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1200:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.21.self_attn.v_proj">(%1198:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1201:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.21.self_attn.View.0">(%1199:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1199:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.21.self_attn.Transpose.0">(%1199:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1202:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.21.self_attn.View.1">(%1200:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1200:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.21.self_attn.Transpose.1">(%1200:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1203:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.21.self_attn.View.2">(%1201:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1201:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.21.self_attn.Transpose.2">(%1201:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1204:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.21.self_attn.q_norm">(%1202:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1205:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.21.self_attn.k_norm">(%1203:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1206:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.21.self_attn.q_rope">(%1205:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>) -> (%1207:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.21.self_attn.k_rope">(%1206:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>) -> (%1208:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.21.self_attn.CastType.0">(%1208:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1209:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.21.self_attn.CastType.1">(%1209:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1210:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.21.self_attn.Transpose.3">(%1210:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1211:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.21.self_attn.Concat.0">(%360:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %1211:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%1212:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.21.self_attn.Concat.1">(%361:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %1204:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1213:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.21.self_attn.Repeat.0">(%1212:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%1214:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.21.self_attn.Repeat.1">(%1213:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%1215:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.21.self_attn.MatMul.0">(%1207:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %1214:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%1216:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.21.self_attn.Mul.0">(%1216:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1217:tensor<[1], Int16PerTensor, CPU>[constant: [0]][constant:[0]]) -> (%1218:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.21.self_attn.ReduceMin.0">(%1218:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1219:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.21.self_attn.Add.0">(%1219:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %1220:tensor<[1], Int16PerTensor, CPU>[constant: [9.1807e-41]][constant:[9.1807e-41]]) -> (%1221:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.21.self_attn.Equal.0">(%317:tensor<[1, 1, 32, 1024], Float32, CPU>, %1222:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%1223:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.21.self_attn.Argsort.0">(%1223:tensor<[1, 1, 32, 1024], UInt8, CPU>, %1218:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1221:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%1224:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.21.self_attn.Softmax.0">(%1224:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1225:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.21.self_attn.MatMul.1">(%1225:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1215:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%1226:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.21.self_attn.Transpose.4">(%1226:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1227:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.21.self_attn.View.3">(%1227:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1227:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.21.self_attn.o_proj">(%1227:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1228:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1228:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1211:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1204:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.21.mlp <CPU> [using_qnn:true, symbol:model.layers.21.mlp] {
        (%1230:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1235:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.21.mlp.gate_proj">(%1230:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1231:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.21.mlp.act">(%1231:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1232:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.21.mlp.up_proj">(%1230:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1233:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.21.mlp.Mul.0">(%1232:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %1233:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1234:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.21.mlp.down_proj">(%1234:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1235:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1235:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.22 <CPU> [using_qnn:true, symbol:model.layers.22] {
        (%1236:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %362:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %363:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1275:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1250:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1243:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.22.input_layernorm">(%1236:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1237:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.22.self_attn (%1237:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %362:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %363:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1267:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1250:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1243:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.22.Add.0">(%1267:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1236:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1268:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.22.post_attention_layernorm">(%1268:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1269:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.22.mlp (%1269:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1274:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.22.Add.1">(%1274:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1268:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1275:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1275:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1250:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1243:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.22.self_attn <CPU> [using_qnn:true, symbol:model.layers.22.self_attn] {
        (%1237:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %362:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %363:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1267:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1250:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1243:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.22.self_attn.q_proj">(%1237:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1238:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.22.self_attn.k_proj">(%1237:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1239:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.22.self_attn.v_proj">(%1237:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1240:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.22.self_attn.View.0">(%1238:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1238:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.22.self_attn.Transpose.0">(%1238:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1241:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.22.self_attn.View.1">(%1239:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1239:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.22.self_attn.Transpose.1">(%1239:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1242:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.22.self_attn.View.2">(%1240:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1240:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.22.self_attn.Transpose.2">(%1240:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1243:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.22.self_attn.q_norm">(%1241:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1244:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.22.self_attn.k_norm">(%1242:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1245:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.22.self_attn.q_rope">(%1244:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>) -> (%1246:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.22.self_attn.k_rope">(%1245:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>) -> (%1247:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.22.self_attn.CastType.0">(%1247:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1248:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.22.self_attn.CastType.1">(%1248:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1249:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.22.self_attn.Transpose.3">(%1249:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1250:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.22.self_attn.Concat.0">(%362:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %1250:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%1251:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.22.self_attn.Concat.1">(%363:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %1243:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1252:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.22.self_attn.Repeat.0">(%1251:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%1253:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.22.self_attn.Repeat.1">(%1252:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%1254:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.22.self_attn.MatMul.0">(%1246:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %1253:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%1255:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.22.self_attn.Mul.0">(%1255:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1256:tensor<[1], Int16PerTensor, CPU>[constant: [0]][constant:[0]]) -> (%1257:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.22.self_attn.ReduceMin.0">(%1257:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1258:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.22.self_attn.Add.0">(%1258:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %1259:tensor<[1], Int16PerTensor, CPU>[constant: [9.1807e-41]][constant:[9.1807e-41]]) -> (%1260:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.22.self_attn.Equal.0">(%317:tensor<[1, 1, 32, 1024], Float32, CPU>, %1261:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%1262:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.22.self_attn.Argsort.0">(%1262:tensor<[1, 1, 32, 1024], UInt8, CPU>, %1257:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1260:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%1263:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.22.self_attn.Softmax.0">(%1263:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1264:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.22.self_attn.MatMul.1">(%1264:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1254:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%1265:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.22.self_attn.Transpose.4">(%1265:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1266:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.22.self_attn.View.3">(%1266:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1266:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.22.self_attn.o_proj">(%1266:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1267:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1267:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1250:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1243:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.22.mlp <CPU> [using_qnn:true, symbol:model.layers.22.mlp] {
        (%1269:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1274:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.22.mlp.gate_proj">(%1269:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1270:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.22.mlp.act">(%1270:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1271:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.22.mlp.up_proj">(%1269:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1272:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.22.mlp.Mul.0">(%1271:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %1272:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1273:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.22.mlp.down_proj">(%1273:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1274:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1274:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.23 <CPU> [using_qnn:true, symbol:model.layers.23] {
        (%1275:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %364:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %365:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1314:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1289:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1282:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.23.input_layernorm">(%1275:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1276:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.23.self_attn (%1276:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %364:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %365:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1306:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1289:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1282:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.23.Add.0">(%1306:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1275:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1307:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.23.post_attention_layernorm">(%1307:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1308:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.23.mlp (%1308:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1313:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.23.Add.1">(%1313:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1307:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1314:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1314:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1289:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1282:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.23.self_attn <CPU> [using_qnn:true, symbol:model.layers.23.self_attn] {
        (%1276:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %364:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %365:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1306:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1289:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1282:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.23.self_attn.q_proj">(%1276:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1277:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.23.self_attn.k_proj">(%1276:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1278:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.23.self_attn.v_proj">(%1276:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1279:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.23.self_attn.View.0">(%1277:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1277:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.23.self_attn.Transpose.0">(%1277:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1280:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.23.self_attn.View.1">(%1278:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1278:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.23.self_attn.Transpose.1">(%1278:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1281:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.23.self_attn.View.2">(%1279:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1279:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.23.self_attn.Transpose.2">(%1279:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1282:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.23.self_attn.q_norm">(%1280:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1283:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.23.self_attn.k_norm">(%1281:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1284:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.23.self_attn.q_rope">(%1283:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>) -> (%1285:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.23.self_attn.k_rope">(%1284:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>) -> (%1286:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.23.self_attn.CastType.0">(%1286:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1287:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.23.self_attn.CastType.1">(%1287:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1288:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.23.self_attn.Transpose.3">(%1288:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1289:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.23.self_attn.Concat.0">(%364:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %1289:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%1290:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.23.self_attn.Concat.1">(%365:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %1282:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1291:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.23.self_attn.Repeat.0">(%1290:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%1292:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.23.self_attn.Repeat.1">(%1291:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%1293:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.23.self_attn.MatMul.0">(%1285:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %1292:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%1294:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.23.self_attn.Mul.0">(%1294:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1295:tensor<[1], Int16PerTensor, CPU>[constant: [0]][constant:[0]]) -> (%1296:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.23.self_attn.ReduceMin.0">(%1296:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1297:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.23.self_attn.Add.0">(%1297:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %1298:tensor<[1], Int16PerTensor, CPU>[constant: [9.1807e-41]][constant:[9.1807e-41]]) -> (%1299:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.23.self_attn.Equal.0">(%317:tensor<[1, 1, 32, 1024], Float32, CPU>, %1300:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%1301:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.23.self_attn.Argsort.0">(%1301:tensor<[1, 1, 32, 1024], UInt8, CPU>, %1296:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1299:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%1302:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.23.self_attn.Softmax.0">(%1302:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1303:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.23.self_attn.MatMul.1">(%1303:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1293:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%1304:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.23.self_attn.Transpose.4">(%1304:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1305:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.23.self_attn.View.3">(%1305:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1305:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.23.self_attn.o_proj">(%1305:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1306:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1306:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1289:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1282:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.23.mlp <CPU> [using_qnn:true, symbol:model.layers.23.mlp] {
        (%1308:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1313:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.23.mlp.gate_proj">(%1308:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1309:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.23.mlp.act">(%1309:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1310:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.23.mlp.up_proj">(%1308:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1311:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.23.mlp.Mul.0">(%1310:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %1311:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1312:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.23.mlp.down_proj">(%1312:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1313:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1313:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.24 <CPU> [using_qnn:true, symbol:model.layers.24] {
        (%1314:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %366:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %367:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1353:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1328:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1321:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.24.input_layernorm">(%1314:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1315:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.24.self_attn (%1315:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %366:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %367:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1345:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1328:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1321:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.24.Add.0">(%1345:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1314:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1346:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.24.post_attention_layernorm">(%1346:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1347:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.24.mlp (%1347:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1352:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.24.Add.1">(%1352:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1346:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1353:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1353:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1328:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1321:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.24.self_attn <CPU> [using_qnn:true, symbol:model.layers.24.self_attn] {
        (%1315:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %366:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %367:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1345:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1328:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1321:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.24.self_attn.q_proj">(%1315:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1316:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.24.self_attn.k_proj">(%1315:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1317:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.24.self_attn.v_proj">(%1315:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1318:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.24.self_attn.View.0">(%1316:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1316:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.24.self_attn.Transpose.0">(%1316:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1319:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.24.self_attn.View.1">(%1317:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1317:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.24.self_attn.Transpose.1">(%1317:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1320:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.24.self_attn.View.2">(%1318:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1318:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.24.self_attn.Transpose.2">(%1318:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1321:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.24.self_attn.q_norm">(%1319:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1322:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.24.self_attn.k_norm">(%1320:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1323:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.24.self_attn.q_rope">(%1322:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>) -> (%1324:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.24.self_attn.k_rope">(%1323:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>) -> (%1325:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.24.self_attn.CastType.0">(%1325:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1326:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.24.self_attn.CastType.1">(%1326:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1327:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.24.self_attn.Transpose.3">(%1327:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1328:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.24.self_attn.Concat.0">(%366:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %1328:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%1329:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.24.self_attn.Concat.1">(%367:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %1321:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1330:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.24.self_attn.Repeat.0">(%1329:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%1331:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.24.self_attn.Repeat.1">(%1330:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%1332:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.24.self_attn.MatMul.0">(%1324:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %1331:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%1333:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.24.self_attn.Mul.0">(%1333:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1334:tensor<[1], Int16PerTensor, CPU>[constant: [0]][constant:[0]]) -> (%1335:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.24.self_attn.ReduceMin.0">(%1335:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1336:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.24.self_attn.Add.0">(%1336:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %1337:tensor<[1], Int16PerTensor, CPU>[constant: [9.1807e-41]][constant:[9.1807e-41]]) -> (%1338:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.24.self_attn.Equal.0">(%317:tensor<[1, 1, 32, 1024], Float32, CPU>, %1339:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%1340:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.24.self_attn.Argsort.0">(%1340:tensor<[1, 1, 32, 1024], UInt8, CPU>, %1335:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1338:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%1341:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.24.self_attn.Softmax.0">(%1341:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1342:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.24.self_attn.MatMul.1">(%1342:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1332:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%1343:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.24.self_attn.Transpose.4">(%1343:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1344:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.24.self_attn.View.3">(%1344:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1344:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.24.self_attn.o_proj">(%1344:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1345:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1345:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1328:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1321:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.24.mlp <CPU> [using_qnn:true, symbol:model.layers.24.mlp] {
        (%1347:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1352:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.24.mlp.gate_proj">(%1347:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1348:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.24.mlp.act">(%1348:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1349:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.24.mlp.up_proj">(%1347:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1350:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.24.mlp.Mul.0">(%1349:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %1350:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1351:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.24.mlp.down_proj">(%1351:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1352:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1352:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.25 <CPU> [using_qnn:true, symbol:model.layers.25] {
        (%1353:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %368:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %369:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1392:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1367:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1360:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.25.input_layernorm">(%1353:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1354:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.25.self_attn (%1354:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %368:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %369:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1384:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1367:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1360:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.25.Add.0">(%1384:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1353:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1385:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.25.post_attention_layernorm">(%1385:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1386:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.25.mlp (%1386:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1391:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.25.Add.1">(%1391:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1385:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1392:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1392:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1367:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1360:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.25.self_attn <CPU> [using_qnn:true, symbol:model.layers.25.self_attn] {
        (%1354:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %368:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %369:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1384:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1367:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1360:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.25.self_attn.q_proj">(%1354:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1355:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.25.self_attn.k_proj">(%1354:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1356:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.25.self_attn.v_proj">(%1354:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1357:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.25.self_attn.View.0">(%1355:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1355:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.25.self_attn.Transpose.0">(%1355:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1358:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.25.self_attn.View.1">(%1356:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1356:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.25.self_attn.Transpose.1">(%1356:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1359:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.25.self_attn.View.2">(%1357:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1357:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.25.self_attn.Transpose.2">(%1357:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1360:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.25.self_attn.q_norm">(%1358:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1361:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.25.self_attn.k_norm">(%1359:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1362:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.25.self_attn.q_rope">(%1361:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>) -> (%1363:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.25.self_attn.k_rope">(%1362:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>) -> (%1364:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.25.self_attn.CastType.0">(%1364:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1365:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.25.self_attn.CastType.1">(%1365:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1366:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.25.self_attn.Transpose.3">(%1366:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1367:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.25.self_attn.Concat.0">(%368:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %1367:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%1368:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.25.self_attn.Concat.1">(%369:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %1360:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1369:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.25.self_attn.Repeat.0">(%1368:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%1370:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.25.self_attn.Repeat.1">(%1369:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%1371:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.25.self_attn.MatMul.0">(%1363:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %1370:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%1372:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.25.self_attn.Mul.0">(%1372:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1373:tensor<[1], Int16PerTensor, CPU>[constant: [0]][constant:[0]]) -> (%1374:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.25.self_attn.ReduceMin.0">(%1374:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1375:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.25.self_attn.Add.0">(%1375:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %1376:tensor<[1], Int16PerTensor, CPU>[constant: [9.1807e-41]][constant:[9.1807e-41]]) -> (%1377:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.25.self_attn.Equal.0">(%317:tensor<[1, 1, 32, 1024], Float32, CPU>, %1378:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%1379:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.25.self_attn.Argsort.0">(%1379:tensor<[1, 1, 32, 1024], UInt8, CPU>, %1374:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1377:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%1380:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.25.self_attn.Softmax.0">(%1380:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1381:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.25.self_attn.MatMul.1">(%1381:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1371:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%1382:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.25.self_attn.Transpose.4">(%1382:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1383:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.25.self_attn.View.3">(%1383:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1383:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.25.self_attn.o_proj">(%1383:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1384:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1384:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1367:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1360:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.25.mlp <CPU> [using_qnn:true, symbol:model.layers.25.mlp] {
        (%1386:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1391:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.25.mlp.gate_proj">(%1386:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1387:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.25.mlp.act">(%1387:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1388:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.25.mlp.up_proj">(%1386:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1389:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.25.mlp.Mul.0">(%1388:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %1389:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1390:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.25.mlp.down_proj">(%1390:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1391:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1391:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.26 <CPU> [using_qnn:true, symbol:model.layers.26] {
        (%1392:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %370:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %371:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1431:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1406:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1399:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.26.input_layernorm">(%1392:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1393:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.26.self_attn (%1393:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %370:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %371:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1423:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1406:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1399:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.26.Add.0">(%1423:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1392:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1424:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.26.post_attention_layernorm">(%1424:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1425:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.26.mlp (%1425:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1430:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.26.Add.1">(%1430:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1424:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1431:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1431:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1406:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1399:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.26.self_attn <CPU> [using_qnn:true, symbol:model.layers.26.self_attn] {
        (%1393:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %370:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %371:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1423:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1406:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1399:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.26.self_attn.q_proj">(%1393:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1394:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.26.self_attn.k_proj">(%1393:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1395:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.26.self_attn.v_proj">(%1393:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1396:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.26.self_attn.View.0">(%1394:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1394:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.26.self_attn.Transpose.0">(%1394:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1397:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.26.self_attn.View.1">(%1395:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1395:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.26.self_attn.Transpose.1">(%1395:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1398:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.26.self_attn.View.2">(%1396:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1396:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.26.self_attn.Transpose.2">(%1396:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1399:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.26.self_attn.q_norm">(%1397:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1400:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.26.self_attn.k_norm">(%1398:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1401:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.26.self_attn.q_rope">(%1400:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>) -> (%1402:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.26.self_attn.k_rope">(%1401:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>) -> (%1403:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.26.self_attn.CastType.0">(%1403:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1404:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.26.self_attn.CastType.1">(%1404:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1405:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.26.self_attn.Transpose.3">(%1405:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1406:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.26.self_attn.Concat.0">(%370:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %1406:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%1407:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.26.self_attn.Concat.1">(%371:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %1399:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1408:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.26.self_attn.Repeat.0">(%1407:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%1409:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.26.self_attn.Repeat.1">(%1408:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%1410:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.26.self_attn.MatMul.0">(%1402:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %1409:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%1411:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.26.self_attn.Mul.0">(%1411:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1412:tensor<[1], Int16PerTensor, CPU>[constant: [0]][constant:[0]]) -> (%1413:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.26.self_attn.ReduceMin.0">(%1413:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1414:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.26.self_attn.Add.0">(%1414:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %1415:tensor<[1], Int16PerTensor, CPU>[constant: [9.1807e-41]][constant:[9.1807e-41]]) -> (%1416:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.26.self_attn.Equal.0">(%317:tensor<[1, 1, 32, 1024], Float32, CPU>, %1417:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%1418:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.26.self_attn.Argsort.0">(%1418:tensor<[1, 1, 32, 1024], UInt8, CPU>, %1413:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1416:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%1419:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.26.self_attn.Softmax.0">(%1419:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1420:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.26.self_attn.MatMul.1">(%1420:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1410:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%1421:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.26.self_attn.Transpose.4">(%1421:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1422:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.26.self_attn.View.3">(%1422:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1422:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.26.self_attn.o_proj">(%1422:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1423:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1423:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1406:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1399:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.26.mlp <CPU> [using_qnn:true, symbol:model.layers.26.mlp] {
        (%1425:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1430:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.26.mlp.gate_proj">(%1425:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1426:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.26.mlp.act">(%1426:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1427:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.26.mlp.up_proj">(%1425:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1428:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.26.mlp.Mul.0">(%1427:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %1428:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1429:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.26.mlp.down_proj">(%1429:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1430:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1430:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.27 <CPU> [using_qnn:true, symbol:model.layers.27] {
        (%1431:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %372:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %373:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1470:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1445:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1438:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.27.input_layernorm">(%1431:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1432:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.27.self_attn (%1432:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %372:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %373:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1462:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1445:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1438:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.27.Add.0">(%1462:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1431:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1463:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.27.post_attention_layernorm">(%1463:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1464:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.27.mlp (%1464:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1469:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.27.Add.1">(%1469:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1463:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1470:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1470:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1445:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1438:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.27.self_attn <CPU> [using_qnn:true, symbol:model.layers.27.self_attn] {
        (%1432:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>, %317:tensor<[1, 1, 32, 1024], Float32, CPU>, %372:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %373:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1462:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1445:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1438:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.27.self_attn.q_proj">(%1432:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1433:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.27.self_attn.k_proj">(%1432:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1434:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.27.self_attn.v_proj">(%1432:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1435:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.27.self_attn.View.0">(%1433:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1433:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.27.self_attn.Transpose.0">(%1433:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1436:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.27.self_attn.View.1">(%1434:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1434:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.27.self_attn.Transpose.1">(%1434:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1437:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.27.self_attn.View.2">(%1435:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1435:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.27.self_attn.Transpose.2">(%1435:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1438:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.27.self_attn.q_norm">(%1436:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1439:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.27.self_attn.k_norm">(%1437:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1440:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.27.self_attn.q_rope">(%1439:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>) -> (%1441:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.27.self_attn.k_rope">(%1440:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %377:tensor<[1, 32, 128], Float32, CPU>, %378:tensor<[1, 32, 128], Float32, CPU>) -> (%1442:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.27.self_attn.CastType.0">(%1442:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1443:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.27.self_attn.CastType.1">(%1443:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1444:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.27.self_attn.Transpose.3">(%1444:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1445:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.27.self_attn.Concat.0">(%372:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %1445:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%1446:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.27.self_attn.Concat.1">(%373:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %1438:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1447:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.27.self_attn.Repeat.0">(%1446:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%1448:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.27.self_attn.Repeat.1">(%1447:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%1449:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.27.self_attn.MatMul.0">(%1441:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %1448:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%1450:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.27.self_attn.Mul.0">(%1450:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1451:tensor<[1], Int16PerTensor, CPU>[constant: [0]][constant:[0]]) -> (%1452:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.27.self_attn.ReduceMin.0">(%1452:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1453:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.27.self_attn.Add.0">(%1453:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %1454:tensor<[1], Int16PerTensor, CPU>[constant: [9.1807e-41]][constant:[9.1807e-41]]) -> (%1455:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.27.self_attn.Equal.0">(%317:tensor<[1, 1, 32, 1024], Float32, CPU>, %1456:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%1457:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.27.self_attn.Argsort.0">(%1457:tensor<[1, 1, 32, 1024], UInt8, CPU>, %1452:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1455:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%1458:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.27.self_attn.Softmax.0">(%1458:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1459:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.27.self_attn.MatMul.1">(%1459:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1449:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%1460:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.27.self_attn.Transpose.4">(%1460:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1461:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.27.self_attn.View.3">(%1461:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1461:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.27.self_attn.o_proj">(%1461:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1462:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1462:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1445:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1438:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.27.mlp <CPU> [using_qnn:true, symbol:model.layers.27.mlp] {
        (%1464:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1469:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.27.mlp.gate_proj">(%1464:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1465:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.27.mlp.act">(%1465:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1466:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.27.mlp.up_proj">(%1464:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1467:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.27.mlp.Mul.0">(%1466:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %1467:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1468:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.27.mlp.down_proj">(%1468:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1469:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1469:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    linalg.CPU.LinearOp <name="lm_head"> [using_qnn:true] (%1471:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1472:tensor<[1, 32, 151936], Int16PerTensor, CPU>)
    //        
    //      o o    
    //            
    //       
    //             
    //        
}
 
