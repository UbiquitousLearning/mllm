@main () -> () {
    graph.SubGraphOp @init <notype> [symbol:init] {
        () -> () {
            tensor.CPU.register () -> (%105:tensor<[151936, 2048], Float32, CPU>[@model.embed_tokens.weight][symbol:model.embed_tokens.weight])[symbol:model.embed_tokens.weight]
            tensor.CPU.register () -> (%199:tensor<[2048], Float32, CPU>[@model.layers.0.input_layernorm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=66), symbol:model.layers.0.input_layernorm.weight])[symbol:model.layers.0.input_layernorm.weight]
            tensor.CPU.register () -> (%76:tensor<[2048, 2048], Float32, CPU>[@model.layers.0.self_attn.q_proj.weight][symbol:model.layers.0.self_attn.q_proj.weight])[symbol:model.layers.0.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%133:tensor<[1024, 2048], Float32, CPU>[@model.layers.0.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=67), symbol:model.layers.0.self_attn.k_proj.weight])[symbol:model.layers.0.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%179:tensor<[1024, 2048], Float32, CPU>[@model.layers.0.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=69), symbol:model.layers.0.self_attn.v_proj.weight])[symbol:model.layers.0.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%200:tensor<[128], Float32, CPU>[@model.layers.0.self_attn.q_norm.weight][qnn_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=73), symbol:model.layers.0.self_attn.q_norm.weight])[symbol:model.layers.0.self_attn.q_norm.weight]
            tensor.CPU.register () -> (%291:tensor<[128], Float32, CPU>[@model.layers.0.self_attn.k_norm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=75), symbol:model.layers.0.self_attn.k_norm.weight])[symbol:model.layers.0.self_attn.k_norm.weight]
            tensor.CPU.register () -> (%269:tensor<[2048, 2048], Float32, CPU>[@model.layers.0.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=88), symbol:model.layers.0.self_attn.o_proj.weight])[symbol:model.layers.0.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%40:tensor<[2048], Float32, CPU>[@model.layers.0.post_attention_layernorm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=91), symbol:model.layers.0.post_attention_layernorm.weight])[symbol:model.layers.0.post_attention_layernorm.weight]
            tensor.CPU.register () -> (%9:tensor<[6144, 2048], Float32, CPU>[@model.layers.0.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=92), symbol:model.layers.0.mlp.gate_proj.weight])[symbol:model.layers.0.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%111:tensor<[6144, 2048], Float32, CPU>[@model.layers.0.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=95), symbol:model.layers.0.mlp.up_proj.weight])[symbol:model.layers.0.mlp.up_proj.weight]
            tensor.CPU.register () -> (%184:tensor<[2048, 6144], Float32, CPU>[@model.layers.0.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=97), symbol:model.layers.0.mlp.down_proj.weight])[symbol:model.layers.0.mlp.down_proj.weight]
            tensor.CPU.register () -> (%180:tensor<[2048], Float32, CPU>[@model.layers.1.input_layernorm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=100), symbol:model.layers.1.input_layernorm.weight])[symbol:model.layers.1.input_layernorm.weight]
            tensor.CPU.register () -> (%285:tensor<[2048, 2048], Float32, CPU>[@model.layers.1.self_attn.q_proj.weight][symbol:model.layers.1.self_attn.q_proj.weight])[symbol:model.layers.1.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%32:tensor<[1024, 2048], Float32, CPU>[@model.layers.1.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=101), symbol:model.layers.1.self_attn.k_proj.weight])[symbol:model.layers.1.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%154:tensor<[1024, 2048], Float32, CPU>[@model.layers.1.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=103), symbol:model.layers.1.self_attn.v_proj.weight])[symbol:model.layers.1.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%131:tensor<[128], Float32, CPU>[@model.layers.1.self_attn.q_norm.weight][qnn_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=107), symbol:model.layers.1.self_attn.q_norm.weight])[symbol:model.layers.1.self_attn.q_norm.weight]
            tensor.CPU.register () -> (%68:tensor<[128], Float32, CPU>[@model.layers.1.self_attn.k_norm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=109), symbol:model.layers.1.self_attn.k_norm.weight])[symbol:model.layers.1.self_attn.k_norm.weight]
            tensor.CPU.register () -> (%20:tensor<[2048, 2048], Float32, CPU>[@model.layers.1.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=122), symbol:model.layers.1.self_attn.o_proj.weight])[symbol:model.layers.1.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%73:tensor<[2048], Float32, CPU>[@model.layers.1.post_attention_layernorm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=125), symbol:model.layers.1.post_attention_layernorm.weight])[symbol:model.layers.1.post_attention_layernorm.weight]
            tensor.CPU.register () -> (%245:tensor<[6144, 2048], Float32, CPU>[@model.layers.1.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=126), symbol:model.layers.1.mlp.gate_proj.weight])[symbol:model.layers.1.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%230:tensor<[6144, 2048], Float32, CPU>[@model.layers.1.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=129), symbol:model.layers.1.mlp.up_proj.weight])[symbol:model.layers.1.mlp.up_proj.weight]
            tensor.CPU.register () -> (%43:tensor<[2048, 6144], Float32, CPU>[@model.layers.1.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=131), symbol:model.layers.1.mlp.down_proj.weight])[symbol:model.layers.1.mlp.down_proj.weight]
            tensor.CPU.register () -> (%86:tensor<[2048], Float32, CPU>[@model.layers.2.input_layernorm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=134), symbol:model.layers.2.input_layernorm.weight])[symbol:model.layers.2.input_layernorm.weight]
            tensor.CPU.register () -> (%221:tensor<[2048, 2048], Float32, CPU>[@model.layers.2.self_attn.q_proj.weight][symbol:model.layers.2.self_attn.q_proj.weight])[symbol:model.layers.2.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%103:tensor<[1024, 2048], Float32, CPU>[@model.layers.2.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=135), symbol:model.layers.2.self_attn.k_proj.weight])[symbol:model.layers.2.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%47:tensor<[1024, 2048], Float32, CPU>[@model.layers.2.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=137), symbol:model.layers.2.self_attn.v_proj.weight])[symbol:model.layers.2.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%65:tensor<[128], Float32, CPU>[@model.layers.2.self_attn.q_norm.weight][qnn_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=141), symbol:model.layers.2.self_attn.q_norm.weight])[symbol:model.layers.2.self_attn.q_norm.weight]
            tensor.CPU.register () -> (%16:tensor<[128], Float32, CPU>[@model.layers.2.self_attn.k_norm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=143), symbol:model.layers.2.self_attn.k_norm.weight])[symbol:model.layers.2.self_attn.k_norm.weight]
            tensor.CPU.register () -> (%85:tensor<[2048, 2048], Float32, CPU>[@model.layers.2.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=156), symbol:model.layers.2.self_attn.o_proj.weight])[symbol:model.layers.2.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%128:tensor<[2048], Float32, CPU>[@model.layers.2.post_attention_layernorm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=159), symbol:model.layers.2.post_attention_layernorm.weight])[symbol:model.layers.2.post_attention_layernorm.weight]
            tensor.CPU.register () -> (%252:tensor<[6144, 2048], Float32, CPU>[@model.layers.2.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=160), symbol:model.layers.2.mlp.gate_proj.weight])[symbol:model.layers.2.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%24:tensor<[6144, 2048], Float32, CPU>[@model.layers.2.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=163), symbol:model.layers.2.mlp.up_proj.weight])[symbol:model.layers.2.mlp.up_proj.weight]
            tensor.CPU.register () -> (%28:tensor<[2048, 6144], Float32, CPU>[@model.layers.2.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=165), symbol:model.layers.2.mlp.down_proj.weight])[symbol:model.layers.2.mlp.down_proj.weight]
            tensor.CPU.register () -> (%1:tensor<[2048], Float32, CPU>[@model.layers.3.input_layernorm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=168), symbol:model.layers.3.input_layernorm.weight])[symbol:model.layers.3.input_layernorm.weight]
            tensor.CPU.register () -> (%283:tensor<[2048, 2048], Float32, CPU>[@model.layers.3.self_attn.q_proj.weight][symbol:model.layers.3.self_attn.q_proj.weight])[symbol:model.layers.3.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%48:tensor<[1024, 2048], Float32, CPU>[@model.layers.3.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=169), symbol:model.layers.3.self_attn.k_proj.weight])[symbol:model.layers.3.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%244:tensor<[1024, 2048], Float32, CPU>[@model.layers.3.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=171), symbol:model.layers.3.self_attn.v_proj.weight])[symbol:model.layers.3.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%33:tensor<[128], Float32, CPU>[@model.layers.3.self_attn.q_norm.weight][qnn_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=175), symbol:model.layers.3.self_attn.q_norm.weight])[symbol:model.layers.3.self_attn.q_norm.weight]
            tensor.CPU.register () -> (%202:tensor<[128], Float32, CPU>[@model.layers.3.self_attn.k_norm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=177), symbol:model.layers.3.self_attn.k_norm.weight])[symbol:model.layers.3.self_attn.k_norm.weight]
            tensor.CPU.register () -> (%301:tensor<[2048, 2048], Float32, CPU>[@model.layers.3.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=190), symbol:model.layers.3.self_attn.o_proj.weight])[symbol:model.layers.3.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%223:tensor<[2048], Float32, CPU>[@model.layers.3.post_attention_layernorm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=193), symbol:model.layers.3.post_attention_layernorm.weight])[symbol:model.layers.3.post_attention_layernorm.weight]
            tensor.CPU.register () -> (%129:tensor<[6144, 2048], Float32, CPU>[@model.layers.3.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=194), symbol:model.layers.3.mlp.gate_proj.weight])[symbol:model.layers.3.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%188:tensor<[6144, 2048], Float32, CPU>[@model.layers.3.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=197), symbol:model.layers.3.mlp.up_proj.weight])[symbol:model.layers.3.mlp.up_proj.weight]
            tensor.CPU.register () -> (%97:tensor<[2048, 6144], Float32, CPU>[@model.layers.3.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=199), symbol:model.layers.3.mlp.down_proj.weight])[symbol:model.layers.3.mlp.down_proj.weight]
            tensor.CPU.register () -> (%3:tensor<[2048], Float32, CPU>[@model.layers.4.input_layernorm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=202), symbol:model.layers.4.input_layernorm.weight])[symbol:model.layers.4.input_layernorm.weight]
            tensor.CPU.register () -> (%164:tensor<[2048, 2048], Float32, CPU>[@model.layers.4.self_attn.q_proj.weight][symbol:model.layers.4.self_attn.q_proj.weight])[symbol:model.layers.4.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%148:tensor<[1024, 2048], Float32, CPU>[@model.layers.4.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=203), symbol:model.layers.4.self_attn.k_proj.weight])[symbol:model.layers.4.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%279:tensor<[1024, 2048], Float32, CPU>[@model.layers.4.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=205), symbol:model.layers.4.self_attn.v_proj.weight])[symbol:model.layers.4.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%145:tensor<[128], Float32, CPU>[@model.layers.4.self_attn.q_norm.weight][qnn_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=209), symbol:model.layers.4.self_attn.q_norm.weight])[symbol:model.layers.4.self_attn.q_norm.weight]
            tensor.CPU.register () -> (%282:tensor<[128], Float32, CPU>[@model.layers.4.self_attn.k_norm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=211), symbol:model.layers.4.self_attn.k_norm.weight])[symbol:model.layers.4.self_attn.k_norm.weight]
            tensor.CPU.register () -> (%91:tensor<[2048, 2048], Float32, CPU>[@model.layers.4.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=224), symbol:model.layers.4.self_attn.o_proj.weight])[symbol:model.layers.4.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%258:tensor<[2048], Float32, CPU>[@model.layers.4.post_attention_layernorm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=227), symbol:model.layers.4.post_attention_layernorm.weight])[symbol:model.layers.4.post_attention_layernorm.weight]
            tensor.CPU.register () -> (%189:tensor<[6144, 2048], Float32, CPU>[@model.layers.4.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=228), symbol:model.layers.4.mlp.gate_proj.weight])[symbol:model.layers.4.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%156:tensor<[6144, 2048], Float32, CPU>[@model.layers.4.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=231), symbol:model.layers.4.mlp.up_proj.weight])[symbol:model.layers.4.mlp.up_proj.weight]
            tensor.CPU.register () -> (%153:tensor<[2048, 6144], Float32, CPU>[@model.layers.4.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=233), symbol:model.layers.4.mlp.down_proj.weight])[symbol:model.layers.4.mlp.down_proj.weight]
            tensor.CPU.register () -> (%256:tensor<[2048], Float32, CPU>[@model.layers.5.input_layernorm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=236), symbol:model.layers.5.input_layernorm.weight])[symbol:model.layers.5.input_layernorm.weight]
            tensor.CPU.register () -> (%78:tensor<[2048, 2048], Float32, CPU>[@model.layers.5.self_attn.q_proj.weight][symbol:model.layers.5.self_attn.q_proj.weight])[symbol:model.layers.5.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%72:tensor<[1024, 2048], Float32, CPU>[@model.layers.5.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=237), symbol:model.layers.5.self_attn.k_proj.weight])[symbol:model.layers.5.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%289:tensor<[1024, 2048], Float32, CPU>[@model.layers.5.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=239), symbol:model.layers.5.self_attn.v_proj.weight])[symbol:model.layers.5.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%225:tensor<[128], Float32, CPU>[@model.layers.5.self_attn.q_norm.weight][qnn_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=243), symbol:model.layers.5.self_attn.q_norm.weight])[symbol:model.layers.5.self_attn.q_norm.weight]
            tensor.CPU.register () -> (%7:tensor<[128], Float32, CPU>[@model.layers.5.self_attn.k_norm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=245), symbol:model.layers.5.self_attn.k_norm.weight])[symbol:model.layers.5.self_attn.k_norm.weight]
            tensor.CPU.register () -> (%264:tensor<[2048, 2048], Float32, CPU>[@model.layers.5.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=258), symbol:model.layers.5.self_attn.o_proj.weight])[symbol:model.layers.5.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%99:tensor<[2048], Float32, CPU>[@model.layers.5.post_attention_layernorm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=261), symbol:model.layers.5.post_attention_layernorm.weight])[symbol:model.layers.5.post_attention_layernorm.weight]
            tensor.CPU.register () -> (%4:tensor<[6144, 2048], Float32, CPU>[@model.layers.5.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=262), symbol:model.layers.5.mlp.gate_proj.weight])[symbol:model.layers.5.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%308:tensor<[6144, 2048], Float32, CPU>[@model.layers.5.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=265), symbol:model.layers.5.mlp.up_proj.weight])[symbol:model.layers.5.mlp.up_proj.weight]
            tensor.CPU.register () -> (%74:tensor<[2048, 6144], Float32, CPU>[@model.layers.5.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=267), symbol:model.layers.5.mlp.down_proj.weight])[symbol:model.layers.5.mlp.down_proj.weight]
            tensor.CPU.register () -> (%132:tensor<[2048], Float32, CPU>[@model.layers.6.input_layernorm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=270), symbol:model.layers.6.input_layernorm.weight])[symbol:model.layers.6.input_layernorm.weight]
            tensor.CPU.register () -> (%59:tensor<[2048, 2048], Float32, CPU>[@model.layers.6.self_attn.q_proj.weight][symbol:model.layers.6.self_attn.q_proj.weight])[symbol:model.layers.6.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%208:tensor<[1024, 2048], Float32, CPU>[@model.layers.6.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=271), symbol:model.layers.6.self_attn.k_proj.weight])[symbol:model.layers.6.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%238:tensor<[1024, 2048], Float32, CPU>[@model.layers.6.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=273), symbol:model.layers.6.self_attn.v_proj.weight])[symbol:model.layers.6.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%294:tensor<[128], Float32, CPU>[@model.layers.6.self_attn.q_norm.weight][qnn_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=277), symbol:model.layers.6.self_attn.q_norm.weight])[symbol:model.layers.6.self_attn.q_norm.weight]
            tensor.CPU.register () -> (%71:tensor<[128], Float32, CPU>[@model.layers.6.self_attn.k_norm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=279), symbol:model.layers.6.self_attn.k_norm.weight])[symbol:model.layers.6.self_attn.k_norm.weight]
            tensor.CPU.register () -> (%52:tensor<[2048, 2048], Float32, CPU>[@model.layers.6.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=292), symbol:model.layers.6.self_attn.o_proj.weight])[symbol:model.layers.6.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%108:tensor<[2048], Float32, CPU>[@model.layers.6.post_attention_layernorm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=295), symbol:model.layers.6.post_attention_layernorm.weight])[symbol:model.layers.6.post_attention_layernorm.weight]
            tensor.CPU.register () -> (%80:tensor<[6144, 2048], Float32, CPU>[@model.layers.6.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=296), symbol:model.layers.6.mlp.gate_proj.weight])[symbol:model.layers.6.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%276:tensor<[6144, 2048], Float32, CPU>[@model.layers.6.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=299), symbol:model.layers.6.mlp.up_proj.weight])[symbol:model.layers.6.mlp.up_proj.weight]
            tensor.CPU.register () -> (%227:tensor<[2048, 6144], Float32, CPU>[@model.layers.6.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=301), symbol:model.layers.6.mlp.down_proj.weight])[symbol:model.layers.6.mlp.down_proj.weight]
            tensor.CPU.register () -> (%107:tensor<[2048], Float32, CPU>[@model.layers.7.input_layernorm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=304), symbol:model.layers.7.input_layernorm.weight])[symbol:model.layers.7.input_layernorm.weight]
            tensor.CPU.register () -> (%287:tensor<[2048, 2048], Float32, CPU>[@model.layers.7.self_attn.q_proj.weight][symbol:model.layers.7.self_attn.q_proj.weight])[symbol:model.layers.7.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%135:tensor<[1024, 2048], Float32, CPU>[@model.layers.7.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=305), symbol:model.layers.7.self_attn.k_proj.weight])[symbol:model.layers.7.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%300:tensor<[1024, 2048], Float32, CPU>[@model.layers.7.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=307), symbol:model.layers.7.self_attn.v_proj.weight])[symbol:model.layers.7.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%23:tensor<[128], Float32, CPU>[@model.layers.7.self_attn.q_norm.weight][qnn_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=311), symbol:model.layers.7.self_attn.q_norm.weight])[symbol:model.layers.7.self_attn.q_norm.weight]
            tensor.CPU.register () -> (%137:tensor<[128], Float32, CPU>[@model.layers.7.self_attn.k_norm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=313), symbol:model.layers.7.self_attn.k_norm.weight])[symbol:model.layers.7.self_attn.k_norm.weight]
            tensor.CPU.register () -> (%251:tensor<[2048, 2048], Float32, CPU>[@model.layers.7.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=326), symbol:model.layers.7.self_attn.o_proj.weight])[symbol:model.layers.7.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%53:tensor<[2048], Float32, CPU>[@model.layers.7.post_attention_layernorm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=329), symbol:model.layers.7.post_attention_layernorm.weight])[symbol:model.layers.7.post_attention_layernorm.weight]
            tensor.CPU.register () -> (%155:tensor<[6144, 2048], Float32, CPU>[@model.layers.7.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=330), symbol:model.layers.7.mlp.gate_proj.weight])[symbol:model.layers.7.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%218:tensor<[6144, 2048], Float32, CPU>[@model.layers.7.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=333), symbol:model.layers.7.mlp.up_proj.weight])[symbol:model.layers.7.mlp.up_proj.weight]
            tensor.CPU.register () -> (%275:tensor<[2048, 6144], Float32, CPU>[@model.layers.7.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=335), symbol:model.layers.7.mlp.down_proj.weight])[symbol:model.layers.7.mlp.down_proj.weight]
            tensor.CPU.register () -> (%171:tensor<[2048], Float32, CPU>[@model.layers.8.input_layernorm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=338), symbol:model.layers.8.input_layernorm.weight])[symbol:model.layers.8.input_layernorm.weight]
            tensor.CPU.register () -> (%165:tensor<[2048, 2048], Float32, CPU>[@model.layers.8.self_attn.q_proj.weight][symbol:model.layers.8.self_attn.q_proj.weight])[symbol:model.layers.8.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%194:tensor<[1024, 2048], Float32, CPU>[@model.layers.8.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=339), symbol:model.layers.8.self_attn.k_proj.weight])[symbol:model.layers.8.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%181:tensor<[1024, 2048], Float32, CPU>[@model.layers.8.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=341), symbol:model.layers.8.self_attn.v_proj.weight])[symbol:model.layers.8.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%309:tensor<[128], Float32, CPU>[@model.layers.8.self_attn.q_norm.weight][qnn_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=345), symbol:model.layers.8.self_attn.q_norm.weight])[symbol:model.layers.8.self_attn.q_norm.weight]
            tensor.CPU.register () -> (%92:tensor<[128], Float32, CPU>[@model.layers.8.self_attn.k_norm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=347), symbol:model.layers.8.self_attn.k_norm.weight])[symbol:model.layers.8.self_attn.k_norm.weight]
            tensor.CPU.register () -> (%197:tensor<[2048, 2048], Float32, CPU>[@model.layers.8.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=360), symbol:model.layers.8.self_attn.o_proj.weight])[symbol:model.layers.8.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%122:tensor<[2048], Float32, CPU>[@model.layers.8.post_attention_layernorm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=363), symbol:model.layers.8.post_attention_layernorm.weight])[symbol:model.layers.8.post_attention_layernorm.weight]
            tensor.CPU.register () -> (%110:tensor<[6144, 2048], Float32, CPU>[@model.layers.8.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=364), symbol:model.layers.8.mlp.gate_proj.weight])[symbol:model.layers.8.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%236:tensor<[6144, 2048], Float32, CPU>[@model.layers.8.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=367), symbol:model.layers.8.mlp.up_proj.weight])[symbol:model.layers.8.mlp.up_proj.weight]
            tensor.CPU.register () -> (%106:tensor<[2048, 6144], Float32, CPU>[@model.layers.8.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=369), symbol:model.layers.8.mlp.down_proj.weight])[symbol:model.layers.8.mlp.down_proj.weight]
            tensor.CPU.register () -> (%178:tensor<[2048], Float32, CPU>[@model.layers.9.input_layernorm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=372), symbol:model.layers.9.input_layernorm.weight])[symbol:model.layers.9.input_layernorm.weight]
            tensor.CPU.register () -> (%235:tensor<[2048, 2048], Float32, CPU>[@model.layers.9.self_attn.q_proj.weight][symbol:model.layers.9.self_attn.q_proj.weight])[symbol:model.layers.9.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%69:tensor<[1024, 2048], Float32, CPU>[@model.layers.9.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=373), symbol:model.layers.9.self_attn.k_proj.weight])[symbol:model.layers.9.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%120:tensor<[1024, 2048], Float32, CPU>[@model.layers.9.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=375), symbol:model.layers.9.self_attn.v_proj.weight])[symbol:model.layers.9.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%140:tensor<[128], Float32, CPU>[@model.layers.9.self_attn.q_norm.weight][qnn_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=379), symbol:model.layers.9.self_attn.q_norm.weight])[symbol:model.layers.9.self_attn.q_norm.weight]
            tensor.CPU.register () -> (%29:tensor<[128], Float32, CPU>[@model.layers.9.self_attn.k_norm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=381), symbol:model.layers.9.self_attn.k_norm.weight])[symbol:model.layers.9.self_attn.k_norm.weight]
            tensor.CPU.register () -> (%205:tensor<[2048, 2048], Float32, CPU>[@model.layers.9.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=394), symbol:model.layers.9.self_attn.o_proj.weight])[symbol:model.layers.9.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%304:tensor<[2048], Float32, CPU>[@model.layers.9.post_attention_layernorm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=397), symbol:model.layers.9.post_attention_layernorm.weight])[symbol:model.layers.9.post_attention_layernorm.weight]
            tensor.CPU.register () -> (%263:tensor<[6144, 2048], Float32, CPU>[@model.layers.9.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=398), symbol:model.layers.9.mlp.gate_proj.weight])[symbol:model.layers.9.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%102:tensor<[6144, 2048], Float32, CPU>[@model.layers.9.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=401), symbol:model.layers.9.mlp.up_proj.weight])[symbol:model.layers.9.mlp.up_proj.weight]
            tensor.CPU.register () -> (%136:tensor<[2048, 6144], Float32, CPU>[@model.layers.9.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=403), symbol:model.layers.9.mlp.down_proj.weight])[symbol:model.layers.9.mlp.down_proj.weight]
            tensor.CPU.register () -> (%186:tensor<[2048], Float32, CPU>[@model.layers.10.input_layernorm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=406), symbol:model.layers.10.input_layernorm.weight])[symbol:model.layers.10.input_layernorm.weight]
            tensor.CPU.register () -> (%278:tensor<[2048, 2048], Float32, CPU>[@model.layers.10.self_attn.q_proj.weight][symbol:model.layers.10.self_attn.q_proj.weight])[symbol:model.layers.10.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%182:tensor<[1024, 2048], Float32, CPU>[@model.layers.10.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=407), symbol:model.layers.10.self_attn.k_proj.weight])[symbol:model.layers.10.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%138:tensor<[1024, 2048], Float32, CPU>[@model.layers.10.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=409), symbol:model.layers.10.self_attn.v_proj.weight])[symbol:model.layers.10.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%305:tensor<[128], Float32, CPU>[@model.layers.10.self_attn.q_norm.weight][qnn_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=413), symbol:model.layers.10.self_attn.q_norm.weight])[symbol:model.layers.10.self_attn.q_norm.weight]
            tensor.CPU.register () -> (%272:tensor<[128], Float32, CPU>[@model.layers.10.self_attn.k_norm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=415), symbol:model.layers.10.self_attn.k_norm.weight])[symbol:model.layers.10.self_attn.k_norm.weight]
            tensor.CPU.register () -> (%233:tensor<[2048, 2048], Float32, CPU>[@model.layers.10.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=428), symbol:model.layers.10.self_attn.o_proj.weight])[symbol:model.layers.10.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%266:tensor<[2048], Float32, CPU>[@model.layers.10.post_attention_layernorm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=431), symbol:model.layers.10.post_attention_layernorm.weight])[symbol:model.layers.10.post_attention_layernorm.weight]
            tensor.CPU.register () -> (%124:tensor<[6144, 2048], Float32, CPU>[@model.layers.10.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=432), symbol:model.layers.10.mlp.gate_proj.weight])[symbol:model.layers.10.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%261:tensor<[6144, 2048], Float32, CPU>[@model.layers.10.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=435), symbol:model.layers.10.mlp.up_proj.weight])[symbol:model.layers.10.mlp.up_proj.weight]
            tensor.CPU.register () -> (%45:tensor<[2048, 6144], Float32, CPU>[@model.layers.10.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=437), symbol:model.layers.10.mlp.down_proj.weight])[symbol:model.layers.10.mlp.down_proj.weight]
            tensor.CPU.register () -> (%219:tensor<[2048], Float32, CPU>[@model.layers.11.input_layernorm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=440), symbol:model.layers.11.input_layernorm.weight])[symbol:model.layers.11.input_layernorm.weight]
            tensor.CPU.register () -> (%274:tensor<[2048, 2048], Float32, CPU>[@model.layers.11.self_attn.q_proj.weight][symbol:model.layers.11.self_attn.q_proj.weight])[symbol:model.layers.11.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%157:tensor<[1024, 2048], Float32, CPU>[@model.layers.11.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=441), symbol:model.layers.11.self_attn.k_proj.weight])[symbol:model.layers.11.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%63:tensor<[1024, 2048], Float32, CPU>[@model.layers.11.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=443), symbol:model.layers.11.self_attn.v_proj.weight])[symbol:model.layers.11.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%214:tensor<[128], Float32, CPU>[@model.layers.11.self_attn.q_norm.weight][qnn_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=447), symbol:model.layers.11.self_attn.q_norm.weight])[symbol:model.layers.11.self_attn.q_norm.weight]
            tensor.CPU.register () -> (%201:tensor<[128], Float32, CPU>[@model.layers.11.self_attn.k_norm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=449), symbol:model.layers.11.self_attn.k_norm.weight])[symbol:model.layers.11.self_attn.k_norm.weight]
            tensor.CPU.register () -> (%118:tensor<[2048, 2048], Float32, CPU>[@model.layers.11.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=462), symbol:model.layers.11.self_attn.o_proj.weight])[symbol:model.layers.11.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%151:tensor<[2048], Float32, CPU>[@model.layers.11.post_attention_layernorm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=465), symbol:model.layers.11.post_attention_layernorm.weight])[symbol:model.layers.11.post_attention_layernorm.weight]
            tensor.CPU.register () -> (%207:tensor<[6144, 2048], Float32, CPU>[@model.layers.11.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=466), symbol:model.layers.11.mlp.gate_proj.weight])[symbol:model.layers.11.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%226:tensor<[6144, 2048], Float32, CPU>[@model.layers.11.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=469), symbol:model.layers.11.mlp.up_proj.weight])[symbol:model.layers.11.mlp.up_proj.weight]
            tensor.CPU.register () -> (%224:tensor<[2048, 6144], Float32, CPU>[@model.layers.11.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=471), symbol:model.layers.11.mlp.down_proj.weight])[symbol:model.layers.11.mlp.down_proj.weight]
            tensor.CPU.register () -> (%55:tensor<[2048], Float32, CPU>[@model.layers.12.input_layernorm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=474), symbol:model.layers.12.input_layernorm.weight])[symbol:model.layers.12.input_layernorm.weight]
            tensor.CPU.register () -> (%217:tensor<[2048, 2048], Float32, CPU>[@model.layers.12.self_attn.q_proj.weight][symbol:model.layers.12.self_attn.q_proj.weight])[symbol:model.layers.12.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%297:tensor<[1024, 2048], Float32, CPU>[@model.layers.12.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=475), symbol:model.layers.12.self_attn.k_proj.weight])[symbol:model.layers.12.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%94:tensor<[1024, 2048], Float32, CPU>[@model.layers.12.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=477), symbol:model.layers.12.self_attn.v_proj.weight])[symbol:model.layers.12.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%161:tensor<[128], Float32, CPU>[@model.layers.12.self_attn.q_norm.weight][qnn_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=481), symbol:model.layers.12.self_attn.q_norm.weight])[symbol:model.layers.12.self_attn.q_norm.weight]
            tensor.CPU.register () -> (%277:tensor<[128], Float32, CPU>[@model.layers.12.self_attn.k_norm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=483), symbol:model.layers.12.self_attn.k_norm.weight])[symbol:model.layers.12.self_attn.k_norm.weight]
            tensor.CPU.register () -> (%49:tensor<[2048, 2048], Float32, CPU>[@model.layers.12.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=496), symbol:model.layers.12.self_attn.o_proj.weight])[symbol:model.layers.12.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%14:tensor<[2048], Float32, CPU>[@model.layers.12.post_attention_layernorm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=499), symbol:model.layers.12.post_attention_layernorm.weight])[symbol:model.layers.12.post_attention_layernorm.weight]
            tensor.CPU.register () -> (%262:tensor<[6144, 2048], Float32, CPU>[@model.layers.12.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=500), symbol:model.layers.12.mlp.gate_proj.weight])[symbol:model.layers.12.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%255:tensor<[6144, 2048], Float32, CPU>[@model.layers.12.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=503), symbol:model.layers.12.mlp.up_proj.weight])[symbol:model.layers.12.mlp.up_proj.weight]
            tensor.CPU.register () -> (%22:tensor<[2048, 6144], Float32, CPU>[@model.layers.12.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=505), symbol:model.layers.12.mlp.down_proj.weight])[symbol:model.layers.12.mlp.down_proj.weight]
            tensor.CPU.register () -> (%212:tensor<[2048], Float32, CPU>[@model.layers.13.input_layernorm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=508), symbol:model.layers.13.input_layernorm.weight])[symbol:model.layers.13.input_layernorm.weight]
            tensor.CPU.register () -> (%114:tensor<[2048, 2048], Float32, CPU>[@model.layers.13.self_attn.q_proj.weight][symbol:model.layers.13.self_attn.q_proj.weight])[symbol:model.layers.13.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%152:tensor<[1024, 2048], Float32, CPU>[@model.layers.13.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=509), symbol:model.layers.13.self_attn.k_proj.weight])[symbol:model.layers.13.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%15:tensor<[1024, 2048], Float32, CPU>[@model.layers.13.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=511), symbol:model.layers.13.self_attn.v_proj.weight])[symbol:model.layers.13.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%307:tensor<[128], Float32, CPU>[@model.layers.13.self_attn.q_norm.weight][qnn_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=515), symbol:model.layers.13.self_attn.q_norm.weight])[symbol:model.layers.13.self_attn.q_norm.weight]
            tensor.CPU.register () -> (%30:tensor<[128], Float32, CPU>[@model.layers.13.self_attn.k_norm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=517), symbol:model.layers.13.self_attn.k_norm.weight])[symbol:model.layers.13.self_attn.k_norm.weight]
            tensor.CPU.register () -> (%250:tensor<[2048, 2048], Float32, CPU>[@model.layers.13.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=530), symbol:model.layers.13.self_attn.o_proj.weight])[symbol:model.layers.13.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%160:tensor<[2048], Float32, CPU>[@model.layers.13.post_attention_layernorm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=533), symbol:model.layers.13.post_attention_layernorm.weight])[symbol:model.layers.13.post_attention_layernorm.weight]
            tensor.CPU.register () -> (%247:tensor<[6144, 2048], Float32, CPU>[@model.layers.13.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=534), symbol:model.layers.13.mlp.gate_proj.weight])[symbol:model.layers.13.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%98:tensor<[6144, 2048], Float32, CPU>[@model.layers.13.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=537), symbol:model.layers.13.mlp.up_proj.weight])[symbol:model.layers.13.mlp.up_proj.weight]
            tensor.CPU.register () -> (%193:tensor<[2048, 6144], Float32, CPU>[@model.layers.13.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=539), symbol:model.layers.13.mlp.down_proj.weight])[symbol:model.layers.13.mlp.down_proj.weight]
            tensor.CPU.register () -> (%246:tensor<[2048], Float32, CPU>[@model.layers.14.input_layernorm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=542), symbol:model.layers.14.input_layernorm.weight])[symbol:model.layers.14.input_layernorm.weight]
            tensor.CPU.register () -> (%209:tensor<[2048, 2048], Float32, CPU>[@model.layers.14.self_attn.q_proj.weight][symbol:model.layers.14.self_attn.q_proj.weight])[symbol:model.layers.14.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%38:tensor<[1024, 2048], Float32, CPU>[@model.layers.14.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=543), symbol:model.layers.14.self_attn.k_proj.weight])[symbol:model.layers.14.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%232:tensor<[1024, 2048], Float32, CPU>[@model.layers.14.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=545), symbol:model.layers.14.self_attn.v_proj.weight])[symbol:model.layers.14.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%0:tensor<[128], Float32, CPU>[@model.layers.14.self_attn.q_norm.weight][qnn_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=549), symbol:model.layers.14.self_attn.q_norm.weight])[symbol:model.layers.14.self_attn.q_norm.weight]
            tensor.CPU.register () -> (%57:tensor<[128], Float32, CPU>[@model.layers.14.self_attn.k_norm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=551), symbol:model.layers.14.self_attn.k_norm.weight])[symbol:model.layers.14.self_attn.k_norm.weight]
            tensor.CPU.register () -> (%168:tensor<[2048, 2048], Float32, CPU>[@model.layers.14.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=564), symbol:model.layers.14.self_attn.o_proj.weight])[symbol:model.layers.14.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%75:tensor<[2048], Float32, CPU>[@model.layers.14.post_attention_layernorm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=567), symbol:model.layers.14.post_attention_layernorm.weight])[symbol:model.layers.14.post_attention_layernorm.weight]
            tensor.CPU.register () -> (%37:tensor<[6144, 2048], Float32, CPU>[@model.layers.14.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=568), symbol:model.layers.14.mlp.gate_proj.weight])[symbol:model.layers.14.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%147:tensor<[6144, 2048], Float32, CPU>[@model.layers.14.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=571), symbol:model.layers.14.mlp.up_proj.weight])[symbol:model.layers.14.mlp.up_proj.weight]
            tensor.CPU.register () -> (%163:tensor<[2048, 6144], Float32, CPU>[@model.layers.14.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=573), symbol:model.layers.14.mlp.down_proj.weight])[symbol:model.layers.14.mlp.down_proj.weight]
            tensor.CPU.register () -> (%67:tensor<[2048], Float32, CPU>[@model.layers.15.input_layernorm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=576), symbol:model.layers.15.input_layernorm.weight])[symbol:model.layers.15.input_layernorm.weight]
            tensor.CPU.register () -> (%46:tensor<[2048, 2048], Float32, CPU>[@model.layers.15.self_attn.q_proj.weight][symbol:model.layers.15.self_attn.q_proj.weight])[symbol:model.layers.15.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%268:tensor<[1024, 2048], Float32, CPU>[@model.layers.15.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=577), symbol:model.layers.15.self_attn.k_proj.weight])[symbol:model.layers.15.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%117:tensor<[1024, 2048], Float32, CPU>[@model.layers.15.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=579), symbol:model.layers.15.self_attn.v_proj.weight])[symbol:model.layers.15.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%213:tensor<[128], Float32, CPU>[@model.layers.15.self_attn.q_norm.weight][qnn_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=583), symbol:model.layers.15.self_attn.q_norm.weight])[symbol:model.layers.15.self_attn.q_norm.weight]
            tensor.CPU.register () -> (%100:tensor<[128], Float32, CPU>[@model.layers.15.self_attn.k_norm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=585), symbol:model.layers.15.self_attn.k_norm.weight])[symbol:model.layers.15.self_attn.k_norm.weight]
            tensor.CPU.register () -> (%303:tensor<[2048, 2048], Float32, CPU>[@model.layers.15.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=598), symbol:model.layers.15.self_attn.o_proj.weight])[symbol:model.layers.15.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%167:tensor<[2048], Float32, CPU>[@model.layers.15.post_attention_layernorm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=601), symbol:model.layers.15.post_attention_layernorm.weight])[symbol:model.layers.15.post_attention_layernorm.weight]
            tensor.CPU.register () -> (%260:tensor<[6144, 2048], Float32, CPU>[@model.layers.15.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=602), symbol:model.layers.15.mlp.gate_proj.weight])[symbol:model.layers.15.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%42:tensor<[6144, 2048], Float32, CPU>[@model.layers.15.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=605), symbol:model.layers.15.mlp.up_proj.weight])[symbol:model.layers.15.mlp.up_proj.weight]
            tensor.CPU.register () -> (%290:tensor<[2048, 6144], Float32, CPU>[@model.layers.15.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=607), symbol:model.layers.15.mlp.down_proj.weight])[symbol:model.layers.15.mlp.down_proj.weight]
            tensor.CPU.register () -> (%93:tensor<[2048], Float32, CPU>[@model.layers.16.input_layernorm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=610), symbol:model.layers.16.input_layernorm.weight])[symbol:model.layers.16.input_layernorm.weight]
            tensor.CPU.register () -> (%17:tensor<[2048, 2048], Float32, CPU>[@model.layers.16.self_attn.q_proj.weight][symbol:model.layers.16.self_attn.q_proj.weight])[symbol:model.layers.16.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%228:tensor<[1024, 2048], Float32, CPU>[@model.layers.16.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=611), symbol:model.layers.16.self_attn.k_proj.weight])[symbol:model.layers.16.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%66:tensor<[1024, 2048], Float32, CPU>[@model.layers.16.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=613), symbol:model.layers.16.self_attn.v_proj.weight])[symbol:model.layers.16.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%240:tensor<[128], Float32, CPU>[@model.layers.16.self_attn.q_norm.weight][qnn_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=617), symbol:model.layers.16.self_attn.q_norm.weight])[symbol:model.layers.16.self_attn.q_norm.weight]
            tensor.CPU.register () -> (%306:tensor<[128], Float32, CPU>[@model.layers.16.self_attn.k_norm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=619), symbol:model.layers.16.self_attn.k_norm.weight])[symbol:model.layers.16.self_attn.k_norm.weight]
            tensor.CPU.register () -> (%211:tensor<[2048, 2048], Float32, CPU>[@model.layers.16.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=632), symbol:model.layers.16.self_attn.o_proj.weight])[symbol:model.layers.16.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%210:tensor<[2048], Float32, CPU>[@model.layers.16.post_attention_layernorm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=635), symbol:model.layers.16.post_attention_layernorm.weight])[symbol:model.layers.16.post_attention_layernorm.weight]
            tensor.CPU.register () -> (%130:tensor<[6144, 2048], Float32, CPU>[@model.layers.16.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=636), symbol:model.layers.16.mlp.gate_proj.weight])[symbol:model.layers.16.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%79:tensor<[6144, 2048], Float32, CPU>[@model.layers.16.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=639), symbol:model.layers.16.mlp.up_proj.weight])[symbol:model.layers.16.mlp.up_proj.weight]
            tensor.CPU.register () -> (%248:tensor<[2048, 6144], Float32, CPU>[@model.layers.16.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=641), symbol:model.layers.16.mlp.down_proj.weight])[symbol:model.layers.16.mlp.down_proj.weight]
            tensor.CPU.register () -> (%231:tensor<[2048], Float32, CPU>[@model.layers.17.input_layernorm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=644), symbol:model.layers.17.input_layernorm.weight])[symbol:model.layers.17.input_layernorm.weight]
            tensor.CPU.register () -> (%64:tensor<[2048, 2048], Float32, CPU>[@model.layers.17.self_attn.q_proj.weight][symbol:model.layers.17.self_attn.q_proj.weight])[symbol:model.layers.17.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%237:tensor<[1024, 2048], Float32, CPU>[@model.layers.17.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=645), symbol:model.layers.17.self_attn.k_proj.weight])[symbol:model.layers.17.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%6:tensor<[1024, 2048], Float32, CPU>[@model.layers.17.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=647), symbol:model.layers.17.self_attn.v_proj.weight])[symbol:model.layers.17.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%222:tensor<[128], Float32, CPU>[@model.layers.17.self_attn.q_norm.weight][qnn_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=651), symbol:model.layers.17.self_attn.q_norm.weight])[symbol:model.layers.17.self_attn.q_norm.weight]
            tensor.CPU.register () -> (%191:tensor<[128], Float32, CPU>[@model.layers.17.self_attn.k_norm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=653), symbol:model.layers.17.self_attn.k_norm.weight])[symbol:model.layers.17.self_attn.k_norm.weight]
            tensor.CPU.register () -> (%125:tensor<[2048, 2048], Float32, CPU>[@model.layers.17.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=666), symbol:model.layers.17.self_attn.o_proj.weight])[symbol:model.layers.17.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%242:tensor<[2048], Float32, CPU>[@model.layers.17.post_attention_layernorm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=669), symbol:model.layers.17.post_attention_layernorm.weight])[symbol:model.layers.17.post_attention_layernorm.weight]
            tensor.CPU.register () -> (%177:tensor<[6144, 2048], Float32, CPU>[@model.layers.17.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=670), symbol:model.layers.17.mlp.gate_proj.weight])[symbol:model.layers.17.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%26:tensor<[6144, 2048], Float32, CPU>[@model.layers.17.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=673), symbol:model.layers.17.mlp.up_proj.weight])[symbol:model.layers.17.mlp.up_proj.weight]
            tensor.CPU.register () -> (%25:tensor<[2048, 6144], Float32, CPU>[@model.layers.17.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=675), symbol:model.layers.17.mlp.down_proj.weight])[symbol:model.layers.17.mlp.down_proj.weight]
            tensor.CPU.register () -> (%296:tensor<[2048], Float32, CPU>[@model.layers.18.input_layernorm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=678), symbol:model.layers.18.input_layernorm.weight])[symbol:model.layers.18.input_layernorm.weight]
            tensor.CPU.register () -> (%273:tensor<[2048, 2048], Float32, CPU>[@model.layers.18.self_attn.q_proj.weight][symbol:model.layers.18.self_attn.q_proj.weight])[symbol:model.layers.18.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%284:tensor<[1024, 2048], Float32, CPU>[@model.layers.18.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=679), symbol:model.layers.18.self_attn.k_proj.weight])[symbol:model.layers.18.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%18:tensor<[1024, 2048], Float32, CPU>[@model.layers.18.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=681), symbol:model.layers.18.self_attn.v_proj.weight])[symbol:model.layers.18.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%51:tensor<[128], Float32, CPU>[@model.layers.18.self_attn.q_norm.weight][qnn_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=685), symbol:model.layers.18.self_attn.q_norm.weight])[symbol:model.layers.18.self_attn.q_norm.weight]
            tensor.CPU.register () -> (%21:tensor<[128], Float32, CPU>[@model.layers.18.self_attn.k_norm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=687), symbol:model.layers.18.self_attn.k_norm.weight])[symbol:model.layers.18.self_attn.k_norm.weight]
            tensor.CPU.register () -> (%2:tensor<[2048, 2048], Float32, CPU>[@model.layers.18.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=700), symbol:model.layers.18.self_attn.o_proj.weight])[symbol:model.layers.18.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%10:tensor<[2048], Float32, CPU>[@model.layers.18.post_attention_layernorm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=703), symbol:model.layers.18.post_attention_layernorm.weight])[symbol:model.layers.18.post_attention_layernorm.weight]
            tensor.CPU.register () -> (%166:tensor<[6144, 2048], Float32, CPU>[@model.layers.18.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=704), symbol:model.layers.18.mlp.gate_proj.weight])[symbol:model.layers.18.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%271:tensor<[6144, 2048], Float32, CPU>[@model.layers.18.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=707), symbol:model.layers.18.mlp.up_proj.weight])[symbol:model.layers.18.mlp.up_proj.weight]
            tensor.CPU.register () -> (%112:tensor<[2048, 6144], Float32, CPU>[@model.layers.18.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=709), symbol:model.layers.18.mlp.down_proj.weight])[symbol:model.layers.18.mlp.down_proj.weight]
            tensor.CPU.register () -> (%113:tensor<[2048], Float32, CPU>[@model.layers.19.input_layernorm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=712), symbol:model.layers.19.input_layernorm.weight])[symbol:model.layers.19.input_layernorm.weight]
            tensor.CPU.register () -> (%8:tensor<[2048, 2048], Float32, CPU>[@model.layers.19.self_attn.q_proj.weight][symbol:model.layers.19.self_attn.q_proj.weight])[symbol:model.layers.19.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%286:tensor<[1024, 2048], Float32, CPU>[@model.layers.19.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=713), symbol:model.layers.19.self_attn.k_proj.weight])[symbol:model.layers.19.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%50:tensor<[1024, 2048], Float32, CPU>[@model.layers.19.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=715), symbol:model.layers.19.self_attn.v_proj.weight])[symbol:model.layers.19.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%116:tensor<[128], Float32, CPU>[@model.layers.19.self_attn.q_norm.weight][qnn_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=719), symbol:model.layers.19.self_attn.q_norm.weight])[symbol:model.layers.19.self_attn.q_norm.weight]
            tensor.CPU.register () -> (%84:tensor<[128], Float32, CPU>[@model.layers.19.self_attn.k_norm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=721), symbol:model.layers.19.self_attn.k_norm.weight])[symbol:model.layers.19.self_attn.k_norm.weight]
            tensor.CPU.register () -> (%58:tensor<[2048, 2048], Float32, CPU>[@model.layers.19.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=734), symbol:model.layers.19.self_attn.o_proj.weight])[symbol:model.layers.19.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%95:tensor<[2048], Float32, CPU>[@model.layers.19.post_attention_layernorm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=737), symbol:model.layers.19.post_attention_layernorm.weight])[symbol:model.layers.19.post_attention_layernorm.weight]
            tensor.CPU.register () -> (%281:tensor<[6144, 2048], Float32, CPU>[@model.layers.19.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=738), symbol:model.layers.19.mlp.gate_proj.weight])[symbol:model.layers.19.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%82:tensor<[6144, 2048], Float32, CPU>[@model.layers.19.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=741), symbol:model.layers.19.mlp.up_proj.weight])[symbol:model.layers.19.mlp.up_proj.weight]
            tensor.CPU.register () -> (%173:tensor<[2048, 6144], Float32, CPU>[@model.layers.19.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=743), symbol:model.layers.19.mlp.down_proj.weight])[symbol:model.layers.19.mlp.down_proj.weight]
            tensor.CPU.register () -> (%203:tensor<[2048], Float32, CPU>[@model.layers.20.input_layernorm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=746), symbol:model.layers.20.input_layernorm.weight])[symbol:model.layers.20.input_layernorm.weight]
            tensor.CPU.register () -> (%280:tensor<[2048, 2048], Float32, CPU>[@model.layers.20.self_attn.q_proj.weight][symbol:model.layers.20.self_attn.q_proj.weight])[symbol:model.layers.20.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%253:tensor<[1024, 2048], Float32, CPU>[@model.layers.20.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=747), symbol:model.layers.20.self_attn.k_proj.weight])[symbol:model.layers.20.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%239:tensor<[1024, 2048], Float32, CPU>[@model.layers.20.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=749), symbol:model.layers.20.self_attn.v_proj.weight])[symbol:model.layers.20.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%143:tensor<[128], Float32, CPU>[@model.layers.20.self_attn.q_norm.weight][qnn_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=753), symbol:model.layers.20.self_attn.q_norm.weight])[symbol:model.layers.20.self_attn.q_norm.weight]
            tensor.CPU.register () -> (%288:tensor<[128], Float32, CPU>[@model.layers.20.self_attn.k_norm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=755), symbol:model.layers.20.self_attn.k_norm.weight])[symbol:model.layers.20.self_attn.k_norm.weight]
            tensor.CPU.register () -> (%41:tensor<[2048, 2048], Float32, CPU>[@model.layers.20.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=768), symbol:model.layers.20.self_attn.o_proj.weight])[symbol:model.layers.20.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%216:tensor<[2048], Float32, CPU>[@model.layers.20.post_attention_layernorm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=771), symbol:model.layers.20.post_attention_layernorm.weight])[symbol:model.layers.20.post_attention_layernorm.weight]
            tensor.CPU.register () -> (%172:tensor<[6144, 2048], Float32, CPU>[@model.layers.20.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=772), symbol:model.layers.20.mlp.gate_proj.weight])[symbol:model.layers.20.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%299:tensor<[6144, 2048], Float32, CPU>[@model.layers.20.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=775), symbol:model.layers.20.mlp.up_proj.weight])[symbol:model.layers.20.mlp.up_proj.weight]
            tensor.CPU.register () -> (%123:tensor<[2048, 6144], Float32, CPU>[@model.layers.20.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=777), symbol:model.layers.20.mlp.down_proj.weight])[symbol:model.layers.20.mlp.down_proj.weight]
            tensor.CPU.register () -> (%229:tensor<[2048], Float32, CPU>[@model.layers.21.input_layernorm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=780), symbol:model.layers.21.input_layernorm.weight])[symbol:model.layers.21.input_layernorm.weight]
            tensor.CPU.register () -> (%295:tensor<[2048, 2048], Float32, CPU>[@model.layers.21.self_attn.q_proj.weight][symbol:model.layers.21.self_attn.q_proj.weight])[symbol:model.layers.21.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%139:tensor<[1024, 2048], Float32, CPU>[@model.layers.21.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=781), symbol:model.layers.21.self_attn.k_proj.weight])[symbol:model.layers.21.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%142:tensor<[1024, 2048], Float32, CPU>[@model.layers.21.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=783), symbol:model.layers.21.self_attn.v_proj.weight])[symbol:model.layers.21.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%87:tensor<[128], Float32, CPU>[@model.layers.21.self_attn.q_norm.weight][qnn_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=787), symbol:model.layers.21.self_attn.q_norm.weight])[symbol:model.layers.21.self_attn.q_norm.weight]
            tensor.CPU.register () -> (%56:tensor<[128], Float32, CPU>[@model.layers.21.self_attn.k_norm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=789), symbol:model.layers.21.self_attn.k_norm.weight])[symbol:model.layers.21.self_attn.k_norm.weight]
            tensor.CPU.register () -> (%115:tensor<[2048, 2048], Float32, CPU>[@model.layers.21.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=802), symbol:model.layers.21.self_attn.o_proj.weight])[symbol:model.layers.21.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%174:tensor<[2048], Float32, CPU>[@model.layers.21.post_attention_layernorm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=805), symbol:model.layers.21.post_attention_layernorm.weight])[symbol:model.layers.21.post_attention_layernorm.weight]
            tensor.CPU.register () -> (%259:tensor<[6144, 2048], Float32, CPU>[@model.layers.21.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=806), symbol:model.layers.21.mlp.gate_proj.weight])[symbol:model.layers.21.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%162:tensor<[6144, 2048], Float32, CPU>[@model.layers.21.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=809), symbol:model.layers.21.mlp.up_proj.weight])[symbol:model.layers.21.mlp.up_proj.weight]
            tensor.CPU.register () -> (%183:tensor<[2048, 6144], Float32, CPU>[@model.layers.21.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=811), symbol:model.layers.21.mlp.down_proj.weight])[symbol:model.layers.21.mlp.down_proj.weight]
            tensor.CPU.register () -> (%257:tensor<[2048], Float32, CPU>[@model.layers.22.input_layernorm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=814), symbol:model.layers.22.input_layernorm.weight])[symbol:model.layers.22.input_layernorm.weight]
            tensor.CPU.register () -> (%89:tensor<[2048, 2048], Float32, CPU>[@model.layers.22.self_attn.q_proj.weight][symbol:model.layers.22.self_attn.q_proj.weight])[symbol:model.layers.22.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%36:tensor<[1024, 2048], Float32, CPU>[@model.layers.22.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=815), symbol:model.layers.22.self_attn.k_proj.weight])[symbol:model.layers.22.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%204:tensor<[1024, 2048], Float32, CPU>[@model.layers.22.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=817), symbol:model.layers.22.self_attn.v_proj.weight])[symbol:model.layers.22.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%158:tensor<[128], Float32, CPU>[@model.layers.22.self_attn.q_norm.weight][qnn_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=821), symbol:model.layers.22.self_attn.q_norm.weight])[symbol:model.layers.22.self_attn.q_norm.weight]
            tensor.CPU.register () -> (%215:tensor<[128], Float32, CPU>[@model.layers.22.self_attn.k_norm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=823), symbol:model.layers.22.self_attn.k_norm.weight])[symbol:model.layers.22.self_attn.k_norm.weight]
            tensor.CPU.register () -> (%234:tensor<[2048, 2048], Float32, CPU>[@model.layers.22.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=836), symbol:model.layers.22.self_attn.o_proj.weight])[symbol:model.layers.22.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%270:tensor<[2048], Float32, CPU>[@model.layers.22.post_attention_layernorm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=839), symbol:model.layers.22.post_attention_layernorm.weight])[symbol:model.layers.22.post_attention_layernorm.weight]
            tensor.CPU.register () -> (%198:tensor<[6144, 2048], Float32, CPU>[@model.layers.22.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=840), symbol:model.layers.22.mlp.gate_proj.weight])[symbol:model.layers.22.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%254:tensor<[6144, 2048], Float32, CPU>[@model.layers.22.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=843), symbol:model.layers.22.mlp.up_proj.weight])[symbol:model.layers.22.mlp.up_proj.weight]
            tensor.CPU.register () -> (%31:tensor<[2048, 6144], Float32, CPU>[@model.layers.22.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=845), symbol:model.layers.22.mlp.down_proj.weight])[symbol:model.layers.22.mlp.down_proj.weight]
            tensor.CPU.register () -> (%292:tensor<[2048], Float32, CPU>[@model.layers.23.input_layernorm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=848), symbol:model.layers.23.input_layernorm.weight])[symbol:model.layers.23.input_layernorm.weight]
            tensor.CPU.register () -> (%109:tensor<[2048, 2048], Float32, CPU>[@model.layers.23.self_attn.q_proj.weight][symbol:model.layers.23.self_attn.q_proj.weight])[symbol:model.layers.23.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%39:tensor<[1024, 2048], Float32, CPU>[@model.layers.23.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=849), symbol:model.layers.23.self_attn.k_proj.weight])[symbol:model.layers.23.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%83:tensor<[1024, 2048], Float32, CPU>[@model.layers.23.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=851), symbol:model.layers.23.self_attn.v_proj.weight])[symbol:model.layers.23.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%293:tensor<[128], Float32, CPU>[@model.layers.23.self_attn.q_norm.weight][qnn_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=855), symbol:model.layers.23.self_attn.q_norm.weight])[symbol:model.layers.23.self_attn.q_norm.weight]
            tensor.CPU.register () -> (%134:tensor<[128], Float32, CPU>[@model.layers.23.self_attn.k_norm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=857), symbol:model.layers.23.self_attn.k_norm.weight])[symbol:model.layers.23.self_attn.k_norm.weight]
            tensor.CPU.register () -> (%176:tensor<[2048, 2048], Float32, CPU>[@model.layers.23.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=870), symbol:model.layers.23.self_attn.o_proj.weight])[symbol:model.layers.23.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%170:tensor<[2048], Float32, CPU>[@model.layers.23.post_attention_layernorm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=873), symbol:model.layers.23.post_attention_layernorm.weight])[symbol:model.layers.23.post_attention_layernorm.weight]
            tensor.CPU.register () -> (%169:tensor<[6144, 2048], Float32, CPU>[@model.layers.23.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=874), symbol:model.layers.23.mlp.gate_proj.weight])[symbol:model.layers.23.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%243:tensor<[6144, 2048], Float32, CPU>[@model.layers.23.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=877), symbol:model.layers.23.mlp.up_proj.weight])[symbol:model.layers.23.mlp.up_proj.weight]
            tensor.CPU.register () -> (%149:tensor<[2048, 6144], Float32, CPU>[@model.layers.23.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=879), symbol:model.layers.23.mlp.down_proj.weight])[symbol:model.layers.23.mlp.down_proj.weight]
            tensor.CPU.register () -> (%13:tensor<[2048], Float32, CPU>[@model.layers.24.input_layernorm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=882), symbol:model.layers.24.input_layernorm.weight])[symbol:model.layers.24.input_layernorm.weight]
            tensor.CPU.register () -> (%11:tensor<[2048, 2048], Float32, CPU>[@model.layers.24.self_attn.q_proj.weight][symbol:model.layers.24.self_attn.q_proj.weight])[symbol:model.layers.24.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%61:tensor<[1024, 2048], Float32, CPU>[@model.layers.24.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=883), symbol:model.layers.24.self_attn.k_proj.weight])[symbol:model.layers.24.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%81:tensor<[1024, 2048], Float32, CPU>[@model.layers.24.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=885), symbol:model.layers.24.self_attn.v_proj.weight])[symbol:model.layers.24.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%90:tensor<[128], Float32, CPU>[@model.layers.24.self_attn.q_norm.weight][qnn_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=889), symbol:model.layers.24.self_attn.q_norm.weight])[symbol:model.layers.24.self_attn.q_norm.weight]
            tensor.CPU.register () -> (%19:tensor<[128], Float32, CPU>[@model.layers.24.self_attn.k_norm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=891), symbol:model.layers.24.self_attn.k_norm.weight])[symbol:model.layers.24.self_attn.k_norm.weight]
            tensor.CPU.register () -> (%127:tensor<[2048, 2048], Float32, CPU>[@model.layers.24.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=904), symbol:model.layers.24.self_attn.o_proj.weight])[symbol:model.layers.24.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%77:tensor<[2048], Float32, CPU>[@model.layers.24.post_attention_layernorm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=907), symbol:model.layers.24.post_attention_layernorm.weight])[symbol:model.layers.24.post_attention_layernorm.weight]
            tensor.CPU.register () -> (%141:tensor<[6144, 2048], Float32, CPU>[@model.layers.24.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=908), symbol:model.layers.24.mlp.gate_proj.weight])[symbol:model.layers.24.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%126:tensor<[6144, 2048], Float32, CPU>[@model.layers.24.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=911), symbol:model.layers.24.mlp.up_proj.weight])[symbol:model.layers.24.mlp.up_proj.weight]
            tensor.CPU.register () -> (%34:tensor<[2048, 6144], Float32, CPU>[@model.layers.24.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=913), symbol:model.layers.24.mlp.down_proj.weight])[symbol:model.layers.24.mlp.down_proj.weight]
            tensor.CPU.register () -> (%196:tensor<[2048], Float32, CPU>[@model.layers.25.input_layernorm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=916), symbol:model.layers.25.input_layernorm.weight])[symbol:model.layers.25.input_layernorm.weight]
            tensor.CPU.register () -> (%206:tensor<[2048, 2048], Float32, CPU>[@model.layers.25.self_attn.q_proj.weight][symbol:model.layers.25.self_attn.q_proj.weight])[symbol:model.layers.25.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%27:tensor<[1024, 2048], Float32, CPU>[@model.layers.25.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=917), symbol:model.layers.25.self_attn.k_proj.weight])[symbol:model.layers.25.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%121:tensor<[1024, 2048], Float32, CPU>[@model.layers.25.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=919), symbol:model.layers.25.self_attn.v_proj.weight])[symbol:model.layers.25.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%310:tensor<[128], Float32, CPU>[@model.layers.25.self_attn.q_norm.weight][qnn_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=923), symbol:model.layers.25.self_attn.q_norm.weight])[symbol:model.layers.25.self_attn.q_norm.weight]
            tensor.CPU.register () -> (%187:tensor<[128], Float32, CPU>[@model.layers.25.self_attn.k_norm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=925), symbol:model.layers.25.self_attn.k_norm.weight])[symbol:model.layers.25.self_attn.k_norm.weight]
            tensor.CPU.register () -> (%150:tensor<[2048, 2048], Float32, CPU>[@model.layers.25.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=938), symbol:model.layers.25.self_attn.o_proj.weight])[symbol:model.layers.25.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%175:tensor<[2048], Float32, CPU>[@model.layers.25.post_attention_layernorm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=941), symbol:model.layers.25.post_attention_layernorm.weight])[symbol:model.layers.25.post_attention_layernorm.weight]
            tensor.CPU.register () -> (%249:tensor<[6144, 2048], Float32, CPU>[@model.layers.25.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=942), symbol:model.layers.25.mlp.gate_proj.weight])[symbol:model.layers.25.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%159:tensor<[6144, 2048], Float32, CPU>[@model.layers.25.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=945), symbol:model.layers.25.mlp.up_proj.weight])[symbol:model.layers.25.mlp.up_proj.weight]
            tensor.CPU.register () -> (%267:tensor<[2048, 6144], Float32, CPU>[@model.layers.25.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=947), symbol:model.layers.25.mlp.down_proj.weight])[symbol:model.layers.25.mlp.down_proj.weight]
            tensor.CPU.register () -> (%302:tensor<[2048], Float32, CPU>[@model.layers.26.input_layernorm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=950), symbol:model.layers.26.input_layernorm.weight])[symbol:model.layers.26.input_layernorm.weight]
            tensor.CPU.register () -> (%265:tensor<[2048, 2048], Float32, CPU>[@model.layers.26.self_attn.q_proj.weight][symbol:model.layers.26.self_attn.q_proj.weight])[symbol:model.layers.26.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%190:tensor<[1024, 2048], Float32, CPU>[@model.layers.26.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=951), symbol:model.layers.26.self_attn.k_proj.weight])[symbol:model.layers.26.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%119:tensor<[1024, 2048], Float32, CPU>[@model.layers.26.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=953), symbol:model.layers.26.self_attn.v_proj.weight])[symbol:model.layers.26.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%70:tensor<[128], Float32, CPU>[@model.layers.26.self_attn.q_norm.weight][qnn_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=957), symbol:model.layers.26.self_attn.q_norm.weight])[symbol:model.layers.26.self_attn.q_norm.weight]
            tensor.CPU.register () -> (%35:tensor<[128], Float32, CPU>[@model.layers.26.self_attn.k_norm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=959), symbol:model.layers.26.self_attn.k_norm.weight])[symbol:model.layers.26.self_attn.k_norm.weight]
            tensor.CPU.register () -> (%88:tensor<[2048, 2048], Float32, CPU>[@model.layers.26.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=972), symbol:model.layers.26.self_attn.o_proj.weight])[symbol:model.layers.26.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%298:tensor<[2048], Float32, CPU>[@model.layers.26.post_attention_layernorm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=975), symbol:model.layers.26.post_attention_layernorm.weight])[symbol:model.layers.26.post_attention_layernorm.weight]
            tensor.CPU.register () -> (%96:tensor<[6144, 2048], Float32, CPU>[@model.layers.26.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=976), symbol:model.layers.26.mlp.gate_proj.weight])[symbol:model.layers.26.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%62:tensor<[6144, 2048], Float32, CPU>[@model.layers.26.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=979), symbol:model.layers.26.mlp.up_proj.weight])[symbol:model.layers.26.mlp.up_proj.weight]
            tensor.CPU.register () -> (%220:tensor<[2048, 6144], Float32, CPU>[@model.layers.26.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=981), symbol:model.layers.26.mlp.down_proj.weight])[symbol:model.layers.26.mlp.down_proj.weight]
            tensor.CPU.register () -> (%44:tensor<[2048], Float32, CPU>[@model.layers.27.input_layernorm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=984), symbol:model.layers.27.input_layernorm.weight])[symbol:model.layers.27.input_layernorm.weight]
            tensor.CPU.register () -> (%185:tensor<[2048, 2048], Float32, CPU>[@model.layers.27.self_attn.q_proj.weight][symbol:model.layers.27.self_attn.q_proj.weight])[symbol:model.layers.27.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%12:tensor<[1024, 2048], Float32, CPU>[@model.layers.27.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=985), symbol:model.layers.27.self_attn.k_proj.weight])[symbol:model.layers.27.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%54:tensor<[1024, 2048], Float32, CPU>[@model.layers.27.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=987), symbol:model.layers.27.self_attn.v_proj.weight])[symbol:model.layers.27.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%192:tensor<[128], Float32, CPU>[@model.layers.27.self_attn.q_norm.weight][qnn_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=991), symbol:model.layers.27.self_attn.q_norm.weight])[symbol:model.layers.27.self_attn.q_norm.weight]
            tensor.CPU.register () -> (%241:tensor<[128], Float32, CPU>[@model.layers.27.self_attn.k_norm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=993), symbol:model.layers.27.self_attn.k_norm.weight])[symbol:model.layers.27.self_attn.k_norm.weight]
            tensor.CPU.register () -> (%60:tensor<[2048, 2048], Float32, CPU>[@model.layers.27.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=1006), symbol:model.layers.27.self_attn.o_proj.weight])[symbol:model.layers.27.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%104:tensor<[2048], Float32, CPU>[@model.layers.27.post_attention_layernorm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1009), symbol:model.layers.27.post_attention_layernorm.weight])[symbol:model.layers.27.post_attention_layernorm.weight]
            tensor.CPU.register () -> (%144:tensor<[6144, 2048], Float32, CPU>[@model.layers.27.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=1010), symbol:model.layers.27.mlp.gate_proj.weight])[symbol:model.layers.27.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%146:tensor<[6144, 2048], Float32, CPU>[@model.layers.27.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=1013), symbol:model.layers.27.mlp.up_proj.weight])[symbol:model.layers.27.mlp.up_proj.weight]
            tensor.CPU.register () -> (%195:tensor<[2048, 6144], Float32, CPU>[@model.layers.27.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=1015), symbol:model.layers.27.mlp.down_proj.weight])[symbol:model.layers.27.mlp.down_proj.weight]
            tensor.CPU.register () -> (%5:tensor<[2048], Float32, CPU>[@model.norm.weight][qnn_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1018), symbol:model.norm.weight])[symbol:model.norm.weight]
            tensor.CPU.register () -> (%101:tensor<[151936, 2048], Float32, CPU>[@lm_head.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=1019), symbol:lm_head.weight])[symbol:lm_head.weight]
        }
    }
    graph.SubGraphOp @deinit <notype> [symbol:deinit] {
        () -> () {
            
        }
    }
    graph.CallGraphOp @model (%318:tensor<[1, 32], Int64, CPU>[quant_recipe:QuantSpec(Raw(type: Int64), uuid=0)], %376:tensor<[1, 32], Int64, CPU>[quant_recipe:QuantSpec(Raw(type: Int64), uuid=1)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %320:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=3)], %322:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=4)], %324:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=5)], %326:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=6)], %328:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=7)], %330:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=8)], %332:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=9)], %334:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=10)], %336:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=11)], %338:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=12)], %340:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=13)], %342:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=14)], %344:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=15)], %346:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=16)], %348:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=17)], %350:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=18)], %352:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=19)], %354:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=20)], %356:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=21)], %358:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=22)], %360:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=23)], %362:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=24)], %364:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=25)], %366:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=26)], %368:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=27)], %370:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=28)], %372:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=29)], %374:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=30)], %321:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=31)], %323:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=32)], %325:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=33)], %327:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=34)], %329:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=35)], %331:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=36)], %333:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=37)], %335:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=38)], %337:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=39)], %339:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=40)], %341:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=41)], %343:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=42)], %345:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=43)], %347:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=44)], %349:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=45)], %351:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=46)], %353:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=47)], %355:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=48)], %357:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=49)], %359:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=50)], %361:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=51)], %363:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=52)], %365:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=53)], %367:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=54)], %369:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=55)], %371:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=56)], %373:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=57)], %375:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=58)]) -> (%1530:tensor<[1, 32, 151936], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1020)], %394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=77)], %435:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=111)], %476:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=145)], %517:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=179)], %558:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=213)], %599:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=247)], %640:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=281)], %681:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=315)], %722:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=349)], %763:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=383)], %804:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=417)], %845:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=451)], %886:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=485)], %927:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=519)], %968:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=553)], %1009:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=587)], %1050:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=621)], %1091:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=655)], %1132:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=689)], %1173:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=723)], %1214:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=757)], %1255:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=791)], %1296:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=825)], %1337:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=859)], %1378:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=893)], %1419:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=927)], %1460:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=961)], %1501:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=995)], %396:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=79)], %437:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=113)], %478:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=147)], %519:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=181)], %560:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=215)], %601:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=249)], %642:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=283)], %683:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=317)], %724:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=351)], %765:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=385)], %806:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=419)], %847:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=453)], %888:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=487)], %929:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=521)], %970:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=555)], %1011:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=589)], %1052:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=623)], %1093:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=657)], %1134:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=691)], %1175:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=725)], %1216:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=759)], %1257:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=793)], %1298:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=827)], %1339:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=861)], %1380:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=895)], %1421:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=929)], %1462:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=963)], %1503:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=997)])
    graph.SubGraphOp @model <CPU> [using_qnn:true, symbol:model] {
        (%318:tensor<[1, 32], Int64, CPU>[quant_recipe:QuantSpec(Raw(type: Int64), uuid=0)], %376:tensor<[1, 32], Int64, CPU>[quant_recipe:QuantSpec(Raw(type: Int64), uuid=1)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %320:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=3)], %322:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=4)], %324:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=5)], %326:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=6)], %328:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=7)], %330:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=8)], %332:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=9)], %334:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=10)], %336:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=11)], %338:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=12)], %340:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=13)], %342:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=14)], %344:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=15)], %346:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=16)], %348:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=17)], %350:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=18)], %352:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=19)], %354:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=20)], %356:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=21)], %358:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=22)], %360:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=23)], %362:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=24)], %364:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=25)], %366:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=26)], %368:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=27)], %370:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=28)], %372:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=29)], %374:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=30)], %321:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=31)], %323:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=32)], %325:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=33)], %327:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=34)], %329:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=35)], %331:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=36)], %333:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=37)], %335:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=38)], %337:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=39)], %339:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=40)], %341:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=41)], %343:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=42)], %345:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=43)], %347:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=44)], %349:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=45)], %351:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=46)], %353:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=47)], %355:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=48)], %357:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=49)], %359:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=50)], %361:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=51)], %363:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=52)], %365:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=53)], %367:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=54)], %369:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=55)], %371:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=56)], %373:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=57)], %375:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=58)]) -> (%1530:tensor<[1, 32, 151936], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1020)], %394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=77)], %435:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=111)], %476:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=145)], %517:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=179)], %558:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=213)], %599:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=247)], %640:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=281)], %681:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=315)], %722:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=349)], %763:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=383)], %804:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=417)], %845:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=451)], %886:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=485)], %927:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=519)], %968:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=553)], %1009:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=587)], %1050:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=621)], %1091:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=655)], %1132:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=689)], %1173:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=723)], %1214:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=757)], %1255:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=791)], %1296:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=825)], %1337:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=859)], %1378:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=893)], %1419:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=927)], %1460:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=961)], %1501:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=995)], %396:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=79)], %437:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=113)], %478:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=147)], %519:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=181)], %560:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=215)], %601:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=249)], %642:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=283)], %683:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=317)], %724:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=351)], %765:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=385)], %806:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=419)], %847:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=453)], %888:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=487)], %929:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=521)], %970:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=555)], %1011:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=589)], %1052:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=623)], %1093:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=657)], %1134:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=691)], %1175:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=725)], %1216:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=759)], %1257:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=793)], %1298:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=827)], %1339:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=861)], %1380:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=895)], %1421:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=929)], %1462:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=963)], %1503:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=997)]) {
            linalg.CPU.EmbeddingOp <name="model.embed_tokens">(%318:tensor<[1, 32], Int64, CPU>[quant_recipe:QuantSpec(Raw(type: Int64), uuid=0)]) -> (%377:tensor<[1, 32, 2048], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=59)])
            linalg.CPU.CastTypeOp <name="model.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float32), uuid=59), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=60), )] (%377:tensor<[1, 32, 2048], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=59)]) -> (%378:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=60)])
            linalg.CPU.ViewOp <name="model.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int64), uuid=1), outputs_0:QuantSpec(Raw(type: Int64), uuid=1), )] (%376:tensor<[1, 32], Int64, CPU>[quant_recipe:QuantSpec(Raw(type: Int64), uuid=1)]) -> (%376:tensor<[32], Int64, CPU>[quant_recipe:QuantSpec(Raw(type: Int64), uuid=1)])
            linalg.CPU.IndexOp <name="model.Index.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=61), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), )] (%316:tensor<[1, 1024, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=61)]) -> (%379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)])
            linalg.CPU.IndexOp <name="model.Index.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), )] (%317:tensor<[1, 1024, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)]) -> (%380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)])
            graph.CallGraphOp @model.layers.0 (%378:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=60)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %320:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=3)], %321:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=31)]) -> (%421:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=98)], %394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=77)], %396:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=79)])
            graph.CallGraphOp @model.layers.1 (%421:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=98)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %322:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=4)], %323:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=32)]) -> (%462:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=132)], %435:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=111)], %437:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=113)])
            graph.CallGraphOp @model.layers.2 (%462:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=132)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %324:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=5)], %325:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=33)]) -> (%503:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=166)], %476:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=145)], %478:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=147)])
            graph.CallGraphOp @model.layers.3 (%503:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=166)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %326:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=6)], %327:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=34)]) -> (%544:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=200)], %517:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=179)], %519:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=181)])
            graph.CallGraphOp @model.layers.4 (%544:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=200)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %328:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=7)], %329:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=35)]) -> (%585:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=234)], %558:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=213)], %560:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=215)])
            graph.CallGraphOp @model.layers.5 (%585:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=234)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %330:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=8)], %331:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=36)]) -> (%626:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=268)], %599:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=247)], %601:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=249)])
            graph.CallGraphOp @model.layers.6 (%626:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=268)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %332:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=9)], %333:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=37)]) -> (%667:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=302)], %640:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=281)], %642:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=283)])
            graph.CallGraphOp @model.layers.7 (%667:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=302)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %334:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=10)], %335:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=38)]) -> (%708:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=336)], %681:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=315)], %683:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=317)])
            graph.CallGraphOp @model.layers.8 (%708:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=336)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %336:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=11)], %337:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=39)]) -> (%749:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=370)], %722:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=349)], %724:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=351)])
            graph.CallGraphOp @model.layers.9 (%749:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=370)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %338:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=12)], %339:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=40)]) -> (%790:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=404)], %763:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=383)], %765:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=385)])
            graph.CallGraphOp @model.layers.10 (%790:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=404)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %340:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=13)], %341:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=41)]) -> (%831:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=438)], %804:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=417)], %806:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=419)])
            graph.CallGraphOp @model.layers.11 (%831:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=438)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %342:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=14)], %343:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=42)]) -> (%872:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=472)], %845:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=451)], %847:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=453)])
            graph.CallGraphOp @model.layers.12 (%872:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=472)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %344:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=15)], %345:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=43)]) -> (%913:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=506)], %886:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=485)], %888:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=487)])
            graph.CallGraphOp @model.layers.13 (%913:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=506)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %346:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=16)], %347:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=44)]) -> (%954:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=540)], %927:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=519)], %929:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=521)])
            graph.CallGraphOp @model.layers.14 (%954:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=540)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %348:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=17)], %349:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=45)]) -> (%995:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=574)], %968:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=553)], %970:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=555)])
            graph.CallGraphOp @model.layers.15 (%995:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=574)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %350:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=18)], %351:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=46)]) -> (%1036:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=608)], %1009:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=587)], %1011:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=589)])
            graph.CallGraphOp @model.layers.16 (%1036:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=608)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %352:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=19)], %353:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=47)]) -> (%1077:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=642)], %1050:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=621)], %1052:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=623)])
            graph.CallGraphOp @model.layers.17 (%1077:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=642)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %354:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=20)], %355:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=48)]) -> (%1118:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=676)], %1091:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=655)], %1093:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=657)])
            graph.CallGraphOp @model.layers.18 (%1118:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=676)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %356:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=21)], %357:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=49)]) -> (%1159:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=710)], %1132:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=689)], %1134:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=691)])
            graph.CallGraphOp @model.layers.19 (%1159:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=710)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %358:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=22)], %359:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=50)]) -> (%1200:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=744)], %1173:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=723)], %1175:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=725)])
            graph.CallGraphOp @model.layers.20 (%1200:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=744)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %360:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=23)], %361:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=51)]) -> (%1241:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=778)], %1214:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=757)], %1216:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=759)])
            graph.CallGraphOp @model.layers.21 (%1241:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=778)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %362:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=24)], %363:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=52)]) -> (%1282:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=812)], %1255:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=791)], %1257:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=793)])
            graph.CallGraphOp @model.layers.22 (%1282:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=812)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %364:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=25)], %365:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=53)]) -> (%1323:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=846)], %1296:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=825)], %1298:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=827)])
            graph.CallGraphOp @model.layers.23 (%1323:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=846)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %366:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=26)], %367:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=54)]) -> (%1364:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=880)], %1337:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=859)], %1339:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=861)])
            graph.CallGraphOp @model.layers.24 (%1364:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=880)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %368:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=27)], %369:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=55)]) -> (%1405:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=914)], %1378:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=893)], %1380:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=895)])
            graph.CallGraphOp @model.layers.25 (%1405:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=914)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %370:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=28)], %371:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=56)]) -> (%1446:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=948)], %1419:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=927)], %1421:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=929)])
            graph.CallGraphOp @model.layers.26 (%1446:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=948)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %372:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=29)], %373:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=57)]) -> (%1487:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=982)], %1460:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=961)], %1462:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=963)])
            graph.CallGraphOp @model.layers.27 (%1487:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=982)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %374:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=30)], %375:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=58)]) -> (%1528:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1016)], %1501:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=995)], %1503:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=997)])
            linalg.CPU.RMSNormOp <name="model.norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1016), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1017), )] (%1528:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1016)]) -> (%1529:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1017)])
            linalg.CPU.LinearOp <name="lm_head"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1017), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1020), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=1019)), using_qnn:true] (%1529:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1017)]) -> (%1530:tensor<[1, 32, 151936], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1020)])
            cf.ReturnOp (%1530:tensor<[1, 32, 151936], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1020)], %394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=77)], %435:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=111)], %476:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=145)], %517:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=179)], %558:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=213)], %599:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=247)], %640:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=281)], %681:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=315)], %722:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=349)], %763:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=383)], %804:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=417)], %845:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=451)], %886:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=485)], %927:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=519)], %968:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=553)], %1009:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=587)], %1050:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=621)], %1091:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=655)], %1132:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=689)], %1173:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=723)], %1214:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=757)], %1255:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=791)], %1296:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=825)], %1337:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=859)], %1378:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=893)], %1419:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=927)], %1460:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=961)], %1501:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=995)], %396:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=79)], %437:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=113)], %478:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=147)], %519:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=181)], %560:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=215)], %601:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=249)], %642:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=283)], %683:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=317)], %724:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=351)], %765:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=385)], %806:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=419)], %847:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=453)], %888:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=487)], %929:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=521)], %970:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=555)], %1011:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=589)], %1052:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=623)], %1093:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=657)], %1134:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=691)], %1175:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=725)], %1216:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=759)], %1257:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=793)], %1298:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=827)], %1339:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=861)], %1380:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=895)], %1421:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=929)], %1462:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=963)], %1503:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=997)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.0 <CPU> [using_qnn:true, symbol:model.layers.0] {
        (%378:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=60)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %320:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=3)], %321:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=31)]) -> (%421:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=98)], %394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=77)], %396:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=79)]) {
            linalg.CPU.RMSNormOp <name="model.layers.0.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=60), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), )] (%378:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=60)]) -> (%381:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)])
            graph.CallGraphOp @model.layers.0.self_attn (%381:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %320:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=3)], %321:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=31)]) -> (%413:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=89)], %394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=77)], %396:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=79)])
            linalg.CPU.AddOp <name="model.layers.0.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=89), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=60), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=89), )] (%413:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=89)], %378:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=60)]) -> (%414:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=89)])
            linalg.CPU.RMSNormOp <name="model.layers.0.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=89), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=90), )] (%414:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=89)]) -> (%415:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=90)])
            graph.CallGraphOp @model.layers.0.mlp (%415:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=90)]) -> (%420:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=98)])
            linalg.CPU.AddOp <name="model.layers.0.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=98), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=89), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=98), )] (%420:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=98)], %414:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=89)]) -> (%421:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=98)])
            cf.ReturnOp (%421:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=98)], %394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=77)], %396:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=79)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.0.self_attn <CPU> [using_qnn:true, symbol:model.layers.0.self_attn] {
        (%381:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %320:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=3)], %321:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=31)]) -> (%413:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=89)], %394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=77)], %396:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=79)]) {
            linalg.CPU.LinearOp <name="model.layers.0.self_attn.q_proj">(%381:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%382:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=71)])
            linalg.CPU.LinearOp <name="model.layers.0.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=68), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=67))] (%381:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%383:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=68)])
            linalg.CPU.LinearOp <name="model.layers.0.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=70), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=69))] (%381:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%384:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=70)])
            linalg.CPU.ViewOp <name="model.layers.0.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=71), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=71), )] (%382:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=71)]) -> (%382:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=71)])
            linalg.CPU.TransposeOp <name="model.layers.0.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=71), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=71), )] (%382:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=71)]) -> (%385:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=71)])
            linalg.CPU.ViewOp <name="model.layers.0.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=68), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=68), )] (%383:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=68)]) -> (%383:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=68)])
            linalg.CPU.TransposeOp <name="model.layers.0.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=68), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=68), )] (%383:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=68)]) -> (%386:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=68)])
            linalg.CPU.ViewOp <name="model.layers.0.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=70), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=70), )] (%384:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=70)]) -> (%384:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=70)])
            linalg.CPU.TransposeOp <name="model.layers.0.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=70), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=70), )] (%384:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=70)]) -> (%387:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=70)])
            linalg.CPU.RMSNormOp <name="model.layers.0.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=71), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=72), )] (%385:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=71)]) -> (%388:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=72)])
            linalg.CPU.RMSNormOp <name="model.layers.0.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=68), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=74), )] (%386:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=68)]) -> (%389:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=74)])
            linalg.CPU.RoPEOp <name="model.layers.0.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=72), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=72), )] (%388:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=72)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%390:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=72)])
            linalg.CPU.RoPEOp <name="model.layers.0.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=74), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=74), )] (%389:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=74)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%391:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=74)])
            linalg.CPU.CastTypeOp <name="model.layers.0.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=74), outputs_0:QuantSpec(Raw(type: Float16), uuid=76), )] (%391:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=74)]) -> (%392:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=76)])
            linalg.CPU.CastTypeOp <name="model.layers.0.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=76), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=77), )] (%392:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=76)]) -> (%393:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=77)])
            linalg.CPU.TransposeOp <name="model.layers.0.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=77), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=77), )] (%393:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=77)]) -> (%394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=77)])
            linalg.CPU.CastTypeOp <name="model.layers.0.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=70), outputs_0:QuantSpec(Raw(type: Float16), uuid=78), )] (%387:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=70)]) -> (%395:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=78)])
            linalg.CPU.CastTypeOp <name="model.layers.0.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=78), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=79), )] (%395:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=78)]) -> (%396:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=79)])
            linalg.CPU.ConcatOp <name="model.layers.0.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=3), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=77), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=3), )] (%320:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=3)], %394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=77)]) -> (%397:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=3)])
            linalg.CPU.ConcatOp <name="model.layers.0.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=31), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=79), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=31), )] (%321:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=31)], %396:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=79)]) -> (%398:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=31)])
            linalg.CPU.RepeatOp <name="model.layers.0.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=3), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=3), )] (%397:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=3)]) -> (%399:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=3)])
            linalg.CPU.RepeatOp <name="model.layers.0.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=31), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=31), )] (%398:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=31)]) -> (%400:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=31)])
            linalg.CPU.MatMulOp <name="model.layers.0.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=72), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=3), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=80), )] (%390:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=72)], %399:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=3)]) -> (%401:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=80)])
            linalg.CPU.MulOp <name="model.layers.0.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=80), inputs_1:QuantSpec(Raw(type: Float32), uuid=81), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=80), )] (%401:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=80)], %402:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=81), constant:[0.088388346]]) -> (%403:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=80)])
            linalg.CPU.ReduceMinOp <name="model.layers.0.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=80), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=82), )] (%403:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=80)]) -> (%404:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=82)])
            linalg.CPU.AddOp <name="model.layers.0.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=82), inputs_1:QuantSpec(Raw(type: Int16), uuid=83), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=82), )] (%404:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=82)], %405:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=83), constant:[-20]]) -> (%406:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=82)])
            linalg.CPU.EqualOp <name="model.layers.0.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=84), outputs_0:QuantSpec(Raw(type: UInt8), uuid=85), )] (%319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %407:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=84), constant:[0]]) -> (%408:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=85)])
            linalg.CPU.WhereOp <name="model.layers.0.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=85), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=80), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=82), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=82), )] (%408:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=85)], %403:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=80)], %406:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=82)]) -> (%409:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=82)])
            linalg.CPU.SoftmaxOp <name="model.layers.0.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=82), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=86), )] (%409:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=82)]) -> (%410:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=86)])
            linalg.CPU.MatMulOp <name="model.layers.0.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=86), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=31), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=87), )] (%410:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=86)], %400:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=31)]) -> (%411:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=87)])
            linalg.CPU.TransposeOp <name="model.layers.0.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=87), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=87), )] (%411:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=87)]) -> (%412:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=87)])
            linalg.CPU.ViewOp <name="model.layers.0.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=87), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=87), )] (%412:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=87)]) -> (%412:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=87)])
            linalg.CPU.LinearOp <name="model.layers.0.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=87), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=89), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=88))] (%412:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=87)]) -> (%413:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=89)])
            cf.ReturnOp (%413:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=89)], %394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=77)], %396:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=79)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.0.mlp <CPU> [using_qnn:true, symbol:model.layers.0.mlp] {
        (%415:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=90)]) -> (%420:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=98)]) {
            linalg.CPU.LinearOp <name="model.layers.0.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=90), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=93), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=92))] (%415:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=90)]) -> (%416:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=93)])
            linalg.CPU.SiLUOp <name="model.layers.0.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=93), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=94), )] (%416:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=93)]) -> (%417:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=94)])
            linalg.CPU.LinearOp <name="model.layers.0.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=90), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=96), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=95))] (%415:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=90)]) -> (%418:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=96)])
            linalg.CPU.MulOp <name="model.layers.0.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=94), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=96), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=94), )] (%417:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=94)], %418:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=96)]) -> (%419:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=94)])
            linalg.CPU.LinearOp <name="model.layers.0.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=94), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=98), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=97))] (%419:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=94)]) -> (%420:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=98)])
            cf.ReturnOp (%420:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=98)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.1 <CPU> [using_qnn:true, symbol:model.layers.1] {
        (%421:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=98)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %322:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=4)], %323:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=32)]) -> (%462:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=132)], %435:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=111)], %437:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=113)]) {
            linalg.CPU.RMSNormOp <name="model.layers.1.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=98), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=99), )] (%421:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=98)]) -> (%422:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=99)])
            graph.CallGraphOp @model.layers.1.self_attn (%422:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=99)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %322:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=4)], %323:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=32)]) -> (%454:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=123)], %435:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=111)], %437:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=113)])
            linalg.CPU.AddOp <name="model.layers.1.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=123), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=98), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=123), )] (%454:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=123)], %421:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=98)]) -> (%455:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=123)])
            linalg.CPU.RMSNormOp <name="model.layers.1.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=123), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=124), )] (%455:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=123)]) -> (%456:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=124)])
            graph.CallGraphOp @model.layers.1.mlp (%456:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=124)]) -> (%461:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=132)])
            linalg.CPU.AddOp <name="model.layers.1.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=132), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=123), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=132), )] (%461:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=132)], %455:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=123)]) -> (%462:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=132)])
            cf.ReturnOp (%462:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=132)], %435:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=111)], %437:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=113)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.1.self_attn <CPU> [using_qnn:true, symbol:model.layers.1.self_attn] {
        (%422:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=99)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %322:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=4)], %323:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=32)]) -> (%454:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=123)], %435:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=111)], %437:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=113)]) {
            linalg.CPU.LinearOp <name="model.layers.1.self_attn.q_proj">(%422:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=99)]) -> (%423:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=105)])
            linalg.CPU.LinearOp <name="model.layers.1.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=99), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=102), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=101))] (%422:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=99)]) -> (%424:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=102)])
            linalg.CPU.LinearOp <name="model.layers.1.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=99), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=104), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=103))] (%422:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=99)]) -> (%425:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=104)])
            linalg.CPU.ViewOp <name="model.layers.1.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=105), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=105), )] (%423:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=105)]) -> (%423:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=105)])
            linalg.CPU.TransposeOp <name="model.layers.1.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=105), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=105), )] (%423:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=105)]) -> (%426:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=105)])
            linalg.CPU.ViewOp <name="model.layers.1.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=102), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=102), )] (%424:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=102)]) -> (%424:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=102)])
            linalg.CPU.TransposeOp <name="model.layers.1.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=102), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=102), )] (%424:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=102)]) -> (%427:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=102)])
            linalg.CPU.ViewOp <name="model.layers.1.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=104), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=104), )] (%425:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=104)]) -> (%425:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=104)])
            linalg.CPU.TransposeOp <name="model.layers.1.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=104), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=104), )] (%425:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=104)]) -> (%428:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=104)])
            linalg.CPU.RMSNormOp <name="model.layers.1.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=105), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=106), )] (%426:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=105)]) -> (%429:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=106)])
            linalg.CPU.RMSNormOp <name="model.layers.1.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=102), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=108), )] (%427:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=102)]) -> (%430:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=108)])
            linalg.CPU.RoPEOp <name="model.layers.1.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=106), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=106), )] (%429:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=106)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%431:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=106)])
            linalg.CPU.RoPEOp <name="model.layers.1.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=108), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=108), )] (%430:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=108)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%432:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=108)])
            linalg.CPU.CastTypeOp <name="model.layers.1.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=108), outputs_0:QuantSpec(Raw(type: Float16), uuid=110), )] (%432:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=108)]) -> (%433:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=110)])
            linalg.CPU.CastTypeOp <name="model.layers.1.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=110), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=111), )] (%433:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=110)]) -> (%434:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=111)])
            linalg.CPU.TransposeOp <name="model.layers.1.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=111), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=111), )] (%434:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=111)]) -> (%435:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=111)])
            linalg.CPU.CastTypeOp <name="model.layers.1.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=104), outputs_0:QuantSpec(Raw(type: Float16), uuid=112), )] (%428:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=104)]) -> (%436:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=112)])
            linalg.CPU.CastTypeOp <name="model.layers.1.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=112), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=113), )] (%436:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=112)]) -> (%437:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=113)])
            linalg.CPU.ConcatOp <name="model.layers.1.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=4), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=111), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=4), )] (%322:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=4)], %435:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=111)]) -> (%438:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=4)])
            linalg.CPU.ConcatOp <name="model.layers.1.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=32), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=113), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=32), )] (%323:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=32)], %437:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=113)]) -> (%439:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=32)])
            linalg.CPU.RepeatOp <name="model.layers.1.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=4), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=4), )] (%438:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=4)]) -> (%440:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=4)])
            linalg.CPU.RepeatOp <name="model.layers.1.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=32), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=32), )] (%439:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=32)]) -> (%441:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=32)])
            linalg.CPU.MatMulOp <name="model.layers.1.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=106), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=4), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=114), )] (%431:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=106)], %440:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=4)]) -> (%442:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=114)])
            linalg.CPU.MulOp <name="model.layers.1.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=114), inputs_1:QuantSpec(Raw(type: Float32), uuid=115), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=114), )] (%442:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=114)], %443:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=115), constant:[0.088388346]]) -> (%444:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=114)])
            linalg.CPU.ReduceMinOp <name="model.layers.1.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=114), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=116), )] (%444:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=114)]) -> (%445:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=116)])
            linalg.CPU.AddOp <name="model.layers.1.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=116), inputs_1:QuantSpec(Raw(type: Int16), uuid=117), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=116), )] (%445:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=116)], %446:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=117), constant:[-20]]) -> (%447:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=116)])
            linalg.CPU.EqualOp <name="model.layers.1.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=118), outputs_0:QuantSpec(Raw(type: UInt8), uuid=119), )] (%319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %448:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=118), constant:[0]]) -> (%449:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=119)])
            linalg.CPU.WhereOp <name="model.layers.1.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=119), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=114), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=116), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=116), )] (%449:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=119)], %444:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=114)], %447:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=116)]) -> (%450:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=116)])
            linalg.CPU.SoftmaxOp <name="model.layers.1.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=116), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=120), )] (%450:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=116)]) -> (%451:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=120)])
            linalg.CPU.MatMulOp <name="model.layers.1.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=120), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=32), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=121), )] (%451:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=120)], %441:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=32)]) -> (%452:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=121)])
            linalg.CPU.TransposeOp <name="model.layers.1.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=121), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=121), )] (%452:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=121)]) -> (%453:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=121)])
            linalg.CPU.ViewOp <name="model.layers.1.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=121), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=121), )] (%453:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=121)]) -> (%453:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=121)])
            linalg.CPU.LinearOp <name="model.layers.1.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=121), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=123), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=122))] (%453:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=121)]) -> (%454:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=123)])
            cf.ReturnOp (%454:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=123)], %435:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=111)], %437:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=113)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.1.mlp <CPU> [using_qnn:true, symbol:model.layers.1.mlp] {
        (%456:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=124)]) -> (%461:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=132)]) {
            linalg.CPU.LinearOp <name="model.layers.1.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=124), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=127), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=126))] (%456:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=124)]) -> (%457:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=127)])
            linalg.CPU.SiLUOp <name="model.layers.1.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=127), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=128), )] (%457:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=127)]) -> (%458:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=128)])
            linalg.CPU.LinearOp <name="model.layers.1.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=124), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=130), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=129))] (%456:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=124)]) -> (%459:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=130)])
            linalg.CPU.MulOp <name="model.layers.1.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=128), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=130), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=128), )] (%458:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=128)], %459:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=130)]) -> (%460:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=128)])
            linalg.CPU.LinearOp <name="model.layers.1.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=128), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=132), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=131))] (%460:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=128)]) -> (%461:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=132)])
            cf.ReturnOp (%461:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=132)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.2 <CPU> [using_qnn:true, symbol:model.layers.2] {
        (%462:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=132)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %324:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=5)], %325:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=33)]) -> (%503:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=166)], %476:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=145)], %478:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=147)]) {
            linalg.CPU.RMSNormOp <name="model.layers.2.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=132), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=133), )] (%462:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=132)]) -> (%463:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=133)])
            graph.CallGraphOp @model.layers.2.self_attn (%463:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=133)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %324:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=5)], %325:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=33)]) -> (%495:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=157)], %476:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=145)], %478:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=147)])
            linalg.CPU.AddOp <name="model.layers.2.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=157), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=132), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=157), )] (%495:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=157)], %462:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=132)]) -> (%496:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=157)])
            linalg.CPU.RMSNormOp <name="model.layers.2.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=157), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=158), )] (%496:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=157)]) -> (%497:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=158)])
            graph.CallGraphOp @model.layers.2.mlp (%497:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=158)]) -> (%502:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=166)])
            linalg.CPU.AddOp <name="model.layers.2.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=166), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=157), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=166), )] (%502:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=166)], %496:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=157)]) -> (%503:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=166)])
            cf.ReturnOp (%503:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=166)], %476:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=145)], %478:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=147)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.2.self_attn <CPU> [using_qnn:true, symbol:model.layers.2.self_attn] {
        (%463:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=133)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %324:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=5)], %325:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=33)]) -> (%495:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=157)], %476:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=145)], %478:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=147)]) {
            linalg.CPU.LinearOp <name="model.layers.2.self_attn.q_proj">(%463:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=133)]) -> (%464:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=139)])
            linalg.CPU.LinearOp <name="model.layers.2.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=133), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=136), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=135))] (%463:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=133)]) -> (%465:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=136)])
            linalg.CPU.LinearOp <name="model.layers.2.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=133), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=138), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=137))] (%463:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=133)]) -> (%466:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=138)])
            linalg.CPU.ViewOp <name="model.layers.2.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=139), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=139), )] (%464:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=139)]) -> (%464:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=139)])
            linalg.CPU.TransposeOp <name="model.layers.2.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=139), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=139), )] (%464:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=139)]) -> (%467:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=139)])
            linalg.CPU.ViewOp <name="model.layers.2.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=136), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=136), )] (%465:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=136)]) -> (%465:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=136)])
            linalg.CPU.TransposeOp <name="model.layers.2.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=136), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=136), )] (%465:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=136)]) -> (%468:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=136)])
            linalg.CPU.ViewOp <name="model.layers.2.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=138), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=138), )] (%466:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=138)]) -> (%466:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=138)])
            linalg.CPU.TransposeOp <name="model.layers.2.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=138), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=138), )] (%466:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=138)]) -> (%469:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=138)])
            linalg.CPU.RMSNormOp <name="model.layers.2.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=139), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=140), )] (%467:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=139)]) -> (%470:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=140)])
            linalg.CPU.RMSNormOp <name="model.layers.2.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=136), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=142), )] (%468:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=136)]) -> (%471:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=142)])
            linalg.CPU.RoPEOp <name="model.layers.2.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=140), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=140), )] (%470:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=140)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%472:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=140)])
            linalg.CPU.RoPEOp <name="model.layers.2.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=142), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=142), )] (%471:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=142)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%473:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=142)])
            linalg.CPU.CastTypeOp <name="model.layers.2.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=142), outputs_0:QuantSpec(Raw(type: Float16), uuid=144), )] (%473:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=142)]) -> (%474:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=144)])
            linalg.CPU.CastTypeOp <name="model.layers.2.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=144), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=145), )] (%474:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=144)]) -> (%475:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=145)])
            linalg.CPU.TransposeOp <name="model.layers.2.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=145), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=145), )] (%475:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=145)]) -> (%476:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=145)])
            linalg.CPU.CastTypeOp <name="model.layers.2.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=138), outputs_0:QuantSpec(Raw(type: Float16), uuid=146), )] (%469:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=138)]) -> (%477:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=146)])
            linalg.CPU.CastTypeOp <name="model.layers.2.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=146), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=147), )] (%477:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=146)]) -> (%478:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=147)])
            linalg.CPU.ConcatOp <name="model.layers.2.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=5), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=145), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=5), )] (%324:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=5)], %476:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=145)]) -> (%479:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=5)])
            linalg.CPU.ConcatOp <name="model.layers.2.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=33), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=147), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=33), )] (%325:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=33)], %478:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=147)]) -> (%480:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=33)])
            linalg.CPU.RepeatOp <name="model.layers.2.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=5), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=5), )] (%479:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=5)]) -> (%481:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=5)])
            linalg.CPU.RepeatOp <name="model.layers.2.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=33), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=33), )] (%480:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=33)]) -> (%482:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=33)])
            linalg.CPU.MatMulOp <name="model.layers.2.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=140), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=5), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=148), )] (%472:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=140)], %481:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=5)]) -> (%483:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=148)])
            linalg.CPU.MulOp <name="model.layers.2.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=148), inputs_1:QuantSpec(Raw(type: Float32), uuid=149), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=148), )] (%483:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=148)], %484:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=149), constant:[0.088388346]]) -> (%485:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=148)])
            linalg.CPU.ReduceMinOp <name="model.layers.2.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=148), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=150), )] (%485:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=148)]) -> (%486:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=150)])
            linalg.CPU.AddOp <name="model.layers.2.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=150), inputs_1:QuantSpec(Raw(type: Int16), uuid=151), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=150), )] (%486:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=150)], %487:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=151), constant:[-20]]) -> (%488:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=150)])
            linalg.CPU.EqualOp <name="model.layers.2.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=152), outputs_0:QuantSpec(Raw(type: UInt8), uuid=153), )] (%319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %489:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=152), constant:[0]]) -> (%490:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=153)])
            linalg.CPU.WhereOp <name="model.layers.2.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=153), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=148), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=150), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=150), )] (%490:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=153)], %485:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=148)], %488:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=150)]) -> (%491:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=150)])
            linalg.CPU.SoftmaxOp <name="model.layers.2.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=150), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=154), )] (%491:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=150)]) -> (%492:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=154)])
            linalg.CPU.MatMulOp <name="model.layers.2.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=154), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=33), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=155), )] (%492:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=154)], %482:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=33)]) -> (%493:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=155)])
            linalg.CPU.TransposeOp <name="model.layers.2.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=155), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=155), )] (%493:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=155)]) -> (%494:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=155)])
            linalg.CPU.ViewOp <name="model.layers.2.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=155), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=155), )] (%494:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=155)]) -> (%494:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=155)])
            linalg.CPU.LinearOp <name="model.layers.2.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=155), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=157), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=156))] (%494:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=155)]) -> (%495:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=157)])
            cf.ReturnOp (%495:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=157)], %476:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=145)], %478:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=147)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.2.mlp <CPU> [using_qnn:true, symbol:model.layers.2.mlp] {
        (%497:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=158)]) -> (%502:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=166)]) {
            linalg.CPU.LinearOp <name="model.layers.2.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=158), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=161), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=160))] (%497:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=158)]) -> (%498:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=161)])
            linalg.CPU.SiLUOp <name="model.layers.2.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=161), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=162), )] (%498:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=161)]) -> (%499:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=162)])
            linalg.CPU.LinearOp <name="model.layers.2.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=158), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=164), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=163))] (%497:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=158)]) -> (%500:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=164)])
            linalg.CPU.MulOp <name="model.layers.2.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=162), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=164), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=162), )] (%499:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=162)], %500:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=164)]) -> (%501:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=162)])
            linalg.CPU.LinearOp <name="model.layers.2.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=162), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=166), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=165))] (%501:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=162)]) -> (%502:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=166)])
            cf.ReturnOp (%502:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=166)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.3 <CPU> [using_qnn:true, symbol:model.layers.3] {
        (%503:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=166)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %326:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=6)], %327:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=34)]) -> (%544:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=200)], %517:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=179)], %519:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=181)]) {
            linalg.CPU.RMSNormOp <name="model.layers.3.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=166), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=167), )] (%503:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=166)]) -> (%504:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=167)])
            graph.CallGraphOp @model.layers.3.self_attn (%504:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=167)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %326:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=6)], %327:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=34)]) -> (%536:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=191)], %517:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=179)], %519:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=181)])
            linalg.CPU.AddOp <name="model.layers.3.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=191), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=166), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=191), )] (%536:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=191)], %503:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=166)]) -> (%537:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=191)])
            linalg.CPU.RMSNormOp <name="model.layers.3.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=191), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=192), )] (%537:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=191)]) -> (%538:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=192)])
            graph.CallGraphOp @model.layers.3.mlp (%538:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=192)]) -> (%543:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=200)])
            linalg.CPU.AddOp <name="model.layers.3.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=200), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=191), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=200), )] (%543:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=200)], %537:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=191)]) -> (%544:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=200)])
            cf.ReturnOp (%544:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=200)], %517:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=179)], %519:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=181)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.3.self_attn <CPU> [using_qnn:true, symbol:model.layers.3.self_attn] {
        (%504:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=167)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %326:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=6)], %327:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=34)]) -> (%536:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=191)], %517:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=179)], %519:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=181)]) {
            linalg.CPU.LinearOp <name="model.layers.3.self_attn.q_proj">(%504:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=167)]) -> (%505:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=173)])
            linalg.CPU.LinearOp <name="model.layers.3.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=167), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=170), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=169))] (%504:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=167)]) -> (%506:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=170)])
            linalg.CPU.LinearOp <name="model.layers.3.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=167), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=172), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=171))] (%504:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=167)]) -> (%507:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=172)])
            linalg.CPU.ViewOp <name="model.layers.3.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=173), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=173), )] (%505:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=173)]) -> (%505:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=173)])
            linalg.CPU.TransposeOp <name="model.layers.3.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=173), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=173), )] (%505:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=173)]) -> (%508:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=173)])
            linalg.CPU.ViewOp <name="model.layers.3.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=170), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=170), )] (%506:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=170)]) -> (%506:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=170)])
            linalg.CPU.TransposeOp <name="model.layers.3.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=170), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=170), )] (%506:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=170)]) -> (%509:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=170)])
            linalg.CPU.ViewOp <name="model.layers.3.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=172), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=172), )] (%507:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=172)]) -> (%507:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=172)])
            linalg.CPU.TransposeOp <name="model.layers.3.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=172), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=172), )] (%507:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=172)]) -> (%510:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=172)])
            linalg.CPU.RMSNormOp <name="model.layers.3.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=173), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=174), )] (%508:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=173)]) -> (%511:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=174)])
            linalg.CPU.RMSNormOp <name="model.layers.3.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=170), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=176), )] (%509:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=170)]) -> (%512:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=176)])
            linalg.CPU.RoPEOp <name="model.layers.3.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=174), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=174), )] (%511:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=174)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%513:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=174)])
            linalg.CPU.RoPEOp <name="model.layers.3.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=176), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=176), )] (%512:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=176)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%514:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=176)])
            linalg.CPU.CastTypeOp <name="model.layers.3.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=176), outputs_0:QuantSpec(Raw(type: Float16), uuid=178), )] (%514:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=176)]) -> (%515:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=178)])
            linalg.CPU.CastTypeOp <name="model.layers.3.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=178), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=179), )] (%515:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=178)]) -> (%516:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=179)])
            linalg.CPU.TransposeOp <name="model.layers.3.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=179), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=179), )] (%516:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=179)]) -> (%517:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=179)])
            linalg.CPU.CastTypeOp <name="model.layers.3.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=172), outputs_0:QuantSpec(Raw(type: Float16), uuid=180), )] (%510:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=172)]) -> (%518:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=180)])
            linalg.CPU.CastTypeOp <name="model.layers.3.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=180), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=181), )] (%518:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=180)]) -> (%519:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=181)])
            linalg.CPU.ConcatOp <name="model.layers.3.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=6), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=179), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=6), )] (%326:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=6)], %517:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=179)]) -> (%520:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=6)])
            linalg.CPU.ConcatOp <name="model.layers.3.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=34), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=181), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=34), )] (%327:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=34)], %519:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=181)]) -> (%521:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=34)])
            linalg.CPU.RepeatOp <name="model.layers.3.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=6), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=6), )] (%520:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=6)]) -> (%522:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=6)])
            linalg.CPU.RepeatOp <name="model.layers.3.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=34), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=34), )] (%521:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=34)]) -> (%523:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=34)])
            linalg.CPU.MatMulOp <name="model.layers.3.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=174), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=6), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=182), )] (%513:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=174)], %522:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=6)]) -> (%524:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=182)])
            linalg.CPU.MulOp <name="model.layers.3.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=182), inputs_1:QuantSpec(Raw(type: Float32), uuid=183), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=182), )] (%524:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=182)], %525:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=183), constant:[0.088388346]]) -> (%526:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=182)])
            linalg.CPU.ReduceMinOp <name="model.layers.3.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=182), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=184), )] (%526:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=182)]) -> (%527:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=184)])
            linalg.CPU.AddOp <name="model.layers.3.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=184), inputs_1:QuantSpec(Raw(type: Int16), uuid=185), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=184), )] (%527:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=184)], %528:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=185), constant:[-20]]) -> (%529:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=184)])
            linalg.CPU.EqualOp <name="model.layers.3.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=186), outputs_0:QuantSpec(Raw(type: UInt8), uuid=187), )] (%319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %530:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=186), constant:[0]]) -> (%531:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=187)])
            linalg.CPU.WhereOp <name="model.layers.3.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=187), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=182), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=184), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=184), )] (%531:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=187)], %526:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=182)], %529:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=184)]) -> (%532:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=184)])
            linalg.CPU.SoftmaxOp <name="model.layers.3.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=184), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=188), )] (%532:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=184)]) -> (%533:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=188)])
            linalg.CPU.MatMulOp <name="model.layers.3.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=188), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=34), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=189), )] (%533:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=188)], %523:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=34)]) -> (%534:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=189)])
            linalg.CPU.TransposeOp <name="model.layers.3.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=189), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=189), )] (%534:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=189)]) -> (%535:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=189)])
            linalg.CPU.ViewOp <name="model.layers.3.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=189), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=189), )] (%535:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=189)]) -> (%535:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=189)])
            linalg.CPU.LinearOp <name="model.layers.3.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=189), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=191), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=190))] (%535:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=189)]) -> (%536:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=191)])
            cf.ReturnOp (%536:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=191)], %517:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=179)], %519:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=181)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.3.mlp <CPU> [using_qnn:true, symbol:model.layers.3.mlp] {
        (%538:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=192)]) -> (%543:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=200)]) {
            linalg.CPU.LinearOp <name="model.layers.3.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=192), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=195), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=194))] (%538:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=192)]) -> (%539:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=195)])
            linalg.CPU.SiLUOp <name="model.layers.3.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=195), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=196), )] (%539:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=195)]) -> (%540:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=196)])
            linalg.CPU.LinearOp <name="model.layers.3.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=192), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=198), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=197))] (%538:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=192)]) -> (%541:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=198)])
            linalg.CPU.MulOp <name="model.layers.3.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=196), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=198), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=196), )] (%540:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=196)], %541:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=198)]) -> (%542:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=196)])
            linalg.CPU.LinearOp <name="model.layers.3.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=196), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=200), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=199))] (%542:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=196)]) -> (%543:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=200)])
            cf.ReturnOp (%543:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=200)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.4 <CPU> [using_qnn:true, symbol:model.layers.4] {
        (%544:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=200)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %328:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=7)], %329:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=35)]) -> (%585:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=234)], %558:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=213)], %560:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=215)]) {
            linalg.CPU.RMSNormOp <name="model.layers.4.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=200), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=201), )] (%544:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=200)]) -> (%545:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=201)])
            graph.CallGraphOp @model.layers.4.self_attn (%545:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=201)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %328:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=7)], %329:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=35)]) -> (%577:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=225)], %558:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=213)], %560:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=215)])
            linalg.CPU.AddOp <name="model.layers.4.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=225), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=200), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=225), )] (%577:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=225)], %544:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=200)]) -> (%578:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=225)])
            linalg.CPU.RMSNormOp <name="model.layers.4.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=225), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=226), )] (%578:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=225)]) -> (%579:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=226)])
            graph.CallGraphOp @model.layers.4.mlp (%579:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=226)]) -> (%584:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=234)])
            linalg.CPU.AddOp <name="model.layers.4.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=234), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=225), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=234), )] (%584:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=234)], %578:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=225)]) -> (%585:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=234)])
            cf.ReturnOp (%585:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=234)], %558:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=213)], %560:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=215)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.4.self_attn <CPU> [using_qnn:true, symbol:model.layers.4.self_attn] {
        (%545:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=201)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %328:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=7)], %329:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=35)]) -> (%577:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=225)], %558:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=213)], %560:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=215)]) {
            linalg.CPU.LinearOp <name="model.layers.4.self_attn.q_proj">(%545:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=201)]) -> (%546:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=207)])
            linalg.CPU.LinearOp <name="model.layers.4.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=201), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=204), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=203))] (%545:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=201)]) -> (%547:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=204)])
            linalg.CPU.LinearOp <name="model.layers.4.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=201), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=206), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=205))] (%545:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=201)]) -> (%548:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=206)])
            linalg.CPU.ViewOp <name="model.layers.4.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=207), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=207), )] (%546:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=207)]) -> (%546:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=207)])
            linalg.CPU.TransposeOp <name="model.layers.4.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=207), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=207), )] (%546:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=207)]) -> (%549:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=207)])
            linalg.CPU.ViewOp <name="model.layers.4.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=204), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=204), )] (%547:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=204)]) -> (%547:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=204)])
            linalg.CPU.TransposeOp <name="model.layers.4.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=204), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=204), )] (%547:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=204)]) -> (%550:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=204)])
            linalg.CPU.ViewOp <name="model.layers.4.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=206), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=206), )] (%548:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=206)]) -> (%548:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=206)])
            linalg.CPU.TransposeOp <name="model.layers.4.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=206), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=206), )] (%548:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=206)]) -> (%551:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=206)])
            linalg.CPU.RMSNormOp <name="model.layers.4.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=207), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=208), )] (%549:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=207)]) -> (%552:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=208)])
            linalg.CPU.RMSNormOp <name="model.layers.4.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=204), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=210), )] (%550:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=204)]) -> (%553:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=210)])
            linalg.CPU.RoPEOp <name="model.layers.4.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=208), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=208), )] (%552:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=208)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%554:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=208)])
            linalg.CPU.RoPEOp <name="model.layers.4.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=210), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=210), )] (%553:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=210)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%555:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=210)])
            linalg.CPU.CastTypeOp <name="model.layers.4.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=210), outputs_0:QuantSpec(Raw(type: Float16), uuid=212), )] (%555:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=210)]) -> (%556:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=212)])
            linalg.CPU.CastTypeOp <name="model.layers.4.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=212), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=213), )] (%556:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=212)]) -> (%557:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=213)])
            linalg.CPU.TransposeOp <name="model.layers.4.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=213), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=213), )] (%557:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=213)]) -> (%558:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=213)])
            linalg.CPU.CastTypeOp <name="model.layers.4.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=206), outputs_0:QuantSpec(Raw(type: Float16), uuid=214), )] (%551:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=206)]) -> (%559:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=214)])
            linalg.CPU.CastTypeOp <name="model.layers.4.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=214), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=215), )] (%559:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=214)]) -> (%560:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=215)])
            linalg.CPU.ConcatOp <name="model.layers.4.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=7), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=213), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=7), )] (%328:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=7)], %558:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=213)]) -> (%561:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=7)])
            linalg.CPU.ConcatOp <name="model.layers.4.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=35), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=215), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=35), )] (%329:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=35)], %560:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=215)]) -> (%562:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=35)])
            linalg.CPU.RepeatOp <name="model.layers.4.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=7), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=7), )] (%561:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=7)]) -> (%563:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=7)])
            linalg.CPU.RepeatOp <name="model.layers.4.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=35), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=35), )] (%562:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=35)]) -> (%564:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=35)])
            linalg.CPU.MatMulOp <name="model.layers.4.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=208), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=7), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=216), )] (%554:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=208)], %563:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=7)]) -> (%565:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=216)])
            linalg.CPU.MulOp <name="model.layers.4.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=216), inputs_1:QuantSpec(Raw(type: Float32), uuid=217), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=216), )] (%565:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=216)], %566:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=217), constant:[0.088388346]]) -> (%567:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=216)])
            linalg.CPU.ReduceMinOp <name="model.layers.4.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=216), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=218), )] (%567:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=216)]) -> (%568:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=218)])
            linalg.CPU.AddOp <name="model.layers.4.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=218), inputs_1:QuantSpec(Raw(type: Int16), uuid=219), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=218), )] (%568:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=218)], %569:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=219), constant:[-20]]) -> (%570:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=218)])
            linalg.CPU.EqualOp <name="model.layers.4.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=220), outputs_0:QuantSpec(Raw(type: UInt8), uuid=221), )] (%319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %571:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=220), constant:[0]]) -> (%572:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=221)])
            linalg.CPU.WhereOp <name="model.layers.4.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=221), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=216), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=218), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=218), )] (%572:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=221)], %567:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=216)], %570:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=218)]) -> (%573:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=218)])
            linalg.CPU.SoftmaxOp <name="model.layers.4.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=218), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=222), )] (%573:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=218)]) -> (%574:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=222)])
            linalg.CPU.MatMulOp <name="model.layers.4.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=222), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=35), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=223), )] (%574:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=222)], %564:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=35)]) -> (%575:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=223)])
            linalg.CPU.TransposeOp <name="model.layers.4.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=223), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=223), )] (%575:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=223)]) -> (%576:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=223)])
            linalg.CPU.ViewOp <name="model.layers.4.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=223), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=223), )] (%576:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=223)]) -> (%576:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=223)])
            linalg.CPU.LinearOp <name="model.layers.4.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=223), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=225), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=224))] (%576:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=223)]) -> (%577:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=225)])
            cf.ReturnOp (%577:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=225)], %558:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=213)], %560:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=215)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.4.mlp <CPU> [using_qnn:true, symbol:model.layers.4.mlp] {
        (%579:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=226)]) -> (%584:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=234)]) {
            linalg.CPU.LinearOp <name="model.layers.4.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=226), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=229), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=228))] (%579:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=226)]) -> (%580:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=229)])
            linalg.CPU.SiLUOp <name="model.layers.4.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=229), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=230), )] (%580:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=229)]) -> (%581:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=230)])
            linalg.CPU.LinearOp <name="model.layers.4.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=226), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=232), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=231))] (%579:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=226)]) -> (%582:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=232)])
            linalg.CPU.MulOp <name="model.layers.4.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=230), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=232), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=230), )] (%581:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=230)], %582:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=232)]) -> (%583:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=230)])
            linalg.CPU.LinearOp <name="model.layers.4.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=230), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=234), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=233))] (%583:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=230)]) -> (%584:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=234)])
            cf.ReturnOp (%584:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=234)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.5 <CPU> [using_qnn:true, symbol:model.layers.5] {
        (%585:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=234)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %330:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=8)], %331:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=36)]) -> (%626:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=268)], %599:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=247)], %601:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=249)]) {
            linalg.CPU.RMSNormOp <name="model.layers.5.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=234), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=235), )] (%585:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=234)]) -> (%586:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=235)])
            graph.CallGraphOp @model.layers.5.self_attn (%586:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=235)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %330:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=8)], %331:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=36)]) -> (%618:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=259)], %599:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=247)], %601:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=249)])
            linalg.CPU.AddOp <name="model.layers.5.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=259), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=234), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=259), )] (%618:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=259)], %585:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=234)]) -> (%619:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=259)])
            linalg.CPU.RMSNormOp <name="model.layers.5.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=259), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=260), )] (%619:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=259)]) -> (%620:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=260)])
            graph.CallGraphOp @model.layers.5.mlp (%620:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=260)]) -> (%625:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=268)])
            linalg.CPU.AddOp <name="model.layers.5.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=268), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=259), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=268), )] (%625:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=268)], %619:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=259)]) -> (%626:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=268)])
            cf.ReturnOp (%626:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=268)], %599:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=247)], %601:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=249)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.5.self_attn <CPU> [using_qnn:true, symbol:model.layers.5.self_attn] {
        (%586:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=235)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %330:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=8)], %331:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=36)]) -> (%618:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=259)], %599:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=247)], %601:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=249)]) {
            linalg.CPU.LinearOp <name="model.layers.5.self_attn.q_proj">(%586:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=235)]) -> (%587:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=241)])
            linalg.CPU.LinearOp <name="model.layers.5.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=235), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=238), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=237))] (%586:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=235)]) -> (%588:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=238)])
            linalg.CPU.LinearOp <name="model.layers.5.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=235), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=240), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=239))] (%586:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=235)]) -> (%589:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=240)])
            linalg.CPU.ViewOp <name="model.layers.5.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=241), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=241), )] (%587:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=241)]) -> (%587:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=241)])
            linalg.CPU.TransposeOp <name="model.layers.5.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=241), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=241), )] (%587:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=241)]) -> (%590:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=241)])
            linalg.CPU.ViewOp <name="model.layers.5.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=238), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=238), )] (%588:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=238)]) -> (%588:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=238)])
            linalg.CPU.TransposeOp <name="model.layers.5.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=238), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=238), )] (%588:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=238)]) -> (%591:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=238)])
            linalg.CPU.ViewOp <name="model.layers.5.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=240), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=240), )] (%589:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=240)]) -> (%589:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=240)])
            linalg.CPU.TransposeOp <name="model.layers.5.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=240), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=240), )] (%589:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=240)]) -> (%592:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=240)])
            linalg.CPU.RMSNormOp <name="model.layers.5.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=241), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=242), )] (%590:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=241)]) -> (%593:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=242)])
            linalg.CPU.RMSNormOp <name="model.layers.5.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=238), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=244), )] (%591:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=238)]) -> (%594:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=244)])
            linalg.CPU.RoPEOp <name="model.layers.5.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=242), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=242), )] (%593:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=242)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%595:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=242)])
            linalg.CPU.RoPEOp <name="model.layers.5.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=244), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=244), )] (%594:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=244)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%596:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=244)])
            linalg.CPU.CastTypeOp <name="model.layers.5.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=244), outputs_0:QuantSpec(Raw(type: Float16), uuid=246), )] (%596:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=244)]) -> (%597:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=246)])
            linalg.CPU.CastTypeOp <name="model.layers.5.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=246), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=247), )] (%597:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=246)]) -> (%598:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=247)])
            linalg.CPU.TransposeOp <name="model.layers.5.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=247), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=247), )] (%598:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=247)]) -> (%599:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=247)])
            linalg.CPU.CastTypeOp <name="model.layers.5.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=240), outputs_0:QuantSpec(Raw(type: Float16), uuid=248), )] (%592:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=240)]) -> (%600:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=248)])
            linalg.CPU.CastTypeOp <name="model.layers.5.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=248), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=249), )] (%600:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=248)]) -> (%601:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=249)])
            linalg.CPU.ConcatOp <name="model.layers.5.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=8), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=247), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=8), )] (%330:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=8)], %599:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=247)]) -> (%602:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=8)])
            linalg.CPU.ConcatOp <name="model.layers.5.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=36), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=249), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=36), )] (%331:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=36)], %601:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=249)]) -> (%603:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=36)])
            linalg.CPU.RepeatOp <name="model.layers.5.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=8), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=8), )] (%602:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=8)]) -> (%604:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=8)])
            linalg.CPU.RepeatOp <name="model.layers.5.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=36), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=36), )] (%603:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=36)]) -> (%605:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=36)])
            linalg.CPU.MatMulOp <name="model.layers.5.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=242), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=8), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=250), )] (%595:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=242)], %604:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=8)]) -> (%606:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=250)])
            linalg.CPU.MulOp <name="model.layers.5.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=250), inputs_1:QuantSpec(Raw(type: Float32), uuid=251), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=250), )] (%606:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=250)], %607:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=251), constant:[0.088388346]]) -> (%608:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=250)])
            linalg.CPU.ReduceMinOp <name="model.layers.5.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=250), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=252), )] (%608:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=250)]) -> (%609:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=252)])
            linalg.CPU.AddOp <name="model.layers.5.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=252), inputs_1:QuantSpec(Raw(type: Int16), uuid=253), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=252), )] (%609:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=252)], %610:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=253), constant:[-20]]) -> (%611:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=252)])
            linalg.CPU.EqualOp <name="model.layers.5.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=254), outputs_0:QuantSpec(Raw(type: UInt8), uuid=255), )] (%319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %612:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=254), constant:[0]]) -> (%613:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=255)])
            linalg.CPU.WhereOp <name="model.layers.5.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=255), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=250), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=252), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=252), )] (%613:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=255)], %608:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=250)], %611:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=252)]) -> (%614:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=252)])
            linalg.CPU.SoftmaxOp <name="model.layers.5.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=252), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=256), )] (%614:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=252)]) -> (%615:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=256)])
            linalg.CPU.MatMulOp <name="model.layers.5.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=256), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=36), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=257), )] (%615:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=256)], %605:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=36)]) -> (%616:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=257)])
            linalg.CPU.TransposeOp <name="model.layers.5.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=257), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=257), )] (%616:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=257)]) -> (%617:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=257)])
            linalg.CPU.ViewOp <name="model.layers.5.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=257), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=257), )] (%617:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=257)]) -> (%617:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=257)])
            linalg.CPU.LinearOp <name="model.layers.5.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=257), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=259), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=258))] (%617:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=257)]) -> (%618:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=259)])
            cf.ReturnOp (%618:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=259)], %599:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=247)], %601:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=249)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.5.mlp <CPU> [using_qnn:true, symbol:model.layers.5.mlp] {
        (%620:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=260)]) -> (%625:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=268)]) {
            linalg.CPU.LinearOp <name="model.layers.5.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=260), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=263), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=262))] (%620:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=260)]) -> (%621:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=263)])
            linalg.CPU.SiLUOp <name="model.layers.5.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=263), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=264), )] (%621:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=263)]) -> (%622:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=264)])
            linalg.CPU.LinearOp <name="model.layers.5.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=260), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=266), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=265))] (%620:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=260)]) -> (%623:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=266)])
            linalg.CPU.MulOp <name="model.layers.5.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=264), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=266), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=264), )] (%622:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=264)], %623:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=266)]) -> (%624:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=264)])
            linalg.CPU.LinearOp <name="model.layers.5.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=264), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=268), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=267))] (%624:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=264)]) -> (%625:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=268)])
            cf.ReturnOp (%625:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=268)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.6 <CPU> [using_qnn:true, symbol:model.layers.6] {
        (%626:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=268)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %332:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=9)], %333:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=37)]) -> (%667:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=302)], %640:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=281)], %642:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=283)]) {
            linalg.CPU.RMSNormOp <name="model.layers.6.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=268), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=269), )] (%626:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=268)]) -> (%627:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=269)])
            graph.CallGraphOp @model.layers.6.self_attn (%627:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=269)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %332:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=9)], %333:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=37)]) -> (%659:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=293)], %640:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=281)], %642:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=283)])
            linalg.CPU.AddOp <name="model.layers.6.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=293), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=268), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=293), )] (%659:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=293)], %626:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=268)]) -> (%660:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=293)])
            linalg.CPU.RMSNormOp <name="model.layers.6.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=293), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=294), )] (%660:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=293)]) -> (%661:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=294)])
            graph.CallGraphOp @model.layers.6.mlp (%661:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=294)]) -> (%666:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=302)])
            linalg.CPU.AddOp <name="model.layers.6.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=302), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=293), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=302), )] (%666:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=302)], %660:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=293)]) -> (%667:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=302)])
            cf.ReturnOp (%667:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=302)], %640:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=281)], %642:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=283)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.6.self_attn <CPU> [using_qnn:true, symbol:model.layers.6.self_attn] {
        (%627:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=269)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %332:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=9)], %333:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=37)]) -> (%659:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=293)], %640:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=281)], %642:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=283)]) {
            linalg.CPU.LinearOp <name="model.layers.6.self_attn.q_proj">(%627:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=269)]) -> (%628:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=275)])
            linalg.CPU.LinearOp <name="model.layers.6.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=269), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=272), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=271))] (%627:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=269)]) -> (%629:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=272)])
            linalg.CPU.LinearOp <name="model.layers.6.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=269), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=274), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=273))] (%627:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=269)]) -> (%630:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=274)])
            linalg.CPU.ViewOp <name="model.layers.6.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=275), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=275), )] (%628:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=275)]) -> (%628:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=275)])
            linalg.CPU.TransposeOp <name="model.layers.6.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=275), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=275), )] (%628:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=275)]) -> (%631:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=275)])
            linalg.CPU.ViewOp <name="model.layers.6.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=272), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=272), )] (%629:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=272)]) -> (%629:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=272)])
            linalg.CPU.TransposeOp <name="model.layers.6.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=272), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=272), )] (%629:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=272)]) -> (%632:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=272)])
            linalg.CPU.ViewOp <name="model.layers.6.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=274), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=274), )] (%630:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=274)]) -> (%630:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=274)])
            linalg.CPU.TransposeOp <name="model.layers.6.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=274), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=274), )] (%630:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=274)]) -> (%633:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=274)])
            linalg.CPU.RMSNormOp <name="model.layers.6.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=275), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=276), )] (%631:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=275)]) -> (%634:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=276)])
            linalg.CPU.RMSNormOp <name="model.layers.6.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=272), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=278), )] (%632:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=272)]) -> (%635:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=278)])
            linalg.CPU.RoPEOp <name="model.layers.6.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=276), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=276), )] (%634:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=276)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%636:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=276)])
            linalg.CPU.RoPEOp <name="model.layers.6.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=278), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=278), )] (%635:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=278)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%637:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=278)])
            linalg.CPU.CastTypeOp <name="model.layers.6.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=278), outputs_0:QuantSpec(Raw(type: Float16), uuid=280), )] (%637:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=278)]) -> (%638:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=280)])
            linalg.CPU.CastTypeOp <name="model.layers.6.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=280), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=281), )] (%638:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=280)]) -> (%639:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=281)])
            linalg.CPU.TransposeOp <name="model.layers.6.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=281), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=281), )] (%639:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=281)]) -> (%640:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=281)])
            linalg.CPU.CastTypeOp <name="model.layers.6.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=274), outputs_0:QuantSpec(Raw(type: Float16), uuid=282), )] (%633:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=274)]) -> (%641:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=282)])
            linalg.CPU.CastTypeOp <name="model.layers.6.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=282), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=283), )] (%641:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=282)]) -> (%642:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=283)])
            linalg.CPU.ConcatOp <name="model.layers.6.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=9), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=281), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=9), )] (%332:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=9)], %640:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=281)]) -> (%643:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=9)])
            linalg.CPU.ConcatOp <name="model.layers.6.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=37), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=283), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=37), )] (%333:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=37)], %642:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=283)]) -> (%644:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=37)])
            linalg.CPU.RepeatOp <name="model.layers.6.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=9), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=9), )] (%643:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=9)]) -> (%645:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=9)])
            linalg.CPU.RepeatOp <name="model.layers.6.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=37), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=37), )] (%644:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=37)]) -> (%646:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=37)])
            linalg.CPU.MatMulOp <name="model.layers.6.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=276), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=9), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=284), )] (%636:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=276)], %645:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=9)]) -> (%647:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=284)])
            linalg.CPU.MulOp <name="model.layers.6.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=284), inputs_1:QuantSpec(Raw(type: Float32), uuid=285), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=284), )] (%647:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=284)], %648:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=285), constant:[0.088388346]]) -> (%649:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=284)])
            linalg.CPU.ReduceMinOp <name="model.layers.6.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=284), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=286), )] (%649:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=284)]) -> (%650:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=286)])
            linalg.CPU.AddOp <name="model.layers.6.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=286), inputs_1:QuantSpec(Raw(type: Int16), uuid=287), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=286), )] (%650:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=286)], %651:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=287), constant:[-20]]) -> (%652:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=286)])
            linalg.CPU.EqualOp <name="model.layers.6.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=288), outputs_0:QuantSpec(Raw(type: UInt8), uuid=289), )] (%319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %653:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=288), constant:[0]]) -> (%654:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=289)])
            linalg.CPU.WhereOp <name="model.layers.6.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=289), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=284), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=286), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=286), )] (%654:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=289)], %649:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=284)], %652:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=286)]) -> (%655:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=286)])
            linalg.CPU.SoftmaxOp <name="model.layers.6.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=286), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=290), )] (%655:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=286)]) -> (%656:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=290)])
            linalg.CPU.MatMulOp <name="model.layers.6.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=290), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=37), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=291), )] (%656:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=290)], %646:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=37)]) -> (%657:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=291)])
            linalg.CPU.TransposeOp <name="model.layers.6.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=291), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=291), )] (%657:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=291)]) -> (%658:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=291)])
            linalg.CPU.ViewOp <name="model.layers.6.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=291), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=291), )] (%658:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=291)]) -> (%658:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=291)])
            linalg.CPU.LinearOp <name="model.layers.6.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=291), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=293), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=292))] (%658:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=291)]) -> (%659:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=293)])
            cf.ReturnOp (%659:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=293)], %640:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=281)], %642:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=283)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.6.mlp <CPU> [using_qnn:true, symbol:model.layers.6.mlp] {
        (%661:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=294)]) -> (%666:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=302)]) {
            linalg.CPU.LinearOp <name="model.layers.6.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=294), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=297), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=296))] (%661:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=294)]) -> (%662:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=297)])
            linalg.CPU.SiLUOp <name="model.layers.6.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=297), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=298), )] (%662:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=297)]) -> (%663:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=298)])
            linalg.CPU.LinearOp <name="model.layers.6.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=294), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=300), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=299))] (%661:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=294)]) -> (%664:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=300)])
            linalg.CPU.MulOp <name="model.layers.6.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=298), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=300), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=298), )] (%663:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=298)], %664:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=300)]) -> (%665:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=298)])
            linalg.CPU.LinearOp <name="model.layers.6.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=298), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=302), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=301))] (%665:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=298)]) -> (%666:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=302)])
            cf.ReturnOp (%666:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=302)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.7 <CPU> [using_qnn:true, symbol:model.layers.7] {
        (%667:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=302)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %334:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=10)], %335:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=38)]) -> (%708:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=336)], %681:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=315)], %683:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=317)]) {
            linalg.CPU.RMSNormOp <name="model.layers.7.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=302), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=303), )] (%667:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=302)]) -> (%668:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=303)])
            graph.CallGraphOp @model.layers.7.self_attn (%668:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=303)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %334:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=10)], %335:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=38)]) -> (%700:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=327)], %681:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=315)], %683:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=317)])
            linalg.CPU.AddOp <name="model.layers.7.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=327), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=302), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=327), )] (%700:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=327)], %667:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=302)]) -> (%701:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=327)])
            linalg.CPU.RMSNormOp <name="model.layers.7.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=327), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=328), )] (%701:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=327)]) -> (%702:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=328)])
            graph.CallGraphOp @model.layers.7.mlp (%702:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=328)]) -> (%707:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=336)])
            linalg.CPU.AddOp <name="model.layers.7.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=336), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=327), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=336), )] (%707:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=336)], %701:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=327)]) -> (%708:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=336)])
            cf.ReturnOp (%708:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=336)], %681:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=315)], %683:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=317)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.7.self_attn <CPU> [using_qnn:true, symbol:model.layers.7.self_attn] {
        (%668:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=303)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %334:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=10)], %335:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=38)]) -> (%700:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=327)], %681:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=315)], %683:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=317)]) {
            linalg.CPU.LinearOp <name="model.layers.7.self_attn.q_proj">(%668:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=303)]) -> (%669:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=309)])
            linalg.CPU.LinearOp <name="model.layers.7.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=303), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=306), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=305))] (%668:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=303)]) -> (%670:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=306)])
            linalg.CPU.LinearOp <name="model.layers.7.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=303), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=308), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=307))] (%668:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=303)]) -> (%671:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=308)])
            linalg.CPU.ViewOp <name="model.layers.7.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=309), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=309), )] (%669:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=309)]) -> (%669:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=309)])
            linalg.CPU.TransposeOp <name="model.layers.7.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=309), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=309), )] (%669:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=309)]) -> (%672:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=309)])
            linalg.CPU.ViewOp <name="model.layers.7.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=306), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=306), )] (%670:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=306)]) -> (%670:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=306)])
            linalg.CPU.TransposeOp <name="model.layers.7.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=306), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=306), )] (%670:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=306)]) -> (%673:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=306)])
            linalg.CPU.ViewOp <name="model.layers.7.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=308), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=308), )] (%671:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=308)]) -> (%671:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=308)])
            linalg.CPU.TransposeOp <name="model.layers.7.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=308), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=308), )] (%671:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=308)]) -> (%674:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=308)])
            linalg.CPU.RMSNormOp <name="model.layers.7.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=309), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=310), )] (%672:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=309)]) -> (%675:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=310)])
            linalg.CPU.RMSNormOp <name="model.layers.7.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=306), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=312), )] (%673:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=306)]) -> (%676:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=312)])
            linalg.CPU.RoPEOp <name="model.layers.7.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=310), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=310), )] (%675:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=310)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%677:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=310)])
            linalg.CPU.RoPEOp <name="model.layers.7.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=312), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=312), )] (%676:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=312)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%678:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=312)])
            linalg.CPU.CastTypeOp <name="model.layers.7.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=312), outputs_0:QuantSpec(Raw(type: Float16), uuid=314), )] (%678:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=312)]) -> (%679:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=314)])
            linalg.CPU.CastTypeOp <name="model.layers.7.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=314), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=315), )] (%679:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=314)]) -> (%680:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=315)])
            linalg.CPU.TransposeOp <name="model.layers.7.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=315), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=315), )] (%680:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=315)]) -> (%681:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=315)])
            linalg.CPU.CastTypeOp <name="model.layers.7.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=308), outputs_0:QuantSpec(Raw(type: Float16), uuid=316), )] (%674:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=308)]) -> (%682:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=316)])
            linalg.CPU.CastTypeOp <name="model.layers.7.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=316), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=317), )] (%682:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=316)]) -> (%683:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=317)])
            linalg.CPU.ConcatOp <name="model.layers.7.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=10), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=315), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=10), )] (%334:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=10)], %681:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=315)]) -> (%684:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=10)])
            linalg.CPU.ConcatOp <name="model.layers.7.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=38), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=317), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=38), )] (%335:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=38)], %683:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=317)]) -> (%685:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=38)])
            linalg.CPU.RepeatOp <name="model.layers.7.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=10), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=10), )] (%684:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=10)]) -> (%686:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=10)])
            linalg.CPU.RepeatOp <name="model.layers.7.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=38), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=38), )] (%685:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=38)]) -> (%687:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=38)])
            linalg.CPU.MatMulOp <name="model.layers.7.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=310), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=10), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=318), )] (%677:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=310)], %686:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=10)]) -> (%688:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=318)])
            linalg.CPU.MulOp <name="model.layers.7.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=318), inputs_1:QuantSpec(Raw(type: Float32), uuid=319), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=318), )] (%688:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=318)], %689:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=319), constant:[0.088388346]]) -> (%690:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=318)])
            linalg.CPU.ReduceMinOp <name="model.layers.7.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=318), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=320), )] (%690:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=318)]) -> (%691:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=320)])
            linalg.CPU.AddOp <name="model.layers.7.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=320), inputs_1:QuantSpec(Raw(type: Int16), uuid=321), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=320), )] (%691:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=320)], %692:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=321), constant:[-20]]) -> (%693:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=320)])
            linalg.CPU.EqualOp <name="model.layers.7.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=322), outputs_0:QuantSpec(Raw(type: UInt8), uuid=323), )] (%319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %694:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=322), constant:[0]]) -> (%695:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=323)])
            linalg.CPU.WhereOp <name="model.layers.7.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=323), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=318), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=320), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=320), )] (%695:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=323)], %690:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=318)], %693:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=320)]) -> (%696:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=320)])
            linalg.CPU.SoftmaxOp <name="model.layers.7.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=320), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=324), )] (%696:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=320)]) -> (%697:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=324)])
            linalg.CPU.MatMulOp <name="model.layers.7.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=324), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=38), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=325), )] (%697:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=324)], %687:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=38)]) -> (%698:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=325)])
            linalg.CPU.TransposeOp <name="model.layers.7.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=325), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=325), )] (%698:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=325)]) -> (%699:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=325)])
            linalg.CPU.ViewOp <name="model.layers.7.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=325), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=325), )] (%699:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=325)]) -> (%699:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=325)])
            linalg.CPU.LinearOp <name="model.layers.7.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=325), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=327), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=326))] (%699:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=325)]) -> (%700:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=327)])
            cf.ReturnOp (%700:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=327)], %681:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=315)], %683:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=317)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.7.mlp <CPU> [using_qnn:true, symbol:model.layers.7.mlp] {
        (%702:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=328)]) -> (%707:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=336)]) {
            linalg.CPU.LinearOp <name="model.layers.7.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=328), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=331), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=330))] (%702:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=328)]) -> (%703:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=331)])
            linalg.CPU.SiLUOp <name="model.layers.7.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=331), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=332), )] (%703:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=331)]) -> (%704:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=332)])
            linalg.CPU.LinearOp <name="model.layers.7.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=328), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=334), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=333))] (%702:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=328)]) -> (%705:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=334)])
            linalg.CPU.MulOp <name="model.layers.7.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=332), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=334), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=332), )] (%704:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=332)], %705:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=334)]) -> (%706:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=332)])
            linalg.CPU.LinearOp <name="model.layers.7.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=332), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=336), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=335))] (%706:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=332)]) -> (%707:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=336)])
            cf.ReturnOp (%707:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=336)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.8 <CPU> [using_qnn:true, symbol:model.layers.8] {
        (%708:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=336)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %336:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=11)], %337:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=39)]) -> (%749:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=370)], %722:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=349)], %724:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=351)]) {
            linalg.CPU.RMSNormOp <name="model.layers.8.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=336), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=337), )] (%708:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=336)]) -> (%709:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=337)])
            graph.CallGraphOp @model.layers.8.self_attn (%709:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=337)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %336:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=11)], %337:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=39)]) -> (%741:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=361)], %722:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=349)], %724:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=351)])
            linalg.CPU.AddOp <name="model.layers.8.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=361), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=336), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=361), )] (%741:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=361)], %708:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=336)]) -> (%742:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=361)])
            linalg.CPU.RMSNormOp <name="model.layers.8.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=361), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=362), )] (%742:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=361)]) -> (%743:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=362)])
            graph.CallGraphOp @model.layers.8.mlp (%743:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=362)]) -> (%748:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=370)])
            linalg.CPU.AddOp <name="model.layers.8.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=370), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=361), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=370), )] (%748:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=370)], %742:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=361)]) -> (%749:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=370)])
            cf.ReturnOp (%749:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=370)], %722:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=349)], %724:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=351)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.8.self_attn <CPU> [using_qnn:true, symbol:model.layers.8.self_attn] {
        (%709:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=337)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %336:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=11)], %337:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=39)]) -> (%741:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=361)], %722:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=349)], %724:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=351)]) {
            linalg.CPU.LinearOp <name="model.layers.8.self_attn.q_proj">(%709:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=337)]) -> (%710:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=343)])
            linalg.CPU.LinearOp <name="model.layers.8.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=337), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=340), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=339))] (%709:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=337)]) -> (%711:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=340)])
            linalg.CPU.LinearOp <name="model.layers.8.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=337), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=342), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=341))] (%709:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=337)]) -> (%712:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=342)])
            linalg.CPU.ViewOp <name="model.layers.8.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=343), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=343), )] (%710:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=343)]) -> (%710:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=343)])
            linalg.CPU.TransposeOp <name="model.layers.8.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=343), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=343), )] (%710:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=343)]) -> (%713:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=343)])
            linalg.CPU.ViewOp <name="model.layers.8.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=340), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=340), )] (%711:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=340)]) -> (%711:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=340)])
            linalg.CPU.TransposeOp <name="model.layers.8.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=340), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=340), )] (%711:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=340)]) -> (%714:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=340)])
            linalg.CPU.ViewOp <name="model.layers.8.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=342), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=342), )] (%712:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=342)]) -> (%712:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=342)])
            linalg.CPU.TransposeOp <name="model.layers.8.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=342), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=342), )] (%712:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=342)]) -> (%715:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=342)])
            linalg.CPU.RMSNormOp <name="model.layers.8.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=343), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=344), )] (%713:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=343)]) -> (%716:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=344)])
            linalg.CPU.RMSNormOp <name="model.layers.8.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=340), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=346), )] (%714:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=340)]) -> (%717:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=346)])
            linalg.CPU.RoPEOp <name="model.layers.8.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=344), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=344), )] (%716:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=344)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%718:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=344)])
            linalg.CPU.RoPEOp <name="model.layers.8.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=346), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=346), )] (%717:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=346)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%719:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=346)])
            linalg.CPU.CastTypeOp <name="model.layers.8.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=346), outputs_0:QuantSpec(Raw(type: Float16), uuid=348), )] (%719:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=346)]) -> (%720:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=348)])
            linalg.CPU.CastTypeOp <name="model.layers.8.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=348), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=349), )] (%720:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=348)]) -> (%721:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=349)])
            linalg.CPU.TransposeOp <name="model.layers.8.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=349), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=349), )] (%721:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=349)]) -> (%722:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=349)])
            linalg.CPU.CastTypeOp <name="model.layers.8.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=342), outputs_0:QuantSpec(Raw(type: Float16), uuid=350), )] (%715:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=342)]) -> (%723:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=350)])
            linalg.CPU.CastTypeOp <name="model.layers.8.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=350), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=351), )] (%723:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=350)]) -> (%724:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=351)])
            linalg.CPU.ConcatOp <name="model.layers.8.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=11), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=349), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=11), )] (%336:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=11)], %722:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=349)]) -> (%725:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=11)])
            linalg.CPU.ConcatOp <name="model.layers.8.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=39), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=351), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=39), )] (%337:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=39)], %724:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=351)]) -> (%726:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=39)])
            linalg.CPU.RepeatOp <name="model.layers.8.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=11), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=11), )] (%725:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=11)]) -> (%727:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=11)])
            linalg.CPU.RepeatOp <name="model.layers.8.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=39), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=39), )] (%726:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=39)]) -> (%728:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=39)])
            linalg.CPU.MatMulOp <name="model.layers.8.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=344), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=11), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=352), )] (%718:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=344)], %727:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=11)]) -> (%729:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=352)])
            linalg.CPU.MulOp <name="model.layers.8.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=352), inputs_1:QuantSpec(Raw(type: Float32), uuid=353), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=352), )] (%729:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=352)], %730:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=353), constant:[0.088388346]]) -> (%731:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=352)])
            linalg.CPU.ReduceMinOp <name="model.layers.8.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=352), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=354), )] (%731:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=352)]) -> (%732:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=354)])
            linalg.CPU.AddOp <name="model.layers.8.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=354), inputs_1:QuantSpec(Raw(type: Int16), uuid=355), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=354), )] (%732:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=354)], %733:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=355), constant:[-20]]) -> (%734:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=354)])
            linalg.CPU.EqualOp <name="model.layers.8.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=356), outputs_0:QuantSpec(Raw(type: UInt8), uuid=357), )] (%319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %735:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=356), constant:[1]]) -> (%736:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=357)])
            linalg.CPU.WhereOp <name="model.layers.8.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=357), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=352), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=354), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=354), )] (%736:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=357)], %731:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=352)], %734:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=354)]) -> (%737:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=354)])
            linalg.CPU.SoftmaxOp <name="model.layers.8.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=354), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=358), )] (%737:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=354)]) -> (%738:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=358)])
            linalg.CPU.MatMulOp <name="model.layers.8.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=358), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=39), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=359), )] (%738:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=358)], %728:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=39)]) -> (%739:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=359)])
            linalg.CPU.TransposeOp <name="model.layers.8.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=359), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=359), )] (%739:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=359)]) -> (%740:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=359)])
            linalg.CPU.ViewOp <name="model.layers.8.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=359), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=359), )] (%740:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=359)]) -> (%740:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=359)])
            linalg.CPU.LinearOp <name="model.layers.8.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=359), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=361), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=360))] (%740:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=359)]) -> (%741:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=361)])
            cf.ReturnOp (%741:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=361)], %722:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=349)], %724:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=351)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.8.mlp <CPU> [using_qnn:true, symbol:model.layers.8.mlp] {
        (%743:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=362)]) -> (%748:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=370)]) {
            linalg.CPU.LinearOp <name="model.layers.8.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=362), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=365), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=364))] (%743:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=362)]) -> (%744:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=365)])
            linalg.CPU.SiLUOp <name="model.layers.8.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=365), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=366), )] (%744:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=365)]) -> (%745:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=366)])
            linalg.CPU.LinearOp <name="model.layers.8.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=362), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=368), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=367))] (%743:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=362)]) -> (%746:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=368)])
            linalg.CPU.MulOp <name="model.layers.8.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=366), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=368), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=366), )] (%745:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=366)], %746:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=368)]) -> (%747:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=366)])
            linalg.CPU.LinearOp <name="model.layers.8.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=366), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=370), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=369))] (%747:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=366)]) -> (%748:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=370)])
            cf.ReturnOp (%748:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=370)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.9 <CPU> [using_qnn:true, symbol:model.layers.9] {
        (%749:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=370)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %338:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=12)], %339:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=40)]) -> (%790:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=404)], %763:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=383)], %765:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=385)]) {
            linalg.CPU.RMSNormOp <name="model.layers.9.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=370), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=371), )] (%749:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=370)]) -> (%750:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=371)])
            graph.CallGraphOp @model.layers.9.self_attn (%750:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=371)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %338:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=12)], %339:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=40)]) -> (%782:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=395)], %763:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=383)], %765:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=385)])
            linalg.CPU.AddOp <name="model.layers.9.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=395), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=370), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=395), )] (%782:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=395)], %749:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=370)]) -> (%783:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=395)])
            linalg.CPU.RMSNormOp <name="model.layers.9.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=395), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=396), )] (%783:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=395)]) -> (%784:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=396)])
            graph.CallGraphOp @model.layers.9.mlp (%784:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=396)]) -> (%789:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=404)])
            linalg.CPU.AddOp <name="model.layers.9.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=404), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=395), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=404), )] (%789:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=404)], %783:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=395)]) -> (%790:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=404)])
            cf.ReturnOp (%790:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=404)], %763:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=383)], %765:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=385)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.9.self_attn <CPU> [using_qnn:true, symbol:model.layers.9.self_attn] {
        (%750:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=371)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %338:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=12)], %339:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=40)]) -> (%782:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=395)], %763:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=383)], %765:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=385)]) {
            linalg.CPU.LinearOp <name="model.layers.9.self_attn.q_proj">(%750:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=371)]) -> (%751:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=377)])
            linalg.CPU.LinearOp <name="model.layers.9.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=371), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=374), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=373))] (%750:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=371)]) -> (%752:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=374)])
            linalg.CPU.LinearOp <name="model.layers.9.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=371), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=376), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=375))] (%750:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=371)]) -> (%753:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=376)])
            linalg.CPU.ViewOp <name="model.layers.9.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=377), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=377), )] (%751:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=377)]) -> (%751:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=377)])
            linalg.CPU.TransposeOp <name="model.layers.9.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=377), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=377), )] (%751:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=377)]) -> (%754:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=377)])
            linalg.CPU.ViewOp <name="model.layers.9.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=374), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=374), )] (%752:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=374)]) -> (%752:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=374)])
            linalg.CPU.TransposeOp <name="model.layers.9.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=374), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=374), )] (%752:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=374)]) -> (%755:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=374)])
            linalg.CPU.ViewOp <name="model.layers.9.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=376), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=376), )] (%753:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=376)]) -> (%753:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=376)])
            linalg.CPU.TransposeOp <name="model.layers.9.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=376), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=376), )] (%753:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=376)]) -> (%756:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=376)])
            linalg.CPU.RMSNormOp <name="model.layers.9.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=377), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=378), )] (%754:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=377)]) -> (%757:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=378)])
            linalg.CPU.RMSNormOp <name="model.layers.9.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=374), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=380), )] (%755:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=374)]) -> (%758:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=380)])
            linalg.CPU.RoPEOp <name="model.layers.9.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=378), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=378), )] (%757:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=378)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%759:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=378)])
            linalg.CPU.RoPEOp <name="model.layers.9.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=380), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=380), )] (%758:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=380)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%760:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=380)])
            linalg.CPU.CastTypeOp <name="model.layers.9.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=380), outputs_0:QuantSpec(Raw(type: Float16), uuid=382), )] (%760:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=380)]) -> (%761:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=382)])
            linalg.CPU.CastTypeOp <name="model.layers.9.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=382), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=383), )] (%761:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=382)]) -> (%762:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=383)])
            linalg.CPU.TransposeOp <name="model.layers.9.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=383), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=383), )] (%762:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=383)]) -> (%763:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=383)])
            linalg.CPU.CastTypeOp <name="model.layers.9.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=376), outputs_0:QuantSpec(Raw(type: Float16), uuid=384), )] (%756:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=376)]) -> (%764:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=384)])
            linalg.CPU.CastTypeOp <name="model.layers.9.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=384), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=385), )] (%764:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=384)]) -> (%765:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=385)])
            linalg.CPU.ConcatOp <name="model.layers.9.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=12), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=383), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=12), )] (%338:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=12)], %763:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=383)]) -> (%766:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=12)])
            linalg.CPU.ConcatOp <name="model.layers.9.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=40), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=385), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=40), )] (%339:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=40)], %765:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=385)]) -> (%767:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=40)])
            linalg.CPU.RepeatOp <name="model.layers.9.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=12), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=12), )] (%766:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=12)]) -> (%768:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=12)])
            linalg.CPU.RepeatOp <name="model.layers.9.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=40), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=40), )] (%767:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=40)]) -> (%769:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=40)])
            linalg.CPU.MatMulOp <name="model.layers.9.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=378), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=12), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=386), )] (%759:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=378)], %768:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=12)]) -> (%770:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=386)])
            linalg.CPU.MulOp <name="model.layers.9.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=386), inputs_1:QuantSpec(Raw(type: Float32), uuid=387), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=386), )] (%770:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=386)], %771:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=387), constant:[0.088388346]]) -> (%772:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=386)])
            linalg.CPU.ReduceMinOp <name="model.layers.9.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=386), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=388), )] (%772:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=386)]) -> (%773:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=388)])
            linalg.CPU.AddOp <name="model.layers.9.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=388), inputs_1:QuantSpec(Raw(type: Int16), uuid=389), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=388), )] (%773:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=388)], %774:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=389), constant:[-20]]) -> (%775:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=388)])
            linalg.CPU.EqualOp <name="model.layers.9.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=390), outputs_0:QuantSpec(Raw(type: UInt8), uuid=391), )] (%319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %776:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=390), constant:[-0.1796875]]) -> (%777:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=391)])
            linalg.CPU.WhereOp <name="model.layers.9.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=391), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=386), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=388), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=388), )] (%777:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=391)], %772:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=386)], %775:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=388)]) -> (%778:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=388)])
            linalg.CPU.SoftmaxOp <name="model.layers.9.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=388), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=392), )] (%778:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=388)]) -> (%779:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=392)])
            linalg.CPU.MatMulOp <name="model.layers.9.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=392), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=40), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=393), )] (%779:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=392)], %769:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=40)]) -> (%780:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=393)])
            linalg.CPU.TransposeOp <name="model.layers.9.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=393), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=393), )] (%780:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=393)]) -> (%781:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=393)])
            linalg.CPU.ViewOp <name="model.layers.9.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=393), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=393), )] (%781:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=393)]) -> (%781:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=393)])
            linalg.CPU.LinearOp <name="model.layers.9.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=393), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=395), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=394))] (%781:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=393)]) -> (%782:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=395)])
            cf.ReturnOp (%782:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=395)], %763:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=383)], %765:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=385)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.9.mlp <CPU> [using_qnn:true, symbol:model.layers.9.mlp] {
        (%784:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=396)]) -> (%789:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=404)]) {
            linalg.CPU.LinearOp <name="model.layers.9.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=396), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=399), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=398))] (%784:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=396)]) -> (%785:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=399)])
            linalg.CPU.SiLUOp <name="model.layers.9.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=399), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=400), )] (%785:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=399)]) -> (%786:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=400)])
            linalg.CPU.LinearOp <name="model.layers.9.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=396), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=402), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=401))] (%784:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=396)]) -> (%787:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=402)])
            linalg.CPU.MulOp <name="model.layers.9.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=400), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=402), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=400), )] (%786:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=400)], %787:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=402)]) -> (%788:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=400)])
            linalg.CPU.LinearOp <name="model.layers.9.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=400), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=404), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=403))] (%788:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=400)]) -> (%789:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=404)])
            cf.ReturnOp (%789:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=404)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.10 <CPU> [using_qnn:true, symbol:model.layers.10] {
        (%790:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=404)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %340:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=13)], %341:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=41)]) -> (%831:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=438)], %804:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=417)], %806:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=419)]) {
            linalg.CPU.RMSNormOp <name="model.layers.10.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=404), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=405), )] (%790:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=404)]) -> (%791:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=405)])
            graph.CallGraphOp @model.layers.10.self_attn (%791:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=405)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %340:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=13)], %341:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=41)]) -> (%823:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=429)], %804:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=417)], %806:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=419)])
            linalg.CPU.AddOp <name="model.layers.10.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=429), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=404), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=429), )] (%823:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=429)], %790:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=404)]) -> (%824:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=429)])
            linalg.CPU.RMSNormOp <name="model.layers.10.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=429), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=430), )] (%824:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=429)]) -> (%825:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=430)])
            graph.CallGraphOp @model.layers.10.mlp (%825:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=430)]) -> (%830:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=438)])
            linalg.CPU.AddOp <name="model.layers.10.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=438), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=429), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=438), )] (%830:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=438)], %824:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=429)]) -> (%831:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=438)])
            cf.ReturnOp (%831:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=438)], %804:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=417)], %806:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=419)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.10.self_attn <CPU> [using_qnn:true, symbol:model.layers.10.self_attn] {
        (%791:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=405)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %340:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=13)], %341:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=41)]) -> (%823:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=429)], %804:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=417)], %806:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=419)]) {
            linalg.CPU.LinearOp <name="model.layers.10.self_attn.q_proj">(%791:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=405)]) -> (%792:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=411)])
            linalg.CPU.LinearOp <name="model.layers.10.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=405), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=408), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=407))] (%791:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=405)]) -> (%793:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=408)])
            linalg.CPU.LinearOp <name="model.layers.10.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=405), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=410), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=409))] (%791:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=405)]) -> (%794:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=410)])
            linalg.CPU.ViewOp <name="model.layers.10.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=411), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=411), )] (%792:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=411)]) -> (%792:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=411)])
            linalg.CPU.TransposeOp <name="model.layers.10.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=411), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=411), )] (%792:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=411)]) -> (%795:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=411)])
            linalg.CPU.ViewOp <name="model.layers.10.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=408), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=408), )] (%793:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=408)]) -> (%793:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=408)])
            linalg.CPU.TransposeOp <name="model.layers.10.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=408), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=408), )] (%793:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=408)]) -> (%796:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=408)])
            linalg.CPU.ViewOp <name="model.layers.10.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=410), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=410), )] (%794:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=410)]) -> (%794:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=410)])
            linalg.CPU.TransposeOp <name="model.layers.10.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=410), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=410), )] (%794:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=410)]) -> (%797:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=410)])
            linalg.CPU.RMSNormOp <name="model.layers.10.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=411), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=412), )] (%795:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=411)]) -> (%798:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=412)])
            linalg.CPU.RMSNormOp <name="model.layers.10.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=408), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=414), )] (%796:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=408)]) -> (%799:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=414)])
            linalg.CPU.RoPEOp <name="model.layers.10.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=412), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=412), )] (%798:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=412)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%800:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=412)])
            linalg.CPU.RoPEOp <name="model.layers.10.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=414), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=414), )] (%799:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=414)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%801:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=414)])
            linalg.CPU.CastTypeOp <name="model.layers.10.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=414), outputs_0:QuantSpec(Raw(type: Float16), uuid=416), )] (%801:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=414)]) -> (%802:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=416)])
            linalg.CPU.CastTypeOp <name="model.layers.10.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=416), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=417), )] (%802:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=416)]) -> (%803:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=417)])
            linalg.CPU.TransposeOp <name="model.layers.10.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=417), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=417), )] (%803:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=417)]) -> (%804:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=417)])
            linalg.CPU.CastTypeOp <name="model.layers.10.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=410), outputs_0:QuantSpec(Raw(type: Float16), uuid=418), )] (%797:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=410)]) -> (%805:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=418)])
            linalg.CPU.CastTypeOp <name="model.layers.10.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=418), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=419), )] (%805:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=418)]) -> (%806:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=419)])
            linalg.CPU.ConcatOp <name="model.layers.10.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=13), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=417), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=13), )] (%340:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=13)], %804:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=417)]) -> (%807:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=13)])
            linalg.CPU.ConcatOp <name="model.layers.10.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=41), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=419), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=41), )] (%341:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=41)], %806:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=419)]) -> (%808:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=41)])
            linalg.CPU.RepeatOp <name="model.layers.10.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=13), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=13), )] (%807:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=13)]) -> (%809:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=13)])
            linalg.CPU.RepeatOp <name="model.layers.10.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=41), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=41), )] (%808:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=41)]) -> (%810:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=41)])
            linalg.CPU.MatMulOp <name="model.layers.10.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=412), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=13), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=420), )] (%800:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=412)], %809:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=13)]) -> (%811:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=420)])
            linalg.CPU.MulOp <name="model.layers.10.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=420), inputs_1:QuantSpec(Raw(type: Float32), uuid=421), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=420), )] (%811:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=420)], %812:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=421), constant:[0.088388346]]) -> (%813:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=420)])
            linalg.CPU.ReduceMinOp <name="model.layers.10.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=420), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=422), )] (%813:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=420)]) -> (%814:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=422)])
            linalg.CPU.AddOp <name="model.layers.10.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=422), inputs_1:QuantSpec(Raw(type: Int16), uuid=423), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=422), )] (%814:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=422)], %815:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=423), constant:[-20]]) -> (%816:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=422)])
            linalg.CPU.EqualOp <name="model.layers.10.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=424), outputs_0:QuantSpec(Raw(type: UInt8), uuid=425), )] (%319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %817:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=424), constant:[-0.93359375]]) -> (%818:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=425)])
            linalg.CPU.WhereOp <name="model.layers.10.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=425), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=420), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=422), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=422), )] (%818:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=425)], %813:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=420)], %816:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=422)]) -> (%819:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=422)])
            linalg.CPU.SoftmaxOp <name="model.layers.10.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=422), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=426), )] (%819:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=422)]) -> (%820:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=426)])
            linalg.CPU.MatMulOp <name="model.layers.10.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=426), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=41), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=427), )] (%820:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=426)], %810:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=41)]) -> (%821:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=427)])
            linalg.CPU.TransposeOp <name="model.layers.10.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=427), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=427), )] (%821:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=427)]) -> (%822:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=427)])
            linalg.CPU.ViewOp <name="model.layers.10.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=427), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=427), )] (%822:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=427)]) -> (%822:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=427)])
            linalg.CPU.LinearOp <name="model.layers.10.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=427), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=429), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=428))] (%822:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=427)]) -> (%823:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=429)])
            cf.ReturnOp (%823:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=429)], %804:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=417)], %806:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=419)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.10.mlp <CPU> [using_qnn:true, symbol:model.layers.10.mlp] {
        (%825:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=430)]) -> (%830:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=438)]) {
            linalg.CPU.LinearOp <name="model.layers.10.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=430), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=433), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=432))] (%825:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=430)]) -> (%826:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=433)])
            linalg.CPU.SiLUOp <name="model.layers.10.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=433), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=434), )] (%826:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=433)]) -> (%827:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=434)])
            linalg.CPU.LinearOp <name="model.layers.10.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=430), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=436), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=435))] (%825:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=430)]) -> (%828:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=436)])
            linalg.CPU.MulOp <name="model.layers.10.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=434), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=436), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=434), )] (%827:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=434)], %828:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=436)]) -> (%829:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=434)])
            linalg.CPU.LinearOp <name="model.layers.10.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=434), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=438), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=437))] (%829:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=434)]) -> (%830:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=438)])
            cf.ReturnOp (%830:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=438)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.11 <CPU> [using_qnn:true, symbol:model.layers.11] {
        (%831:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=438)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %342:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=14)], %343:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=42)]) -> (%872:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=472)], %845:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=451)], %847:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=453)]) {
            linalg.CPU.RMSNormOp <name="model.layers.11.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=438), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=439), )] (%831:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=438)]) -> (%832:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=439)])
            graph.CallGraphOp @model.layers.11.self_attn (%832:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=439)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %342:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=14)], %343:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=42)]) -> (%864:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=463)], %845:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=451)], %847:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=453)])
            linalg.CPU.AddOp <name="model.layers.11.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=463), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=438), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=463), )] (%864:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=463)], %831:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=438)]) -> (%865:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=463)])
            linalg.CPU.RMSNormOp <name="model.layers.11.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=463), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=464), )] (%865:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=463)]) -> (%866:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=464)])
            graph.CallGraphOp @model.layers.11.mlp (%866:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=464)]) -> (%871:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=472)])
            linalg.CPU.AddOp <name="model.layers.11.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=472), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=463), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=472), )] (%871:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=472)], %865:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=463)]) -> (%872:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=472)])
            cf.ReturnOp (%872:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=472)], %845:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=451)], %847:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=453)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.11.self_attn <CPU> [using_qnn:true, symbol:model.layers.11.self_attn] {
        (%832:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=439)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %342:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=14)], %343:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=42)]) -> (%864:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=463)], %845:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=451)], %847:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=453)]) {
            linalg.CPU.LinearOp <name="model.layers.11.self_attn.q_proj">(%832:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=439)]) -> (%833:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=445)])
            linalg.CPU.LinearOp <name="model.layers.11.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=439), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=442), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=441))] (%832:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=439)]) -> (%834:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=442)])
            linalg.CPU.LinearOp <name="model.layers.11.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=439), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=444), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=443))] (%832:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=439)]) -> (%835:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=444)])
            linalg.CPU.ViewOp <name="model.layers.11.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=445), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=445), )] (%833:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=445)]) -> (%833:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=445)])
            linalg.CPU.TransposeOp <name="model.layers.11.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=445), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=445), )] (%833:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=445)]) -> (%836:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=445)])
            linalg.CPU.ViewOp <name="model.layers.11.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=442), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=442), )] (%834:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=442)]) -> (%834:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=442)])
            linalg.CPU.TransposeOp <name="model.layers.11.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=442), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=442), )] (%834:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=442)]) -> (%837:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=442)])
            linalg.CPU.ViewOp <name="model.layers.11.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=444), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=444), )] (%835:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=444)]) -> (%835:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=444)])
            linalg.CPU.TransposeOp <name="model.layers.11.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=444), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=444), )] (%835:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=444)]) -> (%838:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=444)])
            linalg.CPU.RMSNormOp <name="model.layers.11.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=445), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=446), )] (%836:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=445)]) -> (%839:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=446)])
            linalg.CPU.RMSNormOp <name="model.layers.11.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=442), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=448), )] (%837:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=442)]) -> (%840:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=448)])
            linalg.CPU.RoPEOp <name="model.layers.11.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=446), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=446), )] (%839:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=446)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%841:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=446)])
            linalg.CPU.RoPEOp <name="model.layers.11.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=448), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=448), )] (%840:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=448)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%842:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=448)])
            linalg.CPU.CastTypeOp <name="model.layers.11.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=448), outputs_0:QuantSpec(Raw(type: Float16), uuid=450), )] (%842:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=448)]) -> (%843:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=450)])
            linalg.CPU.CastTypeOp <name="model.layers.11.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=450), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=451), )] (%843:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=450)]) -> (%844:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=451)])
            linalg.CPU.TransposeOp <name="model.layers.11.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=451), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=451), )] (%844:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=451)]) -> (%845:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=451)])
            linalg.CPU.CastTypeOp <name="model.layers.11.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=444), outputs_0:QuantSpec(Raw(type: Float16), uuid=452), )] (%838:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=444)]) -> (%846:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=452)])
            linalg.CPU.CastTypeOp <name="model.layers.11.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=452), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=453), )] (%846:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=452)]) -> (%847:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=453)])
            linalg.CPU.ConcatOp <name="model.layers.11.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=14), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=451), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=14), )] (%342:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=14)], %845:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=451)]) -> (%848:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=14)])
            linalg.CPU.ConcatOp <name="model.layers.11.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=42), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=453), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=42), )] (%343:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=42)], %847:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=453)]) -> (%849:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=42)])
            linalg.CPU.RepeatOp <name="model.layers.11.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=14), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=14), )] (%848:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=14)]) -> (%850:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=14)])
            linalg.CPU.RepeatOp <name="model.layers.11.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=42), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=42), )] (%849:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=42)]) -> (%851:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=42)])
            linalg.CPU.MatMulOp <name="model.layers.11.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=446), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=14), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=454), )] (%841:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=446)], %850:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=14)]) -> (%852:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=454)])
            linalg.CPU.MulOp <name="model.layers.11.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=454), inputs_1:QuantSpec(Raw(type: Float32), uuid=455), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=454), )] (%852:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=454)], %853:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=455), constant:[0.088388346]]) -> (%854:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=454)])
            linalg.CPU.ReduceMinOp <name="model.layers.11.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=454), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=456), )] (%854:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=454)]) -> (%855:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=456)])
            linalg.CPU.AddOp <name="model.layers.11.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=456), inputs_1:QuantSpec(Raw(type: Int16), uuid=457), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=456), )] (%855:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=456)], %856:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=457), constant:[-20]]) -> (%857:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=456)])
            linalg.CPU.EqualOp <name="model.layers.11.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=458), outputs_0:QuantSpec(Raw(type: UInt8), uuid=459), )] (%319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %858:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=458), constant:[0.515625]]) -> (%859:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=459)])
            linalg.CPU.WhereOp <name="model.layers.11.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=459), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=454), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=456), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=456), )] (%859:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=459)], %854:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=454)], %857:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=456)]) -> (%860:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=456)])
            linalg.CPU.SoftmaxOp <name="model.layers.11.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=456), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=460), )] (%860:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=456)]) -> (%861:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=460)])
            linalg.CPU.MatMulOp <name="model.layers.11.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=460), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=42), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=461), )] (%861:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=460)], %851:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=42)]) -> (%862:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=461)])
            linalg.CPU.TransposeOp <name="model.layers.11.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=461), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=461), )] (%862:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=461)]) -> (%863:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=461)])
            linalg.CPU.ViewOp <name="model.layers.11.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=461), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=461), )] (%863:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=461)]) -> (%863:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=461)])
            linalg.CPU.LinearOp <name="model.layers.11.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=461), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=463), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=462))] (%863:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=461)]) -> (%864:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=463)])
            cf.ReturnOp (%864:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=463)], %845:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=451)], %847:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=453)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.11.mlp <CPU> [using_qnn:true, symbol:model.layers.11.mlp] {
        (%866:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=464)]) -> (%871:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=472)]) {
            linalg.CPU.LinearOp <name="model.layers.11.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=464), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=467), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=466))] (%866:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=464)]) -> (%867:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=467)])
            linalg.CPU.SiLUOp <name="model.layers.11.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=467), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=468), )] (%867:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=467)]) -> (%868:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=468)])
            linalg.CPU.LinearOp <name="model.layers.11.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=464), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=470), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=469))] (%866:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=464)]) -> (%869:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=470)])
            linalg.CPU.MulOp <name="model.layers.11.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=468), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=470), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=468), )] (%868:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=468)], %869:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=470)]) -> (%870:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=468)])
            linalg.CPU.LinearOp <name="model.layers.11.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=468), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=472), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=471))] (%870:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=468)]) -> (%871:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=472)])
            cf.ReturnOp (%871:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=472)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.12 <CPU> [using_qnn:true, symbol:model.layers.12] {
        (%872:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=472)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %344:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=15)], %345:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=43)]) -> (%913:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=506)], %886:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=485)], %888:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=487)]) {
            linalg.CPU.RMSNormOp <name="model.layers.12.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=472), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=473), )] (%872:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=472)]) -> (%873:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=473)])
            graph.CallGraphOp @model.layers.12.self_attn (%873:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=473)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %344:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=15)], %345:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=43)]) -> (%905:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=497)], %886:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=485)], %888:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=487)])
            linalg.CPU.AddOp <name="model.layers.12.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=497), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=472), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=497), )] (%905:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=497)], %872:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=472)]) -> (%906:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=497)])
            linalg.CPU.RMSNormOp <name="model.layers.12.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=497), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=498), )] (%906:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=497)]) -> (%907:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=498)])
            graph.CallGraphOp @model.layers.12.mlp (%907:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=498)]) -> (%912:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=506)])
            linalg.CPU.AddOp <name="model.layers.12.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=506), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=497), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=506), )] (%912:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=506)], %906:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=497)]) -> (%913:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=506)])
            cf.ReturnOp (%913:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=506)], %886:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=485)], %888:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=487)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.12.self_attn <CPU> [using_qnn:true, symbol:model.layers.12.self_attn] {
        (%873:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=473)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %344:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=15)], %345:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=43)]) -> (%905:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=497)], %886:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=485)], %888:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=487)]) {
            linalg.CPU.LinearOp <name="model.layers.12.self_attn.q_proj">(%873:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=473)]) -> (%874:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=479)])
            linalg.CPU.LinearOp <name="model.layers.12.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=473), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=476), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=475))] (%873:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=473)]) -> (%875:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=476)])
            linalg.CPU.LinearOp <name="model.layers.12.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=473), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=478), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=477))] (%873:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=473)]) -> (%876:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=478)])
            linalg.CPU.ViewOp <name="model.layers.12.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=479), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=479), )] (%874:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=479)]) -> (%874:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=479)])
            linalg.CPU.TransposeOp <name="model.layers.12.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=479), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=479), )] (%874:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=479)]) -> (%877:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=479)])
            linalg.CPU.ViewOp <name="model.layers.12.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=476), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=476), )] (%875:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=476)]) -> (%875:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=476)])
            linalg.CPU.TransposeOp <name="model.layers.12.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=476), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=476), )] (%875:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=476)]) -> (%878:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=476)])
            linalg.CPU.ViewOp <name="model.layers.12.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=478), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=478), )] (%876:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=478)]) -> (%876:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=478)])
            linalg.CPU.TransposeOp <name="model.layers.12.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=478), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=478), )] (%876:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=478)]) -> (%879:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=478)])
            linalg.CPU.RMSNormOp <name="model.layers.12.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=479), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=480), )] (%877:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=479)]) -> (%880:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=480)])
            linalg.CPU.RMSNormOp <name="model.layers.12.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=476), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=482), )] (%878:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=476)]) -> (%881:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=482)])
            linalg.CPU.RoPEOp <name="model.layers.12.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=480), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=480), )] (%880:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=480)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%882:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=480)])
            linalg.CPU.RoPEOp <name="model.layers.12.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=482), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=482), )] (%881:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=482)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%883:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=482)])
            linalg.CPU.CastTypeOp <name="model.layers.12.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=482), outputs_0:QuantSpec(Raw(type: Float16), uuid=484), )] (%883:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=482)]) -> (%884:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=484)])
            linalg.CPU.CastTypeOp <name="model.layers.12.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=484), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=485), )] (%884:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=484)]) -> (%885:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=485)])
            linalg.CPU.TransposeOp <name="model.layers.12.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=485), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=485), )] (%885:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=485)]) -> (%886:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=485)])
            linalg.CPU.CastTypeOp <name="model.layers.12.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=478), outputs_0:QuantSpec(Raw(type: Float16), uuid=486), )] (%879:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=478)]) -> (%887:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=486)])
            linalg.CPU.CastTypeOp <name="model.layers.12.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=486), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=487), )] (%887:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=486)]) -> (%888:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=487)])
            linalg.CPU.ConcatOp <name="model.layers.12.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=15), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=485), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=15), )] (%344:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=15)], %886:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=485)]) -> (%889:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=15)])
            linalg.CPU.ConcatOp <name="model.layers.12.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=43), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=487), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=43), )] (%345:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=43)], %888:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=487)]) -> (%890:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=43)])
            linalg.CPU.RepeatOp <name="model.layers.12.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=15), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=15), )] (%889:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=15)]) -> (%891:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=15)])
            linalg.CPU.RepeatOp <name="model.layers.12.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=43), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=43), )] (%890:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=43)]) -> (%892:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=43)])
            linalg.CPU.MatMulOp <name="model.layers.12.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=480), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=15), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=488), )] (%882:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=480)], %891:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=15)]) -> (%893:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=488)])
            linalg.CPU.MulOp <name="model.layers.12.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=488), inputs_1:QuantSpec(Raw(type: Float32), uuid=489), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=488), )] (%893:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=488)], %894:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=489), constant:[0.088388346]]) -> (%895:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=488)])
            linalg.CPU.ReduceMinOp <name="model.layers.12.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=488), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=490), )] (%895:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=488)]) -> (%896:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=490)])
            linalg.CPU.AddOp <name="model.layers.12.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=490), inputs_1:QuantSpec(Raw(type: Int16), uuid=491), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=490), )] (%896:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=490)], %897:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=491), constant:[-20]]) -> (%898:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=490)])
            linalg.CPU.EqualOp <name="model.layers.12.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=492), outputs_0:QuantSpec(Raw(type: UInt8), uuid=493), )] (%319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %899:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=492), constant:[0.74609375]]) -> (%900:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=493)])
            linalg.CPU.WhereOp <name="model.layers.12.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=493), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=488), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=490), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=490), )] (%900:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=493)], %895:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=488)], %898:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=490)]) -> (%901:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=490)])
            linalg.CPU.SoftmaxOp <name="model.layers.12.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=490), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=494), )] (%901:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=490)]) -> (%902:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=494)])
            linalg.CPU.MatMulOp <name="model.layers.12.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=494), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=43), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=495), )] (%902:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=494)], %892:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=43)]) -> (%903:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=495)])
            linalg.CPU.TransposeOp <name="model.layers.12.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=495), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=495), )] (%903:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=495)]) -> (%904:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=495)])
            linalg.CPU.ViewOp <name="model.layers.12.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=495), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=495), )] (%904:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=495)]) -> (%904:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=495)])
            linalg.CPU.LinearOp <name="model.layers.12.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=495), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=497), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=496))] (%904:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=495)]) -> (%905:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=497)])
            cf.ReturnOp (%905:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=497)], %886:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=485)], %888:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=487)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.12.mlp <CPU> [using_qnn:true, symbol:model.layers.12.mlp] {
        (%907:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=498)]) -> (%912:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=506)]) {
            linalg.CPU.LinearOp <name="model.layers.12.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=498), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=501), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=500))] (%907:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=498)]) -> (%908:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=501)])
            linalg.CPU.SiLUOp <name="model.layers.12.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=501), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=502), )] (%908:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=501)]) -> (%909:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=502)])
            linalg.CPU.LinearOp <name="model.layers.12.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=498), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=504), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=503))] (%907:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=498)]) -> (%910:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=504)])
            linalg.CPU.MulOp <name="model.layers.12.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=502), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=504), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=502), )] (%909:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=502)], %910:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=504)]) -> (%911:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=502)])
            linalg.CPU.LinearOp <name="model.layers.12.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=502), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=506), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=505))] (%911:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=502)]) -> (%912:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=506)])
            cf.ReturnOp (%912:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=506)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.13 <CPU> [using_qnn:true, symbol:model.layers.13] {
        (%913:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=506)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %346:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=16)], %347:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=44)]) -> (%954:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=540)], %927:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=519)], %929:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=521)]) {
            linalg.CPU.RMSNormOp <name="model.layers.13.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=506), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=507), )] (%913:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=506)]) -> (%914:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=507)])
            graph.CallGraphOp @model.layers.13.self_attn (%914:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=507)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %346:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=16)], %347:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=44)]) -> (%946:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=531)], %927:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=519)], %929:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=521)])
            linalg.CPU.AddOp <name="model.layers.13.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=531), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=506), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=531), )] (%946:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=531)], %913:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=506)]) -> (%947:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=531)])
            linalg.CPU.RMSNormOp <name="model.layers.13.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=531), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=532), )] (%947:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=531)]) -> (%948:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=532)])
            graph.CallGraphOp @model.layers.13.mlp (%948:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=532)]) -> (%953:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=540)])
            linalg.CPU.AddOp <name="model.layers.13.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=540), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=531), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=540), )] (%953:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=540)], %947:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=531)]) -> (%954:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=540)])
            cf.ReturnOp (%954:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=540)], %927:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=519)], %929:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=521)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.13.self_attn <CPU> [using_qnn:true, symbol:model.layers.13.self_attn] {
        (%914:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=507)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %346:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=16)], %347:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=44)]) -> (%946:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=531)], %927:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=519)], %929:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=521)]) {
            linalg.CPU.LinearOp <name="model.layers.13.self_attn.q_proj">(%914:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=507)]) -> (%915:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=513)])
            linalg.CPU.LinearOp <name="model.layers.13.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=507), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=510), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=509))] (%914:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=507)]) -> (%916:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=510)])
            linalg.CPU.LinearOp <name="model.layers.13.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=507), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=512), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=511))] (%914:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=507)]) -> (%917:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=512)])
            linalg.CPU.ViewOp <name="model.layers.13.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=513), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=513), )] (%915:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=513)]) -> (%915:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=513)])
            linalg.CPU.TransposeOp <name="model.layers.13.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=513), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=513), )] (%915:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=513)]) -> (%918:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=513)])
            linalg.CPU.ViewOp <name="model.layers.13.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=510), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=510), )] (%916:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=510)]) -> (%916:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=510)])
            linalg.CPU.TransposeOp <name="model.layers.13.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=510), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=510), )] (%916:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=510)]) -> (%919:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=510)])
            linalg.CPU.ViewOp <name="model.layers.13.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=512), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=512), )] (%917:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=512)]) -> (%917:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=512)])
            linalg.CPU.TransposeOp <name="model.layers.13.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=512), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=512), )] (%917:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=512)]) -> (%920:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=512)])
            linalg.CPU.RMSNormOp <name="model.layers.13.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=513), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=514), )] (%918:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=513)]) -> (%921:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=514)])
            linalg.CPU.RMSNormOp <name="model.layers.13.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=510), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=516), )] (%919:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=510)]) -> (%922:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=516)])
            linalg.CPU.RoPEOp <name="model.layers.13.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=514), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=514), )] (%921:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=514)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%923:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=514)])
            linalg.CPU.RoPEOp <name="model.layers.13.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=516), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=516), )] (%922:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=516)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%924:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=516)])
            linalg.CPU.CastTypeOp <name="model.layers.13.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=516), outputs_0:QuantSpec(Raw(type: Float16), uuid=518), )] (%924:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=516)]) -> (%925:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=518)])
            linalg.CPU.CastTypeOp <name="model.layers.13.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=518), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=519), )] (%925:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=518)]) -> (%926:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=519)])
            linalg.CPU.TransposeOp <name="model.layers.13.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=519), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=519), )] (%926:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=519)]) -> (%927:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=519)])
            linalg.CPU.CastTypeOp <name="model.layers.13.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=512), outputs_0:QuantSpec(Raw(type: Float16), uuid=520), )] (%920:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=512)]) -> (%928:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=520)])
            linalg.CPU.CastTypeOp <name="model.layers.13.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=520), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=521), )] (%928:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=520)]) -> (%929:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=521)])
            linalg.CPU.ConcatOp <name="model.layers.13.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=16), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=519), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=16), )] (%346:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=16)], %927:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=519)]) -> (%930:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=16)])
            linalg.CPU.ConcatOp <name="model.layers.13.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=44), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=521), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=44), )] (%347:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=44)], %929:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=521)]) -> (%931:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=44)])
            linalg.CPU.RepeatOp <name="model.layers.13.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=16), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=16), )] (%930:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=16)]) -> (%932:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=16)])
            linalg.CPU.RepeatOp <name="model.layers.13.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=44), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=44), )] (%931:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=44)]) -> (%933:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=44)])
            linalg.CPU.MatMulOp <name="model.layers.13.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=514), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=16), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=522), )] (%923:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=514)], %932:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=16)]) -> (%934:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=522)])
            linalg.CPU.MulOp <name="model.layers.13.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=522), inputs_1:QuantSpec(Raw(type: Float32), uuid=523), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=522), )] (%934:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=522)], %935:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=523), constant:[0.088388346]]) -> (%936:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=522)])
            linalg.CPU.ReduceMinOp <name="model.layers.13.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=522), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=524), )] (%936:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=522)]) -> (%937:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=524)])
            linalg.CPU.AddOp <name="model.layers.13.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=524), inputs_1:QuantSpec(Raw(type: Int16), uuid=525), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=524), )] (%937:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=524)], %938:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=525), constant:[-20]]) -> (%939:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=524)])
            linalg.CPU.EqualOp <name="model.layers.13.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=526), outputs_0:QuantSpec(Raw(type: UInt8), uuid=527), )] (%319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %940:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=526), constant:[-0.78515625]]) -> (%941:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=527)])
            linalg.CPU.WhereOp <name="model.layers.13.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=527), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=522), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=524), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=524), )] (%941:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=527)], %936:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=522)], %939:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=524)]) -> (%942:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=524)])
            linalg.CPU.SoftmaxOp <name="model.layers.13.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=524), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=528), )] (%942:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=524)]) -> (%943:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=528)])
            linalg.CPU.MatMulOp <name="model.layers.13.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=528), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=44), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=529), )] (%943:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=528)], %933:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=44)]) -> (%944:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=529)])
            linalg.CPU.TransposeOp <name="model.layers.13.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=529), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=529), )] (%944:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=529)]) -> (%945:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=529)])
            linalg.CPU.ViewOp <name="model.layers.13.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=529), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=529), )] (%945:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=529)]) -> (%945:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=529)])
            linalg.CPU.LinearOp <name="model.layers.13.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=529), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=531), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=530))] (%945:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=529)]) -> (%946:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=531)])
            cf.ReturnOp (%946:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=531)], %927:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=519)], %929:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=521)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.13.mlp <CPU> [using_qnn:true, symbol:model.layers.13.mlp] {
        (%948:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=532)]) -> (%953:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=540)]) {
            linalg.CPU.LinearOp <name="model.layers.13.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=532), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=535), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=534))] (%948:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=532)]) -> (%949:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=535)])
            linalg.CPU.SiLUOp <name="model.layers.13.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=535), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=536), )] (%949:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=535)]) -> (%950:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=536)])
            linalg.CPU.LinearOp <name="model.layers.13.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=532), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=538), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=537))] (%948:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=532)]) -> (%951:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=538)])
            linalg.CPU.MulOp <name="model.layers.13.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=536), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=538), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=536), )] (%950:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=536)], %951:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=538)]) -> (%952:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=536)])
            linalg.CPU.LinearOp <name="model.layers.13.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=536), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=540), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=539))] (%952:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=536)]) -> (%953:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=540)])
            cf.ReturnOp (%953:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=540)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.14 <CPU> [using_qnn:true, symbol:model.layers.14] {
        (%954:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=540)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %348:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=17)], %349:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=45)]) -> (%995:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=574)], %968:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=553)], %970:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=555)]) {
            linalg.CPU.RMSNormOp <name="model.layers.14.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=540), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=541), )] (%954:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=540)]) -> (%955:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=541)])
            graph.CallGraphOp @model.layers.14.self_attn (%955:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=541)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %348:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=17)], %349:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=45)]) -> (%987:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=565)], %968:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=553)], %970:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=555)])
            linalg.CPU.AddOp <name="model.layers.14.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=565), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=540), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=565), )] (%987:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=565)], %954:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=540)]) -> (%988:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=565)])
            linalg.CPU.RMSNormOp <name="model.layers.14.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=565), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=566), )] (%988:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=565)]) -> (%989:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=566)])
            graph.CallGraphOp @model.layers.14.mlp (%989:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=566)]) -> (%994:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=574)])
            linalg.CPU.AddOp <name="model.layers.14.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=574), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=565), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=574), )] (%994:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=574)], %988:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=565)]) -> (%995:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=574)])
            cf.ReturnOp (%995:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=574)], %968:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=553)], %970:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=555)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.14.self_attn <CPU> [using_qnn:true, symbol:model.layers.14.self_attn] {
        (%955:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=541)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %348:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=17)], %349:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=45)]) -> (%987:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=565)], %968:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=553)], %970:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=555)]) {
            linalg.CPU.LinearOp <name="model.layers.14.self_attn.q_proj">(%955:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=541)]) -> (%956:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=547)])
            linalg.CPU.LinearOp <name="model.layers.14.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=541), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=544), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=543))] (%955:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=541)]) -> (%957:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=544)])
            linalg.CPU.LinearOp <name="model.layers.14.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=541), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=546), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=545))] (%955:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=541)]) -> (%958:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=546)])
            linalg.CPU.ViewOp <name="model.layers.14.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=547), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=547), )] (%956:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=547)]) -> (%956:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=547)])
            linalg.CPU.TransposeOp <name="model.layers.14.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=547), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=547), )] (%956:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=547)]) -> (%959:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=547)])
            linalg.CPU.ViewOp <name="model.layers.14.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=544), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=544), )] (%957:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=544)]) -> (%957:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=544)])
            linalg.CPU.TransposeOp <name="model.layers.14.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=544), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=544), )] (%957:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=544)]) -> (%960:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=544)])
            linalg.CPU.ViewOp <name="model.layers.14.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=546), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=546), )] (%958:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=546)]) -> (%958:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=546)])
            linalg.CPU.TransposeOp <name="model.layers.14.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=546), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=546), )] (%958:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=546)]) -> (%961:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=546)])
            linalg.CPU.RMSNormOp <name="model.layers.14.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=547), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=548), )] (%959:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=547)]) -> (%962:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=548)])
            linalg.CPU.RMSNormOp <name="model.layers.14.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=544), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=550), )] (%960:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=544)]) -> (%963:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=550)])
            linalg.CPU.RoPEOp <name="model.layers.14.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=548), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=548), )] (%962:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=548)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%964:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=548)])
            linalg.CPU.RoPEOp <name="model.layers.14.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=550), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=550), )] (%963:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=550)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%965:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=550)])
            linalg.CPU.CastTypeOp <name="model.layers.14.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=550), outputs_0:QuantSpec(Raw(type: Float16), uuid=552), )] (%965:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=550)]) -> (%966:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=552)])
            linalg.CPU.CastTypeOp <name="model.layers.14.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=552), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=553), )] (%966:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=552)]) -> (%967:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=553)])
            linalg.CPU.TransposeOp <name="model.layers.14.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=553), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=553), )] (%967:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=553)]) -> (%968:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=553)])
            linalg.CPU.CastTypeOp <name="model.layers.14.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=546), outputs_0:QuantSpec(Raw(type: Float16), uuid=554), )] (%961:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=546)]) -> (%969:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=554)])
            linalg.CPU.CastTypeOp <name="model.layers.14.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=554), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=555), )] (%969:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=554)]) -> (%970:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=555)])
            linalg.CPU.ConcatOp <name="model.layers.14.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=17), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=553), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=17), )] (%348:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=17)], %968:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=553)]) -> (%971:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=17)])
            linalg.CPU.ConcatOp <name="model.layers.14.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=45), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=555), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=45), )] (%349:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=45)], %970:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=555)]) -> (%972:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=45)])
            linalg.CPU.RepeatOp <name="model.layers.14.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=17), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=17), )] (%971:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=17)]) -> (%973:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=17)])
            linalg.CPU.RepeatOp <name="model.layers.14.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=45), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=45), )] (%972:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=45)]) -> (%974:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=45)])
            linalg.CPU.MatMulOp <name="model.layers.14.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=548), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=17), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=556), )] (%964:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=548)], %973:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=17)]) -> (%975:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=556)])
            linalg.CPU.MulOp <name="model.layers.14.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=556), inputs_1:QuantSpec(Raw(type: Float32), uuid=557), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=556), )] (%975:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=556)], %976:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=557), constant:[0.088388346]]) -> (%977:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=556)])
            linalg.CPU.ReduceMinOp <name="model.layers.14.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=556), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=558), )] (%977:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=556)]) -> (%978:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=558)])
            linalg.CPU.AddOp <name="model.layers.14.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=558), inputs_1:QuantSpec(Raw(type: Int16), uuid=559), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=558), )] (%978:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=558)], %979:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=559), constant:[-20]]) -> (%980:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=558)])
            linalg.CPU.EqualOp <name="model.layers.14.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=560), outputs_0:QuantSpec(Raw(type: UInt8), uuid=561), )] (%319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %981:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=560), constant:[-0.46289062]]) -> (%982:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=561)])
            linalg.CPU.WhereOp <name="model.layers.14.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=561), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=556), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=558), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=558), )] (%982:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=561)], %977:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=556)], %980:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=558)]) -> (%983:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=558)])
            linalg.CPU.SoftmaxOp <name="model.layers.14.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=558), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=562), )] (%983:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=558)]) -> (%984:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=562)])
            linalg.CPU.MatMulOp <name="model.layers.14.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=562), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=45), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=563), )] (%984:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=562)], %974:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=45)]) -> (%985:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=563)])
            linalg.CPU.TransposeOp <name="model.layers.14.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=563), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=563), )] (%985:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=563)]) -> (%986:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=563)])
            linalg.CPU.ViewOp <name="model.layers.14.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=563), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=563), )] (%986:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=563)]) -> (%986:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=563)])
            linalg.CPU.LinearOp <name="model.layers.14.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=563), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=565), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=564))] (%986:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=563)]) -> (%987:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=565)])
            cf.ReturnOp (%987:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=565)], %968:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=553)], %970:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=555)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.14.mlp <CPU> [using_qnn:true, symbol:model.layers.14.mlp] {
        (%989:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=566)]) -> (%994:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=574)]) {
            linalg.CPU.LinearOp <name="model.layers.14.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=566), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=569), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=568))] (%989:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=566)]) -> (%990:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=569)])
            linalg.CPU.SiLUOp <name="model.layers.14.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=569), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=570), )] (%990:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=569)]) -> (%991:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=570)])
            linalg.CPU.LinearOp <name="model.layers.14.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=566), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=572), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=571))] (%989:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=566)]) -> (%992:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=572)])
            linalg.CPU.MulOp <name="model.layers.14.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=570), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=572), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=570), )] (%991:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=570)], %992:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=572)]) -> (%993:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=570)])
            linalg.CPU.LinearOp <name="model.layers.14.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=570), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=574), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=573))] (%993:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=570)]) -> (%994:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=574)])
            cf.ReturnOp (%994:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=574)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.15 <CPU> [using_qnn:true, symbol:model.layers.15] {
        (%995:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=574)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %350:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=18)], %351:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=46)]) -> (%1036:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=608)], %1009:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=587)], %1011:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=589)]) {
            linalg.CPU.RMSNormOp <name="model.layers.15.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=574), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=575), )] (%995:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=574)]) -> (%996:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=575)])
            graph.CallGraphOp @model.layers.15.self_attn (%996:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=575)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %350:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=18)], %351:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=46)]) -> (%1028:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=599)], %1009:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=587)], %1011:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=589)])
            linalg.CPU.AddOp <name="model.layers.15.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=599), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=574), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=599), )] (%1028:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=599)], %995:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=574)]) -> (%1029:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=599)])
            linalg.CPU.RMSNormOp <name="model.layers.15.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=599), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=600), )] (%1029:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=599)]) -> (%1030:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=600)])
            graph.CallGraphOp @model.layers.15.mlp (%1030:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=600)]) -> (%1035:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=608)])
            linalg.CPU.AddOp <name="model.layers.15.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=608), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=599), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=608), )] (%1035:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=608)], %1029:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=599)]) -> (%1036:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=608)])
            cf.ReturnOp (%1036:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=608)], %1009:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=587)], %1011:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=589)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.15.self_attn <CPU> [using_qnn:true, symbol:model.layers.15.self_attn] {
        (%996:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=575)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %350:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=18)], %351:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=46)]) -> (%1028:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=599)], %1009:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=587)], %1011:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=589)]) {
            linalg.CPU.LinearOp <name="model.layers.15.self_attn.q_proj">(%996:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=575)]) -> (%997:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=581)])
            linalg.CPU.LinearOp <name="model.layers.15.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=575), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=578), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=577))] (%996:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=575)]) -> (%998:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=578)])
            linalg.CPU.LinearOp <name="model.layers.15.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=575), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=580), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=579))] (%996:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=575)]) -> (%999:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=580)])
            linalg.CPU.ViewOp <name="model.layers.15.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=581), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=581), )] (%997:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=581)]) -> (%997:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=581)])
            linalg.CPU.TransposeOp <name="model.layers.15.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=581), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=581), )] (%997:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=581)]) -> (%1000:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=581)])
            linalg.CPU.ViewOp <name="model.layers.15.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=578), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=578), )] (%998:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=578)]) -> (%998:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=578)])
            linalg.CPU.TransposeOp <name="model.layers.15.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=578), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=578), )] (%998:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=578)]) -> (%1001:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=578)])
            linalg.CPU.ViewOp <name="model.layers.15.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=580), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=580), )] (%999:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=580)]) -> (%999:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=580)])
            linalg.CPU.TransposeOp <name="model.layers.15.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=580), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=580), )] (%999:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=580)]) -> (%1002:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=580)])
            linalg.CPU.RMSNormOp <name="model.layers.15.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=581), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=582), )] (%1000:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=581)]) -> (%1003:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=582)])
            linalg.CPU.RMSNormOp <name="model.layers.15.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=578), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=584), )] (%1001:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=578)]) -> (%1004:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=584)])
            linalg.CPU.RoPEOp <name="model.layers.15.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=582), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=582), )] (%1003:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=582)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%1005:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=582)])
            linalg.CPU.RoPEOp <name="model.layers.15.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=584), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=584), )] (%1004:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=584)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%1006:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=584)])
            linalg.CPU.CastTypeOp <name="model.layers.15.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=584), outputs_0:QuantSpec(Raw(type: Float16), uuid=586), )] (%1006:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=584)]) -> (%1007:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=586)])
            linalg.CPU.CastTypeOp <name="model.layers.15.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=586), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=587), )] (%1007:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=586)]) -> (%1008:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=587)])
            linalg.CPU.TransposeOp <name="model.layers.15.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=587), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=587), )] (%1008:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=587)]) -> (%1009:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=587)])
            linalg.CPU.CastTypeOp <name="model.layers.15.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=580), outputs_0:QuantSpec(Raw(type: Float16), uuid=588), )] (%1002:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=580)]) -> (%1010:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=588)])
            linalg.CPU.CastTypeOp <name="model.layers.15.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=588), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=589), )] (%1010:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=588)]) -> (%1011:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=589)])
            linalg.CPU.ConcatOp <name="model.layers.15.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=18), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=587), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=18), )] (%350:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=18)], %1009:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=587)]) -> (%1012:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=18)])
            linalg.CPU.ConcatOp <name="model.layers.15.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=46), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=589), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=46), )] (%351:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=46)], %1011:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=589)]) -> (%1013:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=46)])
            linalg.CPU.RepeatOp <name="model.layers.15.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=18), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=18), )] (%1012:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=18)]) -> (%1014:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=18)])
            linalg.CPU.RepeatOp <name="model.layers.15.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=46), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=46), )] (%1013:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=46)]) -> (%1015:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=46)])
            linalg.CPU.MatMulOp <name="model.layers.15.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=582), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=18), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=590), )] (%1005:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=582)], %1014:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=18)]) -> (%1016:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=590)])
            linalg.CPU.MulOp <name="model.layers.15.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=590), inputs_1:QuantSpec(Raw(type: Float32), uuid=591), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=590), )] (%1016:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=590)], %1017:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=591), constant:[0.088388346]]) -> (%1018:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=590)])
            linalg.CPU.ReduceMinOp <name="model.layers.15.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=590), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=592), )] (%1018:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=590)]) -> (%1019:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=592)])
            linalg.CPU.AddOp <name="model.layers.15.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=592), inputs_1:QuantSpec(Raw(type: Int16), uuid=593), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=592), )] (%1019:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=592)], %1020:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=593), constant:[-20]]) -> (%1021:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=592)])
            linalg.CPU.EqualOp <name="model.layers.15.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=594), outputs_0:QuantSpec(Raw(type: UInt8), uuid=595), )] (%319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %1022:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=594), constant:[0.953125]]) -> (%1023:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=595)])
            linalg.CPU.WhereOp <name="model.layers.15.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=595), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=590), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=592), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=592), )] (%1023:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=595)], %1018:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=590)], %1021:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=592)]) -> (%1024:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=592)])
            linalg.CPU.SoftmaxOp <name="model.layers.15.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=592), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=596), )] (%1024:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=592)]) -> (%1025:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=596)])
            linalg.CPU.MatMulOp <name="model.layers.15.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=596), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=46), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=597), )] (%1025:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=596)], %1015:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=46)]) -> (%1026:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=597)])
            linalg.CPU.TransposeOp <name="model.layers.15.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=597), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=597), )] (%1026:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=597)]) -> (%1027:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=597)])
            linalg.CPU.ViewOp <name="model.layers.15.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=597), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=597), )] (%1027:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=597)]) -> (%1027:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=597)])
            linalg.CPU.LinearOp <name="model.layers.15.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=597), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=599), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=598))] (%1027:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=597)]) -> (%1028:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=599)])
            cf.ReturnOp (%1028:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=599)], %1009:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=587)], %1011:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=589)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.15.mlp <CPU> [using_qnn:true, symbol:model.layers.15.mlp] {
        (%1030:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=600)]) -> (%1035:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=608)]) {
            linalg.CPU.LinearOp <name="model.layers.15.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=600), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=603), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=602))] (%1030:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=600)]) -> (%1031:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=603)])
            linalg.CPU.SiLUOp <name="model.layers.15.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=603), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=604), )] (%1031:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=603)]) -> (%1032:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=604)])
            linalg.CPU.LinearOp <name="model.layers.15.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=600), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=606), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=605))] (%1030:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=600)]) -> (%1033:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=606)])
            linalg.CPU.MulOp <name="model.layers.15.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=604), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=606), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=604), )] (%1032:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=604)], %1033:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=606)]) -> (%1034:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=604)])
            linalg.CPU.LinearOp <name="model.layers.15.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=604), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=608), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=607))] (%1034:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=604)]) -> (%1035:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=608)])
            cf.ReturnOp (%1035:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=608)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.16 <CPU> [using_qnn:true, symbol:model.layers.16] {
        (%1036:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=608)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %352:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=19)], %353:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=47)]) -> (%1077:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=642)], %1050:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=621)], %1052:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=623)]) {
            linalg.CPU.RMSNormOp <name="model.layers.16.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=608), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=609), )] (%1036:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=608)]) -> (%1037:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=609)])
            graph.CallGraphOp @model.layers.16.self_attn (%1037:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=609)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %352:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=19)], %353:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=47)]) -> (%1069:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=633)], %1050:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=621)], %1052:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=623)])
            linalg.CPU.AddOp <name="model.layers.16.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=633), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=608), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=633), )] (%1069:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=633)], %1036:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=608)]) -> (%1070:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=633)])
            linalg.CPU.RMSNormOp <name="model.layers.16.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=633), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=634), )] (%1070:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=633)]) -> (%1071:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=634)])
            graph.CallGraphOp @model.layers.16.mlp (%1071:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=634)]) -> (%1076:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=642)])
            linalg.CPU.AddOp <name="model.layers.16.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=642), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=633), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=642), )] (%1076:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=642)], %1070:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=633)]) -> (%1077:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=642)])
            cf.ReturnOp (%1077:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=642)], %1050:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=621)], %1052:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=623)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.16.self_attn <CPU> [using_qnn:true, symbol:model.layers.16.self_attn] {
        (%1037:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=609)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %352:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=19)], %353:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=47)]) -> (%1069:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=633)], %1050:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=621)], %1052:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=623)]) {
            linalg.CPU.LinearOp <name="model.layers.16.self_attn.q_proj">(%1037:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=609)]) -> (%1038:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=615)])
            linalg.CPU.LinearOp <name="model.layers.16.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=609), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=612), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=611))] (%1037:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=609)]) -> (%1039:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=612)])
            linalg.CPU.LinearOp <name="model.layers.16.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=609), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=614), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=613))] (%1037:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=609)]) -> (%1040:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=614)])
            linalg.CPU.ViewOp <name="model.layers.16.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=615), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=615), )] (%1038:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=615)]) -> (%1038:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=615)])
            linalg.CPU.TransposeOp <name="model.layers.16.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=615), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=615), )] (%1038:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=615)]) -> (%1041:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=615)])
            linalg.CPU.ViewOp <name="model.layers.16.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=612), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=612), )] (%1039:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=612)]) -> (%1039:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=612)])
            linalg.CPU.TransposeOp <name="model.layers.16.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=612), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=612), )] (%1039:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=612)]) -> (%1042:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=612)])
            linalg.CPU.ViewOp <name="model.layers.16.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=614), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=614), )] (%1040:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=614)]) -> (%1040:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=614)])
            linalg.CPU.TransposeOp <name="model.layers.16.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=614), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=614), )] (%1040:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=614)]) -> (%1043:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=614)])
            linalg.CPU.RMSNormOp <name="model.layers.16.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=615), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=616), )] (%1041:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=615)]) -> (%1044:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=616)])
            linalg.CPU.RMSNormOp <name="model.layers.16.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=612), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=618), )] (%1042:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=612)]) -> (%1045:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=618)])
            linalg.CPU.RoPEOp <name="model.layers.16.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=616), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=616), )] (%1044:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=616)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%1046:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=616)])
            linalg.CPU.RoPEOp <name="model.layers.16.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=618), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=618), )] (%1045:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=618)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%1047:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=618)])
            linalg.CPU.CastTypeOp <name="model.layers.16.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=618), outputs_0:QuantSpec(Raw(type: Float16), uuid=620), )] (%1047:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=618)]) -> (%1048:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=620)])
            linalg.CPU.CastTypeOp <name="model.layers.16.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=620), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=621), )] (%1048:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=620)]) -> (%1049:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=621)])
            linalg.CPU.TransposeOp <name="model.layers.16.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=621), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=621), )] (%1049:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=621)]) -> (%1050:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=621)])
            linalg.CPU.CastTypeOp <name="model.layers.16.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=614), outputs_0:QuantSpec(Raw(type: Float16), uuid=622), )] (%1043:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=614)]) -> (%1051:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=622)])
            linalg.CPU.CastTypeOp <name="model.layers.16.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=622), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=623), )] (%1051:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=622)]) -> (%1052:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=623)])
            linalg.CPU.ConcatOp <name="model.layers.16.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=19), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=621), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=19), )] (%352:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=19)], %1050:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=621)]) -> (%1053:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=19)])
            linalg.CPU.ConcatOp <name="model.layers.16.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=47), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=623), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=47), )] (%353:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=47)], %1052:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=623)]) -> (%1054:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=47)])
            linalg.CPU.RepeatOp <name="model.layers.16.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=19), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=19), )] (%1053:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=19)]) -> (%1055:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=19)])
            linalg.CPU.RepeatOp <name="model.layers.16.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=47), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=47), )] (%1054:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=47)]) -> (%1056:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=47)])
            linalg.CPU.MatMulOp <name="model.layers.16.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=616), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=19), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=624), )] (%1046:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=616)], %1055:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=19)]) -> (%1057:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=624)])
            linalg.CPU.MulOp <name="model.layers.16.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=624), inputs_1:QuantSpec(Raw(type: Float32), uuid=625), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=624), )] (%1057:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=624)], %1058:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=625), constant:[0.088388346]]) -> (%1059:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=624)])
            linalg.CPU.ReduceMinOp <name="model.layers.16.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=624), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=626), )] (%1059:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=624)]) -> (%1060:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=626)])
            linalg.CPU.AddOp <name="model.layers.16.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=626), inputs_1:QuantSpec(Raw(type: Int16), uuid=627), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=626), )] (%1060:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=626)], %1061:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=627), constant:[-20]]) -> (%1062:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=626)])
            linalg.CPU.EqualOp <name="model.layers.16.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=628), outputs_0:QuantSpec(Raw(type: UInt8), uuid=629), )] (%319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %1063:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=628), constant:[0.118652344]]) -> (%1064:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=629)])
            linalg.CPU.WhereOp <name="model.layers.16.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=629), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=624), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=626), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=626), )] (%1064:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=629)], %1059:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=624)], %1062:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=626)]) -> (%1065:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=626)])
            linalg.CPU.SoftmaxOp <name="model.layers.16.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=626), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=630), )] (%1065:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=626)]) -> (%1066:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=630)])
            linalg.CPU.MatMulOp <name="model.layers.16.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=630), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=47), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=631), )] (%1066:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=630)], %1056:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=47)]) -> (%1067:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=631)])
            linalg.CPU.TransposeOp <name="model.layers.16.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=631), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=631), )] (%1067:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=631)]) -> (%1068:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=631)])
            linalg.CPU.ViewOp <name="model.layers.16.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=631), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=631), )] (%1068:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=631)]) -> (%1068:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=631)])
            linalg.CPU.LinearOp <name="model.layers.16.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=631), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=633), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=632))] (%1068:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=631)]) -> (%1069:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=633)])
            cf.ReturnOp (%1069:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=633)], %1050:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=621)], %1052:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=623)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.16.mlp <CPU> [using_qnn:true, symbol:model.layers.16.mlp] {
        (%1071:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=634)]) -> (%1076:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=642)]) {
            linalg.CPU.LinearOp <name="model.layers.16.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=634), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=637), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=636))] (%1071:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=634)]) -> (%1072:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=637)])
            linalg.CPU.SiLUOp <name="model.layers.16.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=637), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=638), )] (%1072:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=637)]) -> (%1073:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=638)])
            linalg.CPU.LinearOp <name="model.layers.16.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=634), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=640), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=639))] (%1071:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=634)]) -> (%1074:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=640)])
            linalg.CPU.MulOp <name="model.layers.16.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=638), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=640), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=638), )] (%1073:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=638)], %1074:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=640)]) -> (%1075:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=638)])
            linalg.CPU.LinearOp <name="model.layers.16.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=638), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=642), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=641))] (%1075:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=638)]) -> (%1076:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=642)])
            cf.ReturnOp (%1076:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=642)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.17 <CPU> [using_qnn:true, symbol:model.layers.17] {
        (%1077:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=642)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %354:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=20)], %355:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=48)]) -> (%1118:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=676)], %1091:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=655)], %1093:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=657)]) {
            linalg.CPU.RMSNormOp <name="model.layers.17.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=642), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=643), )] (%1077:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=642)]) -> (%1078:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=643)])
            graph.CallGraphOp @model.layers.17.self_attn (%1078:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=643)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %354:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=20)], %355:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=48)]) -> (%1110:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=667)], %1091:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=655)], %1093:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=657)])
            linalg.CPU.AddOp <name="model.layers.17.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=667), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=642), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=667), )] (%1110:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=667)], %1077:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=642)]) -> (%1111:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=667)])
            linalg.CPU.RMSNormOp <name="model.layers.17.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=667), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=668), )] (%1111:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=667)]) -> (%1112:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=668)])
            graph.CallGraphOp @model.layers.17.mlp (%1112:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=668)]) -> (%1117:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=676)])
            linalg.CPU.AddOp <name="model.layers.17.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=676), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=667), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=676), )] (%1117:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=676)], %1111:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=667)]) -> (%1118:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=676)])
            cf.ReturnOp (%1118:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=676)], %1091:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=655)], %1093:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=657)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.17.self_attn <CPU> [using_qnn:true, symbol:model.layers.17.self_attn] {
        (%1078:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=643)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %354:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=20)], %355:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=48)]) -> (%1110:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=667)], %1091:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=655)], %1093:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=657)]) {
            linalg.CPU.LinearOp <name="model.layers.17.self_attn.q_proj">(%1078:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=643)]) -> (%1079:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=649)])
            linalg.CPU.LinearOp <name="model.layers.17.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=643), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=646), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=645))] (%1078:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=643)]) -> (%1080:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=646)])
            linalg.CPU.LinearOp <name="model.layers.17.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=643), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=648), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=647))] (%1078:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=643)]) -> (%1081:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=648)])
            linalg.CPU.ViewOp <name="model.layers.17.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=649), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=649), )] (%1079:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=649)]) -> (%1079:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=649)])
            linalg.CPU.TransposeOp <name="model.layers.17.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=649), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=649), )] (%1079:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=649)]) -> (%1082:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=649)])
            linalg.CPU.ViewOp <name="model.layers.17.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=646), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=646), )] (%1080:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=646)]) -> (%1080:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=646)])
            linalg.CPU.TransposeOp <name="model.layers.17.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=646), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=646), )] (%1080:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=646)]) -> (%1083:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=646)])
            linalg.CPU.ViewOp <name="model.layers.17.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=648), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=648), )] (%1081:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=648)]) -> (%1081:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=648)])
            linalg.CPU.TransposeOp <name="model.layers.17.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=648), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=648), )] (%1081:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=648)]) -> (%1084:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=648)])
            linalg.CPU.RMSNormOp <name="model.layers.17.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=649), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=650), )] (%1082:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=649)]) -> (%1085:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=650)])
            linalg.CPU.RMSNormOp <name="model.layers.17.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=646), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=652), )] (%1083:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=646)]) -> (%1086:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=652)])
            linalg.CPU.RoPEOp <name="model.layers.17.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=650), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=650), )] (%1085:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=650)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%1087:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=650)])
            linalg.CPU.RoPEOp <name="model.layers.17.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=652), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=652), )] (%1086:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=652)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%1088:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=652)])
            linalg.CPU.CastTypeOp <name="model.layers.17.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=652), outputs_0:QuantSpec(Raw(type: Float16), uuid=654), )] (%1088:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=652)]) -> (%1089:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=654)])
            linalg.CPU.CastTypeOp <name="model.layers.17.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=654), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=655), )] (%1089:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=654)]) -> (%1090:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=655)])
            linalg.CPU.TransposeOp <name="model.layers.17.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=655), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=655), )] (%1090:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=655)]) -> (%1091:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=655)])
            linalg.CPU.CastTypeOp <name="model.layers.17.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=648), outputs_0:QuantSpec(Raw(type: Float16), uuid=656), )] (%1084:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=648)]) -> (%1092:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=656)])
            linalg.CPU.CastTypeOp <name="model.layers.17.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=656), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=657), )] (%1092:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=656)]) -> (%1093:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=657)])
            linalg.CPU.ConcatOp <name="model.layers.17.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=20), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=655), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=20), )] (%354:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=20)], %1091:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=655)]) -> (%1094:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=20)])
            linalg.CPU.ConcatOp <name="model.layers.17.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=48), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=657), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=48), )] (%355:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=48)], %1093:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=657)]) -> (%1095:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=48)])
            linalg.CPU.RepeatOp <name="model.layers.17.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=20), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=20), )] (%1094:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=20)]) -> (%1096:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=20)])
            linalg.CPU.RepeatOp <name="model.layers.17.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=48), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=48), )] (%1095:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=48)]) -> (%1097:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=48)])
            linalg.CPU.MatMulOp <name="model.layers.17.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=650), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=20), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=658), )] (%1087:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=650)], %1096:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=20)]) -> (%1098:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=658)])
            linalg.CPU.MulOp <name="model.layers.17.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=658), inputs_1:QuantSpec(Raw(type: Float32), uuid=659), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=658), )] (%1098:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=658)], %1099:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=659), constant:[0.088388346]]) -> (%1100:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=658)])
            linalg.CPU.ReduceMinOp <name="model.layers.17.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=658), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=660), )] (%1100:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=658)]) -> (%1101:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=660)])
            linalg.CPU.AddOp <name="model.layers.17.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=660), inputs_1:QuantSpec(Raw(type: Int16), uuid=661), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=660), )] (%1101:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=660)], %1102:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=661), constant:[-20]]) -> (%1103:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=660)])
            linalg.CPU.EqualOp <name="model.layers.17.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=662), outputs_0:QuantSpec(Raw(type: UInt8), uuid=663), )] (%319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %1104:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=662), constant:[-0.99609375]]) -> (%1105:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=663)])
            linalg.CPU.WhereOp <name="model.layers.17.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=663), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=658), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=660), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=660), )] (%1105:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=663)], %1100:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=658)], %1103:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=660)]) -> (%1106:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=660)])
            linalg.CPU.SoftmaxOp <name="model.layers.17.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=660), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=664), )] (%1106:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=660)]) -> (%1107:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=664)])
            linalg.CPU.MatMulOp <name="model.layers.17.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=664), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=48), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=665), )] (%1107:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=664)], %1097:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=48)]) -> (%1108:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=665)])
            linalg.CPU.TransposeOp <name="model.layers.17.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=665), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=665), )] (%1108:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=665)]) -> (%1109:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=665)])
            linalg.CPU.ViewOp <name="model.layers.17.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=665), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=665), )] (%1109:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=665)]) -> (%1109:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=665)])
            linalg.CPU.LinearOp <name="model.layers.17.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=665), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=667), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=666))] (%1109:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=665)]) -> (%1110:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=667)])
            cf.ReturnOp (%1110:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=667)], %1091:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=655)], %1093:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=657)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.17.mlp <CPU> [using_qnn:true, symbol:model.layers.17.mlp] {
        (%1112:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=668)]) -> (%1117:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=676)]) {
            linalg.CPU.LinearOp <name="model.layers.17.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=668), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=671), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=670))] (%1112:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=668)]) -> (%1113:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=671)])
            linalg.CPU.SiLUOp <name="model.layers.17.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=671), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=672), )] (%1113:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=671)]) -> (%1114:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=672)])
            linalg.CPU.LinearOp <name="model.layers.17.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=668), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=674), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=673))] (%1112:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=668)]) -> (%1115:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=674)])
            linalg.CPU.MulOp <name="model.layers.17.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=672), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=674), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=672), )] (%1114:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=672)], %1115:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=674)]) -> (%1116:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=672)])
            linalg.CPU.LinearOp <name="model.layers.17.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=672), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=676), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=675))] (%1116:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=672)]) -> (%1117:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=676)])
            cf.ReturnOp (%1117:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=676)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.18 <CPU> [using_qnn:true, symbol:model.layers.18] {
        (%1118:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=676)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %356:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=21)], %357:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=49)]) -> (%1159:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=710)], %1132:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=689)], %1134:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=691)]) {
            linalg.CPU.RMSNormOp <name="model.layers.18.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=676), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=677), )] (%1118:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=676)]) -> (%1119:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=677)])
            graph.CallGraphOp @model.layers.18.self_attn (%1119:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=677)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %356:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=21)], %357:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=49)]) -> (%1151:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=701)], %1132:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=689)], %1134:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=691)])
            linalg.CPU.AddOp <name="model.layers.18.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=701), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=676), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=701), )] (%1151:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=701)], %1118:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=676)]) -> (%1152:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=701)])
            linalg.CPU.RMSNormOp <name="model.layers.18.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=701), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=702), )] (%1152:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=701)]) -> (%1153:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=702)])
            graph.CallGraphOp @model.layers.18.mlp (%1153:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=702)]) -> (%1158:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=710)])
            linalg.CPU.AddOp <name="model.layers.18.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=710), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=701), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=710), )] (%1158:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=710)], %1152:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=701)]) -> (%1159:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=710)])
            cf.ReturnOp (%1159:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=710)], %1132:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=689)], %1134:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=691)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.18.self_attn <CPU> [using_qnn:true, symbol:model.layers.18.self_attn] {
        (%1119:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=677)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %356:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=21)], %357:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=49)]) -> (%1151:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=701)], %1132:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=689)], %1134:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=691)]) {
            linalg.CPU.LinearOp <name="model.layers.18.self_attn.q_proj">(%1119:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=677)]) -> (%1120:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=683)])
            linalg.CPU.LinearOp <name="model.layers.18.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=677), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=680), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=679))] (%1119:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=677)]) -> (%1121:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=680)])
            linalg.CPU.LinearOp <name="model.layers.18.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=677), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=682), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=681))] (%1119:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=677)]) -> (%1122:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=682)])
            linalg.CPU.ViewOp <name="model.layers.18.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=683), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=683), )] (%1120:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=683)]) -> (%1120:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=683)])
            linalg.CPU.TransposeOp <name="model.layers.18.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=683), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=683), )] (%1120:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=683)]) -> (%1123:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=683)])
            linalg.CPU.ViewOp <name="model.layers.18.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=680), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=680), )] (%1121:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=680)]) -> (%1121:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=680)])
            linalg.CPU.TransposeOp <name="model.layers.18.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=680), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=680), )] (%1121:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=680)]) -> (%1124:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=680)])
            linalg.CPU.ViewOp <name="model.layers.18.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=682), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=682), )] (%1122:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=682)]) -> (%1122:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=682)])
            linalg.CPU.TransposeOp <name="model.layers.18.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=682), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=682), )] (%1122:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=682)]) -> (%1125:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=682)])
            linalg.CPU.RMSNormOp <name="model.layers.18.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=683), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=684), )] (%1123:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=683)]) -> (%1126:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=684)])
            linalg.CPU.RMSNormOp <name="model.layers.18.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=680), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=686), )] (%1124:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=680)]) -> (%1127:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=686)])
            linalg.CPU.RoPEOp <name="model.layers.18.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=684), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=684), )] (%1126:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=684)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%1128:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=684)])
            linalg.CPU.RoPEOp <name="model.layers.18.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=686), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=686), )] (%1127:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=686)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%1129:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=686)])
            linalg.CPU.CastTypeOp <name="model.layers.18.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=686), outputs_0:QuantSpec(Raw(type: Float16), uuid=688), )] (%1129:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=686)]) -> (%1130:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=688)])
            linalg.CPU.CastTypeOp <name="model.layers.18.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=688), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=689), )] (%1130:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=688)]) -> (%1131:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=689)])
            linalg.CPU.TransposeOp <name="model.layers.18.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=689), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=689), )] (%1131:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=689)]) -> (%1132:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=689)])
            linalg.CPU.CastTypeOp <name="model.layers.18.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=682), outputs_0:QuantSpec(Raw(type: Float16), uuid=690), )] (%1125:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=682)]) -> (%1133:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=690)])
            linalg.CPU.CastTypeOp <name="model.layers.18.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=690), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=691), )] (%1133:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=690)]) -> (%1134:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=691)])
            linalg.CPU.ConcatOp <name="model.layers.18.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=21), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=689), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=21), )] (%356:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=21)], %1132:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=689)]) -> (%1135:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=21)])
            linalg.CPU.ConcatOp <name="model.layers.18.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=49), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=691), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=49), )] (%357:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=49)], %1134:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=691)]) -> (%1136:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=49)])
            linalg.CPU.RepeatOp <name="model.layers.18.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=21), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=21), )] (%1135:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=21)]) -> (%1137:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=21)])
            linalg.CPU.RepeatOp <name="model.layers.18.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=49), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=49), )] (%1136:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=49)]) -> (%1138:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=49)])
            linalg.CPU.MatMulOp <name="model.layers.18.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=684), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=21), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=692), )] (%1128:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=684)], %1137:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=21)]) -> (%1139:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=692)])
            linalg.CPU.MulOp <name="model.layers.18.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=692), inputs_1:QuantSpec(Raw(type: Float32), uuid=693), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=692), )] (%1139:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=692)], %1140:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=693), constant:[0.088388346]]) -> (%1141:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=692)])
            linalg.CPU.ReduceMinOp <name="model.layers.18.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=692), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=694), )] (%1141:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=692)]) -> (%1142:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=694)])
            linalg.CPU.AddOp <name="model.layers.18.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=694), inputs_1:QuantSpec(Raw(type: Int16), uuid=695), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=694), )] (%1142:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=694)], %1143:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=695), constant:[-20]]) -> (%1144:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=694)])
            linalg.CPU.EqualOp <name="model.layers.18.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=696), outputs_0:QuantSpec(Raw(type: UInt8), uuid=697), )] (%319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %1145:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=696), constant:[0.24023438]]) -> (%1146:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=697)])
            linalg.CPU.WhereOp <name="model.layers.18.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=697), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=692), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=694), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=694), )] (%1146:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=697)], %1141:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=692)], %1144:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=694)]) -> (%1147:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=694)])
            linalg.CPU.SoftmaxOp <name="model.layers.18.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=694), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=698), )] (%1147:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=694)]) -> (%1148:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=698)])
            linalg.CPU.MatMulOp <name="model.layers.18.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=698), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=49), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=699), )] (%1148:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=698)], %1138:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=49)]) -> (%1149:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=699)])
            linalg.CPU.TransposeOp <name="model.layers.18.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=699), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=699), )] (%1149:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=699)]) -> (%1150:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=699)])
            linalg.CPU.ViewOp <name="model.layers.18.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=699), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=699), )] (%1150:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=699)]) -> (%1150:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=699)])
            linalg.CPU.LinearOp <name="model.layers.18.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=699), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=701), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=700))] (%1150:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=699)]) -> (%1151:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=701)])
            cf.ReturnOp (%1151:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=701)], %1132:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=689)], %1134:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=691)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.18.mlp <CPU> [using_qnn:true, symbol:model.layers.18.mlp] {
        (%1153:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=702)]) -> (%1158:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=710)]) {
            linalg.CPU.LinearOp <name="model.layers.18.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=702), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=705), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=704))] (%1153:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=702)]) -> (%1154:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=705)])
            linalg.CPU.SiLUOp <name="model.layers.18.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=705), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=706), )] (%1154:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=705)]) -> (%1155:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=706)])
            linalg.CPU.LinearOp <name="model.layers.18.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=702), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=708), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=707))] (%1153:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=702)]) -> (%1156:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=708)])
            linalg.CPU.MulOp <name="model.layers.18.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=706), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=708), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=706), )] (%1155:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=706)], %1156:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=708)]) -> (%1157:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=706)])
            linalg.CPU.LinearOp <name="model.layers.18.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=706), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=710), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=709))] (%1157:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=706)]) -> (%1158:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=710)])
            cf.ReturnOp (%1158:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=710)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.19 <CPU> [using_qnn:true, symbol:model.layers.19] {
        (%1159:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=710)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %358:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=22)], %359:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=50)]) -> (%1200:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=744)], %1173:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=723)], %1175:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=725)]) {
            linalg.CPU.RMSNormOp <name="model.layers.19.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=710), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=711), )] (%1159:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=710)]) -> (%1160:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=711)])
            graph.CallGraphOp @model.layers.19.self_attn (%1160:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=711)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %358:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=22)], %359:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=50)]) -> (%1192:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=735)], %1173:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=723)], %1175:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=725)])
            linalg.CPU.AddOp <name="model.layers.19.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=735), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=710), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=735), )] (%1192:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=735)], %1159:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=710)]) -> (%1193:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=735)])
            linalg.CPU.RMSNormOp <name="model.layers.19.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=735), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=736), )] (%1193:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=735)]) -> (%1194:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=736)])
            graph.CallGraphOp @model.layers.19.mlp (%1194:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=736)]) -> (%1199:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=744)])
            linalg.CPU.AddOp <name="model.layers.19.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=744), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=735), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=744), )] (%1199:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=744)], %1193:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=735)]) -> (%1200:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=744)])
            cf.ReturnOp (%1200:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=744)], %1173:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=723)], %1175:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=725)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.19.self_attn <CPU> [using_qnn:true, symbol:model.layers.19.self_attn] {
        (%1160:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=711)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %358:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=22)], %359:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=50)]) -> (%1192:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=735)], %1173:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=723)], %1175:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=725)]) {
            linalg.CPU.LinearOp <name="model.layers.19.self_attn.q_proj">(%1160:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=711)]) -> (%1161:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=717)])
            linalg.CPU.LinearOp <name="model.layers.19.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=711), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=714), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=713))] (%1160:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=711)]) -> (%1162:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=714)])
            linalg.CPU.LinearOp <name="model.layers.19.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=711), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=716), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=715))] (%1160:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=711)]) -> (%1163:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=716)])
            linalg.CPU.ViewOp <name="model.layers.19.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=717), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=717), )] (%1161:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=717)]) -> (%1161:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=717)])
            linalg.CPU.TransposeOp <name="model.layers.19.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=717), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=717), )] (%1161:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=717)]) -> (%1164:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=717)])
            linalg.CPU.ViewOp <name="model.layers.19.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=714), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=714), )] (%1162:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=714)]) -> (%1162:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=714)])
            linalg.CPU.TransposeOp <name="model.layers.19.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=714), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=714), )] (%1162:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=714)]) -> (%1165:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=714)])
            linalg.CPU.ViewOp <name="model.layers.19.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=716), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=716), )] (%1163:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=716)]) -> (%1163:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=716)])
            linalg.CPU.TransposeOp <name="model.layers.19.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=716), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=716), )] (%1163:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=716)]) -> (%1166:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=716)])
            linalg.CPU.RMSNormOp <name="model.layers.19.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=717), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=718), )] (%1164:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=717)]) -> (%1167:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=718)])
            linalg.CPU.RMSNormOp <name="model.layers.19.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=714), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=720), )] (%1165:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=714)]) -> (%1168:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=720)])
            linalg.CPU.RoPEOp <name="model.layers.19.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=718), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=718), )] (%1167:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=718)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%1169:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=718)])
            linalg.CPU.RoPEOp <name="model.layers.19.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=720), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=720), )] (%1168:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=720)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%1170:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=720)])
            linalg.CPU.CastTypeOp <name="model.layers.19.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=720), outputs_0:QuantSpec(Raw(type: Float16), uuid=722), )] (%1170:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=720)]) -> (%1171:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=722)])
            linalg.CPU.CastTypeOp <name="model.layers.19.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=722), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=723), )] (%1171:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=722)]) -> (%1172:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=723)])
            linalg.CPU.TransposeOp <name="model.layers.19.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=723), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=723), )] (%1172:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=723)]) -> (%1173:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=723)])
            linalg.CPU.CastTypeOp <name="model.layers.19.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=716), outputs_0:QuantSpec(Raw(type: Float16), uuid=724), )] (%1166:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=716)]) -> (%1174:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=724)])
            linalg.CPU.CastTypeOp <name="model.layers.19.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=724), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=725), )] (%1174:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=724)]) -> (%1175:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=725)])
            linalg.CPU.ConcatOp <name="model.layers.19.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=22), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=723), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=22), )] (%358:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=22)], %1173:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=723)]) -> (%1176:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=22)])
            linalg.CPU.ConcatOp <name="model.layers.19.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=50), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=725), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=50), )] (%359:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=50)], %1175:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=725)]) -> (%1177:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=50)])
            linalg.CPU.RepeatOp <name="model.layers.19.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=22), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=22), )] (%1176:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=22)]) -> (%1178:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=22)])
            linalg.CPU.RepeatOp <name="model.layers.19.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=50), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=50), )] (%1177:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=50)]) -> (%1179:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=50)])
            linalg.CPU.MatMulOp <name="model.layers.19.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=718), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=22), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=726), )] (%1169:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=718)], %1178:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=22)]) -> (%1180:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=726)])
            linalg.CPU.MulOp <name="model.layers.19.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=726), inputs_1:QuantSpec(Raw(type: Float32), uuid=727), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=726), )] (%1180:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=726)], %1181:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=727), constant:[0.088388346]]) -> (%1182:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=726)])
            linalg.CPU.ReduceMinOp <name="model.layers.19.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=726), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=728), )] (%1182:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=726)]) -> (%1183:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=728)])
            linalg.CPU.AddOp <name="model.layers.19.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=728), inputs_1:QuantSpec(Raw(type: Int16), uuid=729), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=728), )] (%1183:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=728)], %1184:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=729), constant:[-20]]) -> (%1185:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=728)])
            linalg.CPU.EqualOp <name="model.layers.19.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=730), outputs_0:QuantSpec(Raw(type: UInt8), uuid=731), )] (%319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %1186:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=730), constant:[0.55078125]]) -> (%1187:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=731)])
            linalg.CPU.WhereOp <name="model.layers.19.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=731), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=726), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=728), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=728), )] (%1187:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=731)], %1182:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=726)], %1185:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=728)]) -> (%1188:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=728)])
            linalg.CPU.SoftmaxOp <name="model.layers.19.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=728), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=732), )] (%1188:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=728)]) -> (%1189:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=732)])
            linalg.CPU.MatMulOp <name="model.layers.19.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=732), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=50), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=733), )] (%1189:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=732)], %1179:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=50)]) -> (%1190:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=733)])
            linalg.CPU.TransposeOp <name="model.layers.19.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=733), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=733), )] (%1190:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=733)]) -> (%1191:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=733)])
            linalg.CPU.ViewOp <name="model.layers.19.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=733), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=733), )] (%1191:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=733)]) -> (%1191:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=733)])
            linalg.CPU.LinearOp <name="model.layers.19.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=733), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=735), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=734))] (%1191:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=733)]) -> (%1192:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=735)])
            cf.ReturnOp (%1192:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=735)], %1173:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=723)], %1175:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=725)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.19.mlp <CPU> [using_qnn:true, symbol:model.layers.19.mlp] {
        (%1194:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=736)]) -> (%1199:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=744)]) {
            linalg.CPU.LinearOp <name="model.layers.19.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=736), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=739), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=738))] (%1194:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=736)]) -> (%1195:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=739)])
            linalg.CPU.SiLUOp <name="model.layers.19.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=739), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=740), )] (%1195:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=739)]) -> (%1196:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=740)])
            linalg.CPU.LinearOp <name="model.layers.19.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=736), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=742), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=741))] (%1194:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=736)]) -> (%1197:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=742)])
            linalg.CPU.MulOp <name="model.layers.19.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=740), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=742), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=740), )] (%1196:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=740)], %1197:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=742)]) -> (%1198:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=740)])
            linalg.CPU.LinearOp <name="model.layers.19.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=740), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=744), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=743))] (%1198:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=740)]) -> (%1199:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=744)])
            cf.ReturnOp (%1199:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=744)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.20 <CPU> [using_qnn:true, symbol:model.layers.20] {
        (%1200:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=744)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %360:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=23)], %361:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=51)]) -> (%1241:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=778)], %1214:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=757)], %1216:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=759)]) {
            linalg.CPU.RMSNormOp <name="model.layers.20.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=744), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=745), )] (%1200:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=744)]) -> (%1201:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=745)])
            graph.CallGraphOp @model.layers.20.self_attn (%1201:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=745)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %360:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=23)], %361:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=51)]) -> (%1233:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=769)], %1214:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=757)], %1216:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=759)])
            linalg.CPU.AddOp <name="model.layers.20.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=769), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=744), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=769), )] (%1233:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=769)], %1200:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=744)]) -> (%1234:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=769)])
            linalg.CPU.RMSNormOp <name="model.layers.20.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=769), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=770), )] (%1234:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=769)]) -> (%1235:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=770)])
            graph.CallGraphOp @model.layers.20.mlp (%1235:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=770)]) -> (%1240:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=778)])
            linalg.CPU.AddOp <name="model.layers.20.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=778), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=769), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=778), )] (%1240:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=778)], %1234:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=769)]) -> (%1241:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=778)])
            cf.ReturnOp (%1241:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=778)], %1214:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=757)], %1216:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=759)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.20.self_attn <CPU> [using_qnn:true, symbol:model.layers.20.self_attn] {
        (%1201:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=745)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %360:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=23)], %361:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=51)]) -> (%1233:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=769)], %1214:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=757)], %1216:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=759)]) {
            linalg.CPU.LinearOp <name="model.layers.20.self_attn.q_proj">(%1201:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=745)]) -> (%1202:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=751)])
            linalg.CPU.LinearOp <name="model.layers.20.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=745), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=748), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=747))] (%1201:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=745)]) -> (%1203:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=748)])
            linalg.CPU.LinearOp <name="model.layers.20.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=745), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=750), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=749))] (%1201:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=745)]) -> (%1204:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=750)])
            linalg.CPU.ViewOp <name="model.layers.20.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=751), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=751), )] (%1202:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=751)]) -> (%1202:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=751)])
            linalg.CPU.TransposeOp <name="model.layers.20.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=751), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=751), )] (%1202:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=751)]) -> (%1205:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=751)])
            linalg.CPU.ViewOp <name="model.layers.20.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=748), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=748), )] (%1203:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=748)]) -> (%1203:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=748)])
            linalg.CPU.TransposeOp <name="model.layers.20.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=748), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=748), )] (%1203:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=748)]) -> (%1206:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=748)])
            linalg.CPU.ViewOp <name="model.layers.20.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=750), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=750), )] (%1204:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=750)]) -> (%1204:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=750)])
            linalg.CPU.TransposeOp <name="model.layers.20.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=750), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=750), )] (%1204:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=750)]) -> (%1207:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=750)])
            linalg.CPU.RMSNormOp <name="model.layers.20.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=751), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=752), )] (%1205:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=751)]) -> (%1208:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=752)])
            linalg.CPU.RMSNormOp <name="model.layers.20.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=748), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=754), )] (%1206:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=748)]) -> (%1209:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=754)])
            linalg.CPU.RoPEOp <name="model.layers.20.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=752), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=752), )] (%1208:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=752)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%1210:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=752)])
            linalg.CPU.RoPEOp <name="model.layers.20.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=754), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=754), )] (%1209:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=754)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%1211:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=754)])
            linalg.CPU.CastTypeOp <name="model.layers.20.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=754), outputs_0:QuantSpec(Raw(type: Float16), uuid=756), )] (%1211:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=754)]) -> (%1212:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=756)])
            linalg.CPU.CastTypeOp <name="model.layers.20.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=756), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=757), )] (%1212:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=756)]) -> (%1213:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=757)])
            linalg.CPU.TransposeOp <name="model.layers.20.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=757), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=757), )] (%1213:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=757)]) -> (%1214:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=757)])
            linalg.CPU.CastTypeOp <name="model.layers.20.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=750), outputs_0:QuantSpec(Raw(type: Float16), uuid=758), )] (%1207:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=750)]) -> (%1215:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=758)])
            linalg.CPU.CastTypeOp <name="model.layers.20.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=758), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=759), )] (%1215:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=758)]) -> (%1216:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=759)])
            linalg.CPU.ConcatOp <name="model.layers.20.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=23), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=757), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=23), )] (%360:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=23)], %1214:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=757)]) -> (%1217:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=23)])
            linalg.CPU.ConcatOp <name="model.layers.20.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=51), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=759), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=51), )] (%361:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=51)], %1216:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=759)]) -> (%1218:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=51)])
            linalg.CPU.RepeatOp <name="model.layers.20.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=23), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=23), )] (%1217:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=23)]) -> (%1219:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=23)])
            linalg.CPU.RepeatOp <name="model.layers.20.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=51), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=51), )] (%1218:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=51)]) -> (%1220:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=51)])
            linalg.CPU.MatMulOp <name="model.layers.20.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=752), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=23), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=760), )] (%1210:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=752)], %1219:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=23)]) -> (%1221:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=760)])
            linalg.CPU.MulOp <name="model.layers.20.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=760), inputs_1:QuantSpec(Raw(type: Float32), uuid=761), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=760), )] (%1221:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=760)], %1222:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=761), constant:[0.088388346]]) -> (%1223:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=760)])
            linalg.CPU.ReduceMinOp <name="model.layers.20.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=760), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=762), )] (%1223:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=760)]) -> (%1224:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=762)])
            linalg.CPU.AddOp <name="model.layers.20.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=762), inputs_1:QuantSpec(Raw(type: Int16), uuid=763), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=762), )] (%1224:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=762)], %1225:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=763), constant:[-20]]) -> (%1226:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=762)])
            linalg.CPU.EqualOp <name="model.layers.20.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=764), outputs_0:QuantSpec(Raw(type: UInt8), uuid=765), )] (%319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %1227:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=764), constant:[0.71875]]) -> (%1228:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=765)])
            linalg.CPU.WhereOp <name="model.layers.20.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=765), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=760), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=762), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=762), )] (%1228:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=765)], %1223:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=760)], %1226:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=762)]) -> (%1229:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=762)])
            linalg.CPU.SoftmaxOp <name="model.layers.20.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=762), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=766), )] (%1229:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=762)]) -> (%1230:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=766)])
            linalg.CPU.MatMulOp <name="model.layers.20.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=766), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=51), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=767), )] (%1230:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=766)], %1220:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=51)]) -> (%1231:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=767)])
            linalg.CPU.TransposeOp <name="model.layers.20.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=767), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=767), )] (%1231:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=767)]) -> (%1232:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=767)])
            linalg.CPU.ViewOp <name="model.layers.20.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=767), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=767), )] (%1232:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=767)]) -> (%1232:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=767)])
            linalg.CPU.LinearOp <name="model.layers.20.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=767), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=769), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=768))] (%1232:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=767)]) -> (%1233:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=769)])
            cf.ReturnOp (%1233:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=769)], %1214:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=757)], %1216:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=759)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.20.mlp <CPU> [using_qnn:true, symbol:model.layers.20.mlp] {
        (%1235:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=770)]) -> (%1240:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=778)]) {
            linalg.CPU.LinearOp <name="model.layers.20.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=770), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=773), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=772))] (%1235:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=770)]) -> (%1236:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=773)])
            linalg.CPU.SiLUOp <name="model.layers.20.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=773), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=774), )] (%1236:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=773)]) -> (%1237:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=774)])
            linalg.CPU.LinearOp <name="model.layers.20.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=770), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=776), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=775))] (%1235:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=770)]) -> (%1238:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=776)])
            linalg.CPU.MulOp <name="model.layers.20.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=774), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=776), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=774), )] (%1237:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=774)], %1238:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=776)]) -> (%1239:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=774)])
            linalg.CPU.LinearOp <name="model.layers.20.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=774), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=778), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=777))] (%1239:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=774)]) -> (%1240:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=778)])
            cf.ReturnOp (%1240:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=778)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.21 <CPU> [using_qnn:true, symbol:model.layers.21] {
        (%1241:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=778)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %362:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=24)], %363:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=52)]) -> (%1282:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=812)], %1255:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=791)], %1257:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=793)]) {
            linalg.CPU.RMSNormOp <name="model.layers.21.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=778), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=779), )] (%1241:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=778)]) -> (%1242:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=779)])
            graph.CallGraphOp @model.layers.21.self_attn (%1242:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=779)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %362:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=24)], %363:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=52)]) -> (%1274:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=803)], %1255:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=791)], %1257:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=793)])
            linalg.CPU.AddOp <name="model.layers.21.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=803), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=778), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=803), )] (%1274:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=803)], %1241:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=778)]) -> (%1275:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=803)])
            linalg.CPU.RMSNormOp <name="model.layers.21.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=803), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=804), )] (%1275:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=803)]) -> (%1276:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=804)])
            graph.CallGraphOp @model.layers.21.mlp (%1276:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=804)]) -> (%1281:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=812)])
            linalg.CPU.AddOp <name="model.layers.21.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=812), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=803), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=812), )] (%1281:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=812)], %1275:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=803)]) -> (%1282:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=812)])
            cf.ReturnOp (%1282:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=812)], %1255:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=791)], %1257:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=793)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.21.self_attn <CPU> [using_qnn:true, symbol:model.layers.21.self_attn] {
        (%1242:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=779)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %362:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=24)], %363:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=52)]) -> (%1274:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=803)], %1255:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=791)], %1257:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=793)]) {
            linalg.CPU.LinearOp <name="model.layers.21.self_attn.q_proj">(%1242:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=779)]) -> (%1243:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=785)])
            linalg.CPU.LinearOp <name="model.layers.21.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=779), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=782), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=781))] (%1242:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=779)]) -> (%1244:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=782)])
            linalg.CPU.LinearOp <name="model.layers.21.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=779), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=784), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=783))] (%1242:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=779)]) -> (%1245:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=784)])
            linalg.CPU.ViewOp <name="model.layers.21.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=785), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=785), )] (%1243:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=785)]) -> (%1243:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=785)])
            linalg.CPU.TransposeOp <name="model.layers.21.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=785), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=785), )] (%1243:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=785)]) -> (%1246:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=785)])
            linalg.CPU.ViewOp <name="model.layers.21.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=782), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=782), )] (%1244:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=782)]) -> (%1244:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=782)])
            linalg.CPU.TransposeOp <name="model.layers.21.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=782), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=782), )] (%1244:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=782)]) -> (%1247:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=782)])
            linalg.CPU.ViewOp <name="model.layers.21.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=784), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=784), )] (%1245:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=784)]) -> (%1245:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=784)])
            linalg.CPU.TransposeOp <name="model.layers.21.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=784), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=784), )] (%1245:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=784)]) -> (%1248:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=784)])
            linalg.CPU.RMSNormOp <name="model.layers.21.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=785), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=786), )] (%1246:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=785)]) -> (%1249:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=786)])
            linalg.CPU.RMSNormOp <name="model.layers.21.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=782), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=788), )] (%1247:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=782)]) -> (%1250:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=788)])
            linalg.CPU.RoPEOp <name="model.layers.21.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=786), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=786), )] (%1249:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=786)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%1251:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=786)])
            linalg.CPU.RoPEOp <name="model.layers.21.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=788), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=788), )] (%1250:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=788)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%1252:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=788)])
            linalg.CPU.CastTypeOp <name="model.layers.21.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=788), outputs_0:QuantSpec(Raw(type: Float16), uuid=790), )] (%1252:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=788)]) -> (%1253:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=790)])
            linalg.CPU.CastTypeOp <name="model.layers.21.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=790), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=791), )] (%1253:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=790)]) -> (%1254:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=791)])
            linalg.CPU.TransposeOp <name="model.layers.21.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=791), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=791), )] (%1254:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=791)]) -> (%1255:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=791)])
            linalg.CPU.CastTypeOp <name="model.layers.21.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=784), outputs_0:QuantSpec(Raw(type: Float16), uuid=792), )] (%1248:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=784)]) -> (%1256:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=792)])
            linalg.CPU.CastTypeOp <name="model.layers.21.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=792), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=793), )] (%1256:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=792)]) -> (%1257:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=793)])
            linalg.CPU.ConcatOp <name="model.layers.21.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=24), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=791), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=24), )] (%362:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=24)], %1255:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=791)]) -> (%1258:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=24)])
            linalg.CPU.ConcatOp <name="model.layers.21.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=52), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=793), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=52), )] (%363:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=52)], %1257:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=793)]) -> (%1259:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=52)])
            linalg.CPU.RepeatOp <name="model.layers.21.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=24), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=24), )] (%1258:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=24)]) -> (%1260:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=24)])
            linalg.CPU.RepeatOp <name="model.layers.21.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=52), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=52), )] (%1259:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=52)]) -> (%1261:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=52)])
            linalg.CPU.MatMulOp <name="model.layers.21.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=786), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=24), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=794), )] (%1251:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=786)], %1260:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=24)]) -> (%1262:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=794)])
            linalg.CPU.MulOp <name="model.layers.21.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=794), inputs_1:QuantSpec(Raw(type: Float32), uuid=795), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=794), )] (%1262:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=794)], %1263:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=795), constant:[0.088388346]]) -> (%1264:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=794)])
            linalg.CPU.ReduceMinOp <name="model.layers.21.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=794), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=796), )] (%1264:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=794)]) -> (%1265:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=796)])
            linalg.CPU.AddOp <name="model.layers.21.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=796), inputs_1:QuantSpec(Raw(type: Int16), uuid=797), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=796), )] (%1265:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=796)], %1266:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=797), constant:[-20]]) -> (%1267:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=796)])
            linalg.CPU.EqualOp <name="model.layers.21.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=798), outputs_0:QuantSpec(Raw(type: UInt8), uuid=799), )] (%319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %1268:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=798), constant:[-0.80859375]]) -> (%1269:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=799)])
            linalg.CPU.WhereOp <name="model.layers.21.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=799), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=794), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=796), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=796), )] (%1269:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=799)], %1264:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=794)], %1267:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=796)]) -> (%1270:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=796)])
            linalg.CPU.SoftmaxOp <name="model.layers.21.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=796), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=800), )] (%1270:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=796)]) -> (%1271:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=800)])
            linalg.CPU.MatMulOp <name="model.layers.21.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=800), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=52), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=801), )] (%1271:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=800)], %1261:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=52)]) -> (%1272:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=801)])
            linalg.CPU.TransposeOp <name="model.layers.21.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=801), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=801), )] (%1272:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=801)]) -> (%1273:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=801)])
            linalg.CPU.ViewOp <name="model.layers.21.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=801), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=801), )] (%1273:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=801)]) -> (%1273:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=801)])
            linalg.CPU.LinearOp <name="model.layers.21.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=801), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=803), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=802))] (%1273:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=801)]) -> (%1274:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=803)])
            cf.ReturnOp (%1274:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=803)], %1255:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=791)], %1257:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=793)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.21.mlp <CPU> [using_qnn:true, symbol:model.layers.21.mlp] {
        (%1276:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=804)]) -> (%1281:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=812)]) {
            linalg.CPU.LinearOp <name="model.layers.21.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=804), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=807), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=806))] (%1276:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=804)]) -> (%1277:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=807)])
            linalg.CPU.SiLUOp <name="model.layers.21.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=807), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=808), )] (%1277:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=807)]) -> (%1278:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=808)])
            linalg.CPU.LinearOp <name="model.layers.21.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=804), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=810), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=809))] (%1276:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=804)]) -> (%1279:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=810)])
            linalg.CPU.MulOp <name="model.layers.21.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=808), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=810), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=808), )] (%1278:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=808)], %1279:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=810)]) -> (%1280:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=808)])
            linalg.CPU.LinearOp <name="model.layers.21.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=808), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=812), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=811))] (%1280:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=808)]) -> (%1281:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=812)])
            cf.ReturnOp (%1281:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=812)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.22 <CPU> [using_qnn:true, symbol:model.layers.22] {
        (%1282:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=812)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %364:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=25)], %365:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=53)]) -> (%1323:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=846)], %1296:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=825)], %1298:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=827)]) {
            linalg.CPU.RMSNormOp <name="model.layers.22.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=812), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=813), )] (%1282:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=812)]) -> (%1283:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=813)])
            graph.CallGraphOp @model.layers.22.self_attn (%1283:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=813)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %364:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=25)], %365:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=53)]) -> (%1315:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=837)], %1296:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=825)], %1298:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=827)])
            linalg.CPU.AddOp <name="model.layers.22.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=837), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=812), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=837), )] (%1315:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=837)], %1282:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=812)]) -> (%1316:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=837)])
            linalg.CPU.RMSNormOp <name="model.layers.22.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=837), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=838), )] (%1316:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=837)]) -> (%1317:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=838)])
            graph.CallGraphOp @model.layers.22.mlp (%1317:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=838)]) -> (%1322:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=846)])
            linalg.CPU.AddOp <name="model.layers.22.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=846), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=837), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=846), )] (%1322:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=846)], %1316:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=837)]) -> (%1323:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=846)])
            cf.ReturnOp (%1323:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=846)], %1296:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=825)], %1298:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=827)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.22.self_attn <CPU> [using_qnn:true, symbol:model.layers.22.self_attn] {
        (%1283:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=813)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %364:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=25)], %365:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=53)]) -> (%1315:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=837)], %1296:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=825)], %1298:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=827)]) {
            linalg.CPU.LinearOp <name="model.layers.22.self_attn.q_proj">(%1283:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=813)]) -> (%1284:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=819)])
            linalg.CPU.LinearOp <name="model.layers.22.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=813), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=816), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=815))] (%1283:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=813)]) -> (%1285:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=816)])
            linalg.CPU.LinearOp <name="model.layers.22.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=813), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=818), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=817))] (%1283:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=813)]) -> (%1286:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=818)])
            linalg.CPU.ViewOp <name="model.layers.22.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=819), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=819), )] (%1284:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=819)]) -> (%1284:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=819)])
            linalg.CPU.TransposeOp <name="model.layers.22.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=819), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=819), )] (%1284:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=819)]) -> (%1287:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=819)])
            linalg.CPU.ViewOp <name="model.layers.22.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=816), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=816), )] (%1285:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=816)]) -> (%1285:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=816)])
            linalg.CPU.TransposeOp <name="model.layers.22.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=816), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=816), )] (%1285:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=816)]) -> (%1288:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=816)])
            linalg.CPU.ViewOp <name="model.layers.22.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=818), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=818), )] (%1286:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=818)]) -> (%1286:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=818)])
            linalg.CPU.TransposeOp <name="model.layers.22.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=818), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=818), )] (%1286:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=818)]) -> (%1289:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=818)])
            linalg.CPU.RMSNormOp <name="model.layers.22.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=819), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=820), )] (%1287:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=819)]) -> (%1290:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=820)])
            linalg.CPU.RMSNormOp <name="model.layers.22.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=816), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=822), )] (%1288:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=816)]) -> (%1291:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=822)])
            linalg.CPU.RoPEOp <name="model.layers.22.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=820), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=820), )] (%1290:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=820)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%1292:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=820)])
            linalg.CPU.RoPEOp <name="model.layers.22.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=822), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=822), )] (%1291:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=822)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%1293:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=822)])
            linalg.CPU.CastTypeOp <name="model.layers.22.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=822), outputs_0:QuantSpec(Raw(type: Float16), uuid=824), )] (%1293:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=822)]) -> (%1294:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=824)])
            linalg.CPU.CastTypeOp <name="model.layers.22.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=824), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=825), )] (%1294:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=824)]) -> (%1295:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=825)])
            linalg.CPU.TransposeOp <name="model.layers.22.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=825), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=825), )] (%1295:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=825)]) -> (%1296:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=825)])
            linalg.CPU.CastTypeOp <name="model.layers.22.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=818), outputs_0:QuantSpec(Raw(type: Float16), uuid=826), )] (%1289:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=818)]) -> (%1297:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=826)])
            linalg.CPU.CastTypeOp <name="model.layers.22.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=826), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=827), )] (%1297:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=826)]) -> (%1298:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=827)])
            linalg.CPU.ConcatOp <name="model.layers.22.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=25), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=825), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=25), )] (%364:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=25)], %1296:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=825)]) -> (%1299:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=25)])
            linalg.CPU.ConcatOp <name="model.layers.22.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=53), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=827), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=53), )] (%365:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=53)], %1298:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=827)]) -> (%1300:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=53)])
            linalg.CPU.RepeatOp <name="model.layers.22.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=25), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=25), )] (%1299:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=25)]) -> (%1301:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=25)])
            linalg.CPU.RepeatOp <name="model.layers.22.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=53), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=53), )] (%1300:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=53)]) -> (%1302:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=53)])
            linalg.CPU.MatMulOp <name="model.layers.22.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=820), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=25), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=828), )] (%1292:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=820)], %1301:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=25)]) -> (%1303:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=828)])
            linalg.CPU.MulOp <name="model.layers.22.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=828), inputs_1:QuantSpec(Raw(type: Float32), uuid=829), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=828), )] (%1303:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=828)], %1304:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=829), constant:[0.088388346]]) -> (%1305:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=828)])
            linalg.CPU.ReduceMinOp <name="model.layers.22.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=828), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=830), )] (%1305:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=828)]) -> (%1306:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=830)])
            linalg.CPU.AddOp <name="model.layers.22.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=830), inputs_1:QuantSpec(Raw(type: Int16), uuid=831), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=830), )] (%1306:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=830)], %1307:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=831), constant:[-20]]) -> (%1308:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=830)])
            linalg.CPU.EqualOp <name="model.layers.22.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=832), outputs_0:QuantSpec(Raw(type: UInt8), uuid=833), )] (%319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %1309:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=832), constant:[-0.42773438]]) -> (%1310:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=833)])
            linalg.CPU.WhereOp <name="model.layers.22.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=833), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=828), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=830), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=830), )] (%1310:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=833)], %1305:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=828)], %1308:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=830)]) -> (%1311:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=830)])
            linalg.CPU.SoftmaxOp <name="model.layers.22.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=830), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=834), )] (%1311:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=830)]) -> (%1312:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=834)])
            linalg.CPU.MatMulOp <name="model.layers.22.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=834), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=53), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=835), )] (%1312:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=834)], %1302:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=53)]) -> (%1313:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=835)])
            linalg.CPU.TransposeOp <name="model.layers.22.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=835), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=835), )] (%1313:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=835)]) -> (%1314:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=835)])
            linalg.CPU.ViewOp <name="model.layers.22.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=835), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=835), )] (%1314:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=835)]) -> (%1314:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=835)])
            linalg.CPU.LinearOp <name="model.layers.22.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=835), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=837), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=836))] (%1314:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=835)]) -> (%1315:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=837)])
            cf.ReturnOp (%1315:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=837)], %1296:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=825)], %1298:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=827)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.22.mlp <CPU> [using_qnn:true, symbol:model.layers.22.mlp] {
        (%1317:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=838)]) -> (%1322:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=846)]) {
            linalg.CPU.LinearOp <name="model.layers.22.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=838), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=841), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=840))] (%1317:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=838)]) -> (%1318:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=841)])
            linalg.CPU.SiLUOp <name="model.layers.22.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=841), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=842), )] (%1318:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=841)]) -> (%1319:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=842)])
            linalg.CPU.LinearOp <name="model.layers.22.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=838), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=844), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=843))] (%1317:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=838)]) -> (%1320:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=844)])
            linalg.CPU.MulOp <name="model.layers.22.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=842), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=844), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=842), )] (%1319:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=842)], %1320:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=844)]) -> (%1321:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=842)])
            linalg.CPU.LinearOp <name="model.layers.22.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=842), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=846), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=845))] (%1321:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=842)]) -> (%1322:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=846)])
            cf.ReturnOp (%1322:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=846)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.23 <CPU> [using_qnn:true, symbol:model.layers.23] {
        (%1323:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=846)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %366:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=26)], %367:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=54)]) -> (%1364:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=880)], %1337:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=859)], %1339:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=861)]) {
            linalg.CPU.RMSNormOp <name="model.layers.23.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=846), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=847), )] (%1323:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=846)]) -> (%1324:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=847)])
            graph.CallGraphOp @model.layers.23.self_attn (%1324:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=847)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %366:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=26)], %367:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=54)]) -> (%1356:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=871)], %1337:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=859)], %1339:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=861)])
            linalg.CPU.AddOp <name="model.layers.23.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=871), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=846), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=871), )] (%1356:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=871)], %1323:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=846)]) -> (%1357:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=871)])
            linalg.CPU.RMSNormOp <name="model.layers.23.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=871), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=872), )] (%1357:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=871)]) -> (%1358:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=872)])
            graph.CallGraphOp @model.layers.23.mlp (%1358:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=872)]) -> (%1363:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=880)])
            linalg.CPU.AddOp <name="model.layers.23.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=880), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=871), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=880), )] (%1363:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=880)], %1357:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=871)]) -> (%1364:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=880)])
            cf.ReturnOp (%1364:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=880)], %1337:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=859)], %1339:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=861)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.23.self_attn <CPU> [using_qnn:true, symbol:model.layers.23.self_attn] {
        (%1324:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=847)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %366:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=26)], %367:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=54)]) -> (%1356:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=871)], %1337:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=859)], %1339:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=861)]) {
            linalg.CPU.LinearOp <name="model.layers.23.self_attn.q_proj">(%1324:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=847)]) -> (%1325:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=853)])
            linalg.CPU.LinearOp <name="model.layers.23.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=847), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=850), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=849))] (%1324:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=847)]) -> (%1326:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=850)])
            linalg.CPU.LinearOp <name="model.layers.23.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=847), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=852), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=851))] (%1324:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=847)]) -> (%1327:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=852)])
            linalg.CPU.ViewOp <name="model.layers.23.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=853), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=853), )] (%1325:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=853)]) -> (%1325:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=853)])
            linalg.CPU.TransposeOp <name="model.layers.23.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=853), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=853), )] (%1325:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=853)]) -> (%1328:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=853)])
            linalg.CPU.ViewOp <name="model.layers.23.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=850), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=850), )] (%1326:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=850)]) -> (%1326:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=850)])
            linalg.CPU.TransposeOp <name="model.layers.23.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=850), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=850), )] (%1326:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=850)]) -> (%1329:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=850)])
            linalg.CPU.ViewOp <name="model.layers.23.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=852), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=852), )] (%1327:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=852)]) -> (%1327:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=852)])
            linalg.CPU.TransposeOp <name="model.layers.23.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=852), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=852), )] (%1327:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=852)]) -> (%1330:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=852)])
            linalg.CPU.RMSNormOp <name="model.layers.23.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=853), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=854), )] (%1328:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=853)]) -> (%1331:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=854)])
            linalg.CPU.RMSNormOp <name="model.layers.23.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=850), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=856), )] (%1329:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=850)]) -> (%1332:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=856)])
            linalg.CPU.RoPEOp <name="model.layers.23.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=854), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=854), )] (%1331:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=854)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%1333:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=854)])
            linalg.CPU.RoPEOp <name="model.layers.23.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=856), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=856), )] (%1332:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=856)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%1334:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=856)])
            linalg.CPU.CastTypeOp <name="model.layers.23.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=856), outputs_0:QuantSpec(Raw(type: Float16), uuid=858), )] (%1334:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=856)]) -> (%1335:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=858)])
            linalg.CPU.CastTypeOp <name="model.layers.23.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=858), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=859), )] (%1335:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=858)]) -> (%1336:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=859)])
            linalg.CPU.TransposeOp <name="model.layers.23.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=859), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=859), )] (%1336:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=859)]) -> (%1337:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=859)])
            linalg.CPU.CastTypeOp <name="model.layers.23.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=852), outputs_0:QuantSpec(Raw(type: Float16), uuid=860), )] (%1330:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=852)]) -> (%1338:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=860)])
            linalg.CPU.CastTypeOp <name="model.layers.23.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=860), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=861), )] (%1338:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=860)]) -> (%1339:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=861)])
            linalg.CPU.ConcatOp <name="model.layers.23.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=26), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=859), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=26), )] (%366:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=26)], %1337:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=859)]) -> (%1340:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=26)])
            linalg.CPU.ConcatOp <name="model.layers.23.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=54), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=861), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=54), )] (%367:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=54)], %1339:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=861)]) -> (%1341:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=54)])
            linalg.CPU.RepeatOp <name="model.layers.23.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=26), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=26), )] (%1340:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=26)]) -> (%1342:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=26)])
            linalg.CPU.RepeatOp <name="model.layers.23.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=54), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=54), )] (%1341:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=54)]) -> (%1343:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=54)])
            linalg.CPU.MatMulOp <name="model.layers.23.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=854), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=26), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=862), )] (%1333:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=854)], %1342:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=26)]) -> (%1344:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=862)])
            linalg.CPU.MulOp <name="model.layers.23.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=862), inputs_1:QuantSpec(Raw(type: Float32), uuid=863), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=862), )] (%1344:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=862)], %1345:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=863), constant:[0.088388346]]) -> (%1346:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=862)])
            linalg.CPU.ReduceMinOp <name="model.layers.23.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=862), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=864), )] (%1346:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=862)]) -> (%1347:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=864)])
            linalg.CPU.AddOp <name="model.layers.23.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=864), inputs_1:QuantSpec(Raw(type: Int16), uuid=865), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=864), )] (%1347:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=864)], %1348:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=865), constant:[-20]]) -> (%1349:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=864)])
            linalg.CPU.EqualOp <name="model.layers.23.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=866), outputs_0:QuantSpec(Raw(type: UInt8), uuid=867), )] (%319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %1350:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=866), constant:[0.96484375]]) -> (%1351:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=867)])
            linalg.CPU.WhereOp <name="model.layers.23.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=867), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=862), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=864), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=864), )] (%1351:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=867)], %1346:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=862)], %1349:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=864)]) -> (%1352:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=864)])
            linalg.CPU.SoftmaxOp <name="model.layers.23.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=864), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=868), )] (%1352:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=864)]) -> (%1353:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=868)])
            linalg.CPU.MatMulOp <name="model.layers.23.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=868), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=54), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=869), )] (%1353:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=868)], %1343:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=54)]) -> (%1354:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=869)])
            linalg.CPU.TransposeOp <name="model.layers.23.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=869), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=869), )] (%1354:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=869)]) -> (%1355:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=869)])
            linalg.CPU.ViewOp <name="model.layers.23.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=869), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=869), )] (%1355:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=869)]) -> (%1355:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=869)])
            linalg.CPU.LinearOp <name="model.layers.23.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=869), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=871), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=870))] (%1355:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=869)]) -> (%1356:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=871)])
            cf.ReturnOp (%1356:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=871)], %1337:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=859)], %1339:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=861)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.23.mlp <CPU> [using_qnn:true, symbol:model.layers.23.mlp] {
        (%1358:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=872)]) -> (%1363:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=880)]) {
            linalg.CPU.LinearOp <name="model.layers.23.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=872), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=875), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=874))] (%1358:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=872)]) -> (%1359:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=875)])
            linalg.CPU.SiLUOp <name="model.layers.23.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=875), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=876), )] (%1359:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=875)]) -> (%1360:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=876)])
            linalg.CPU.LinearOp <name="model.layers.23.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=872), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=878), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=877))] (%1358:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=872)]) -> (%1361:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=878)])
            linalg.CPU.MulOp <name="model.layers.23.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=876), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=878), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=876), )] (%1360:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=876)], %1361:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=878)]) -> (%1362:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=876)])
            linalg.CPU.LinearOp <name="model.layers.23.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=876), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=880), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=879))] (%1362:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=876)]) -> (%1363:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=880)])
            cf.ReturnOp (%1363:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=880)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.24 <CPU> [using_qnn:true, symbol:model.layers.24] {
        (%1364:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=880)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %368:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=27)], %369:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=55)]) -> (%1405:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=914)], %1378:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=893)], %1380:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=895)]) {
            linalg.CPU.RMSNormOp <name="model.layers.24.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=880), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=881), )] (%1364:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=880)]) -> (%1365:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=881)])
            graph.CallGraphOp @model.layers.24.self_attn (%1365:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=881)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %368:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=27)], %369:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=55)]) -> (%1397:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=905)], %1378:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=893)], %1380:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=895)])
            linalg.CPU.AddOp <name="model.layers.24.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=905), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=880), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=905), )] (%1397:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=905)], %1364:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=880)]) -> (%1398:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=905)])
            linalg.CPU.RMSNormOp <name="model.layers.24.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=905), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=906), )] (%1398:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=905)]) -> (%1399:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=906)])
            graph.CallGraphOp @model.layers.24.mlp (%1399:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=906)]) -> (%1404:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=914)])
            linalg.CPU.AddOp <name="model.layers.24.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=914), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=905), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=914), )] (%1404:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=914)], %1398:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=905)]) -> (%1405:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=914)])
            cf.ReturnOp (%1405:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=914)], %1378:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=893)], %1380:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=895)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.24.self_attn <CPU> [using_qnn:true, symbol:model.layers.24.self_attn] {
        (%1365:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=881)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %368:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=27)], %369:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=55)]) -> (%1397:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=905)], %1378:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=893)], %1380:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=895)]) {
            linalg.CPU.LinearOp <name="model.layers.24.self_attn.q_proj">(%1365:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=881)]) -> (%1366:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=887)])
            linalg.CPU.LinearOp <name="model.layers.24.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=881), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=884), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=883))] (%1365:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=881)]) -> (%1367:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=884)])
            linalg.CPU.LinearOp <name="model.layers.24.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=881), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=886), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=885))] (%1365:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=881)]) -> (%1368:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=886)])
            linalg.CPU.ViewOp <name="model.layers.24.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=887), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=887), )] (%1366:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=887)]) -> (%1366:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=887)])
            linalg.CPU.TransposeOp <name="model.layers.24.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=887), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=887), )] (%1366:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=887)]) -> (%1369:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=887)])
            linalg.CPU.ViewOp <name="model.layers.24.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=884), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=884), )] (%1367:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=884)]) -> (%1367:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=884)])
            linalg.CPU.TransposeOp <name="model.layers.24.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=884), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=884), )] (%1367:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=884)]) -> (%1370:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=884)])
            linalg.CPU.ViewOp <name="model.layers.24.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=886), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=886), )] (%1368:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=886)]) -> (%1368:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=886)])
            linalg.CPU.TransposeOp <name="model.layers.24.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=886), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=886), )] (%1368:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=886)]) -> (%1371:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=886)])
            linalg.CPU.RMSNormOp <name="model.layers.24.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=887), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=888), )] (%1369:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=887)]) -> (%1372:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=888)])
            linalg.CPU.RMSNormOp <name="model.layers.24.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=884), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=890), )] (%1370:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=884)]) -> (%1373:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=890)])
            linalg.CPU.RoPEOp <name="model.layers.24.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=888), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=888), )] (%1372:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=888)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%1374:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=888)])
            linalg.CPU.RoPEOp <name="model.layers.24.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=890), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=890), )] (%1373:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=890)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%1375:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=890)])
            linalg.CPU.CastTypeOp <name="model.layers.24.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=890), outputs_0:QuantSpec(Raw(type: Float16), uuid=892), )] (%1375:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=890)]) -> (%1376:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=892)])
            linalg.CPU.CastTypeOp <name="model.layers.24.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=892), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=893), )] (%1376:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=892)]) -> (%1377:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=893)])
            linalg.CPU.TransposeOp <name="model.layers.24.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=893), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=893), )] (%1377:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=893)]) -> (%1378:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=893)])
            linalg.CPU.CastTypeOp <name="model.layers.24.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=886), outputs_0:QuantSpec(Raw(type: Float16), uuid=894), )] (%1371:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=886)]) -> (%1379:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=894)])
            linalg.CPU.CastTypeOp <name="model.layers.24.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=894), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=895), )] (%1379:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=894)]) -> (%1380:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=895)])
            linalg.CPU.ConcatOp <name="model.layers.24.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=27), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=893), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=27), )] (%368:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=27)], %1378:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=893)]) -> (%1381:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=27)])
            linalg.CPU.ConcatOp <name="model.layers.24.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=55), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=895), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=55), )] (%369:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=55)], %1380:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=895)]) -> (%1382:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=55)])
            linalg.CPU.RepeatOp <name="model.layers.24.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=27), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=27), )] (%1381:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=27)]) -> (%1383:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=27)])
            linalg.CPU.RepeatOp <name="model.layers.24.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=55), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=55), )] (%1382:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=55)]) -> (%1384:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=55)])
            linalg.CPU.MatMulOp <name="model.layers.24.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=888), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=27), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=896), )] (%1374:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=888)], %1383:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=27)]) -> (%1385:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=896)])
            linalg.CPU.MulOp <name="model.layers.24.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=896), inputs_1:QuantSpec(Raw(type: Float32), uuid=897), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=896), )] (%1385:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=896)], %1386:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=897), constant:[0.088388346]]) -> (%1387:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=896)])
            linalg.CPU.ReduceMinOp <name="model.layers.24.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=896), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=898), )] (%1387:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=896)]) -> (%1388:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=898)])
            linalg.CPU.AddOp <name="model.layers.24.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=898), inputs_1:QuantSpec(Raw(type: Int16), uuid=899), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=898), )] (%1388:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=898)], %1389:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=899), constant:[-20]]) -> (%1390:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=898)])
            linalg.CPU.EqualOp <name="model.layers.24.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=900), outputs_0:QuantSpec(Raw(type: UInt8), uuid=901), )] (%319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %1391:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=900), constant:[0.07910156]]) -> (%1392:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=901)])
            linalg.CPU.WhereOp <name="model.layers.24.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=901), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=896), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=898), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=898), )] (%1392:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=901)], %1387:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=896)], %1390:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=898)]) -> (%1393:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=898)])
            linalg.CPU.SoftmaxOp <name="model.layers.24.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=898), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=902), )] (%1393:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=898)]) -> (%1394:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=902)])
            linalg.CPU.MatMulOp <name="model.layers.24.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=902), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=55), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=903), )] (%1394:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=902)], %1384:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=55)]) -> (%1395:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=903)])
            linalg.CPU.TransposeOp <name="model.layers.24.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=903), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=903), )] (%1395:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=903)]) -> (%1396:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=903)])
            linalg.CPU.ViewOp <name="model.layers.24.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=903), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=903), )] (%1396:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=903)]) -> (%1396:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=903)])
            linalg.CPU.LinearOp <name="model.layers.24.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=903), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=905), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=904))] (%1396:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=903)]) -> (%1397:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=905)])
            cf.ReturnOp (%1397:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=905)], %1378:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=893)], %1380:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=895)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.24.mlp <CPU> [using_qnn:true, symbol:model.layers.24.mlp] {
        (%1399:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=906)]) -> (%1404:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=914)]) {
            linalg.CPU.LinearOp <name="model.layers.24.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=906), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=909), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=908))] (%1399:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=906)]) -> (%1400:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=909)])
            linalg.CPU.SiLUOp <name="model.layers.24.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=909), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=910), )] (%1400:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=909)]) -> (%1401:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=910)])
            linalg.CPU.LinearOp <name="model.layers.24.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=906), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=912), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=911))] (%1399:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=906)]) -> (%1402:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=912)])
            linalg.CPU.MulOp <name="model.layers.24.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=910), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=912), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=910), )] (%1401:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=910)], %1402:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=912)]) -> (%1403:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=910)])
            linalg.CPU.LinearOp <name="model.layers.24.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=910), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=914), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=913))] (%1403:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=910)]) -> (%1404:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=914)])
            cf.ReturnOp (%1404:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=914)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.25 <CPU> [using_qnn:true, symbol:model.layers.25] {
        (%1405:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=914)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %370:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=28)], %371:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=56)]) -> (%1446:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=948)], %1419:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=927)], %1421:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=929)]) {
            linalg.CPU.RMSNormOp <name="model.layers.25.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=914), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=915), )] (%1405:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=914)]) -> (%1406:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=915)])
            graph.CallGraphOp @model.layers.25.self_attn (%1406:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=915)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %370:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=28)], %371:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=56)]) -> (%1438:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=939)], %1419:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=927)], %1421:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=929)])
            linalg.CPU.AddOp <name="model.layers.25.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=939), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=914), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=939), )] (%1438:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=939)], %1405:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=914)]) -> (%1439:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=939)])
            linalg.CPU.RMSNormOp <name="model.layers.25.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=939), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=940), )] (%1439:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=939)]) -> (%1440:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=940)])
            graph.CallGraphOp @model.layers.25.mlp (%1440:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=940)]) -> (%1445:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=948)])
            linalg.CPU.AddOp <name="model.layers.25.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=948), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=939), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=948), )] (%1445:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=948)], %1439:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=939)]) -> (%1446:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=948)])
            cf.ReturnOp (%1446:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=948)], %1419:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=927)], %1421:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=929)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.25.self_attn <CPU> [using_qnn:true, symbol:model.layers.25.self_attn] {
        (%1406:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=915)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %370:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=28)], %371:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=56)]) -> (%1438:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=939)], %1419:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=927)], %1421:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=929)]) {
            linalg.CPU.LinearOp <name="model.layers.25.self_attn.q_proj">(%1406:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=915)]) -> (%1407:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=921)])
            linalg.CPU.LinearOp <name="model.layers.25.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=915), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=918), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=917))] (%1406:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=915)]) -> (%1408:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=918)])
            linalg.CPU.LinearOp <name="model.layers.25.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=915), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=920), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=919))] (%1406:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=915)]) -> (%1409:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=920)])
            linalg.CPU.ViewOp <name="model.layers.25.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=921), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=921), )] (%1407:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=921)]) -> (%1407:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=921)])
            linalg.CPU.TransposeOp <name="model.layers.25.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=921), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=921), )] (%1407:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=921)]) -> (%1410:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=921)])
            linalg.CPU.ViewOp <name="model.layers.25.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=918), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=918), )] (%1408:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=918)]) -> (%1408:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=918)])
            linalg.CPU.TransposeOp <name="model.layers.25.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=918), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=918), )] (%1408:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=918)]) -> (%1411:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=918)])
            linalg.CPU.ViewOp <name="model.layers.25.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=920), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=920), )] (%1409:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=920)]) -> (%1409:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=920)])
            linalg.CPU.TransposeOp <name="model.layers.25.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=920), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=920), )] (%1409:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=920)]) -> (%1412:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=920)])
            linalg.CPU.RMSNormOp <name="model.layers.25.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=921), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=922), )] (%1410:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=921)]) -> (%1413:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=922)])
            linalg.CPU.RMSNormOp <name="model.layers.25.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=918), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=924), )] (%1411:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=918)]) -> (%1414:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=924)])
            linalg.CPU.RoPEOp <name="model.layers.25.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=922), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=922), )] (%1413:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=922)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%1415:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=922)])
            linalg.CPU.RoPEOp <name="model.layers.25.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=924), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=924), )] (%1414:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=924)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%1416:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=924)])
            linalg.CPU.CastTypeOp <name="model.layers.25.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=924), outputs_0:QuantSpec(Raw(type: Float16), uuid=926), )] (%1416:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=924)]) -> (%1417:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=926)])
            linalg.CPU.CastTypeOp <name="model.layers.25.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=926), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=927), )] (%1417:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=926)]) -> (%1418:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=927)])
            linalg.CPU.TransposeOp <name="model.layers.25.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=927), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=927), )] (%1418:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=927)]) -> (%1419:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=927)])
            linalg.CPU.CastTypeOp <name="model.layers.25.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=920), outputs_0:QuantSpec(Raw(type: Float16), uuid=928), )] (%1412:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=920)]) -> (%1420:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=928)])
            linalg.CPU.CastTypeOp <name="model.layers.25.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=928), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=929), )] (%1420:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=928)]) -> (%1421:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=929)])
            linalg.CPU.ConcatOp <name="model.layers.25.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=28), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=927), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=28), )] (%370:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=28)], %1419:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=927)]) -> (%1422:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=28)])
            linalg.CPU.ConcatOp <name="model.layers.25.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=56), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=929), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=56), )] (%371:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=56)], %1421:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=929)]) -> (%1423:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=56)])
            linalg.CPU.RepeatOp <name="model.layers.25.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=28), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=28), )] (%1422:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=28)]) -> (%1424:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=28)])
            linalg.CPU.RepeatOp <name="model.layers.25.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=56), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=56), )] (%1423:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=56)]) -> (%1425:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=56)])
            linalg.CPU.MatMulOp <name="model.layers.25.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=922), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=28), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=930), )] (%1415:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=922)], %1424:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=28)]) -> (%1426:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=930)])
            linalg.CPU.MulOp <name="model.layers.25.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=930), inputs_1:QuantSpec(Raw(type: Float32), uuid=931), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=930), )] (%1426:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=930)], %1427:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=931), constant:[0.088388346]]) -> (%1428:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=930)])
            linalg.CPU.ReduceMinOp <name="model.layers.25.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=930), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=932), )] (%1428:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=930)]) -> (%1429:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=932)])
            linalg.CPU.AddOp <name="model.layers.25.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=932), inputs_1:QuantSpec(Raw(type: Int16), uuid=933), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=932), )] (%1429:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=932)], %1430:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=933), constant:[-20]]) -> (%1431:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=932)])
            linalg.CPU.EqualOp <name="model.layers.25.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=934), outputs_0:QuantSpec(Raw(type: UInt8), uuid=935), )] (%319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %1432:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=934), constant:[-0.9921875]]) -> (%1433:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=935)])
            linalg.CPU.WhereOp <name="model.layers.25.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=935), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=930), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=932), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=932), )] (%1433:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=935)], %1428:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=930)], %1431:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=932)]) -> (%1434:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=932)])
            linalg.CPU.SoftmaxOp <name="model.layers.25.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=932), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=936), )] (%1434:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=932)]) -> (%1435:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=936)])
            linalg.CPU.MatMulOp <name="model.layers.25.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=936), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=56), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=937), )] (%1435:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=936)], %1425:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=56)]) -> (%1436:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=937)])
            linalg.CPU.TransposeOp <name="model.layers.25.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=937), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=937), )] (%1436:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=937)]) -> (%1437:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=937)])
            linalg.CPU.ViewOp <name="model.layers.25.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=937), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=937), )] (%1437:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=937)]) -> (%1437:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=937)])
            linalg.CPU.LinearOp <name="model.layers.25.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=937), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=939), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=938))] (%1437:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=937)]) -> (%1438:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=939)])
            cf.ReturnOp (%1438:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=939)], %1419:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=927)], %1421:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=929)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.25.mlp <CPU> [using_qnn:true, symbol:model.layers.25.mlp] {
        (%1440:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=940)]) -> (%1445:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=948)]) {
            linalg.CPU.LinearOp <name="model.layers.25.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=940), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=943), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=942))] (%1440:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=940)]) -> (%1441:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=943)])
            linalg.CPU.SiLUOp <name="model.layers.25.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=943), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=944), )] (%1441:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=943)]) -> (%1442:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=944)])
            linalg.CPU.LinearOp <name="model.layers.25.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=940), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=946), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=945))] (%1440:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=940)]) -> (%1443:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=946)])
            linalg.CPU.MulOp <name="model.layers.25.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=944), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=946), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=944), )] (%1442:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=944)], %1443:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=946)]) -> (%1444:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=944)])
            linalg.CPU.LinearOp <name="model.layers.25.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=944), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=948), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=947))] (%1444:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=944)]) -> (%1445:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=948)])
            cf.ReturnOp (%1445:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=948)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.26 <CPU> [using_qnn:true, symbol:model.layers.26] {
        (%1446:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=948)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %372:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=29)], %373:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=57)]) -> (%1487:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=982)], %1460:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=961)], %1462:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=963)]) {
            linalg.CPU.RMSNormOp <name="model.layers.26.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=948), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=949), )] (%1446:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=948)]) -> (%1447:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=949)])
            graph.CallGraphOp @model.layers.26.self_attn (%1447:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=949)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %372:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=29)], %373:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=57)]) -> (%1479:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=973)], %1460:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=961)], %1462:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=963)])
            linalg.CPU.AddOp <name="model.layers.26.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=973), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=948), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=973), )] (%1479:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=973)], %1446:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=948)]) -> (%1480:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=973)])
            linalg.CPU.RMSNormOp <name="model.layers.26.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=973), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=974), )] (%1480:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=973)]) -> (%1481:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=974)])
            graph.CallGraphOp @model.layers.26.mlp (%1481:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=974)]) -> (%1486:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=982)])
            linalg.CPU.AddOp <name="model.layers.26.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=982), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=973), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=982), )] (%1486:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=982)], %1480:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=973)]) -> (%1487:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=982)])
            cf.ReturnOp (%1487:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=982)], %1460:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=961)], %1462:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=963)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.26.self_attn <CPU> [using_qnn:true, symbol:model.layers.26.self_attn] {
        (%1447:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=949)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %372:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=29)], %373:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=57)]) -> (%1479:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=973)], %1460:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=961)], %1462:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=963)]) {
            linalg.CPU.LinearOp <name="model.layers.26.self_attn.q_proj">(%1447:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=949)]) -> (%1448:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=955)])
            linalg.CPU.LinearOp <name="model.layers.26.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=949), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=952), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=951))] (%1447:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=949)]) -> (%1449:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=952)])
            linalg.CPU.LinearOp <name="model.layers.26.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=949), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=954), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=953))] (%1447:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=949)]) -> (%1450:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=954)])
            linalg.CPU.ViewOp <name="model.layers.26.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=955), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=955), )] (%1448:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=955)]) -> (%1448:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=955)])
            linalg.CPU.TransposeOp <name="model.layers.26.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=955), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=955), )] (%1448:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=955)]) -> (%1451:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=955)])
            linalg.CPU.ViewOp <name="model.layers.26.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=952), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=952), )] (%1449:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=952)]) -> (%1449:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=952)])
            linalg.CPU.TransposeOp <name="model.layers.26.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=952), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=952), )] (%1449:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=952)]) -> (%1452:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=952)])
            linalg.CPU.ViewOp <name="model.layers.26.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=954), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=954), )] (%1450:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=954)]) -> (%1450:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=954)])
            linalg.CPU.TransposeOp <name="model.layers.26.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=954), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=954), )] (%1450:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=954)]) -> (%1453:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=954)])
            linalg.CPU.RMSNormOp <name="model.layers.26.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=955), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=956), )] (%1451:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=955)]) -> (%1454:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=956)])
            linalg.CPU.RMSNormOp <name="model.layers.26.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=952), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=958), )] (%1452:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=952)]) -> (%1455:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=958)])
            linalg.CPU.RoPEOp <name="model.layers.26.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=956), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=956), )] (%1454:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=956)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%1456:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=956)])
            linalg.CPU.RoPEOp <name="model.layers.26.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=958), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=958), )] (%1455:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=958)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%1457:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=958)])
            linalg.CPU.CastTypeOp <name="model.layers.26.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=958), outputs_0:QuantSpec(Raw(type: Float16), uuid=960), )] (%1457:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=958)]) -> (%1458:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=960)])
            linalg.CPU.CastTypeOp <name="model.layers.26.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=960), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=961), )] (%1458:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=960)]) -> (%1459:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=961)])
            linalg.CPU.TransposeOp <name="model.layers.26.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=961), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=961), )] (%1459:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=961)]) -> (%1460:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=961)])
            linalg.CPU.CastTypeOp <name="model.layers.26.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=954), outputs_0:QuantSpec(Raw(type: Float16), uuid=962), )] (%1453:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=954)]) -> (%1461:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=962)])
            linalg.CPU.CastTypeOp <name="model.layers.26.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=962), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=963), )] (%1461:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=962)]) -> (%1462:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=963)])
            linalg.CPU.ConcatOp <name="model.layers.26.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=29), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=961), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=29), )] (%372:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=29)], %1460:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=961)]) -> (%1463:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=29)])
            linalg.CPU.ConcatOp <name="model.layers.26.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=57), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=963), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=57), )] (%373:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=57)], %1462:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=963)]) -> (%1464:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=57)])
            linalg.CPU.RepeatOp <name="model.layers.26.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=29), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=29), )] (%1463:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=29)]) -> (%1465:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=29)])
            linalg.CPU.RepeatOp <name="model.layers.26.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=57), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=57), )] (%1464:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=57)]) -> (%1466:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=57)])
            linalg.CPU.MatMulOp <name="model.layers.26.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=956), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=29), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=964), )] (%1456:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=956)], %1465:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=29)]) -> (%1467:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=964)])
            linalg.CPU.MulOp <name="model.layers.26.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=964), inputs_1:QuantSpec(Raw(type: Float32), uuid=965), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=964), )] (%1467:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=964)], %1468:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=965), constant:[0.088388346]]) -> (%1469:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=964)])
            linalg.CPU.ReduceMinOp <name="model.layers.26.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=964), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=966), )] (%1469:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=964)]) -> (%1470:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=966)])
            linalg.CPU.AddOp <name="model.layers.26.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=966), inputs_1:QuantSpec(Raw(type: Int16), uuid=967), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=966), )] (%1470:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=966)], %1471:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=967), constant:[-20]]) -> (%1472:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=966)])
            linalg.CPU.EqualOp <name="model.layers.26.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=968), outputs_0:QuantSpec(Raw(type: UInt8), uuid=969), )] (%319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %1473:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=968), constant:[0.27929688]]) -> (%1474:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=969)])
            linalg.CPU.WhereOp <name="model.layers.26.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=969), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=964), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=966), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=966), )] (%1474:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=969)], %1469:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=964)], %1472:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=966)]) -> (%1475:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=966)])
            linalg.CPU.SoftmaxOp <name="model.layers.26.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=966), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=970), )] (%1475:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=966)]) -> (%1476:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=970)])
            linalg.CPU.MatMulOp <name="model.layers.26.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=970), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=57), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=971), )] (%1476:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=970)], %1466:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=57)]) -> (%1477:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=971)])
            linalg.CPU.TransposeOp <name="model.layers.26.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=971), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=971), )] (%1477:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=971)]) -> (%1478:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=971)])
            linalg.CPU.ViewOp <name="model.layers.26.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=971), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=971), )] (%1478:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=971)]) -> (%1478:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=971)])
            linalg.CPU.LinearOp <name="model.layers.26.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=971), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=973), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=972))] (%1478:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=971)]) -> (%1479:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=973)])
            cf.ReturnOp (%1479:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=973)], %1460:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=961)], %1462:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=963)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.26.mlp <CPU> [using_qnn:true, symbol:model.layers.26.mlp] {
        (%1481:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=974)]) -> (%1486:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=982)]) {
            linalg.CPU.LinearOp <name="model.layers.26.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=974), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=977), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=976))] (%1481:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=974)]) -> (%1482:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=977)])
            linalg.CPU.SiLUOp <name="model.layers.26.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=977), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=978), )] (%1482:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=977)]) -> (%1483:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=978)])
            linalg.CPU.LinearOp <name="model.layers.26.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=974), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=980), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=979))] (%1481:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=974)]) -> (%1484:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=980)])
            linalg.CPU.MulOp <name="model.layers.26.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=978), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=980), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=978), )] (%1483:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=978)], %1484:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=980)]) -> (%1485:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=978)])
            linalg.CPU.LinearOp <name="model.layers.26.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=978), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=982), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=981))] (%1485:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=978)]) -> (%1486:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=982)])
            cf.ReturnOp (%1486:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=982)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.27 <CPU> [using_qnn:true, symbol:model.layers.27] {
        (%1487:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=982)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %374:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=30)], %375:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=58)]) -> (%1528:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1016)], %1501:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=995)], %1503:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=997)]) {
            linalg.CPU.RMSNormOp <name="model.layers.27.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=982), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=983), )] (%1487:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=982)]) -> (%1488:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=983)])
            graph.CallGraphOp @model.layers.27.self_attn (%1488:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=983)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %374:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=30)], %375:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=58)]) -> (%1520:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1007)], %1501:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=995)], %1503:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=997)])
            linalg.CPU.AddOp <name="model.layers.27.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1007), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=982), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1007), )] (%1520:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1007)], %1487:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=982)]) -> (%1521:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1007)])
            linalg.CPU.RMSNormOp <name="model.layers.27.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1007), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1008), )] (%1521:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1007)]) -> (%1522:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1008)])
            graph.CallGraphOp @model.layers.27.mlp (%1522:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1008)]) -> (%1527:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1016)])
            linalg.CPU.AddOp <name="model.layers.27.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1016), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1007), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1016), )] (%1527:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1016)], %1521:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1007)]) -> (%1528:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1016)])
            cf.ReturnOp (%1528:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1016)], %1501:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=995)], %1503:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=997)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.27.self_attn <CPU> [using_qnn:true, symbol:model.layers.27.self_attn] {
        (%1488:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=983)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %374:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=30)], %375:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=58)]) -> (%1520:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1007)], %1501:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=995)], %1503:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=997)]) {
            linalg.CPU.LinearOp <name="model.layers.27.self_attn.q_proj">(%1488:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=983)]) -> (%1489:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=989)])
            linalg.CPU.LinearOp <name="model.layers.27.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=983), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=986), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=985))] (%1488:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=983)]) -> (%1490:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=986)])
            linalg.CPU.LinearOp <name="model.layers.27.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=983), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=988), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=987))] (%1488:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=983)]) -> (%1491:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=988)])
            linalg.CPU.ViewOp <name="model.layers.27.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=989), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=989), )] (%1489:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=989)]) -> (%1489:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=989)])
            linalg.CPU.TransposeOp <name="model.layers.27.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=989), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=989), )] (%1489:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=989)]) -> (%1492:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=989)])
            linalg.CPU.ViewOp <name="model.layers.27.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=986), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=986), )] (%1490:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=986)]) -> (%1490:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=986)])
            linalg.CPU.TransposeOp <name="model.layers.27.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=986), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=986), )] (%1490:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=986)]) -> (%1493:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=986)])
            linalg.CPU.ViewOp <name="model.layers.27.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=988), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=988), )] (%1491:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=988)]) -> (%1491:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=988)])
            linalg.CPU.TransposeOp <name="model.layers.27.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=988), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=988), )] (%1491:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=988)]) -> (%1494:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=988)])
            linalg.CPU.RMSNormOp <name="model.layers.27.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=989), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=990), )] (%1492:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=989)]) -> (%1495:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=990)])
            linalg.CPU.RMSNormOp <name="model.layers.27.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=986), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=992), )] (%1493:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=986)]) -> (%1496:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=992)])
            linalg.CPU.RoPEOp <name="model.layers.27.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=990), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=990), )] (%1495:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=990)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%1497:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=990)])
            linalg.CPU.RoPEOp <name="model.layers.27.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=992), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=992), )] (%1496:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=992)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%1498:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=992)])
            linalg.CPU.CastTypeOp <name="model.layers.27.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=992), outputs_0:QuantSpec(Raw(type: Float16), uuid=994), )] (%1498:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=992)]) -> (%1499:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=994)])
            linalg.CPU.CastTypeOp <name="model.layers.27.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=994), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=995), )] (%1499:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=994)]) -> (%1500:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=995)])
            linalg.CPU.TransposeOp <name="model.layers.27.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=995), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=995), )] (%1500:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=995)]) -> (%1501:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=995)])
            linalg.CPU.CastTypeOp <name="model.layers.27.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=988), outputs_0:QuantSpec(Raw(type: Float16), uuid=996), )] (%1494:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=988)]) -> (%1502:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=996)])
            linalg.CPU.CastTypeOp <name="model.layers.27.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=996), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=997), )] (%1502:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=996)]) -> (%1503:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=997)])
            linalg.CPU.ConcatOp <name="model.layers.27.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=30), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=995), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=30), )] (%374:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=30)], %1501:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=995)]) -> (%1504:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=30)])
            linalg.CPU.ConcatOp <name="model.layers.27.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=58), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=997), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=58), )] (%375:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=58)], %1503:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=997)]) -> (%1505:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=58)])
            linalg.CPU.RepeatOp <name="model.layers.27.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=30), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=30), )] (%1504:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=30)]) -> (%1506:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=30)])
            linalg.CPU.RepeatOp <name="model.layers.27.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=58), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=58), )] (%1505:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=58)]) -> (%1507:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=58)])
            linalg.CPU.MatMulOp <name="model.layers.27.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=990), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=30), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=998), )] (%1497:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=990)], %1506:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=30)]) -> (%1508:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=998)])
            linalg.CPU.MulOp <name="model.layers.27.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=998), inputs_1:QuantSpec(Raw(type: Float32), uuid=999), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=998), )] (%1508:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=998)], %1509:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=999), constant:[0.088388346]]) -> (%1510:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=998)])
            linalg.CPU.ReduceMinOp <name="model.layers.27.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=998), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1000), )] (%1510:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=998)]) -> (%1511:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1000)])
            linalg.CPU.AddOp <name="model.layers.27.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1000), inputs_1:QuantSpec(Raw(type: Int16), uuid=1001), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1000), )] (%1511:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1000)], %1512:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=1001), constant:[-20]]) -> (%1513:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1000)])
            linalg.CPU.EqualOp <name="model.layers.27.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=1002), outputs_0:QuantSpec(Raw(type: UInt8), uuid=1003), )] (%319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %1514:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=1002), constant:[0.890625]]) -> (%1515:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=1003)])
            linalg.CPU.WhereOp <name="model.layers.27.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=1003), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=998), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1000), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1000), )] (%1515:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=1003)], %1510:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=998)], %1513:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1000)]) -> (%1516:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1000)])
            linalg.CPU.SoftmaxOp <name="model.layers.27.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1000), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1004), )] (%1516:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1000)]) -> (%1517:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1004)])
            linalg.CPU.MatMulOp <name="model.layers.27.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1004), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=58), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1005), )] (%1517:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1004)], %1507:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=58)]) -> (%1518:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1005)])
            linalg.CPU.TransposeOp <name="model.layers.27.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1005), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1005), )] (%1518:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1005)]) -> (%1519:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1005)])
            linalg.CPU.ViewOp <name="model.layers.27.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1005), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1005), )] (%1519:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1005)]) -> (%1519:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1005)])
            linalg.CPU.LinearOp <name="model.layers.27.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1005), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1007), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=1006))] (%1519:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1005)]) -> (%1520:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1007)])
            cf.ReturnOp (%1520:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1007)], %1501:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=995)], %1503:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=997)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.27.mlp <CPU> [using_qnn:true, symbol:model.layers.27.mlp] {
        (%1522:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1008)]) -> (%1527:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1016)]) {
            linalg.CPU.LinearOp <name="model.layers.27.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1008), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1011), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=1010))] (%1522:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1008)]) -> (%1523:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1011)])
            linalg.CPU.SiLUOp <name="model.layers.27.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1011), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1012), )] (%1523:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1011)]) -> (%1524:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1012)])
            linalg.CPU.LinearOp <name="model.layers.27.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1008), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1014), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=1013))] (%1522:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1008)]) -> (%1525:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1014)])
            linalg.CPU.MulOp <name="model.layers.27.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1012), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1014), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1012), )] (%1524:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1012)], %1525:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1014)]) -> (%1526:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1012)])
            linalg.CPU.LinearOp <name="model.layers.27.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1012), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1016), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=1015))] (%1526:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1012)]) -> (%1527:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1016)])
            cf.ReturnOp (%1527:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=1016)]) -> ()
        }
    }
    //        
    //      o o    
    //            
    //       
    //             
    //        
}
 
