@main () -> () {
    graph.SubGraphOp @init <notype> [symbol:init] {
        () -> () {
            tensor.CPU.register () -> (%105:tensor<[151936, 2048], Float32, CPU>[@model.embed_tokens.weight][symbol:model.embed_tokens.weight])[symbol:model.embed_tokens.weight]
            tensor.CPU.register () -> (%76:tensor<[2048, 2048], Float32, CPU>[@model.layers.0.self_attn.q_proj.weight][symbol:model.layers.0.self_attn.q_proj.weight])[symbol:model.layers.0.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%133:tensor<[1024, 2048], Float32, CPU>[@model.layers.0.self_attn.k_proj.weight][symbol:model.layers.0.self_attn.k_proj.weight])[symbol:model.layers.0.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%179:tensor<[1024, 2048], Float32, CPU>[@model.layers.0.self_attn.v_proj.weight][symbol:model.layers.0.self_attn.v_proj.weight])[symbol:model.layers.0.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%269:tensor<[2048, 2048], Float32, CPU>[@model.layers.0.self_attn.o_proj.weight][symbol:model.layers.0.self_attn.o_proj.weight])[symbol:model.layers.0.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%9:tensor<[6144, 2048], Float32, CPU>[@model.layers.0.mlp.gate_proj.weight][symbol:model.layers.0.mlp.gate_proj.weight])[symbol:model.layers.0.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%111:tensor<[6144, 2048], Float32, CPU>[@model.layers.0.mlp.up_proj.weight][symbol:model.layers.0.mlp.up_proj.weight])[symbol:model.layers.0.mlp.up_proj.weight]
            tensor.CPU.register () -> (%184:tensor<[2048, 6144], Float32, CPU>[@model.layers.0.mlp.down_proj.weight][symbol:model.layers.0.mlp.down_proj.weight])[symbol:model.layers.0.mlp.down_proj.weight]
            tensor.CPU.register () -> (%285:tensor<[2048, 2048], Float32, CPU>[@model.layers.1.self_attn.q_proj.weight][symbol:model.layers.1.self_attn.q_proj.weight])[symbol:model.layers.1.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%32:tensor<[1024, 2048], Float32, CPU>[@model.layers.1.self_attn.k_proj.weight][symbol:model.layers.1.self_attn.k_proj.weight])[symbol:model.layers.1.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%154:tensor<[1024, 2048], Float32, CPU>[@model.layers.1.self_attn.v_proj.weight][symbol:model.layers.1.self_attn.v_proj.weight])[symbol:model.layers.1.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%20:tensor<[2048, 2048], Float32, CPU>[@model.layers.1.self_attn.o_proj.weight][symbol:model.layers.1.self_attn.o_proj.weight])[symbol:model.layers.1.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%245:tensor<[6144, 2048], Float32, CPU>[@model.layers.1.mlp.gate_proj.weight][symbol:model.layers.1.mlp.gate_proj.weight])[symbol:model.layers.1.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%230:tensor<[6144, 2048], Float32, CPU>[@model.layers.1.mlp.up_proj.weight][symbol:model.layers.1.mlp.up_proj.weight])[symbol:model.layers.1.mlp.up_proj.weight]
            tensor.CPU.register () -> (%43:tensor<[2048, 6144], Float32, CPU>[@model.layers.1.mlp.down_proj.weight][symbol:model.layers.1.mlp.down_proj.weight])[symbol:model.layers.1.mlp.down_proj.weight]
            tensor.CPU.register () -> (%221:tensor<[2048, 2048], Float32, CPU>[@model.layers.2.self_attn.q_proj.weight][symbol:model.layers.2.self_attn.q_proj.weight])[symbol:model.layers.2.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%103:tensor<[1024, 2048], Float32, CPU>[@model.layers.2.self_attn.k_proj.weight][symbol:model.layers.2.self_attn.k_proj.weight])[symbol:model.layers.2.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%47:tensor<[1024, 2048], Float32, CPU>[@model.layers.2.self_attn.v_proj.weight][symbol:model.layers.2.self_attn.v_proj.weight])[symbol:model.layers.2.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%85:tensor<[2048, 2048], Float32, CPU>[@model.layers.2.self_attn.o_proj.weight][symbol:model.layers.2.self_attn.o_proj.weight])[symbol:model.layers.2.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%252:tensor<[6144, 2048], Float32, CPU>[@model.layers.2.mlp.gate_proj.weight][symbol:model.layers.2.mlp.gate_proj.weight])[symbol:model.layers.2.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%24:tensor<[6144, 2048], Float32, CPU>[@model.layers.2.mlp.up_proj.weight][symbol:model.layers.2.mlp.up_proj.weight])[symbol:model.layers.2.mlp.up_proj.weight]
            tensor.CPU.register () -> (%28:tensor<[2048, 6144], Float32, CPU>[@model.layers.2.mlp.down_proj.weight][symbol:model.layers.2.mlp.down_proj.weight])[symbol:model.layers.2.mlp.down_proj.weight]
            tensor.CPU.register () -> (%283:tensor<[2048, 2048], Float32, CPU>[@model.layers.3.self_attn.q_proj.weight][symbol:model.layers.3.self_attn.q_proj.weight])[symbol:model.layers.3.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%48:tensor<[1024, 2048], Float32, CPU>[@model.layers.3.self_attn.k_proj.weight][symbol:model.layers.3.self_attn.k_proj.weight])[symbol:model.layers.3.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%244:tensor<[1024, 2048], Float32, CPU>[@model.layers.3.self_attn.v_proj.weight][symbol:model.layers.3.self_attn.v_proj.weight])[symbol:model.layers.3.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%301:tensor<[2048, 2048], Float32, CPU>[@model.layers.3.self_attn.o_proj.weight][symbol:model.layers.3.self_attn.o_proj.weight])[symbol:model.layers.3.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%129:tensor<[6144, 2048], Float32, CPU>[@model.layers.3.mlp.gate_proj.weight][symbol:model.layers.3.mlp.gate_proj.weight])[symbol:model.layers.3.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%188:tensor<[6144, 2048], Float32, CPU>[@model.layers.3.mlp.up_proj.weight][symbol:model.layers.3.mlp.up_proj.weight])[symbol:model.layers.3.mlp.up_proj.weight]
            tensor.CPU.register () -> (%97:tensor<[2048, 6144], Float32, CPU>[@model.layers.3.mlp.down_proj.weight][symbol:model.layers.3.mlp.down_proj.weight])[symbol:model.layers.3.mlp.down_proj.weight]
            tensor.CPU.register () -> (%164:tensor<[2048, 2048], Float32, CPU>[@model.layers.4.self_attn.q_proj.weight][symbol:model.layers.4.self_attn.q_proj.weight])[symbol:model.layers.4.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%148:tensor<[1024, 2048], Float32, CPU>[@model.layers.4.self_attn.k_proj.weight][symbol:model.layers.4.self_attn.k_proj.weight])[symbol:model.layers.4.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%279:tensor<[1024, 2048], Float32, CPU>[@model.layers.4.self_attn.v_proj.weight][symbol:model.layers.4.self_attn.v_proj.weight])[symbol:model.layers.4.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%91:tensor<[2048, 2048], Float32, CPU>[@model.layers.4.self_attn.o_proj.weight][symbol:model.layers.4.self_attn.o_proj.weight])[symbol:model.layers.4.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%189:tensor<[6144, 2048], Float32, CPU>[@model.layers.4.mlp.gate_proj.weight][symbol:model.layers.4.mlp.gate_proj.weight])[symbol:model.layers.4.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%156:tensor<[6144, 2048], Float32, CPU>[@model.layers.4.mlp.up_proj.weight][symbol:model.layers.4.mlp.up_proj.weight])[symbol:model.layers.4.mlp.up_proj.weight]
            tensor.CPU.register () -> (%153:tensor<[2048, 6144], Float32, CPU>[@model.layers.4.mlp.down_proj.weight][symbol:model.layers.4.mlp.down_proj.weight])[symbol:model.layers.4.mlp.down_proj.weight]
            tensor.CPU.register () -> (%78:tensor<[2048, 2048], Float32, CPU>[@model.layers.5.self_attn.q_proj.weight][symbol:model.layers.5.self_attn.q_proj.weight])[symbol:model.layers.5.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%72:tensor<[1024, 2048], Float32, CPU>[@model.layers.5.self_attn.k_proj.weight][symbol:model.layers.5.self_attn.k_proj.weight])[symbol:model.layers.5.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%289:tensor<[1024, 2048], Float32, CPU>[@model.layers.5.self_attn.v_proj.weight][symbol:model.layers.5.self_attn.v_proj.weight])[symbol:model.layers.5.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%264:tensor<[2048, 2048], Float32, CPU>[@model.layers.5.self_attn.o_proj.weight][symbol:model.layers.5.self_attn.o_proj.weight])[symbol:model.layers.5.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%4:tensor<[6144, 2048], Float32, CPU>[@model.layers.5.mlp.gate_proj.weight][symbol:model.layers.5.mlp.gate_proj.weight])[symbol:model.layers.5.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%308:tensor<[6144, 2048], Float32, CPU>[@model.layers.5.mlp.up_proj.weight][symbol:model.layers.5.mlp.up_proj.weight])[symbol:model.layers.5.mlp.up_proj.weight]
            tensor.CPU.register () -> (%74:tensor<[2048, 6144], Float32, CPU>[@model.layers.5.mlp.down_proj.weight][symbol:model.layers.5.mlp.down_proj.weight])[symbol:model.layers.5.mlp.down_proj.weight]
            tensor.CPU.register () -> (%59:tensor<[2048, 2048], Float32, CPU>[@model.layers.6.self_attn.q_proj.weight][symbol:model.layers.6.self_attn.q_proj.weight])[symbol:model.layers.6.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%208:tensor<[1024, 2048], Float32, CPU>[@model.layers.6.self_attn.k_proj.weight][symbol:model.layers.6.self_attn.k_proj.weight])[symbol:model.layers.6.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%238:tensor<[1024, 2048], Float32, CPU>[@model.layers.6.self_attn.v_proj.weight][symbol:model.layers.6.self_attn.v_proj.weight])[symbol:model.layers.6.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%52:tensor<[2048, 2048], Float32, CPU>[@model.layers.6.self_attn.o_proj.weight][symbol:model.layers.6.self_attn.o_proj.weight])[symbol:model.layers.6.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%80:tensor<[6144, 2048], Float32, CPU>[@model.layers.6.mlp.gate_proj.weight][symbol:model.layers.6.mlp.gate_proj.weight])[symbol:model.layers.6.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%276:tensor<[6144, 2048], Float32, CPU>[@model.layers.6.mlp.up_proj.weight][symbol:model.layers.6.mlp.up_proj.weight])[symbol:model.layers.6.mlp.up_proj.weight]
            tensor.CPU.register () -> (%227:tensor<[2048, 6144], Float32, CPU>[@model.layers.6.mlp.down_proj.weight][symbol:model.layers.6.mlp.down_proj.weight])[symbol:model.layers.6.mlp.down_proj.weight]
            tensor.CPU.register () -> (%287:tensor<[2048, 2048], Float32, CPU>[@model.layers.7.self_attn.q_proj.weight][symbol:model.layers.7.self_attn.q_proj.weight])[symbol:model.layers.7.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%135:tensor<[1024, 2048], Float32, CPU>[@model.layers.7.self_attn.k_proj.weight][symbol:model.layers.7.self_attn.k_proj.weight])[symbol:model.layers.7.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%300:tensor<[1024, 2048], Float32, CPU>[@model.layers.7.self_attn.v_proj.weight][symbol:model.layers.7.self_attn.v_proj.weight])[symbol:model.layers.7.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%251:tensor<[2048, 2048], Float32, CPU>[@model.layers.7.self_attn.o_proj.weight][symbol:model.layers.7.self_attn.o_proj.weight])[symbol:model.layers.7.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%155:tensor<[6144, 2048], Float32, CPU>[@model.layers.7.mlp.gate_proj.weight][symbol:model.layers.7.mlp.gate_proj.weight])[symbol:model.layers.7.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%218:tensor<[6144, 2048], Float32, CPU>[@model.layers.7.mlp.up_proj.weight][symbol:model.layers.7.mlp.up_proj.weight])[symbol:model.layers.7.mlp.up_proj.weight]
            tensor.CPU.register () -> (%275:tensor<[2048, 6144], Float32, CPU>[@model.layers.7.mlp.down_proj.weight][symbol:model.layers.7.mlp.down_proj.weight])[symbol:model.layers.7.mlp.down_proj.weight]
            tensor.CPU.register () -> (%165:tensor<[2048, 2048], Float32, CPU>[@model.layers.8.self_attn.q_proj.weight][symbol:model.layers.8.self_attn.q_proj.weight])[symbol:model.layers.8.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%194:tensor<[1024, 2048], Float32, CPU>[@model.layers.8.self_attn.k_proj.weight][symbol:model.layers.8.self_attn.k_proj.weight])[symbol:model.layers.8.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%181:tensor<[1024, 2048], Float32, CPU>[@model.layers.8.self_attn.v_proj.weight][symbol:model.layers.8.self_attn.v_proj.weight])[symbol:model.layers.8.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%197:tensor<[2048, 2048], Float32, CPU>[@model.layers.8.self_attn.o_proj.weight][symbol:model.layers.8.self_attn.o_proj.weight])[symbol:model.layers.8.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%110:tensor<[6144, 2048], Float32, CPU>[@model.layers.8.mlp.gate_proj.weight][symbol:model.layers.8.mlp.gate_proj.weight])[symbol:model.layers.8.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%236:tensor<[6144, 2048], Float32, CPU>[@model.layers.8.mlp.up_proj.weight][symbol:model.layers.8.mlp.up_proj.weight])[symbol:model.layers.8.mlp.up_proj.weight]
            tensor.CPU.register () -> (%106:tensor<[2048, 6144], Float32, CPU>[@model.layers.8.mlp.down_proj.weight][symbol:model.layers.8.mlp.down_proj.weight])[symbol:model.layers.8.mlp.down_proj.weight]
            tensor.CPU.register () -> (%235:tensor<[2048, 2048], Float32, CPU>[@model.layers.9.self_attn.q_proj.weight][symbol:model.layers.9.self_attn.q_proj.weight])[symbol:model.layers.9.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%69:tensor<[1024, 2048], Float32, CPU>[@model.layers.9.self_attn.k_proj.weight][symbol:model.layers.9.self_attn.k_proj.weight])[symbol:model.layers.9.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%120:tensor<[1024, 2048], Float32, CPU>[@model.layers.9.self_attn.v_proj.weight][symbol:model.layers.9.self_attn.v_proj.weight])[symbol:model.layers.9.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%205:tensor<[2048, 2048], Float32, CPU>[@model.layers.9.self_attn.o_proj.weight][symbol:model.layers.9.self_attn.o_proj.weight])[symbol:model.layers.9.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%263:tensor<[6144, 2048], Float32, CPU>[@model.layers.9.mlp.gate_proj.weight][symbol:model.layers.9.mlp.gate_proj.weight])[symbol:model.layers.9.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%102:tensor<[6144, 2048], Float32, CPU>[@model.layers.9.mlp.up_proj.weight][symbol:model.layers.9.mlp.up_proj.weight])[symbol:model.layers.9.mlp.up_proj.weight]
            tensor.CPU.register () -> (%136:tensor<[2048, 6144], Float32, CPU>[@model.layers.9.mlp.down_proj.weight][symbol:model.layers.9.mlp.down_proj.weight])[symbol:model.layers.9.mlp.down_proj.weight]
            tensor.CPU.register () -> (%278:tensor<[2048, 2048], Float32, CPU>[@model.layers.10.self_attn.q_proj.weight][symbol:model.layers.10.self_attn.q_proj.weight])[symbol:model.layers.10.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%182:tensor<[1024, 2048], Float32, CPU>[@model.layers.10.self_attn.k_proj.weight][symbol:model.layers.10.self_attn.k_proj.weight])[symbol:model.layers.10.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%138:tensor<[1024, 2048], Float32, CPU>[@model.layers.10.self_attn.v_proj.weight][symbol:model.layers.10.self_attn.v_proj.weight])[symbol:model.layers.10.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%233:tensor<[2048, 2048], Float32, CPU>[@model.layers.10.self_attn.o_proj.weight][symbol:model.layers.10.self_attn.o_proj.weight])[symbol:model.layers.10.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%124:tensor<[6144, 2048], Float32, CPU>[@model.layers.10.mlp.gate_proj.weight][symbol:model.layers.10.mlp.gate_proj.weight])[symbol:model.layers.10.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%261:tensor<[6144, 2048], Float32, CPU>[@model.layers.10.mlp.up_proj.weight][symbol:model.layers.10.mlp.up_proj.weight])[symbol:model.layers.10.mlp.up_proj.weight]
            tensor.CPU.register () -> (%45:tensor<[2048, 6144], Float32, CPU>[@model.layers.10.mlp.down_proj.weight][symbol:model.layers.10.mlp.down_proj.weight])[symbol:model.layers.10.mlp.down_proj.weight]
            tensor.CPU.register () -> (%274:tensor<[2048, 2048], Float32, CPU>[@model.layers.11.self_attn.q_proj.weight][symbol:model.layers.11.self_attn.q_proj.weight])[symbol:model.layers.11.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%157:tensor<[1024, 2048], Float32, CPU>[@model.layers.11.self_attn.k_proj.weight][symbol:model.layers.11.self_attn.k_proj.weight])[symbol:model.layers.11.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%63:tensor<[1024, 2048], Float32, CPU>[@model.layers.11.self_attn.v_proj.weight][symbol:model.layers.11.self_attn.v_proj.weight])[symbol:model.layers.11.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%118:tensor<[2048, 2048], Float32, CPU>[@model.layers.11.self_attn.o_proj.weight][symbol:model.layers.11.self_attn.o_proj.weight])[symbol:model.layers.11.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%207:tensor<[6144, 2048], Float32, CPU>[@model.layers.11.mlp.gate_proj.weight][symbol:model.layers.11.mlp.gate_proj.weight])[symbol:model.layers.11.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%226:tensor<[6144, 2048], Float32, CPU>[@model.layers.11.mlp.up_proj.weight][symbol:model.layers.11.mlp.up_proj.weight])[symbol:model.layers.11.mlp.up_proj.weight]
            tensor.CPU.register () -> (%224:tensor<[2048, 6144], Float32, CPU>[@model.layers.11.mlp.down_proj.weight][symbol:model.layers.11.mlp.down_proj.weight])[symbol:model.layers.11.mlp.down_proj.weight]
            tensor.CPU.register () -> (%217:tensor<[2048, 2048], Float32, CPU>[@model.layers.12.self_attn.q_proj.weight][symbol:model.layers.12.self_attn.q_proj.weight])[symbol:model.layers.12.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%297:tensor<[1024, 2048], Float32, CPU>[@model.layers.12.self_attn.k_proj.weight][symbol:model.layers.12.self_attn.k_proj.weight])[symbol:model.layers.12.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%94:tensor<[1024, 2048], Float32, CPU>[@model.layers.12.self_attn.v_proj.weight][symbol:model.layers.12.self_attn.v_proj.weight])[symbol:model.layers.12.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%49:tensor<[2048, 2048], Float32, CPU>[@model.layers.12.self_attn.o_proj.weight][symbol:model.layers.12.self_attn.o_proj.weight])[symbol:model.layers.12.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%262:tensor<[6144, 2048], Float32, CPU>[@model.layers.12.mlp.gate_proj.weight][symbol:model.layers.12.mlp.gate_proj.weight])[symbol:model.layers.12.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%255:tensor<[6144, 2048], Float32, CPU>[@model.layers.12.mlp.up_proj.weight][symbol:model.layers.12.mlp.up_proj.weight])[symbol:model.layers.12.mlp.up_proj.weight]
            tensor.CPU.register () -> (%22:tensor<[2048, 6144], Float32, CPU>[@model.layers.12.mlp.down_proj.weight][symbol:model.layers.12.mlp.down_proj.weight])[symbol:model.layers.12.mlp.down_proj.weight]
            tensor.CPU.register () -> (%114:tensor<[2048, 2048], Float32, CPU>[@model.layers.13.self_attn.q_proj.weight][symbol:model.layers.13.self_attn.q_proj.weight])[symbol:model.layers.13.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%152:tensor<[1024, 2048], Float32, CPU>[@model.layers.13.self_attn.k_proj.weight][symbol:model.layers.13.self_attn.k_proj.weight])[symbol:model.layers.13.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%15:tensor<[1024, 2048], Float32, CPU>[@model.layers.13.self_attn.v_proj.weight][symbol:model.layers.13.self_attn.v_proj.weight])[symbol:model.layers.13.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%250:tensor<[2048, 2048], Float32, CPU>[@model.layers.13.self_attn.o_proj.weight][symbol:model.layers.13.self_attn.o_proj.weight])[symbol:model.layers.13.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%247:tensor<[6144, 2048], Float32, CPU>[@model.layers.13.mlp.gate_proj.weight][symbol:model.layers.13.mlp.gate_proj.weight])[symbol:model.layers.13.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%98:tensor<[6144, 2048], Float32, CPU>[@model.layers.13.mlp.up_proj.weight][symbol:model.layers.13.mlp.up_proj.weight])[symbol:model.layers.13.mlp.up_proj.weight]
            tensor.CPU.register () -> (%193:tensor<[2048, 6144], Float32, CPU>[@model.layers.13.mlp.down_proj.weight][symbol:model.layers.13.mlp.down_proj.weight])[symbol:model.layers.13.mlp.down_proj.weight]
            tensor.CPU.register () -> (%209:tensor<[2048, 2048], Float32, CPU>[@model.layers.14.self_attn.q_proj.weight][symbol:model.layers.14.self_attn.q_proj.weight])[symbol:model.layers.14.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%38:tensor<[1024, 2048], Float32, CPU>[@model.layers.14.self_attn.k_proj.weight][symbol:model.layers.14.self_attn.k_proj.weight])[symbol:model.layers.14.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%232:tensor<[1024, 2048], Float32, CPU>[@model.layers.14.self_attn.v_proj.weight][symbol:model.layers.14.self_attn.v_proj.weight])[symbol:model.layers.14.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%168:tensor<[2048, 2048], Float32, CPU>[@model.layers.14.self_attn.o_proj.weight][symbol:model.layers.14.self_attn.o_proj.weight])[symbol:model.layers.14.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%37:tensor<[6144, 2048], Float32, CPU>[@model.layers.14.mlp.gate_proj.weight][symbol:model.layers.14.mlp.gate_proj.weight])[symbol:model.layers.14.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%147:tensor<[6144, 2048], Float32, CPU>[@model.layers.14.mlp.up_proj.weight][symbol:model.layers.14.mlp.up_proj.weight])[symbol:model.layers.14.mlp.up_proj.weight]
            tensor.CPU.register () -> (%163:tensor<[2048, 6144], Float32, CPU>[@model.layers.14.mlp.down_proj.weight][symbol:model.layers.14.mlp.down_proj.weight])[symbol:model.layers.14.mlp.down_proj.weight]
            tensor.CPU.register () -> (%46:tensor<[2048, 2048], Float32, CPU>[@model.layers.15.self_attn.q_proj.weight][symbol:model.layers.15.self_attn.q_proj.weight])[symbol:model.layers.15.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%268:tensor<[1024, 2048], Float32, CPU>[@model.layers.15.self_attn.k_proj.weight][symbol:model.layers.15.self_attn.k_proj.weight])[symbol:model.layers.15.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%117:tensor<[1024, 2048], Float32, CPU>[@model.layers.15.self_attn.v_proj.weight][symbol:model.layers.15.self_attn.v_proj.weight])[symbol:model.layers.15.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%303:tensor<[2048, 2048], Float32, CPU>[@model.layers.15.self_attn.o_proj.weight][symbol:model.layers.15.self_attn.o_proj.weight])[symbol:model.layers.15.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%260:tensor<[6144, 2048], Float32, CPU>[@model.layers.15.mlp.gate_proj.weight][symbol:model.layers.15.mlp.gate_proj.weight])[symbol:model.layers.15.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%42:tensor<[6144, 2048], Float32, CPU>[@model.layers.15.mlp.up_proj.weight][symbol:model.layers.15.mlp.up_proj.weight])[symbol:model.layers.15.mlp.up_proj.weight]
            tensor.CPU.register () -> (%290:tensor<[2048, 6144], Float32, CPU>[@model.layers.15.mlp.down_proj.weight][symbol:model.layers.15.mlp.down_proj.weight])[symbol:model.layers.15.mlp.down_proj.weight]
            tensor.CPU.register () -> (%17:tensor<[2048, 2048], Float32, CPU>[@model.layers.16.self_attn.q_proj.weight][symbol:model.layers.16.self_attn.q_proj.weight])[symbol:model.layers.16.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%228:tensor<[1024, 2048], Float32, CPU>[@model.layers.16.self_attn.k_proj.weight][symbol:model.layers.16.self_attn.k_proj.weight])[symbol:model.layers.16.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%66:tensor<[1024, 2048], Float32, CPU>[@model.layers.16.self_attn.v_proj.weight][symbol:model.layers.16.self_attn.v_proj.weight])[symbol:model.layers.16.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%211:tensor<[2048, 2048], Float32, CPU>[@model.layers.16.self_attn.o_proj.weight][symbol:model.layers.16.self_attn.o_proj.weight])[symbol:model.layers.16.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%130:tensor<[6144, 2048], Float32, CPU>[@model.layers.16.mlp.gate_proj.weight][symbol:model.layers.16.mlp.gate_proj.weight])[symbol:model.layers.16.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%79:tensor<[6144, 2048], Float32, CPU>[@model.layers.16.mlp.up_proj.weight][symbol:model.layers.16.mlp.up_proj.weight])[symbol:model.layers.16.mlp.up_proj.weight]
            tensor.CPU.register () -> (%248:tensor<[2048, 6144], Float32, CPU>[@model.layers.16.mlp.down_proj.weight][symbol:model.layers.16.mlp.down_proj.weight])[symbol:model.layers.16.mlp.down_proj.weight]
            tensor.CPU.register () -> (%64:tensor<[2048, 2048], Float32, CPU>[@model.layers.17.self_attn.q_proj.weight][symbol:model.layers.17.self_attn.q_proj.weight])[symbol:model.layers.17.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%237:tensor<[1024, 2048], Float32, CPU>[@model.layers.17.self_attn.k_proj.weight][symbol:model.layers.17.self_attn.k_proj.weight])[symbol:model.layers.17.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%6:tensor<[1024, 2048], Float32, CPU>[@model.layers.17.self_attn.v_proj.weight][symbol:model.layers.17.self_attn.v_proj.weight])[symbol:model.layers.17.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%125:tensor<[2048, 2048], Float32, CPU>[@model.layers.17.self_attn.o_proj.weight][symbol:model.layers.17.self_attn.o_proj.weight])[symbol:model.layers.17.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%177:tensor<[6144, 2048], Float32, CPU>[@model.layers.17.mlp.gate_proj.weight][symbol:model.layers.17.mlp.gate_proj.weight])[symbol:model.layers.17.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%26:tensor<[6144, 2048], Float32, CPU>[@model.layers.17.mlp.up_proj.weight][symbol:model.layers.17.mlp.up_proj.weight])[symbol:model.layers.17.mlp.up_proj.weight]
            tensor.CPU.register () -> (%25:tensor<[2048, 6144], Float32, CPU>[@model.layers.17.mlp.down_proj.weight][symbol:model.layers.17.mlp.down_proj.weight])[symbol:model.layers.17.mlp.down_proj.weight]
            tensor.CPU.register () -> (%273:tensor<[2048, 2048], Float32, CPU>[@model.layers.18.self_attn.q_proj.weight][symbol:model.layers.18.self_attn.q_proj.weight])[symbol:model.layers.18.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%284:tensor<[1024, 2048], Float32, CPU>[@model.layers.18.self_attn.k_proj.weight][symbol:model.layers.18.self_attn.k_proj.weight])[symbol:model.layers.18.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%18:tensor<[1024, 2048], Float32, CPU>[@model.layers.18.self_attn.v_proj.weight][symbol:model.layers.18.self_attn.v_proj.weight])[symbol:model.layers.18.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%2:tensor<[2048, 2048], Float32, CPU>[@model.layers.18.self_attn.o_proj.weight][symbol:model.layers.18.self_attn.o_proj.weight])[symbol:model.layers.18.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%166:tensor<[6144, 2048], Float32, CPU>[@model.layers.18.mlp.gate_proj.weight][symbol:model.layers.18.mlp.gate_proj.weight])[symbol:model.layers.18.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%271:tensor<[6144, 2048], Float32, CPU>[@model.layers.18.mlp.up_proj.weight][symbol:model.layers.18.mlp.up_proj.weight])[symbol:model.layers.18.mlp.up_proj.weight]
            tensor.CPU.register () -> (%112:tensor<[2048, 6144], Float32, CPU>[@model.layers.18.mlp.down_proj.weight][symbol:model.layers.18.mlp.down_proj.weight])[symbol:model.layers.18.mlp.down_proj.weight]
            tensor.CPU.register () -> (%8:tensor<[2048, 2048], Float32, CPU>[@model.layers.19.self_attn.q_proj.weight][symbol:model.layers.19.self_attn.q_proj.weight])[symbol:model.layers.19.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%286:tensor<[1024, 2048], Float32, CPU>[@model.layers.19.self_attn.k_proj.weight][symbol:model.layers.19.self_attn.k_proj.weight])[symbol:model.layers.19.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%50:tensor<[1024, 2048], Float32, CPU>[@model.layers.19.self_attn.v_proj.weight][symbol:model.layers.19.self_attn.v_proj.weight])[symbol:model.layers.19.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%58:tensor<[2048, 2048], Float32, CPU>[@model.layers.19.self_attn.o_proj.weight][symbol:model.layers.19.self_attn.o_proj.weight])[symbol:model.layers.19.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%281:tensor<[6144, 2048], Float32, CPU>[@model.layers.19.mlp.gate_proj.weight][symbol:model.layers.19.mlp.gate_proj.weight])[symbol:model.layers.19.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%82:tensor<[6144, 2048], Float32, CPU>[@model.layers.19.mlp.up_proj.weight][symbol:model.layers.19.mlp.up_proj.weight])[symbol:model.layers.19.mlp.up_proj.weight]
            tensor.CPU.register () -> (%173:tensor<[2048, 6144], Float32, CPU>[@model.layers.19.mlp.down_proj.weight][symbol:model.layers.19.mlp.down_proj.weight])[symbol:model.layers.19.mlp.down_proj.weight]
            tensor.CPU.register () -> (%280:tensor<[2048, 2048], Float32, CPU>[@model.layers.20.self_attn.q_proj.weight][symbol:model.layers.20.self_attn.q_proj.weight])[symbol:model.layers.20.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%253:tensor<[1024, 2048], Float32, CPU>[@model.layers.20.self_attn.k_proj.weight][symbol:model.layers.20.self_attn.k_proj.weight])[symbol:model.layers.20.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%239:tensor<[1024, 2048], Float32, CPU>[@model.layers.20.self_attn.v_proj.weight][symbol:model.layers.20.self_attn.v_proj.weight])[symbol:model.layers.20.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%41:tensor<[2048, 2048], Float32, CPU>[@model.layers.20.self_attn.o_proj.weight][symbol:model.layers.20.self_attn.o_proj.weight])[symbol:model.layers.20.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%172:tensor<[6144, 2048], Float32, CPU>[@model.layers.20.mlp.gate_proj.weight][symbol:model.layers.20.mlp.gate_proj.weight])[symbol:model.layers.20.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%299:tensor<[6144, 2048], Float32, CPU>[@model.layers.20.mlp.up_proj.weight][symbol:model.layers.20.mlp.up_proj.weight])[symbol:model.layers.20.mlp.up_proj.weight]
            tensor.CPU.register () -> (%123:tensor<[2048, 6144], Float32, CPU>[@model.layers.20.mlp.down_proj.weight][symbol:model.layers.20.mlp.down_proj.weight])[symbol:model.layers.20.mlp.down_proj.weight]
            tensor.CPU.register () -> (%295:tensor<[2048, 2048], Float32, CPU>[@model.layers.21.self_attn.q_proj.weight][symbol:model.layers.21.self_attn.q_proj.weight])[symbol:model.layers.21.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%139:tensor<[1024, 2048], Float32, CPU>[@model.layers.21.self_attn.k_proj.weight][symbol:model.layers.21.self_attn.k_proj.weight])[symbol:model.layers.21.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%142:tensor<[1024, 2048], Float32, CPU>[@model.layers.21.self_attn.v_proj.weight][symbol:model.layers.21.self_attn.v_proj.weight])[symbol:model.layers.21.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%115:tensor<[2048, 2048], Float32, CPU>[@model.layers.21.self_attn.o_proj.weight][symbol:model.layers.21.self_attn.o_proj.weight])[symbol:model.layers.21.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%259:tensor<[6144, 2048], Float32, CPU>[@model.layers.21.mlp.gate_proj.weight][symbol:model.layers.21.mlp.gate_proj.weight])[symbol:model.layers.21.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%162:tensor<[6144, 2048], Float32, CPU>[@model.layers.21.mlp.up_proj.weight][symbol:model.layers.21.mlp.up_proj.weight])[symbol:model.layers.21.mlp.up_proj.weight]
            tensor.CPU.register () -> (%183:tensor<[2048, 6144], Float32, CPU>[@model.layers.21.mlp.down_proj.weight][symbol:model.layers.21.mlp.down_proj.weight])[symbol:model.layers.21.mlp.down_proj.weight]
            tensor.CPU.register () -> (%89:tensor<[2048, 2048], Float32, CPU>[@model.layers.22.self_attn.q_proj.weight][symbol:model.layers.22.self_attn.q_proj.weight])[symbol:model.layers.22.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%36:tensor<[1024, 2048], Float32, CPU>[@model.layers.22.self_attn.k_proj.weight][symbol:model.layers.22.self_attn.k_proj.weight])[symbol:model.layers.22.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%204:tensor<[1024, 2048], Float32, CPU>[@model.layers.22.self_attn.v_proj.weight][symbol:model.layers.22.self_attn.v_proj.weight])[symbol:model.layers.22.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%234:tensor<[2048, 2048], Float32, CPU>[@model.layers.22.self_attn.o_proj.weight][symbol:model.layers.22.self_attn.o_proj.weight])[symbol:model.layers.22.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%198:tensor<[6144, 2048], Float32, CPU>[@model.layers.22.mlp.gate_proj.weight][symbol:model.layers.22.mlp.gate_proj.weight])[symbol:model.layers.22.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%254:tensor<[6144, 2048], Float32, CPU>[@model.layers.22.mlp.up_proj.weight][symbol:model.layers.22.mlp.up_proj.weight])[symbol:model.layers.22.mlp.up_proj.weight]
            tensor.CPU.register () -> (%31:tensor<[2048, 6144], Float32, CPU>[@model.layers.22.mlp.down_proj.weight][symbol:model.layers.22.mlp.down_proj.weight])[symbol:model.layers.22.mlp.down_proj.weight]
            tensor.CPU.register () -> (%109:tensor<[2048, 2048], Float32, CPU>[@model.layers.23.self_attn.q_proj.weight][symbol:model.layers.23.self_attn.q_proj.weight])[symbol:model.layers.23.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%39:tensor<[1024, 2048], Float32, CPU>[@model.layers.23.self_attn.k_proj.weight][symbol:model.layers.23.self_attn.k_proj.weight])[symbol:model.layers.23.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%83:tensor<[1024, 2048], Float32, CPU>[@model.layers.23.self_attn.v_proj.weight][symbol:model.layers.23.self_attn.v_proj.weight])[symbol:model.layers.23.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%176:tensor<[2048, 2048], Float32, CPU>[@model.layers.23.self_attn.o_proj.weight][symbol:model.layers.23.self_attn.o_proj.weight])[symbol:model.layers.23.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%169:tensor<[6144, 2048], Float32, CPU>[@model.layers.23.mlp.gate_proj.weight][symbol:model.layers.23.mlp.gate_proj.weight])[symbol:model.layers.23.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%243:tensor<[6144, 2048], Float32, CPU>[@model.layers.23.mlp.up_proj.weight][symbol:model.layers.23.mlp.up_proj.weight])[symbol:model.layers.23.mlp.up_proj.weight]
            tensor.CPU.register () -> (%149:tensor<[2048, 6144], Float32, CPU>[@model.layers.23.mlp.down_proj.weight][symbol:model.layers.23.mlp.down_proj.weight])[symbol:model.layers.23.mlp.down_proj.weight]
            tensor.CPU.register () -> (%11:tensor<[2048, 2048], Float32, CPU>[@model.layers.24.self_attn.q_proj.weight][symbol:model.layers.24.self_attn.q_proj.weight])[symbol:model.layers.24.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%61:tensor<[1024, 2048], Float32, CPU>[@model.layers.24.self_attn.k_proj.weight][symbol:model.layers.24.self_attn.k_proj.weight])[symbol:model.layers.24.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%81:tensor<[1024, 2048], Float32, CPU>[@model.layers.24.self_attn.v_proj.weight][symbol:model.layers.24.self_attn.v_proj.weight])[symbol:model.layers.24.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%127:tensor<[2048, 2048], Float32, CPU>[@model.layers.24.self_attn.o_proj.weight][symbol:model.layers.24.self_attn.o_proj.weight])[symbol:model.layers.24.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%141:tensor<[6144, 2048], Float32, CPU>[@model.layers.24.mlp.gate_proj.weight][symbol:model.layers.24.mlp.gate_proj.weight])[symbol:model.layers.24.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%126:tensor<[6144, 2048], Float32, CPU>[@model.layers.24.mlp.up_proj.weight][symbol:model.layers.24.mlp.up_proj.weight])[symbol:model.layers.24.mlp.up_proj.weight]
            tensor.CPU.register () -> (%34:tensor<[2048, 6144], Float32, CPU>[@model.layers.24.mlp.down_proj.weight][symbol:model.layers.24.mlp.down_proj.weight])[symbol:model.layers.24.mlp.down_proj.weight]
            tensor.CPU.register () -> (%206:tensor<[2048, 2048], Float32, CPU>[@model.layers.25.self_attn.q_proj.weight][symbol:model.layers.25.self_attn.q_proj.weight])[symbol:model.layers.25.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%27:tensor<[1024, 2048], Float32, CPU>[@model.layers.25.self_attn.k_proj.weight][symbol:model.layers.25.self_attn.k_proj.weight])[symbol:model.layers.25.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%121:tensor<[1024, 2048], Float32, CPU>[@model.layers.25.self_attn.v_proj.weight][symbol:model.layers.25.self_attn.v_proj.weight])[symbol:model.layers.25.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%150:tensor<[2048, 2048], Float32, CPU>[@model.layers.25.self_attn.o_proj.weight][symbol:model.layers.25.self_attn.o_proj.weight])[symbol:model.layers.25.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%249:tensor<[6144, 2048], Float32, CPU>[@model.layers.25.mlp.gate_proj.weight][symbol:model.layers.25.mlp.gate_proj.weight])[symbol:model.layers.25.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%159:tensor<[6144, 2048], Float32, CPU>[@model.layers.25.mlp.up_proj.weight][symbol:model.layers.25.mlp.up_proj.weight])[symbol:model.layers.25.mlp.up_proj.weight]
            tensor.CPU.register () -> (%267:tensor<[2048, 6144], Float32, CPU>[@model.layers.25.mlp.down_proj.weight][symbol:model.layers.25.mlp.down_proj.weight])[symbol:model.layers.25.mlp.down_proj.weight]
            tensor.CPU.register () -> (%265:tensor<[2048, 2048], Float32, CPU>[@model.layers.26.self_attn.q_proj.weight][symbol:model.layers.26.self_attn.q_proj.weight])[symbol:model.layers.26.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%190:tensor<[1024, 2048], Float32, CPU>[@model.layers.26.self_attn.k_proj.weight][symbol:model.layers.26.self_attn.k_proj.weight])[symbol:model.layers.26.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%119:tensor<[1024, 2048], Float32, CPU>[@model.layers.26.self_attn.v_proj.weight][symbol:model.layers.26.self_attn.v_proj.weight])[symbol:model.layers.26.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%88:tensor<[2048, 2048], Float32, CPU>[@model.layers.26.self_attn.o_proj.weight][symbol:model.layers.26.self_attn.o_proj.weight])[symbol:model.layers.26.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%96:tensor<[6144, 2048], Float32, CPU>[@model.layers.26.mlp.gate_proj.weight][symbol:model.layers.26.mlp.gate_proj.weight])[symbol:model.layers.26.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%62:tensor<[6144, 2048], Float32, CPU>[@model.layers.26.mlp.up_proj.weight][symbol:model.layers.26.mlp.up_proj.weight])[symbol:model.layers.26.mlp.up_proj.weight]
            tensor.CPU.register () -> (%220:tensor<[2048, 6144], Float32, CPU>[@model.layers.26.mlp.down_proj.weight][symbol:model.layers.26.mlp.down_proj.weight])[symbol:model.layers.26.mlp.down_proj.weight]
            tensor.CPU.register () -> (%185:tensor<[2048, 2048], Float32, CPU>[@model.layers.27.self_attn.q_proj.weight][symbol:model.layers.27.self_attn.q_proj.weight])[symbol:model.layers.27.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%12:tensor<[1024, 2048], Float32, CPU>[@model.layers.27.self_attn.k_proj.weight][symbol:model.layers.27.self_attn.k_proj.weight])[symbol:model.layers.27.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%54:tensor<[1024, 2048], Float32, CPU>[@model.layers.27.self_attn.v_proj.weight][symbol:model.layers.27.self_attn.v_proj.weight])[symbol:model.layers.27.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%60:tensor<[2048, 2048], Float32, CPU>[@model.layers.27.self_attn.o_proj.weight][symbol:model.layers.27.self_attn.o_proj.weight])[symbol:model.layers.27.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%144:tensor<[6144, 2048], Float32, CPU>[@model.layers.27.mlp.gate_proj.weight][symbol:model.layers.27.mlp.gate_proj.weight])[symbol:model.layers.27.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%146:tensor<[6144, 2048], Float32, CPU>[@model.layers.27.mlp.up_proj.weight][symbol:model.layers.27.mlp.up_proj.weight])[symbol:model.layers.27.mlp.up_proj.weight]
            tensor.CPU.register () -> (%195:tensor<[2048, 6144], Float32, CPU>[@model.layers.27.mlp.down_proj.weight][symbol:model.layers.27.mlp.down_proj.weight])[symbol:model.layers.27.mlp.down_proj.weight]
            tensor.CPU.register () -> (%101:tensor<[151936, 2048], Float32, CPU>[@lm_head.weight][symbol:lm_head.weight])[symbol:lm_head.weight]
        }
    }
    graph.SubGraphOp @deinit <notype> [symbol:deinit] {
        () -> () {
            
        }
    }
    graph.CallGraphOp @model (%318:tensor<[1, 32], Int64, CPU>, %376:tensor<[1, 32], Int64, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %320:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %322:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %324:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %326:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %328:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %330:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %332:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %334:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %336:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %338:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %340:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %342:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %344:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %346:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %348:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %350:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %352:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %354:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %356:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %358:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %360:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %362:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %364:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %366:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %368:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %370:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %372:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %374:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %321:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %323:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %325:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %327:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %329:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %331:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %333:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %335:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %337:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %339:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %341:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %343:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %345:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %347:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %349:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %351:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %353:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %355:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %357:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %359:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %361:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %363:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %365:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %367:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %369:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %371:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %373:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %375:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1529:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %435:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %476:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %517:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %558:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %599:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %640:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %681:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %722:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %763:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %804:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %845:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %886:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %927:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %968:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1009:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1050:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1091:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1132:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1173:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1214:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1255:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1296:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1337:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1378:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1419:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1460:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1501:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %396:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %437:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %478:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %519:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %560:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %601:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %642:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %683:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %724:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %765:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %806:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %847:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %888:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %929:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %970:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %1011:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %1052:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %1093:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %1134:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %1175:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %1216:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %1257:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %1298:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %1339:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %1380:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %1421:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %1462:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %1503:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
    graph.SubGraphOp @model <CPU> [symbol:model] {
        (%318:tensor<[1, 32], Int64, CPU>, %376:tensor<[1, 32], Int64, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %320:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %322:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %324:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %326:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %328:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %330:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %332:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %334:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %336:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %338:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %340:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %342:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %344:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %346:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %348:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %350:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %352:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %354:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %356:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %358:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %360:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %362:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %364:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %366:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %368:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %370:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %372:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %374:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %321:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %323:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %325:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %327:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %329:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %331:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %333:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %335:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %337:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %339:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %341:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %343:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %345:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %347:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %349:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %351:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %353:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %355:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %357:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %359:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %361:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %363:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %365:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %367:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %369:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %371:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %373:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %375:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1529:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %435:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %476:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %517:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %558:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %599:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %640:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %681:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %722:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %763:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %804:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %845:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %886:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %927:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %968:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1009:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1050:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1091:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1132:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1173:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1214:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1255:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1296:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1337:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1378:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1419:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1460:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1501:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %396:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %437:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %478:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %519:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %560:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %601:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %642:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %683:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %724:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %765:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %806:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %847:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %888:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %929:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %970:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %1011:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %1052:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %1093:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %1134:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %1175:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %1216:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %1257:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %1298:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %1339:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %1380:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %1421:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %1462:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %1503:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) {
            linalg.CPU.EmbeddingOp <name="model.embed_tokens">(%318:tensor<[1, 32], Int64, CPU>) -> (%377:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.CastTypeOp(%377:tensor<[1, 32, 2048], Float32, CPU>) -> (%378:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%376:tensor<[1, 32], Int64, CPU>) -> (%376:tensor<[32], Int64, CPU>)
            linalg.CPU.IndexOp(%316:tensor<[1, 1024, 128], Int16PerTensor, CPU>) -> (%379:tensor<[1, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.IndexOp(%317:tensor<[1, 1024, 128], Int16PerTensor, CPU>) -> (%380:tensor<[1, 32, 128], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.0 (%378:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %320:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %321:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%421:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %396:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            graph.CallGraphOp @model.layers.1 (%421:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %322:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %323:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%462:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %435:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %437:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            graph.CallGraphOp @model.layers.2 (%462:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %324:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %325:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%503:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %476:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %478:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            graph.CallGraphOp @model.layers.3 (%503:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %326:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %327:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%544:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %517:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %519:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            graph.CallGraphOp @model.layers.4 (%544:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %328:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %329:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%585:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %558:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %560:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            graph.CallGraphOp @model.layers.5 (%585:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %330:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %331:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%626:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %599:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %601:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            graph.CallGraphOp @model.layers.6 (%626:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %332:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %333:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%667:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %640:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %642:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            graph.CallGraphOp @model.layers.7 (%667:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %334:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %335:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%708:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %681:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %683:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            graph.CallGraphOp @model.layers.8 (%708:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %336:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %337:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%749:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %722:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %724:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            graph.CallGraphOp @model.layers.9 (%749:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %338:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %339:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%790:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %763:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %765:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            graph.CallGraphOp @model.layers.10 (%790:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %340:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %341:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%831:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %804:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %806:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            graph.CallGraphOp @model.layers.11 (%831:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %342:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %343:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%872:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %845:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %847:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            graph.CallGraphOp @model.layers.12 (%872:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %344:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %345:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%913:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %886:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %888:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            graph.CallGraphOp @model.layers.13 (%913:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %346:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %347:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%954:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %927:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %929:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            graph.CallGraphOp @model.layers.14 (%954:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %348:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %349:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%995:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %968:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %970:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            graph.CallGraphOp @model.layers.15 (%995:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %350:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %351:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1036:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1009:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1011:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            graph.CallGraphOp @model.layers.16 (%1036:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %352:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %353:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1077:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1050:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1052:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            graph.CallGraphOp @model.layers.17 (%1077:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %354:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %355:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1118:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1091:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1093:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            graph.CallGraphOp @model.layers.18 (%1118:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %356:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %357:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1159:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1132:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1134:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            graph.CallGraphOp @model.layers.19 (%1159:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %358:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %359:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1200:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1173:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1175:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            graph.CallGraphOp @model.layers.20 (%1200:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %360:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %361:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1241:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1214:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1216:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            graph.CallGraphOp @model.layers.21 (%1241:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %362:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %363:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1282:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1255:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1257:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            graph.CallGraphOp @model.layers.22 (%1282:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %364:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %365:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1323:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1296:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1298:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            graph.CallGraphOp @model.layers.23 (%1323:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %366:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %367:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1364:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1337:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1339:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            graph.CallGraphOp @model.layers.24 (%1364:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %368:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %369:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1405:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1378:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1380:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            graph.CallGraphOp @model.layers.25 (%1405:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %370:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %371:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1446:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1419:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1421:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            graph.CallGraphOp @model.layers.26 (%1446:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %372:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %373:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1487:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1460:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1462:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            graph.CallGraphOp @model.layers.27 (%1487:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %374:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %375:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1528:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1501:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1503:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.norm">(%1528:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1529:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1529:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %435:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %476:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %517:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %558:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %599:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %640:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %681:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %722:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %763:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %804:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %845:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %886:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %927:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %968:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1009:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1050:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1091:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1132:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1173:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1214:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1255:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1296:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1337:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1378:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1419:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1460:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1501:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %396:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %437:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %478:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %519:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %560:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %601:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %642:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %683:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %724:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %765:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %806:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %847:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %888:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %929:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %970:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %1011:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %1052:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %1093:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %1134:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %1175:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %1216:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %1257:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %1298:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %1339:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %1380:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %1421:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %1462:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>, %1503:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.0 <CPU> [symbol:model.layers.0] {
        (%378:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %320:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %321:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%421:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %396:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.0.input_layernorm">(%378:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%381:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.0.self_attn (%381:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %320:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %321:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%413:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %396:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.AddOp(%413:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %378:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%414:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.0.post_attention_layernorm">(%414:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%415:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.0.mlp (%415:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%420:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp(%420:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %414:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%421:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%421:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %396:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.0.self_attn <CPU> [symbol:model.layers.0.self_attn] {
        (%381:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %320:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %321:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%413:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %396:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.0.self_attn.q_proj">(%381:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%382:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.0.self_attn.k_proj">(%381:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%383:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.0.self_attn.v_proj">(%381:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%384:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%382:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%382:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%382:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%385:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%383:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%383:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%383:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%386:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%384:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%384:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%384:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%387:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.0.self_attn.q_norm">(%385:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%388:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.0.self_attn.k_norm">(%386:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%389:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.0.self_attn.q_rope">(%388:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%390:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.0.self_attn.k_rope">(%389:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%391:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp(%391:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%392:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp(%392:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%393:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp(%393:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.CastTypeOp(%387:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%395:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp(%395:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%396:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp(%320:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%397:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp(%321:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %396:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%398:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp(%397:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%399:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp(%398:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%400:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp(%390:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %399:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%401:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp(%401:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %402:tensor<[1], Int16PerTensor, CPU>[constant:[0]]) -> (%403:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp(%403:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%404:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp(%404:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %405:tensor<[1], Int16PerTensor, CPU>[constant:[9.1807e-41]]) -> (%406:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp(%319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %407:tensor<[1], UInt16, CPU>[constant:[0]]) -> (%408:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp(%408:tensor<[1, 1, 32, 1024], UInt8, CPU>, %403:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %406:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%409:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp(%409:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%410:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp(%410:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %400:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%411:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%411:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%412:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%412:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%412:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.0.self_attn.o_proj">(%412:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%413:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%413:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %396:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.0.mlp <CPU> [symbol:model.layers.0.mlp] {
        (%415:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%420:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.0.mlp.gate_proj">(%415:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%416:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.0.mlp.act">(%416:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%417:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.0.mlp.up_proj">(%415:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%418:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp(%417:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %418:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%419:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.0.mlp.down_proj">(%419:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%420:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%420:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.1 <CPU> [symbol:model.layers.1] {
        (%421:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %322:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %323:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%462:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %435:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %437:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.1.input_layernorm">(%421:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%422:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.1.self_attn (%422:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %322:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %323:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%454:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %435:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %437:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.AddOp(%454:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %421:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%455:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.1.post_attention_layernorm">(%455:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%456:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.1.mlp (%456:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%461:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp(%461:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %455:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%462:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%462:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %435:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %437:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.1.self_attn <CPU> [symbol:model.layers.1.self_attn] {
        (%422:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %322:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %323:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%454:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %435:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %437:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.1.self_attn.q_proj">(%422:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%423:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.1.self_attn.k_proj">(%422:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%424:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.1.self_attn.v_proj">(%422:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%425:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%423:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%423:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%423:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%426:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%424:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%424:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%424:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%427:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%425:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%425:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%425:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%428:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.1.self_attn.q_norm">(%426:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%429:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.1.self_attn.k_norm">(%427:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%430:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.1.self_attn.q_rope">(%429:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%431:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.1.self_attn.k_rope">(%430:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%432:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp(%432:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%433:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp(%433:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%434:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp(%434:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%435:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.CastTypeOp(%428:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%436:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp(%436:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%437:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp(%322:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %435:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%438:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp(%323:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %437:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%439:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp(%438:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%440:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp(%439:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%441:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp(%431:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %440:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%442:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp(%442:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %443:tensor<[1], Int16PerTensor, CPU>[constant:[0]]) -> (%444:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp(%444:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%445:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp(%445:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %446:tensor<[1], Int16PerTensor, CPU>[constant:[9.1807e-41]]) -> (%447:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp(%319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %448:tensor<[1], UInt16, CPU>[constant:[0]]) -> (%449:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp(%449:tensor<[1, 1, 32, 1024], UInt8, CPU>, %444:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %447:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%450:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp(%450:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%451:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp(%451:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %441:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%452:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%452:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%453:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%453:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%453:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.1.self_attn.o_proj">(%453:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%454:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%454:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %435:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %437:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.1.mlp <CPU> [symbol:model.layers.1.mlp] {
        (%456:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%461:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.1.mlp.gate_proj">(%456:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%457:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.1.mlp.act">(%457:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%458:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.1.mlp.up_proj">(%456:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%459:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp(%458:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %459:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%460:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.1.mlp.down_proj">(%460:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%461:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%461:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.2 <CPU> [symbol:model.layers.2] {
        (%462:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %324:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %325:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%503:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %476:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %478:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.2.input_layernorm">(%462:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%463:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.2.self_attn (%463:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %324:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %325:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%495:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %476:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %478:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.AddOp(%495:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %462:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%496:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.2.post_attention_layernorm">(%496:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%497:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.2.mlp (%497:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%502:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp(%502:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %496:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%503:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%503:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %476:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %478:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.2.self_attn <CPU> [symbol:model.layers.2.self_attn] {
        (%463:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %324:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %325:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%495:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %476:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %478:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.2.self_attn.q_proj">(%463:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%464:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.2.self_attn.k_proj">(%463:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%465:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.2.self_attn.v_proj">(%463:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%466:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%464:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%464:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%464:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%467:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%465:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%465:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%465:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%468:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%466:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%466:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%466:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%469:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.2.self_attn.q_norm">(%467:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%470:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.2.self_attn.k_norm">(%468:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%471:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.2.self_attn.q_rope">(%470:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%472:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.2.self_attn.k_rope">(%471:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%473:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp(%473:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%474:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp(%474:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%475:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp(%475:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%476:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.CastTypeOp(%469:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%477:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp(%477:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%478:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp(%324:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %476:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%479:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp(%325:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %478:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%480:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp(%479:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%481:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp(%480:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%482:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp(%472:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %481:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%483:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp(%483:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %484:tensor<[1], Int16PerTensor, CPU>[constant:[0]]) -> (%485:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp(%485:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%486:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp(%486:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %487:tensor<[1], Int16PerTensor, CPU>[constant:[9.1807e-41]]) -> (%488:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp(%319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %489:tensor<[1], UInt16, CPU>[constant:[0]]) -> (%490:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp(%490:tensor<[1, 1, 32, 1024], UInt8, CPU>, %485:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %488:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%491:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp(%491:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%492:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp(%492:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %482:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%493:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%493:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%494:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%494:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%494:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.2.self_attn.o_proj">(%494:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%495:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%495:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %476:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %478:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.2.mlp <CPU> [symbol:model.layers.2.mlp] {
        (%497:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%502:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.2.mlp.gate_proj">(%497:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%498:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.2.mlp.act">(%498:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%499:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.2.mlp.up_proj">(%497:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%500:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp(%499:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %500:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%501:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.2.mlp.down_proj">(%501:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%502:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%502:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.3 <CPU> [symbol:model.layers.3] {
        (%503:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %326:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %327:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%544:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %517:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %519:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.3.input_layernorm">(%503:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%504:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.3.self_attn (%504:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %326:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %327:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%536:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %517:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %519:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.AddOp(%536:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %503:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%537:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.3.post_attention_layernorm">(%537:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%538:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.3.mlp (%538:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%543:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp(%543:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %537:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%544:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%544:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %517:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %519:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.3.self_attn <CPU> [symbol:model.layers.3.self_attn] {
        (%504:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %326:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %327:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%536:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %517:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %519:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.3.self_attn.q_proj">(%504:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%505:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.3.self_attn.k_proj">(%504:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%506:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.3.self_attn.v_proj">(%504:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%507:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%505:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%505:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%505:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%508:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%506:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%506:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%506:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%509:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%507:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%507:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%507:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%510:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.3.self_attn.q_norm">(%508:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%511:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.3.self_attn.k_norm">(%509:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%512:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.3.self_attn.q_rope">(%511:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%513:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.3.self_attn.k_rope">(%512:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%514:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp(%514:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%515:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp(%515:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%516:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp(%516:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%517:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.CastTypeOp(%510:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%518:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp(%518:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%519:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp(%326:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %517:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%520:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp(%327:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %519:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%521:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp(%520:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%522:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp(%521:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%523:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp(%513:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %522:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%524:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp(%524:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %525:tensor<[1], Int16PerTensor, CPU>[constant:[0]]) -> (%526:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp(%526:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%527:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp(%527:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %528:tensor<[1], Int16PerTensor, CPU>[constant:[1.0078101]]) -> (%529:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp(%319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %530:tensor<[1], UInt16, CPU>[constant:[0]]) -> (%531:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp(%531:tensor<[1, 1, 32, 1024], UInt8, CPU>, %526:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %529:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%532:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp(%532:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%533:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp(%533:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %523:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%534:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%534:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%535:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%535:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%535:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.3.self_attn.o_proj">(%535:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%536:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%536:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %517:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %519:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.3.mlp <CPU> [symbol:model.layers.3.mlp] {
        (%538:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%543:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.3.mlp.gate_proj">(%538:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%539:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.3.mlp.act">(%539:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%540:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.3.mlp.up_proj">(%538:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%541:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp(%540:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %541:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%542:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.3.mlp.down_proj">(%542:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%543:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%543:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.4 <CPU> [symbol:model.layers.4] {
        (%544:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %328:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %329:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%585:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %558:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %560:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.4.input_layernorm">(%544:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%545:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.4.self_attn (%545:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %328:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %329:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%577:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %558:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %560:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.AddOp(%577:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %544:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%578:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.4.post_attention_layernorm">(%578:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%579:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.4.mlp (%579:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%584:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp(%584:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %578:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%585:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%585:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %558:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %560:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.4.self_attn <CPU> [symbol:model.layers.4.self_attn] {
        (%545:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %328:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %329:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%577:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %558:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %560:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.4.self_attn.q_proj">(%545:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%546:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.4.self_attn.k_proj">(%545:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%547:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.4.self_attn.v_proj">(%545:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%548:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%546:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%546:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%546:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%549:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%547:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%547:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%547:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%550:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%548:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%548:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%548:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%551:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.4.self_attn.q_norm">(%549:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%552:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.4.self_attn.k_norm">(%550:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%553:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.4.self_attn.q_rope">(%552:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%554:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.4.self_attn.k_rope">(%553:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%555:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp(%555:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%556:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp(%556:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%557:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp(%557:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%558:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.CastTypeOp(%551:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%559:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp(%559:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%560:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp(%328:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %558:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%561:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp(%329:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %560:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%562:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp(%561:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%563:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp(%562:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%564:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp(%554:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %563:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%565:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp(%565:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %566:tensor<[1], Int16PerTensor, CPU>[constant:[0]]) -> (%567:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp(%567:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%568:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp(%568:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %569:tensor<[1], Int16PerTensor, CPU>[constant:[9.1807e-41]]) -> (%570:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp(%319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %571:tensor<[1], UInt16, CPU>[constant:[0]]) -> (%572:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp(%572:tensor<[1, 1, 32, 1024], UInt8, CPU>, %567:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %570:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%573:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp(%573:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%574:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp(%574:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %564:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%575:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%575:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%576:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%576:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%576:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.4.self_attn.o_proj">(%576:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%577:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%577:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %558:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %560:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.4.mlp <CPU> [symbol:model.layers.4.mlp] {
        (%579:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%584:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.4.mlp.gate_proj">(%579:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%580:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.4.mlp.act">(%580:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%581:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.4.mlp.up_proj">(%579:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%582:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp(%581:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %582:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%583:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.4.mlp.down_proj">(%583:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%584:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%584:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.5 <CPU> [symbol:model.layers.5] {
        (%585:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %330:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %331:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%626:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %599:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %601:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.5.input_layernorm">(%585:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%586:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.5.self_attn (%586:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %330:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %331:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%618:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %599:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %601:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.AddOp(%618:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %585:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%619:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.5.post_attention_layernorm">(%619:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%620:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.5.mlp (%620:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%625:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp(%625:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %619:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%626:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%626:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %599:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %601:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.5.self_attn <CPU> [symbol:model.layers.5.self_attn] {
        (%586:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %330:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %331:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%618:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %599:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %601:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.5.self_attn.q_proj">(%586:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%587:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.5.self_attn.k_proj">(%586:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%588:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.5.self_attn.v_proj">(%586:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%589:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%587:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%587:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%587:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%590:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%588:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%588:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%588:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%591:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%589:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%589:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%589:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%592:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.5.self_attn.q_norm">(%590:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%593:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.5.self_attn.k_norm">(%591:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%594:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.5.self_attn.q_rope">(%593:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%595:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.5.self_attn.k_rope">(%594:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%596:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp(%596:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%597:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp(%597:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%598:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp(%598:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%599:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.CastTypeOp(%592:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%600:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp(%600:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%601:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp(%330:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %599:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%602:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp(%331:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %601:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%603:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp(%602:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%604:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp(%603:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%605:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp(%595:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %604:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%606:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp(%606:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %607:tensor<[1], Int16PerTensor, CPU>[constant:[0]]) -> (%608:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp(%608:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%609:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp(%609:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %610:tensor<[1], Int16PerTensor, CPU>[constant:[9.1807e-41]]) -> (%611:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp(%319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %612:tensor<[1], UInt16, CPU>[constant:[0]]) -> (%613:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp(%613:tensor<[1, 1, 32, 1024], UInt8, CPU>, %608:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %611:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%614:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp(%614:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%615:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp(%615:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %605:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%616:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%616:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%617:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%617:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%617:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.5.self_attn.o_proj">(%617:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%618:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%618:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %599:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %601:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.5.mlp <CPU> [symbol:model.layers.5.mlp] {
        (%620:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%625:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.5.mlp.gate_proj">(%620:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%621:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.5.mlp.act">(%621:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%622:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.5.mlp.up_proj">(%620:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%623:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp(%622:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %623:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%624:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.5.mlp.down_proj">(%624:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%625:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%625:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.6 <CPU> [symbol:model.layers.6] {
        (%626:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %332:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %333:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%667:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %640:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %642:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.6.input_layernorm">(%626:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%627:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.6.self_attn (%627:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %332:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %333:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%659:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %640:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %642:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.AddOp(%659:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %626:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%660:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.6.post_attention_layernorm">(%660:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%661:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.6.mlp (%661:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%666:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp(%666:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %660:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%667:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%667:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %640:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %642:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.6.self_attn <CPU> [symbol:model.layers.6.self_attn] {
        (%627:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %332:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %333:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%659:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %640:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %642:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.6.self_attn.q_proj">(%627:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%628:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.6.self_attn.k_proj">(%627:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%629:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.6.self_attn.v_proj">(%627:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%630:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%628:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%628:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%628:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%631:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%629:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%629:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%629:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%632:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%630:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%630:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%630:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%633:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.6.self_attn.q_norm">(%631:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%634:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.6.self_attn.k_norm">(%632:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%635:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.6.self_attn.q_rope">(%634:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%636:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.6.self_attn.k_rope">(%635:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%637:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp(%637:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%638:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp(%638:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%639:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp(%639:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%640:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.CastTypeOp(%633:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%641:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp(%641:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%642:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp(%332:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %640:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%643:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp(%333:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %642:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%644:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp(%643:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%645:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp(%644:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%646:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp(%636:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %645:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%647:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp(%647:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %648:tensor<[1], Int16PerTensor, CPU>[constant:[0]]) -> (%649:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp(%649:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%650:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp(%650:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %651:tensor<[1], Int16PerTensor, CPU>[constant:[9.1807e-41]]) -> (%652:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp(%319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %653:tensor<[1], UInt16, CPU>[constant:[0]]) -> (%654:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp(%654:tensor<[1, 1, 32, 1024], UInt8, CPU>, %649:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %652:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%655:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp(%655:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%656:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp(%656:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %646:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%657:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%657:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%658:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%658:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%658:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.6.self_attn.o_proj">(%658:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%659:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%659:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %640:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %642:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.6.mlp <CPU> [symbol:model.layers.6.mlp] {
        (%661:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%666:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.6.mlp.gate_proj">(%661:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%662:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.6.mlp.act">(%662:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%663:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.6.mlp.up_proj">(%661:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%664:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp(%663:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %664:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%665:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.6.mlp.down_proj">(%665:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%666:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%666:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.7 <CPU> [symbol:model.layers.7] {
        (%667:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %334:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %335:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%708:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %681:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %683:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.7.input_layernorm">(%667:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%668:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.7.self_attn (%668:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %334:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %335:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%700:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %681:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %683:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.AddOp(%700:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %667:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%701:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.7.post_attention_layernorm">(%701:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%702:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.7.mlp (%702:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%707:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp(%707:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %701:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%708:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%708:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %681:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %683:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.7.self_attn <CPU> [symbol:model.layers.7.self_attn] {
        (%668:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %334:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %335:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%700:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %681:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %683:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.7.self_attn.q_proj">(%668:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%669:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.7.self_attn.k_proj">(%668:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%670:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.7.self_attn.v_proj">(%668:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%671:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%669:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%669:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%669:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%672:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%670:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%670:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%670:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%673:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%671:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%671:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%671:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%674:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.7.self_attn.q_norm">(%672:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%675:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.7.self_attn.k_norm">(%673:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%676:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.7.self_attn.q_rope">(%675:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%677:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.7.self_attn.k_rope">(%676:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%678:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp(%678:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%679:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp(%679:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%680:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp(%680:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%681:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.CastTypeOp(%674:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%682:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp(%682:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%683:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp(%334:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %681:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%684:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp(%335:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %683:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%685:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp(%684:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%686:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp(%685:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%687:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp(%677:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %686:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%688:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp(%688:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %689:tensor<[1], Int16PerTensor, CPU>[constant:[0]]) -> (%690:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp(%690:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%691:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp(%691:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %692:tensor<[1], Int16PerTensor, CPU>[constant:[9.1807e-41]]) -> (%693:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp(%319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %694:tensor<[1], UInt16, CPU>[constant:[0]]) -> (%695:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp(%695:tensor<[1, 1, 32, 1024], UInt8, CPU>, %690:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %693:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%696:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp(%696:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%697:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp(%697:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %687:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%698:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%698:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%699:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%699:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%699:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.7.self_attn.o_proj">(%699:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%700:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%700:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %681:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %683:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.7.mlp <CPU> [symbol:model.layers.7.mlp] {
        (%702:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%707:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.7.mlp.gate_proj">(%702:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%703:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.7.mlp.act">(%703:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%704:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.7.mlp.up_proj">(%702:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%705:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp(%704:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %705:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%706:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.7.mlp.down_proj">(%706:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%707:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%707:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.8 <CPU> [symbol:model.layers.8] {
        (%708:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %336:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %337:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%749:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %722:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %724:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.8.input_layernorm">(%708:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%709:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.8.self_attn (%709:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %336:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %337:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%741:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %722:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %724:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.AddOp(%741:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %708:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%742:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.8.post_attention_layernorm">(%742:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%743:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.8.mlp (%743:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%748:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp(%748:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %742:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%749:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%749:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %722:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %724:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.8.self_attn <CPU> [symbol:model.layers.8.self_attn] {
        (%709:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %336:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %337:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%741:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %722:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %724:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.8.self_attn.q_proj">(%709:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%710:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.8.self_attn.k_proj">(%709:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%711:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.8.self_attn.v_proj">(%709:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%712:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%710:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%710:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%710:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%713:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%711:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%711:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%711:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%714:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%712:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%712:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%712:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%715:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.8.self_attn.q_norm">(%713:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%716:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.8.self_attn.k_norm">(%714:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%717:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.8.self_attn.q_rope">(%716:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%718:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.8.self_attn.k_rope">(%717:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%719:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp(%719:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%720:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp(%720:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%721:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp(%721:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%722:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.CastTypeOp(%715:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%723:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp(%723:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%724:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp(%336:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %722:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%725:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp(%337:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %724:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%726:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp(%725:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%727:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp(%726:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%728:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp(%718:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %727:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%729:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp(%729:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %730:tensor<[1], Int16PerTensor, CPU>[constant:[0]]) -> (%731:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp(%731:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%732:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp(%732:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %733:tensor<[1], Int16PerTensor, CPU>[constant:[9.1807e-41]]) -> (%734:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp(%319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %735:tensor<[1], UInt16, CPU>[constant:[1]]) -> (%736:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp(%736:tensor<[1, 1, 32, 1024], UInt8, CPU>, %731:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %734:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%737:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp(%737:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%738:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp(%738:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %728:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%739:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%739:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%740:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%740:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%740:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.8.self_attn.o_proj">(%740:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%741:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%741:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %722:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %724:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.8.mlp <CPU> [symbol:model.layers.8.mlp] {
        (%743:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%748:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.8.mlp.gate_proj">(%743:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%744:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.8.mlp.act">(%744:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%745:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.8.mlp.up_proj">(%743:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%746:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp(%745:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %746:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%747:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.8.mlp.down_proj">(%747:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%748:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%748:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.9 <CPU> [symbol:model.layers.9] {
        (%749:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %338:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %339:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%790:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %763:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %765:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.9.input_layernorm">(%749:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%750:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.9.self_attn (%750:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %338:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %339:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%782:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %763:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %765:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.AddOp(%782:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %749:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%783:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.9.post_attention_layernorm">(%783:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%784:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.9.mlp (%784:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%789:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp(%789:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %783:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%790:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%790:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %763:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %765:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.9.self_attn <CPU> [symbol:model.layers.9.self_attn] {
        (%750:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %338:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %339:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%782:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %763:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %765:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.9.self_attn.q_proj">(%750:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%751:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.9.self_attn.k_proj">(%750:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%752:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.9.self_attn.v_proj">(%750:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%753:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%751:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%751:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%751:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%754:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%752:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%752:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%752:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%755:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%753:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%753:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%753:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%756:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.9.self_attn.q_norm">(%754:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%757:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.9.self_attn.k_norm">(%755:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%758:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.9.self_attn.q_rope">(%757:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%759:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.9.self_attn.k_rope">(%758:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%760:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp(%760:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%761:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp(%761:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%762:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp(%762:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%763:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.CastTypeOp(%756:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%764:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp(%764:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%765:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp(%338:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %763:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%766:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp(%339:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %765:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%767:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp(%766:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%768:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp(%767:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%769:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp(%759:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %768:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%770:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp(%770:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %771:tensor<[1], Int16PerTensor, CPU>[constant:[0.83203125]]) -> (%772:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp(%772:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%773:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp(%773:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %774:tensor<[1], Int16PerTensor, CPU>[constant:[0.39257753]]) -> (%775:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp(%319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %776:tensor<[1], UInt16, CPU>[constant:[-0.1796875]]) -> (%777:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp(%777:tensor<[1, 1, 32, 1024], UInt8, CPU>, %772:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %775:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%778:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp(%778:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%779:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp(%779:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %769:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%780:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%780:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%781:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%781:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%781:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.9.self_attn.o_proj">(%781:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%782:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%782:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %763:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %765:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.9.mlp <CPU> [symbol:model.layers.9.mlp] {
        (%784:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%789:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.9.mlp.gate_proj">(%784:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%785:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.9.mlp.act">(%785:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%786:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.9.mlp.up_proj">(%784:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%787:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp(%786:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %787:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%788:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.9.mlp.down_proj">(%788:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%789:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%789:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.10 <CPU> [symbol:model.layers.10] {
        (%790:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %340:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %341:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%831:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %804:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %806:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.10.input_layernorm">(%790:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%791:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.10.self_attn (%791:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %340:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %341:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%823:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %804:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %806:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.AddOp(%823:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %790:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%824:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.10.post_attention_layernorm">(%824:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%825:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.10.mlp (%825:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%830:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp(%830:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %824:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%831:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%831:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %804:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %806:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.10.self_attn <CPU> [symbol:model.layers.10.self_attn] {
        (%791:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %340:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %341:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%823:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %804:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %806:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.10.self_attn.q_proj">(%791:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%792:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.10.self_attn.k_proj">(%791:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%793:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.10.self_attn.v_proj">(%791:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%794:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%792:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%792:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%792:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%795:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%793:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%793:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%793:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%796:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%794:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%794:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%794:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%797:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.10.self_attn.q_norm">(%795:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%798:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.10.self_attn.k_norm">(%796:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%799:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.10.self_attn.q_rope">(%798:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%800:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.10.self_attn.k_rope">(%799:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%801:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp(%801:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%802:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp(%802:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%803:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp(%803:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%804:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.CastTypeOp(%797:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%805:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp(%805:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%806:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp(%340:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %804:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%807:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp(%341:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %806:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%808:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp(%807:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%809:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp(%808:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%810:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp(%800:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %809:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%811:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp(%811:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %812:tensor<[1], Int16PerTensor, CPU>[constant:[-0.69140625]]) -> (%813:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp(%813:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%814:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp(%814:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %815:tensor<[1], Int16PerTensor, CPU>[constant:[-0.9765613]]) -> (%816:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp(%319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %817:tensor<[1], UInt16, CPU>[constant:[-0.93359375]]) -> (%818:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp(%818:tensor<[1, 1, 32, 1024], UInt8, CPU>, %813:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %816:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%819:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp(%819:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%820:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp(%820:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %810:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%821:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%821:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%822:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%822:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%822:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.10.self_attn.o_proj">(%822:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%823:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%823:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %804:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %806:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.10.mlp <CPU> [symbol:model.layers.10.mlp] {
        (%825:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%830:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.10.mlp.gate_proj">(%825:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%826:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.10.mlp.act">(%826:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%827:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.10.mlp.up_proj">(%825:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%828:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp(%827:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %828:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%829:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.10.mlp.down_proj">(%829:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%830:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%830:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.11 <CPU> [symbol:model.layers.11] {
        (%831:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %342:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %343:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%872:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %845:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %847:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.11.input_layernorm">(%831:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%832:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.11.self_attn (%832:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %342:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %343:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%864:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %845:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %847:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.AddOp(%864:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %831:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%865:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.11.post_attention_layernorm">(%865:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%866:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.11.mlp (%866:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%871:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp(%871:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %865:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%872:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%872:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %845:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %847:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.11.self_attn <CPU> [symbol:model.layers.11.self_attn] {
        (%832:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %342:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %343:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%864:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %845:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %847:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.11.self_attn.q_proj">(%832:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%833:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.11.self_attn.k_proj">(%832:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%834:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.11.self_attn.v_proj">(%832:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%835:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%833:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%833:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%833:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%836:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%834:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%834:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%834:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%837:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%835:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%835:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%835:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%838:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.11.self_attn.q_norm">(%836:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%839:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.11.self_attn.k_norm">(%837:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%840:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.11.self_attn.q_rope">(%839:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%841:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.11.self_attn.k_rope">(%840:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%842:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp(%842:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%843:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp(%843:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%844:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp(%844:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%845:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.CastTypeOp(%838:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%846:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp(%846:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%847:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp(%342:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %845:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%848:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp(%343:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %847:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%849:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp(%848:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%850:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp(%849:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%851:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp(%841:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %850:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%852:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp(%852:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %853:tensor<[1], Int16PerTensor, CPU>[constant:[-0.58203125]]) -> (%854:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp(%854:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%855:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp(%855:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %856:tensor<[1], Int16PerTensor, CPU>[constant:[-0.039794847]]) -> (%857:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp(%319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %858:tensor<[1], UInt16, CPU>[constant:[0.515625]]) -> (%859:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp(%859:tensor<[1, 1, 32, 1024], UInt8, CPU>, %854:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %857:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%860:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp(%860:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%861:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp(%861:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %851:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%862:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%862:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%863:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%863:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%863:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.11.self_attn.o_proj">(%863:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%864:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%864:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %845:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %847:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.11.mlp <CPU> [symbol:model.layers.11.mlp] {
        (%866:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%871:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.11.mlp.gate_proj">(%866:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%867:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.11.mlp.act">(%867:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%868:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.11.mlp.up_proj">(%866:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%869:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp(%868:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %869:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%870:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.11.mlp.down_proj">(%870:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%871:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%871:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.12 <CPU> [symbol:model.layers.12] {
        (%872:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %344:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %345:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%913:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %886:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %888:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.12.input_layernorm">(%872:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%873:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.12.self_attn (%873:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %344:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %345:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%905:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %886:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %888:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.AddOp(%905:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %872:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%906:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.12.post_attention_layernorm">(%906:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%907:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.12.mlp (%907:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%912:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp(%912:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %906:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%913:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%913:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %886:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %888:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.12.self_attn <CPU> [symbol:model.layers.12.self_attn] {
        (%873:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %344:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %345:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%905:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %886:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %888:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.12.self_attn.q_proj">(%873:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%874:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.12.self_attn.k_proj">(%873:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%875:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.12.self_attn.v_proj">(%873:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%876:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%874:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%874:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%874:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%877:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%875:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%875:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%875:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%878:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%876:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%876:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%876:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%879:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.12.self_attn.q_norm">(%877:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%880:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.12.self_attn.k_norm">(%878:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%881:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.12.self_attn.q_rope">(%880:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%882:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.12.self_attn.k_rope">(%881:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%883:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp(%883:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%884:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp(%884:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%885:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp(%885:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%886:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.CastTypeOp(%879:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%887:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp(%887:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%888:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp(%344:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %886:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%889:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp(%345:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %888:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%890:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp(%889:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%891:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp(%890:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%892:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp(%882:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %891:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%893:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp(%893:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %894:tensor<[1], Int16PerTensor, CPU>[constant:[0.90234375]]) -> (%895:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp(%895:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%896:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp(%896:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %897:tensor<[1], Int16PerTensor, CPU>[constant:[0.9921863]]) -> (%898:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp(%319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %899:tensor<[1], UInt16, CPU>[constant:[0.74609375]]) -> (%900:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp(%900:tensor<[1, 1, 32, 1024], UInt8, CPU>, %895:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %898:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%901:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp(%901:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%902:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp(%902:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %892:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%903:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%903:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%904:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%904:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%904:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.12.self_attn.o_proj">(%904:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%905:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%905:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %886:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %888:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.12.mlp <CPU> [symbol:model.layers.12.mlp] {
        (%907:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%912:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.12.mlp.gate_proj">(%907:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%908:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.12.mlp.act">(%908:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%909:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.12.mlp.up_proj">(%907:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%910:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp(%909:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %910:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%911:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.12.mlp.down_proj">(%911:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%912:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%912:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.13 <CPU> [symbol:model.layers.13] {
        (%913:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %346:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %347:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%954:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %927:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %929:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.13.input_layernorm">(%913:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%914:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.13.self_attn (%914:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %346:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %347:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%946:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %927:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %929:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.AddOp(%946:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %913:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%947:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.13.post_attention_layernorm">(%947:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%948:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.13.mlp (%948:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%953:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp(%953:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %947:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%954:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%954:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %927:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %929:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.13.self_attn <CPU> [symbol:model.layers.13.self_attn] {
        (%914:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %346:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %347:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%946:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %927:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %929:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.13.self_attn.q_proj">(%914:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%915:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.13.self_attn.k_proj">(%914:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%916:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.13.self_attn.v_proj">(%914:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%917:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%915:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%915:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%915:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%918:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%916:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%916:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%916:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%919:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%917:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%917:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%917:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%920:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.13.self_attn.q_norm">(%918:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%921:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.13.self_attn.k_norm">(%919:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%922:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.13.self_attn.q_rope">(%921:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%923:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.13.self_attn.k_rope">(%922:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%924:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp(%924:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%925:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp(%925:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%926:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp(%926:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%927:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.CastTypeOp(%920:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%928:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp(%928:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%929:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp(%346:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %927:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%930:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp(%347:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %929:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%931:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp(%930:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%932:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp(%931:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%933:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp(%923:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %932:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%934:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp(%934:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %935:tensor<[1], Int16PerTensor, CPU>[constant:[0.2578125]]) -> (%936:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp(%936:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%937:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp(%937:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %938:tensor<[1], Int16PerTensor, CPU>[constant:[-0.31835878]]) -> (%939:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp(%319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %940:tensor<[1], UInt16, CPU>[constant:[-0.78515625]]) -> (%941:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp(%941:tensor<[1, 1, 32, 1024], UInt8, CPU>, %936:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %939:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%942:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp(%942:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%943:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp(%943:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %933:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%944:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%944:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%945:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%945:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%945:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.13.self_attn.o_proj">(%945:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%946:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%946:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %927:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %929:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.13.mlp <CPU> [symbol:model.layers.13.mlp] {
        (%948:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%953:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.13.mlp.gate_proj">(%948:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%949:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.13.mlp.act">(%949:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%950:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.13.mlp.up_proj">(%948:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%951:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp(%950:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %951:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%952:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.13.mlp.down_proj">(%952:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%953:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%953:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.14 <CPU> [symbol:model.layers.14] {
        (%954:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %348:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %349:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%995:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %968:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %970:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.14.input_layernorm">(%954:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%955:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.14.self_attn (%955:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %348:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %349:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%987:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %968:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %970:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.AddOp(%987:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %954:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%988:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.14.post_attention_layernorm">(%988:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%989:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.14.mlp (%989:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%994:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp(%994:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %988:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%995:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%995:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %968:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %970:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.14.self_attn <CPU> [symbol:model.layers.14.self_attn] {
        (%955:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %348:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %349:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%987:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %968:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %970:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.14.self_attn.q_proj">(%955:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%956:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.14.self_attn.k_proj">(%955:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%957:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.14.self_attn.v_proj">(%955:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%958:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%956:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%956:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%956:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%959:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%957:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%957:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%957:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%960:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%958:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%958:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%958:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%961:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.14.self_attn.q_norm">(%959:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%962:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.14.self_attn.k_norm">(%960:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%963:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.14.self_attn.q_rope">(%962:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%964:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.14.self_attn.k_rope">(%963:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%965:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp(%965:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%966:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp(%966:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%967:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp(%967:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%968:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.CastTypeOp(%961:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%969:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp(%969:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%970:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp(%348:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %968:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%971:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp(%349:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %970:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%972:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp(%971:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%973:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp(%972:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%974:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp(%964:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %973:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%975:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp(%975:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %976:tensor<[1], Int16PerTensor, CPU>[constant:[-0.99609375]]) -> (%977:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp(%977:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%978:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp(%978:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %979:tensor<[1], Int16PerTensor, CPU>[constant:[-0.87890506]]) -> (%980:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp(%319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %981:tensor<[1], UInt16, CPU>[constant:[-0.46289062]]) -> (%982:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp(%982:tensor<[1, 1, 32, 1024], UInt8, CPU>, %977:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %980:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%983:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp(%983:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%984:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp(%984:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %974:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%985:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%985:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%986:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%986:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%986:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.14.self_attn.o_proj">(%986:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%987:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%987:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %968:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %970:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.14.mlp <CPU> [symbol:model.layers.14.mlp] {
        (%989:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%994:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.14.mlp.gate_proj">(%989:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%990:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.14.mlp.act">(%990:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%991:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.14.mlp.up_proj">(%989:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%992:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp(%991:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %992:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%993:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.14.mlp.down_proj">(%993:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%994:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%994:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.15 <CPU> [symbol:model.layers.15] {
        (%995:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %350:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %351:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1036:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1009:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1011:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.15.input_layernorm">(%995:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%996:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.15.self_attn (%996:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %350:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %351:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1028:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1009:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1011:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.AddOp(%1028:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %995:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1029:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.15.post_attention_layernorm">(%1029:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1030:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.15.mlp (%1030:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1035:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp(%1035:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1029:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1036:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1036:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1009:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1011:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.15.self_attn <CPU> [symbol:model.layers.15.self_attn] {
        (%996:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %350:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %351:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1028:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1009:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1011:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.15.self_attn.q_proj">(%996:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%997:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.15.self_attn.k_proj">(%996:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%998:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.15.self_attn.v_proj">(%996:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%999:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%997:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%997:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%997:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1000:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%998:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%998:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%998:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1001:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%999:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%999:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%999:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1002:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.15.self_attn.q_norm">(%1000:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1003:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.15.self_attn.k_norm">(%1001:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1004:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.15.self_attn.q_rope">(%1003:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1005:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.15.self_attn.k_rope">(%1004:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1006:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp(%1006:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1007:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp(%1007:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1008:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1008:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1009:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.CastTypeOp(%1002:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1010:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp(%1010:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1011:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp(%350:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %1009:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%1012:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp(%351:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %1011:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1013:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp(%1012:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%1014:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp(%1013:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%1015:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp(%1005:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %1014:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%1016:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp(%1016:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1017:tensor<[1], Int16PerTensor, CPU>[constant:[0.1015625]]) -> (%1018:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp(%1018:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1019:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp(%1019:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %1020:tensor<[1], Int16PerTensor, CPU>[constant:[0.63671756]]) -> (%1021:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp(%319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %1022:tensor<[1], UInt16, CPU>[constant:[0.953125]]) -> (%1023:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp(%1023:tensor<[1, 1, 32, 1024], UInt8, CPU>, %1018:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1021:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%1024:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp(%1024:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1025:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp(%1025:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1015:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%1026:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1026:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1027:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%1027:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1027:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.15.self_attn.o_proj">(%1027:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1028:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1028:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1009:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1011:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.15.mlp <CPU> [symbol:model.layers.15.mlp] {
        (%1030:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1035:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.15.mlp.gate_proj">(%1030:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1031:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.15.mlp.act">(%1031:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1032:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.15.mlp.up_proj">(%1030:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1033:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp(%1032:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %1033:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1034:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.15.mlp.down_proj">(%1034:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1035:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1035:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.16 <CPU> [symbol:model.layers.16] {
        (%1036:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %352:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %353:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1077:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1050:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1052:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.16.input_layernorm">(%1036:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1037:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.16.self_attn (%1037:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %352:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %353:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1069:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1050:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1052:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.AddOp(%1069:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1036:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1070:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.16.post_attention_layernorm">(%1070:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1071:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.16.mlp (%1071:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1076:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp(%1076:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1070:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1077:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1077:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1050:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1052:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.16.self_attn <CPU> [symbol:model.layers.16.self_attn] {
        (%1037:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %352:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %353:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1069:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1050:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1052:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.16.self_attn.q_proj">(%1037:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1038:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.16.self_attn.k_proj">(%1037:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1039:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.16.self_attn.v_proj">(%1037:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1040:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%1038:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1038:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1038:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1041:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%1039:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1039:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1039:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1042:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%1040:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1040:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1040:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1043:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.16.self_attn.q_norm">(%1041:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1044:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.16.self_attn.k_norm">(%1042:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1045:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.16.self_attn.q_rope">(%1044:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1046:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.16.self_attn.k_rope">(%1045:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1047:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp(%1047:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1048:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp(%1048:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1049:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1049:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1050:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.CastTypeOp(%1043:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1051:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp(%1051:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1052:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp(%352:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %1050:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%1053:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp(%353:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %1052:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1054:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp(%1053:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%1055:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp(%1054:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%1056:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp(%1046:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %1055:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%1057:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp(%1057:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1058:tensor<[1], Int16PerTensor, CPU>[constant:[0.95703125]]) -> (%1059:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp(%1059:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1060:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp(%1060:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %1061:tensor<[1], Int16PerTensor, CPU>[constant:[0.6484363]]) -> (%1062:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp(%319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %1063:tensor<[1], UInt16, CPU>[constant:[0.118652344]]) -> (%1064:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp(%1064:tensor<[1, 1, 32, 1024], UInt8, CPU>, %1059:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1062:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%1065:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp(%1065:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1066:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp(%1066:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1056:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%1067:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1067:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1068:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%1068:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1068:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.16.self_attn.o_proj">(%1068:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1069:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1069:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1050:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1052:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.16.mlp <CPU> [symbol:model.layers.16.mlp] {
        (%1071:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1076:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.16.mlp.gate_proj">(%1071:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1072:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.16.mlp.act">(%1072:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1073:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.16.mlp.up_proj">(%1071:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1074:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp(%1073:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %1074:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1075:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.16.mlp.down_proj">(%1075:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1076:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1076:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.17 <CPU> [symbol:model.layers.17] {
        (%1077:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %354:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %355:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1118:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1091:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1093:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.17.input_layernorm">(%1077:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1078:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.17.self_attn (%1078:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %354:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %355:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1110:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1091:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1093:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.AddOp(%1110:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1077:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1111:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.17.post_attention_layernorm">(%1111:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1112:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.17.mlp (%1112:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1117:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp(%1117:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1111:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1118:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1118:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1091:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1093:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.17.self_attn <CPU> [symbol:model.layers.17.self_attn] {
        (%1078:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %354:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %355:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1110:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1091:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1093:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.17.self_attn.q_proj">(%1078:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1079:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.17.self_attn.k_proj">(%1078:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1080:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.17.self_attn.v_proj">(%1078:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1081:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%1079:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1079:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1079:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1082:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%1080:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1080:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1080:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1083:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%1081:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1081:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1081:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1084:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.17.self_attn.q_norm">(%1082:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1085:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.17.self_attn.k_norm">(%1083:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1086:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.17.self_attn.q_rope">(%1085:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1087:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.17.self_attn.k_rope">(%1086:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1088:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp(%1088:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1089:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp(%1089:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1090:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1090:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1091:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.CastTypeOp(%1084:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1092:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp(%1092:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1093:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp(%354:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %1091:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%1094:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp(%355:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %1093:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1095:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp(%1094:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%1096:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp(%1095:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%1097:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp(%1087:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %1096:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%1098:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp(%1098:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1099:tensor<[1], Int16PerTensor, CPU>[constant:[-0.44726562]]) -> (%1100:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp(%1100:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1101:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp(%1101:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %1102:tensor<[1], Int16PerTensor, CPU>[constant:[-0.8671863]]) -> (%1103:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp(%319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %1104:tensor<[1], UInt16, CPU>[constant:[-0.99609375]]) -> (%1105:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp(%1105:tensor<[1, 1, 32, 1024], UInt8, CPU>, %1100:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1103:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%1106:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp(%1106:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1107:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp(%1107:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1097:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%1108:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1108:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1109:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%1109:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1109:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.17.self_attn.o_proj">(%1109:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1110:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1110:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1091:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1093:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.17.mlp <CPU> [symbol:model.layers.17.mlp] {
        (%1112:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1117:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.17.mlp.gate_proj">(%1112:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1113:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.17.mlp.act">(%1113:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1114:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.17.mlp.up_proj">(%1112:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1115:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp(%1114:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %1115:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1116:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.17.mlp.down_proj">(%1116:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1117:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1117:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.18 <CPU> [symbol:model.layers.18] {
        (%1118:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %356:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %357:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1159:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1132:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1134:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.18.input_layernorm">(%1118:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1119:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.18.self_attn (%1119:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %356:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %357:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1151:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1132:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1134:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.AddOp(%1151:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1118:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1152:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.18.post_attention_layernorm">(%1152:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1153:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.18.mlp (%1153:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1158:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp(%1158:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1152:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1159:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1159:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1132:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1134:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.18.self_attn <CPU> [symbol:model.layers.18.self_attn] {
        (%1119:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %356:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %357:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1151:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1132:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1134:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.18.self_attn.q_proj">(%1119:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1120:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.18.self_attn.k_proj">(%1119:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1121:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.18.self_attn.v_proj">(%1119:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1122:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%1120:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1120:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1120:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1123:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%1121:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1121:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1121:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1124:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%1122:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1122:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1122:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1125:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.18.self_attn.q_norm">(%1123:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1126:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.18.self_attn.k_norm">(%1124:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1127:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.18.self_attn.q_rope">(%1126:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1128:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.18.self_attn.k_rope">(%1127:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1129:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp(%1129:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1130:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp(%1130:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1131:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1131:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1132:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.CastTypeOp(%1125:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1133:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp(%1133:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1134:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp(%356:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %1132:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%1135:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp(%357:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %1134:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1136:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp(%1135:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%1137:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp(%1136:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%1138:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp(%1128:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %1137:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%1139:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp(%1139:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1140:tensor<[1], Int16PerTensor, CPU>[constant:[-0.796875]]) -> (%1141:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp(%1141:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1142:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp(%1142:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %1143:tensor<[1], Int16PerTensor, CPU>[constant:[-0.3359369]]) -> (%1144:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp(%319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %1145:tensor<[1], UInt16, CPU>[constant:[0.24023438]]) -> (%1146:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp(%1146:tensor<[1, 1, 32, 1024], UInt8, CPU>, %1141:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1144:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%1147:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp(%1147:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1148:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp(%1148:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1138:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%1149:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1149:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1150:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%1150:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1150:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.18.self_attn.o_proj">(%1150:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1151:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1151:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1132:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1134:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.18.mlp <CPU> [symbol:model.layers.18.mlp] {
        (%1153:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1158:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.18.mlp.gate_proj">(%1153:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1154:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.18.mlp.act">(%1154:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1155:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.18.mlp.up_proj">(%1153:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1156:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp(%1155:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %1156:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1157:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.18.mlp.down_proj">(%1157:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1158:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1158:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.19 <CPU> [symbol:model.layers.19] {
        (%1159:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %358:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %359:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1200:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1173:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1175:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.19.input_layernorm">(%1159:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1160:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.19.self_attn (%1160:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %358:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %359:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1192:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1173:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1175:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.AddOp(%1192:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1159:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1193:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.19.post_attention_layernorm">(%1193:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1194:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.19.mlp (%1194:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1199:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp(%1199:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1193:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1200:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1200:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1173:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1175:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.19.self_attn <CPU> [symbol:model.layers.19.self_attn] {
        (%1160:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %358:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %359:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1192:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1173:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1175:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.19.self_attn.q_proj">(%1160:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1161:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.19.self_attn.k_proj">(%1160:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1162:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.19.self_attn.v_proj">(%1160:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1163:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%1161:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1161:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1161:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1164:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%1162:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1162:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1162:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1165:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%1163:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1163:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1163:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1166:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.19.self_attn.q_norm">(%1164:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1167:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.19.self_attn.k_norm">(%1165:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1168:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.19.self_attn.q_rope">(%1167:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1169:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.19.self_attn.k_rope">(%1168:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1170:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp(%1170:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1171:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp(%1171:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1172:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1172:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1173:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.CastTypeOp(%1166:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1174:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp(%1174:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1175:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp(%358:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %1173:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%1176:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp(%359:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %1175:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1177:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp(%1176:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%1178:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp(%1177:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%1179:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp(%1169:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %1178:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%1180:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp(%1180:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1181:tensor<[1], Int16PerTensor, CPU>[constant:[0.734375]]) -> (%1182:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp(%1182:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1183:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp(%1183:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %1184:tensor<[1], Int16PerTensor, CPU>[constant:[9.1807e-41]]) -> (%1185:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp(%319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %1186:tensor<[1], UInt16, CPU>[constant:[0.55078125]]) -> (%1187:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp(%1187:tensor<[1, 1, 32, 1024], UInt8, CPU>, %1182:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1185:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%1188:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp(%1188:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1189:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp(%1189:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1179:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%1190:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1190:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1191:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%1191:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1191:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.19.self_attn.o_proj">(%1191:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1192:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1192:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1173:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1175:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.19.mlp <CPU> [symbol:model.layers.19.mlp] {
        (%1194:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1199:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.19.mlp.gate_proj">(%1194:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1195:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.19.mlp.act">(%1195:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1196:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.19.mlp.up_proj">(%1194:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1197:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp(%1196:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %1197:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1198:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.19.mlp.down_proj">(%1198:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1199:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1199:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.20 <CPU> [symbol:model.layers.20] {
        (%1200:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %360:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %361:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1241:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1214:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1216:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.20.input_layernorm">(%1200:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1201:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.20.self_attn (%1201:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %360:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %361:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1233:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1214:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1216:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.AddOp(%1233:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1200:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1234:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.20.post_attention_layernorm">(%1234:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1235:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.20.mlp (%1235:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1240:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp(%1240:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1234:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1241:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1241:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1214:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1216:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.20.self_attn <CPU> [symbol:model.layers.20.self_attn] {
        (%1201:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %360:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %361:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1233:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1214:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1216:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.20.self_attn.q_proj">(%1201:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1202:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.20.self_attn.k_proj">(%1201:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1203:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.20.self_attn.v_proj">(%1201:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1204:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%1202:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1202:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1202:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1205:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%1203:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1203:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1203:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1206:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%1204:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1204:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1204:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1207:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.20.self_attn.q_norm">(%1205:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1208:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.20.self_attn.k_norm">(%1206:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1209:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.20.self_attn.q_rope">(%1208:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1210:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.20.self_attn.k_rope">(%1209:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1211:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp(%1211:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1212:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp(%1212:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1213:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1213:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1214:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.CastTypeOp(%1207:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1215:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp(%1215:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1216:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp(%360:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %1214:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%1217:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp(%361:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %1216:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1218:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp(%1217:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%1219:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp(%1218:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%1220:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp(%1210:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %1219:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%1221:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp(%1221:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1222:tensor<[1], Int16PerTensor, CPU>[constant:[0.91796875]]) -> (%1223:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp(%1223:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1224:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp(%1224:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %1225:tensor<[1], Int16PerTensor, CPU>[constant:[0.9843738]]) -> (%1226:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp(%319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %1227:tensor<[1], UInt16, CPU>[constant:[0.71875]]) -> (%1228:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp(%1228:tensor<[1, 1, 32, 1024], UInt8, CPU>, %1223:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1226:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%1229:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp(%1229:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1230:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp(%1230:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1220:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%1231:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1231:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1232:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%1232:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1232:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.20.self_attn.o_proj">(%1232:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1233:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1233:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1214:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1216:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.20.mlp <CPU> [symbol:model.layers.20.mlp] {
        (%1235:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1240:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.20.mlp.gate_proj">(%1235:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1236:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.20.mlp.act">(%1236:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1237:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.20.mlp.up_proj">(%1235:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1238:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp(%1237:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %1238:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1239:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.20.mlp.down_proj">(%1239:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1240:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1240:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.21 <CPU> [symbol:model.layers.21] {
        (%1241:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %362:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %363:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1282:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1255:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1257:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.21.input_layernorm">(%1241:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1242:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.21.self_attn (%1242:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %362:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %363:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1274:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1255:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1257:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.AddOp(%1274:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1241:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1275:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.21.post_attention_layernorm">(%1275:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1276:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.21.mlp (%1276:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1281:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp(%1281:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1275:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1282:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1282:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1255:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1257:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.21.self_attn <CPU> [symbol:model.layers.21.self_attn] {
        (%1242:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %362:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %363:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1274:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1255:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1257:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.21.self_attn.q_proj">(%1242:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1243:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.21.self_attn.k_proj">(%1242:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1244:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.21.self_attn.v_proj">(%1242:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1245:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%1243:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1243:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1243:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1246:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%1244:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1244:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1244:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1247:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%1245:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1245:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1245:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1248:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.21.self_attn.q_norm">(%1246:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1249:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.21.self_attn.k_norm">(%1247:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1250:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.21.self_attn.q_rope">(%1249:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1251:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.21.self_attn.k_rope">(%1250:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1252:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp(%1252:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1253:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp(%1253:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1254:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1254:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1255:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.CastTypeOp(%1248:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1256:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp(%1256:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1257:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp(%362:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %1255:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%1258:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp(%363:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %1257:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1259:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp(%1258:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%1260:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp(%1259:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%1261:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp(%1251:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %1260:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%1262:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp(%1262:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1263:tensor<[1], Int16PerTensor, CPU>[constant:[0.21875]]) -> (%1264:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp(%1264:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1265:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp(%1265:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %1266:tensor<[1], Int16PerTensor, CPU>[constant:[-0.35546815]]) -> (%1267:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp(%319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %1268:tensor<[1], UInt16, CPU>[constant:[-0.80859375]]) -> (%1269:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp(%1269:tensor<[1, 1, 32, 1024], UInt8, CPU>, %1264:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1267:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%1270:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp(%1270:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1271:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp(%1271:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1261:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%1272:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1272:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1273:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%1273:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1273:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.21.self_attn.o_proj">(%1273:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1274:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1274:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1255:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1257:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.21.mlp <CPU> [symbol:model.layers.21.mlp] {
        (%1276:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1281:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.21.mlp.gate_proj">(%1276:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1277:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.21.mlp.act">(%1277:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1278:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.21.mlp.up_proj">(%1276:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1279:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp(%1278:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %1279:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1280:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.21.mlp.down_proj">(%1280:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1281:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1281:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.22 <CPU> [symbol:model.layers.22] {
        (%1282:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %364:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %365:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1323:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1296:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1298:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.22.input_layernorm">(%1282:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1283:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.22.self_attn (%1283:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %364:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %365:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1315:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1296:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1298:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.AddOp(%1315:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1282:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1316:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.22.post_attention_layernorm">(%1316:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1317:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.22.mlp (%1317:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1322:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp(%1322:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1316:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1323:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1323:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1296:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1298:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.22.self_attn <CPU> [symbol:model.layers.22.self_attn] {
        (%1283:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %364:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %365:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1315:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1296:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1298:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.22.self_attn.q_proj">(%1283:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1284:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.22.self_attn.k_proj">(%1283:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1285:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.22.self_attn.v_proj">(%1283:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1286:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%1284:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1284:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1284:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1287:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%1285:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1285:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1285:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1288:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%1286:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1286:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1286:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1289:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.22.self_attn.q_norm">(%1287:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1290:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.22.self_attn.k_norm">(%1288:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1291:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.22.self_attn.q_rope">(%1290:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1292:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.22.self_attn.k_rope">(%1291:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1293:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp(%1293:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1294:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp(%1294:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1295:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1295:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1296:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.CastTypeOp(%1289:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1297:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp(%1297:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1298:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp(%364:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %1296:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%1299:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp(%365:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %1298:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1300:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp(%1299:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%1301:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp(%1300:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%1302:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp(%1292:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %1301:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%1303:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp(%1303:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1304:tensor<[1], Int16PerTensor, CPU>[constant:[-0.99609375]]) -> (%1305:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp(%1305:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1306:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp(%1306:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %1307:tensor<[1], Int16PerTensor, CPU>[constant:[-0.8593738]]) -> (%1308:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp(%319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %1309:tensor<[1], UInt16, CPU>[constant:[-0.42773438]]) -> (%1310:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp(%1310:tensor<[1, 1, 32, 1024], UInt8, CPU>, %1305:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1308:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%1311:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp(%1311:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1312:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp(%1312:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1302:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%1313:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1313:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1314:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%1314:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1314:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.22.self_attn.o_proj">(%1314:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1315:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1315:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1296:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1298:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.22.mlp <CPU> [symbol:model.layers.22.mlp] {
        (%1317:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1322:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.22.mlp.gate_proj">(%1317:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1318:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.22.mlp.act">(%1318:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1319:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.22.mlp.up_proj">(%1317:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1320:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp(%1319:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %1320:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1321:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.22.mlp.down_proj">(%1321:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1322:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1322:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.23 <CPU> [symbol:model.layers.23] {
        (%1323:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %366:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %367:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1364:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1337:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1339:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.23.input_layernorm">(%1323:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1324:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.23.self_attn (%1324:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %366:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %367:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1356:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1337:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1339:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.AddOp(%1356:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1323:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1357:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.23.post_attention_layernorm">(%1357:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1358:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.23.mlp (%1358:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1363:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp(%1363:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1357:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1364:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1364:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1337:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1339:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.23.self_attn <CPU> [symbol:model.layers.23.self_attn] {
        (%1324:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %366:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %367:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1356:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1337:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1339:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.23.self_attn.q_proj">(%1324:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1325:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.23.self_attn.k_proj">(%1324:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1326:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.23.self_attn.v_proj">(%1324:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1327:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%1325:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1325:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1325:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1328:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%1326:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1326:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1326:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1329:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%1327:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1327:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1327:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1330:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.23.self_attn.q_norm">(%1328:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1331:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.23.self_attn.k_norm">(%1329:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1332:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.23.self_attn.q_rope">(%1331:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1333:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.23.self_attn.k_rope">(%1332:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1334:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp(%1334:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1335:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp(%1335:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1336:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1336:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1337:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.CastTypeOp(%1330:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1338:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp(%1338:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1339:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp(%366:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %1337:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%1340:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp(%367:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %1339:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1341:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp(%1340:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%1342:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp(%1341:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%1343:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp(%1333:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %1342:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%1344:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp(%1344:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1345:tensor<[1], Int16PerTensor, CPU>[constant:[0.140625]]) -> (%1346:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp(%1346:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1347:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp(%1347:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %1348:tensor<[1], Int16PerTensor, CPU>[constant:[0.6640613]]) -> (%1349:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp(%319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %1350:tensor<[1], UInt16, CPU>[constant:[0.96484375]]) -> (%1351:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp(%1351:tensor<[1, 1, 32, 1024], UInt8, CPU>, %1346:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1349:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%1352:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp(%1352:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1353:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp(%1353:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1343:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%1354:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1354:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1355:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%1355:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1355:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.23.self_attn.o_proj">(%1355:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1356:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1356:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1337:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1339:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.23.mlp <CPU> [symbol:model.layers.23.mlp] {
        (%1358:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1363:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.23.mlp.gate_proj">(%1358:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1359:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.23.mlp.act">(%1359:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1360:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.23.mlp.up_proj">(%1358:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1361:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp(%1360:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %1361:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1362:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.23.mlp.down_proj">(%1362:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1363:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1363:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.24 <CPU> [symbol:model.layers.24] {
        (%1364:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %368:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %369:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1405:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1378:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1380:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.24.input_layernorm">(%1364:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1365:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.24.self_attn (%1365:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %368:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %369:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1397:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1378:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1380:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.AddOp(%1397:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1364:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1398:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.24.post_attention_layernorm">(%1398:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1399:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.24.mlp (%1399:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1404:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp(%1404:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1398:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1405:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1405:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1378:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1380:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.24.self_attn <CPU> [symbol:model.layers.24.self_attn] {
        (%1365:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %368:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %369:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1397:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1378:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1380:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.24.self_attn.q_proj">(%1365:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1366:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.24.self_attn.k_proj">(%1365:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1367:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.24.self_attn.v_proj">(%1365:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1368:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%1366:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1366:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1366:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1369:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%1367:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1367:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1367:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1370:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%1368:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1368:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1368:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1371:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.24.self_attn.q_norm">(%1369:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1372:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.24.self_attn.k_norm">(%1370:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1373:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.24.self_attn.q_rope">(%1372:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1374:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.24.self_attn.k_rope">(%1373:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1375:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp(%1375:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1376:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp(%1376:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1377:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1377:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1378:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.CastTypeOp(%1371:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1379:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp(%1379:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1380:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp(%368:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %1378:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%1381:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp(%369:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %1380:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1382:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp(%1381:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%1383:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp(%1382:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%1384:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp(%1374:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %1383:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%1385:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp(%1385:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1386:tensor<[1], Int16PerTensor, CPU>[constant:[0.9453125]]) -> (%1387:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp(%1387:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1388:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp(%1388:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %1389:tensor<[1], Int16PerTensor, CPU>[constant:[0.6171863]]) -> (%1390:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp(%319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %1391:tensor<[1], UInt16, CPU>[constant:[0.07910156]]) -> (%1392:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp(%1392:tensor<[1, 1, 32, 1024], UInt8, CPU>, %1387:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1390:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%1393:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp(%1393:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1394:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp(%1394:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1384:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%1395:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1395:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1396:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%1396:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1396:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.24.self_attn.o_proj">(%1396:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1397:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1397:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1378:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1380:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.24.mlp <CPU> [symbol:model.layers.24.mlp] {
        (%1399:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1404:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.24.mlp.gate_proj">(%1399:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1400:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.24.mlp.act">(%1400:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1401:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.24.mlp.up_proj">(%1399:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1402:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp(%1401:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %1402:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1403:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.24.mlp.down_proj">(%1403:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1404:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1404:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.25 <CPU> [symbol:model.layers.25] {
        (%1405:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %370:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %371:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1446:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1419:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1421:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.25.input_layernorm">(%1405:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1406:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.25.self_attn (%1406:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %370:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %371:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1438:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1419:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1421:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.AddOp(%1438:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1405:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1439:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.25.post_attention_layernorm">(%1439:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1440:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.25.mlp (%1440:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1445:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp(%1445:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1439:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1446:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1446:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1419:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1421:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.25.self_attn <CPU> [symbol:model.layers.25.self_attn] {
        (%1406:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %370:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %371:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1438:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1419:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1421:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.25.self_attn.q_proj">(%1406:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1407:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.25.self_attn.k_proj">(%1406:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1408:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.25.self_attn.v_proj">(%1406:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1409:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%1407:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1407:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1407:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1410:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%1408:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1408:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1408:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1411:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%1409:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1409:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1409:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1412:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.25.self_attn.q_norm">(%1410:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1413:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.25.self_attn.k_norm">(%1411:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1414:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.25.self_attn.q_rope">(%1413:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1415:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.25.self_attn.k_rope">(%1414:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1416:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp(%1416:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1417:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp(%1417:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1418:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1418:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1419:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.CastTypeOp(%1412:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1420:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp(%1420:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1421:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp(%370:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %1419:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%1422:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp(%371:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %1421:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1423:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp(%1422:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%1424:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp(%1423:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%1425:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp(%1415:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %1424:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%1426:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp(%1426:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1427:tensor<[1], Int16PerTensor, CPU>[constant:[-0.48242188]]) -> (%1428:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp(%1428:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1429:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp(%1429:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %1430:tensor<[1], Int16PerTensor, CPU>[constant:[-0.88671756]]) -> (%1431:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp(%319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %1432:tensor<[1], UInt16, CPU>[constant:[-0.9921875]]) -> (%1433:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp(%1433:tensor<[1, 1, 32, 1024], UInt8, CPU>, %1428:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1431:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%1434:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp(%1434:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1435:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp(%1435:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1425:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%1436:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1436:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1437:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%1437:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1437:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.25.self_attn.o_proj">(%1437:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1438:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1438:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1419:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1421:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.25.mlp <CPU> [symbol:model.layers.25.mlp] {
        (%1440:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1445:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.25.mlp.gate_proj">(%1440:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1441:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.25.mlp.act">(%1441:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1442:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.25.mlp.up_proj">(%1440:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1443:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp(%1442:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %1443:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1444:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.25.mlp.down_proj">(%1444:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1445:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1445:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.26 <CPU> [symbol:model.layers.26] {
        (%1446:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %372:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %373:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1487:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1460:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1462:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.26.input_layernorm">(%1446:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1447:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.26.self_attn (%1447:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %372:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %373:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1479:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1460:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1462:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.AddOp(%1479:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1446:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1480:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.26.post_attention_layernorm">(%1480:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1481:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.26.mlp (%1481:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1486:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp(%1486:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1480:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1487:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1487:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1460:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1462:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.26.self_attn <CPU> [symbol:model.layers.26.self_attn] {
        (%1447:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %372:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %373:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1479:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1460:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1462:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.26.self_attn.q_proj">(%1447:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1448:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.26.self_attn.k_proj">(%1447:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1449:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.26.self_attn.v_proj">(%1447:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1450:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%1448:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1448:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1448:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1451:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%1449:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1449:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1449:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1452:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%1450:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1450:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1450:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1453:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.26.self_attn.q_norm">(%1451:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1454:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.26.self_attn.k_norm">(%1452:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1455:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.26.self_attn.q_rope">(%1454:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1456:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.26.self_attn.k_rope">(%1455:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1457:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp(%1457:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1458:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp(%1458:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1459:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1459:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1460:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.CastTypeOp(%1453:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1461:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp(%1461:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1462:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp(%372:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %1460:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%1463:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp(%373:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %1462:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1464:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp(%1463:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%1465:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp(%1464:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%1466:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp(%1456:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %1465:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%1467:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp(%1467:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1468:tensor<[1], Int16PerTensor, CPU>[constant:[-0.7734375]]) -> (%1469:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp(%1469:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1470:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp(%1470:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %1471:tensor<[1], Int16PerTensor, CPU>[constant:[-0.2968744]]) -> (%1472:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp(%319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %1473:tensor<[1], UInt16, CPU>[constant:[0.27929688]]) -> (%1474:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp(%1474:tensor<[1, 1, 32, 1024], UInt8, CPU>, %1469:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1472:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%1475:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp(%1475:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1476:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp(%1476:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1466:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%1477:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1477:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1478:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%1478:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1478:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.26.self_attn.o_proj">(%1478:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1479:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1479:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1460:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1462:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.26.mlp <CPU> [symbol:model.layers.26.mlp] {
        (%1481:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1486:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.26.mlp.gate_proj">(%1481:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1482:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.26.mlp.act">(%1482:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1483:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.26.mlp.up_proj">(%1481:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1484:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp(%1483:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %1484:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1485:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.26.mlp.down_proj">(%1485:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1486:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1486:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.27 <CPU> [symbol:model.layers.27] {
        (%1487:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %374:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %375:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1528:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1501:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1503:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.27.input_layernorm">(%1487:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1488:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.27.self_attn (%1488:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %374:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %375:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1520:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1501:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1503:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.AddOp(%1520:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1487:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1521:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.27.post_attention_layernorm">(%1521:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1522:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.27.mlp (%1522:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1527:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp(%1527:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1521:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1528:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1528:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1501:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1503:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.27.self_attn <CPU> [symbol:model.layers.27.self_attn] {
        (%1488:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %374:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %375:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1520:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1501:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1503:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.27.self_attn.q_proj">(%1488:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1489:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.27.self_attn.k_proj">(%1488:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1490:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.27.self_attn.v_proj">(%1488:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1491:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%1489:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1489:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1489:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1492:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%1490:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1490:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1490:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1493:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%1491:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1491:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1491:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1494:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.27.self_attn.q_norm">(%1492:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1495:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.27.self_attn.k_norm">(%1493:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1496:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.27.self_attn.q_rope">(%1495:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1497:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.27.self_attn.k_rope">(%1496:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1498:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp(%1498:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1499:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp(%1499:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1500:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1500:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1501:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.CastTypeOp(%1494:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1502:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp(%1502:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1503:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp(%374:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %1501:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%1504:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp(%375:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %1503:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1505:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp(%1504:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%1506:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp(%1505:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%1507:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp(%1497:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %1506:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%1508:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp(%1508:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1509:tensor<[1], Int16PerTensor, CPU>[constant:[0.76171875]]) -> (%1510:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp(%1510:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1511:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp(%1511:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %1512:tensor<[1], Int16PerTensor, CPU>[constant:[0.99609256]]) -> (%1513:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp(%319:tensor<[1, 1, 32, 1024], UInt16, CPU>, %1514:tensor<[1], UInt16, CPU>[constant:[0.890625]]) -> (%1515:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp(%1515:tensor<[1, 1, 32, 1024], UInt8, CPU>, %1510:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1513:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%1516:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp(%1516:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1517:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp(%1517:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1507:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%1518:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp(%1518:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1519:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp(%1519:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1519:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.27.self_attn.o_proj">(%1519:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1520:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1520:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1501:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1503:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.27.mlp <CPU> [symbol:model.layers.27.mlp] {
        (%1522:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1527:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.27.mlp.gate_proj">(%1522:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1523:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.27.mlp.act">(%1523:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1524:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.27.mlp.up_proj">(%1522:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1525:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp(%1524:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %1525:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1526:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.27.mlp.down_proj">(%1526:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1527:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1527:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    linalg.CPU.LinearOp <name="lm_head">(%1529:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1530:tensor<[1, 32, 151936], Int16PerTensor, CPU>)
    //        
    //      o o    
    //            
    //       
    //             
    //        
}
 
