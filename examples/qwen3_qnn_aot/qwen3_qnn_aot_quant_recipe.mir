@main () -> () {
    graph.SubGraphOp @init <notype> [symbol:init] {
        () -> () {
            tensor.CPU.register () -> (%105:tensor<[151936, 2048], Float32, CPU>[@model.embed_tokens.weight][symbol:model.embed_tokens.weight])[symbol:model.embed_tokens.weight]
            tensor.CPU.register () -> (%76:tensor<[2048, 2048], Float32, CPU>[@model.layers.0.self_attn.q_proj.weight][symbol:model.layers.0.self_attn.q_proj.weight])[symbol:model.layers.0.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%133:tensor<[1024, 2048], Float32, CPU>[@model.layers.0.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=66), symbol:model.layers.0.self_attn.k_proj.weight])[symbol:model.layers.0.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%179:tensor<[1024, 2048], Float32, CPU>[@model.layers.0.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=68), symbol:model.layers.0.self_attn.v_proj.weight])[symbol:model.layers.0.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%269:tensor<[2048, 2048], Float32, CPU>[@model.layers.0.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=85), symbol:model.layers.0.self_attn.o_proj.weight])[symbol:model.layers.0.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%9:tensor<[6144, 2048], Float32, CPU>[@model.layers.0.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=88), symbol:model.layers.0.mlp.gate_proj.weight])[symbol:model.layers.0.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%111:tensor<[6144, 2048], Float32, CPU>[@model.layers.0.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=91), symbol:model.layers.0.mlp.up_proj.weight])[symbol:model.layers.0.mlp.up_proj.weight]
            tensor.CPU.register () -> (%184:tensor<[2048, 6144], Float32, CPU>[@model.layers.0.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=93), symbol:model.layers.0.mlp.down_proj.weight])[symbol:model.layers.0.mlp.down_proj.weight]
            tensor.CPU.register () -> (%285:tensor<[2048, 2048], Float32, CPU>[@model.layers.1.self_attn.q_proj.weight][symbol:model.layers.1.self_attn.q_proj.weight])[symbol:model.layers.1.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%32:tensor<[1024, 2048], Float32, CPU>[@model.layers.1.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=96), symbol:model.layers.1.self_attn.k_proj.weight])[symbol:model.layers.1.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%154:tensor<[1024, 2048], Float32, CPU>[@model.layers.1.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=98), symbol:model.layers.1.self_attn.v_proj.weight])[symbol:model.layers.1.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%20:tensor<[2048, 2048], Float32, CPU>[@model.layers.1.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=115), symbol:model.layers.1.self_attn.o_proj.weight])[symbol:model.layers.1.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%245:tensor<[6144, 2048], Float32, CPU>[@model.layers.1.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=118), symbol:model.layers.1.mlp.gate_proj.weight])[symbol:model.layers.1.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%230:tensor<[6144, 2048], Float32, CPU>[@model.layers.1.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=121), symbol:model.layers.1.mlp.up_proj.weight])[symbol:model.layers.1.mlp.up_proj.weight]
            tensor.CPU.register () -> (%43:tensor<[2048, 6144], Float32, CPU>[@model.layers.1.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=123), symbol:model.layers.1.mlp.down_proj.weight])[symbol:model.layers.1.mlp.down_proj.weight]
            tensor.CPU.register () -> (%221:tensor<[2048, 2048], Float32, CPU>[@model.layers.2.self_attn.q_proj.weight][symbol:model.layers.2.self_attn.q_proj.weight])[symbol:model.layers.2.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%103:tensor<[1024, 2048], Float32, CPU>[@model.layers.2.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=126), symbol:model.layers.2.self_attn.k_proj.weight])[symbol:model.layers.2.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%47:tensor<[1024, 2048], Float32, CPU>[@model.layers.2.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=128), symbol:model.layers.2.self_attn.v_proj.weight])[symbol:model.layers.2.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%85:tensor<[2048, 2048], Float32, CPU>[@model.layers.2.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=145), symbol:model.layers.2.self_attn.o_proj.weight])[symbol:model.layers.2.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%252:tensor<[6144, 2048], Float32, CPU>[@model.layers.2.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=148), symbol:model.layers.2.mlp.gate_proj.weight])[symbol:model.layers.2.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%24:tensor<[6144, 2048], Float32, CPU>[@model.layers.2.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=151), symbol:model.layers.2.mlp.up_proj.weight])[symbol:model.layers.2.mlp.up_proj.weight]
            tensor.CPU.register () -> (%28:tensor<[2048, 6144], Float32, CPU>[@model.layers.2.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=153), symbol:model.layers.2.mlp.down_proj.weight])[symbol:model.layers.2.mlp.down_proj.weight]
            tensor.CPU.register () -> (%283:tensor<[2048, 2048], Float32, CPU>[@model.layers.3.self_attn.q_proj.weight][symbol:model.layers.3.self_attn.q_proj.weight])[symbol:model.layers.3.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%48:tensor<[1024, 2048], Float32, CPU>[@model.layers.3.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=156), symbol:model.layers.3.self_attn.k_proj.weight])[symbol:model.layers.3.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%244:tensor<[1024, 2048], Float32, CPU>[@model.layers.3.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=158), symbol:model.layers.3.self_attn.v_proj.weight])[symbol:model.layers.3.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%301:tensor<[2048, 2048], Float32, CPU>[@model.layers.3.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=175), symbol:model.layers.3.self_attn.o_proj.weight])[symbol:model.layers.3.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%129:tensor<[6144, 2048], Float32, CPU>[@model.layers.3.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=178), symbol:model.layers.3.mlp.gate_proj.weight])[symbol:model.layers.3.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%188:tensor<[6144, 2048], Float32, CPU>[@model.layers.3.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=181), symbol:model.layers.3.mlp.up_proj.weight])[symbol:model.layers.3.mlp.up_proj.weight]
            tensor.CPU.register () -> (%97:tensor<[2048, 6144], Float32, CPU>[@model.layers.3.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=183), symbol:model.layers.3.mlp.down_proj.weight])[symbol:model.layers.3.mlp.down_proj.weight]
            tensor.CPU.register () -> (%164:tensor<[2048, 2048], Float32, CPU>[@model.layers.4.self_attn.q_proj.weight][symbol:model.layers.4.self_attn.q_proj.weight])[symbol:model.layers.4.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%148:tensor<[1024, 2048], Float32, CPU>[@model.layers.4.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=186), symbol:model.layers.4.self_attn.k_proj.weight])[symbol:model.layers.4.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%279:tensor<[1024, 2048], Float32, CPU>[@model.layers.4.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=188), symbol:model.layers.4.self_attn.v_proj.weight])[symbol:model.layers.4.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%91:tensor<[2048, 2048], Float32, CPU>[@model.layers.4.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=205), symbol:model.layers.4.self_attn.o_proj.weight])[symbol:model.layers.4.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%189:tensor<[6144, 2048], Float32, CPU>[@model.layers.4.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=208), symbol:model.layers.4.mlp.gate_proj.weight])[symbol:model.layers.4.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%156:tensor<[6144, 2048], Float32, CPU>[@model.layers.4.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=211), symbol:model.layers.4.mlp.up_proj.weight])[symbol:model.layers.4.mlp.up_proj.weight]
            tensor.CPU.register () -> (%153:tensor<[2048, 6144], Float32, CPU>[@model.layers.4.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=213), symbol:model.layers.4.mlp.down_proj.weight])[symbol:model.layers.4.mlp.down_proj.weight]
            tensor.CPU.register () -> (%78:tensor<[2048, 2048], Float32, CPU>[@model.layers.5.self_attn.q_proj.weight][symbol:model.layers.5.self_attn.q_proj.weight])[symbol:model.layers.5.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%72:tensor<[1024, 2048], Float32, CPU>[@model.layers.5.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=216), symbol:model.layers.5.self_attn.k_proj.weight])[symbol:model.layers.5.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%289:tensor<[1024, 2048], Float32, CPU>[@model.layers.5.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=218), symbol:model.layers.5.self_attn.v_proj.weight])[symbol:model.layers.5.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%264:tensor<[2048, 2048], Float32, CPU>[@model.layers.5.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=235), symbol:model.layers.5.self_attn.o_proj.weight])[symbol:model.layers.5.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%4:tensor<[6144, 2048], Float32, CPU>[@model.layers.5.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=238), symbol:model.layers.5.mlp.gate_proj.weight])[symbol:model.layers.5.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%308:tensor<[6144, 2048], Float32, CPU>[@model.layers.5.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=241), symbol:model.layers.5.mlp.up_proj.weight])[symbol:model.layers.5.mlp.up_proj.weight]
            tensor.CPU.register () -> (%74:tensor<[2048, 6144], Float32, CPU>[@model.layers.5.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=243), symbol:model.layers.5.mlp.down_proj.weight])[symbol:model.layers.5.mlp.down_proj.weight]
            tensor.CPU.register () -> (%59:tensor<[2048, 2048], Float32, CPU>[@model.layers.6.self_attn.q_proj.weight][symbol:model.layers.6.self_attn.q_proj.weight])[symbol:model.layers.6.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%208:tensor<[1024, 2048], Float32, CPU>[@model.layers.6.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=246), symbol:model.layers.6.self_attn.k_proj.weight])[symbol:model.layers.6.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%238:tensor<[1024, 2048], Float32, CPU>[@model.layers.6.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=248), symbol:model.layers.6.self_attn.v_proj.weight])[symbol:model.layers.6.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%52:tensor<[2048, 2048], Float32, CPU>[@model.layers.6.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=265), symbol:model.layers.6.self_attn.o_proj.weight])[symbol:model.layers.6.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%80:tensor<[6144, 2048], Float32, CPU>[@model.layers.6.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=268), symbol:model.layers.6.mlp.gate_proj.weight])[symbol:model.layers.6.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%276:tensor<[6144, 2048], Float32, CPU>[@model.layers.6.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=271), symbol:model.layers.6.mlp.up_proj.weight])[symbol:model.layers.6.mlp.up_proj.weight]
            tensor.CPU.register () -> (%227:tensor<[2048, 6144], Float32, CPU>[@model.layers.6.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=273), symbol:model.layers.6.mlp.down_proj.weight])[symbol:model.layers.6.mlp.down_proj.weight]
            tensor.CPU.register () -> (%287:tensor<[2048, 2048], Float32, CPU>[@model.layers.7.self_attn.q_proj.weight][symbol:model.layers.7.self_attn.q_proj.weight])[symbol:model.layers.7.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%135:tensor<[1024, 2048], Float32, CPU>[@model.layers.7.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=276), symbol:model.layers.7.self_attn.k_proj.weight])[symbol:model.layers.7.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%300:tensor<[1024, 2048], Float32, CPU>[@model.layers.7.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=278), symbol:model.layers.7.self_attn.v_proj.weight])[symbol:model.layers.7.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%251:tensor<[2048, 2048], Float32, CPU>[@model.layers.7.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=295), symbol:model.layers.7.self_attn.o_proj.weight])[symbol:model.layers.7.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%155:tensor<[6144, 2048], Float32, CPU>[@model.layers.7.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=298), symbol:model.layers.7.mlp.gate_proj.weight])[symbol:model.layers.7.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%218:tensor<[6144, 2048], Float32, CPU>[@model.layers.7.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=301), symbol:model.layers.7.mlp.up_proj.weight])[symbol:model.layers.7.mlp.up_proj.weight]
            tensor.CPU.register () -> (%275:tensor<[2048, 6144], Float32, CPU>[@model.layers.7.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=303), symbol:model.layers.7.mlp.down_proj.weight])[symbol:model.layers.7.mlp.down_proj.weight]
            tensor.CPU.register () -> (%165:tensor<[2048, 2048], Float32, CPU>[@model.layers.8.self_attn.q_proj.weight][symbol:model.layers.8.self_attn.q_proj.weight])[symbol:model.layers.8.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%194:tensor<[1024, 2048], Float32, CPU>[@model.layers.8.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=306), symbol:model.layers.8.self_attn.k_proj.weight])[symbol:model.layers.8.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%181:tensor<[1024, 2048], Float32, CPU>[@model.layers.8.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=308), symbol:model.layers.8.self_attn.v_proj.weight])[symbol:model.layers.8.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%197:tensor<[2048, 2048], Float32, CPU>[@model.layers.8.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=325), symbol:model.layers.8.self_attn.o_proj.weight])[symbol:model.layers.8.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%110:tensor<[6144, 2048], Float32, CPU>[@model.layers.8.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=328), symbol:model.layers.8.mlp.gate_proj.weight])[symbol:model.layers.8.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%236:tensor<[6144, 2048], Float32, CPU>[@model.layers.8.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=331), symbol:model.layers.8.mlp.up_proj.weight])[symbol:model.layers.8.mlp.up_proj.weight]
            tensor.CPU.register () -> (%106:tensor<[2048, 6144], Float32, CPU>[@model.layers.8.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=333), symbol:model.layers.8.mlp.down_proj.weight])[symbol:model.layers.8.mlp.down_proj.weight]
            tensor.CPU.register () -> (%235:tensor<[2048, 2048], Float32, CPU>[@model.layers.9.self_attn.q_proj.weight][symbol:model.layers.9.self_attn.q_proj.weight])[symbol:model.layers.9.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%69:tensor<[1024, 2048], Float32, CPU>[@model.layers.9.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=336), symbol:model.layers.9.self_attn.k_proj.weight])[symbol:model.layers.9.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%120:tensor<[1024, 2048], Float32, CPU>[@model.layers.9.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=338), symbol:model.layers.9.self_attn.v_proj.weight])[symbol:model.layers.9.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%205:tensor<[2048, 2048], Float32, CPU>[@model.layers.9.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=355), symbol:model.layers.9.self_attn.o_proj.weight])[symbol:model.layers.9.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%263:tensor<[6144, 2048], Float32, CPU>[@model.layers.9.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=358), symbol:model.layers.9.mlp.gate_proj.weight])[symbol:model.layers.9.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%102:tensor<[6144, 2048], Float32, CPU>[@model.layers.9.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=361), symbol:model.layers.9.mlp.up_proj.weight])[symbol:model.layers.9.mlp.up_proj.weight]
            tensor.CPU.register () -> (%136:tensor<[2048, 6144], Float32, CPU>[@model.layers.9.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=363), symbol:model.layers.9.mlp.down_proj.weight])[symbol:model.layers.9.mlp.down_proj.weight]
            tensor.CPU.register () -> (%278:tensor<[2048, 2048], Float32, CPU>[@model.layers.10.self_attn.q_proj.weight][symbol:model.layers.10.self_attn.q_proj.weight])[symbol:model.layers.10.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%182:tensor<[1024, 2048], Float32, CPU>[@model.layers.10.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=366), symbol:model.layers.10.self_attn.k_proj.weight])[symbol:model.layers.10.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%138:tensor<[1024, 2048], Float32, CPU>[@model.layers.10.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=368), symbol:model.layers.10.self_attn.v_proj.weight])[symbol:model.layers.10.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%233:tensor<[2048, 2048], Float32, CPU>[@model.layers.10.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=385), symbol:model.layers.10.self_attn.o_proj.weight])[symbol:model.layers.10.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%124:tensor<[6144, 2048], Float32, CPU>[@model.layers.10.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=388), symbol:model.layers.10.mlp.gate_proj.weight])[symbol:model.layers.10.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%261:tensor<[6144, 2048], Float32, CPU>[@model.layers.10.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=391), symbol:model.layers.10.mlp.up_proj.weight])[symbol:model.layers.10.mlp.up_proj.weight]
            tensor.CPU.register () -> (%45:tensor<[2048, 6144], Float32, CPU>[@model.layers.10.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=393), symbol:model.layers.10.mlp.down_proj.weight])[symbol:model.layers.10.mlp.down_proj.weight]
            tensor.CPU.register () -> (%274:tensor<[2048, 2048], Float32, CPU>[@model.layers.11.self_attn.q_proj.weight][symbol:model.layers.11.self_attn.q_proj.weight])[symbol:model.layers.11.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%157:tensor<[1024, 2048], Float32, CPU>[@model.layers.11.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=396), symbol:model.layers.11.self_attn.k_proj.weight])[symbol:model.layers.11.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%63:tensor<[1024, 2048], Float32, CPU>[@model.layers.11.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=398), symbol:model.layers.11.self_attn.v_proj.weight])[symbol:model.layers.11.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%118:tensor<[2048, 2048], Float32, CPU>[@model.layers.11.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=415), symbol:model.layers.11.self_attn.o_proj.weight])[symbol:model.layers.11.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%207:tensor<[6144, 2048], Float32, CPU>[@model.layers.11.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=418), symbol:model.layers.11.mlp.gate_proj.weight])[symbol:model.layers.11.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%226:tensor<[6144, 2048], Float32, CPU>[@model.layers.11.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=421), symbol:model.layers.11.mlp.up_proj.weight])[symbol:model.layers.11.mlp.up_proj.weight]
            tensor.CPU.register () -> (%224:tensor<[2048, 6144], Float32, CPU>[@model.layers.11.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=423), symbol:model.layers.11.mlp.down_proj.weight])[symbol:model.layers.11.mlp.down_proj.weight]
            tensor.CPU.register () -> (%217:tensor<[2048, 2048], Float32, CPU>[@model.layers.12.self_attn.q_proj.weight][symbol:model.layers.12.self_attn.q_proj.weight])[symbol:model.layers.12.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%297:tensor<[1024, 2048], Float32, CPU>[@model.layers.12.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=426), symbol:model.layers.12.self_attn.k_proj.weight])[symbol:model.layers.12.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%94:tensor<[1024, 2048], Float32, CPU>[@model.layers.12.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=428), symbol:model.layers.12.self_attn.v_proj.weight])[symbol:model.layers.12.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%49:tensor<[2048, 2048], Float32, CPU>[@model.layers.12.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=445), symbol:model.layers.12.self_attn.o_proj.weight])[symbol:model.layers.12.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%262:tensor<[6144, 2048], Float32, CPU>[@model.layers.12.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=448), symbol:model.layers.12.mlp.gate_proj.weight])[symbol:model.layers.12.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%255:tensor<[6144, 2048], Float32, CPU>[@model.layers.12.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=451), symbol:model.layers.12.mlp.up_proj.weight])[symbol:model.layers.12.mlp.up_proj.weight]
            tensor.CPU.register () -> (%22:tensor<[2048, 6144], Float32, CPU>[@model.layers.12.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=453), symbol:model.layers.12.mlp.down_proj.weight])[symbol:model.layers.12.mlp.down_proj.weight]
            tensor.CPU.register () -> (%114:tensor<[2048, 2048], Float32, CPU>[@model.layers.13.self_attn.q_proj.weight][symbol:model.layers.13.self_attn.q_proj.weight])[symbol:model.layers.13.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%152:tensor<[1024, 2048], Float32, CPU>[@model.layers.13.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=456), symbol:model.layers.13.self_attn.k_proj.weight])[symbol:model.layers.13.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%15:tensor<[1024, 2048], Float32, CPU>[@model.layers.13.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=458), symbol:model.layers.13.self_attn.v_proj.weight])[symbol:model.layers.13.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%250:tensor<[2048, 2048], Float32, CPU>[@model.layers.13.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=475), symbol:model.layers.13.self_attn.o_proj.weight])[symbol:model.layers.13.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%247:tensor<[6144, 2048], Float32, CPU>[@model.layers.13.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=478), symbol:model.layers.13.mlp.gate_proj.weight])[symbol:model.layers.13.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%98:tensor<[6144, 2048], Float32, CPU>[@model.layers.13.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=481), symbol:model.layers.13.mlp.up_proj.weight])[symbol:model.layers.13.mlp.up_proj.weight]
            tensor.CPU.register () -> (%193:tensor<[2048, 6144], Float32, CPU>[@model.layers.13.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=483), symbol:model.layers.13.mlp.down_proj.weight])[symbol:model.layers.13.mlp.down_proj.weight]
            tensor.CPU.register () -> (%209:tensor<[2048, 2048], Float32, CPU>[@model.layers.14.self_attn.q_proj.weight][symbol:model.layers.14.self_attn.q_proj.weight])[symbol:model.layers.14.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%38:tensor<[1024, 2048], Float32, CPU>[@model.layers.14.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=486), symbol:model.layers.14.self_attn.k_proj.weight])[symbol:model.layers.14.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%232:tensor<[1024, 2048], Float32, CPU>[@model.layers.14.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=488), symbol:model.layers.14.self_attn.v_proj.weight])[symbol:model.layers.14.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%168:tensor<[2048, 2048], Float32, CPU>[@model.layers.14.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=505), symbol:model.layers.14.self_attn.o_proj.weight])[symbol:model.layers.14.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%37:tensor<[6144, 2048], Float32, CPU>[@model.layers.14.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=508), symbol:model.layers.14.mlp.gate_proj.weight])[symbol:model.layers.14.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%147:tensor<[6144, 2048], Float32, CPU>[@model.layers.14.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=511), symbol:model.layers.14.mlp.up_proj.weight])[symbol:model.layers.14.mlp.up_proj.weight]
            tensor.CPU.register () -> (%163:tensor<[2048, 6144], Float32, CPU>[@model.layers.14.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=513), symbol:model.layers.14.mlp.down_proj.weight])[symbol:model.layers.14.mlp.down_proj.weight]
            tensor.CPU.register () -> (%46:tensor<[2048, 2048], Float32, CPU>[@model.layers.15.self_attn.q_proj.weight][symbol:model.layers.15.self_attn.q_proj.weight])[symbol:model.layers.15.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%268:tensor<[1024, 2048], Float32, CPU>[@model.layers.15.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=516), symbol:model.layers.15.self_attn.k_proj.weight])[symbol:model.layers.15.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%117:tensor<[1024, 2048], Float32, CPU>[@model.layers.15.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=518), symbol:model.layers.15.self_attn.v_proj.weight])[symbol:model.layers.15.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%303:tensor<[2048, 2048], Float32, CPU>[@model.layers.15.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=535), symbol:model.layers.15.self_attn.o_proj.weight])[symbol:model.layers.15.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%260:tensor<[6144, 2048], Float32, CPU>[@model.layers.15.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=538), symbol:model.layers.15.mlp.gate_proj.weight])[symbol:model.layers.15.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%42:tensor<[6144, 2048], Float32, CPU>[@model.layers.15.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=541), symbol:model.layers.15.mlp.up_proj.weight])[symbol:model.layers.15.mlp.up_proj.weight]
            tensor.CPU.register () -> (%290:tensor<[2048, 6144], Float32, CPU>[@model.layers.15.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=543), symbol:model.layers.15.mlp.down_proj.weight])[symbol:model.layers.15.mlp.down_proj.weight]
            tensor.CPU.register () -> (%17:tensor<[2048, 2048], Float32, CPU>[@model.layers.16.self_attn.q_proj.weight][symbol:model.layers.16.self_attn.q_proj.weight])[symbol:model.layers.16.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%228:tensor<[1024, 2048], Float32, CPU>[@model.layers.16.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=546), symbol:model.layers.16.self_attn.k_proj.weight])[symbol:model.layers.16.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%66:tensor<[1024, 2048], Float32, CPU>[@model.layers.16.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=548), symbol:model.layers.16.self_attn.v_proj.weight])[symbol:model.layers.16.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%211:tensor<[2048, 2048], Float32, CPU>[@model.layers.16.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=565), symbol:model.layers.16.self_attn.o_proj.weight])[symbol:model.layers.16.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%130:tensor<[6144, 2048], Float32, CPU>[@model.layers.16.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=568), symbol:model.layers.16.mlp.gate_proj.weight])[symbol:model.layers.16.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%79:tensor<[6144, 2048], Float32, CPU>[@model.layers.16.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=571), symbol:model.layers.16.mlp.up_proj.weight])[symbol:model.layers.16.mlp.up_proj.weight]
            tensor.CPU.register () -> (%248:tensor<[2048, 6144], Float32, CPU>[@model.layers.16.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=573), symbol:model.layers.16.mlp.down_proj.weight])[symbol:model.layers.16.mlp.down_proj.weight]
            tensor.CPU.register () -> (%64:tensor<[2048, 2048], Float32, CPU>[@model.layers.17.self_attn.q_proj.weight][symbol:model.layers.17.self_attn.q_proj.weight])[symbol:model.layers.17.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%237:tensor<[1024, 2048], Float32, CPU>[@model.layers.17.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=576), symbol:model.layers.17.self_attn.k_proj.weight])[symbol:model.layers.17.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%6:tensor<[1024, 2048], Float32, CPU>[@model.layers.17.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=578), symbol:model.layers.17.self_attn.v_proj.weight])[symbol:model.layers.17.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%125:tensor<[2048, 2048], Float32, CPU>[@model.layers.17.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=595), symbol:model.layers.17.self_attn.o_proj.weight])[symbol:model.layers.17.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%177:tensor<[6144, 2048], Float32, CPU>[@model.layers.17.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=598), symbol:model.layers.17.mlp.gate_proj.weight])[symbol:model.layers.17.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%26:tensor<[6144, 2048], Float32, CPU>[@model.layers.17.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=601), symbol:model.layers.17.mlp.up_proj.weight])[symbol:model.layers.17.mlp.up_proj.weight]
            tensor.CPU.register () -> (%25:tensor<[2048, 6144], Float32, CPU>[@model.layers.17.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=603), symbol:model.layers.17.mlp.down_proj.weight])[symbol:model.layers.17.mlp.down_proj.weight]
            tensor.CPU.register () -> (%273:tensor<[2048, 2048], Float32, CPU>[@model.layers.18.self_attn.q_proj.weight][symbol:model.layers.18.self_attn.q_proj.weight])[symbol:model.layers.18.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%284:tensor<[1024, 2048], Float32, CPU>[@model.layers.18.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=606), symbol:model.layers.18.self_attn.k_proj.weight])[symbol:model.layers.18.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%18:tensor<[1024, 2048], Float32, CPU>[@model.layers.18.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=608), symbol:model.layers.18.self_attn.v_proj.weight])[symbol:model.layers.18.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%2:tensor<[2048, 2048], Float32, CPU>[@model.layers.18.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=625), symbol:model.layers.18.self_attn.o_proj.weight])[symbol:model.layers.18.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%166:tensor<[6144, 2048], Float32, CPU>[@model.layers.18.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=628), symbol:model.layers.18.mlp.gate_proj.weight])[symbol:model.layers.18.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%271:tensor<[6144, 2048], Float32, CPU>[@model.layers.18.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=631), symbol:model.layers.18.mlp.up_proj.weight])[symbol:model.layers.18.mlp.up_proj.weight]
            tensor.CPU.register () -> (%112:tensor<[2048, 6144], Float32, CPU>[@model.layers.18.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=633), symbol:model.layers.18.mlp.down_proj.weight])[symbol:model.layers.18.mlp.down_proj.weight]
            tensor.CPU.register () -> (%8:tensor<[2048, 2048], Float32, CPU>[@model.layers.19.self_attn.q_proj.weight][symbol:model.layers.19.self_attn.q_proj.weight])[symbol:model.layers.19.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%286:tensor<[1024, 2048], Float32, CPU>[@model.layers.19.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=636), symbol:model.layers.19.self_attn.k_proj.weight])[symbol:model.layers.19.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%50:tensor<[1024, 2048], Float32, CPU>[@model.layers.19.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=638), symbol:model.layers.19.self_attn.v_proj.weight])[symbol:model.layers.19.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%58:tensor<[2048, 2048], Float32, CPU>[@model.layers.19.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=655), symbol:model.layers.19.self_attn.o_proj.weight])[symbol:model.layers.19.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%281:tensor<[6144, 2048], Float32, CPU>[@model.layers.19.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=658), symbol:model.layers.19.mlp.gate_proj.weight])[symbol:model.layers.19.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%82:tensor<[6144, 2048], Float32, CPU>[@model.layers.19.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=661), symbol:model.layers.19.mlp.up_proj.weight])[symbol:model.layers.19.mlp.up_proj.weight]
            tensor.CPU.register () -> (%173:tensor<[2048, 6144], Float32, CPU>[@model.layers.19.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=663), symbol:model.layers.19.mlp.down_proj.weight])[symbol:model.layers.19.mlp.down_proj.weight]
            tensor.CPU.register () -> (%280:tensor<[2048, 2048], Float32, CPU>[@model.layers.20.self_attn.q_proj.weight][symbol:model.layers.20.self_attn.q_proj.weight])[symbol:model.layers.20.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%253:tensor<[1024, 2048], Float32, CPU>[@model.layers.20.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=666), symbol:model.layers.20.self_attn.k_proj.weight])[symbol:model.layers.20.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%239:tensor<[1024, 2048], Float32, CPU>[@model.layers.20.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=668), symbol:model.layers.20.self_attn.v_proj.weight])[symbol:model.layers.20.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%41:tensor<[2048, 2048], Float32, CPU>[@model.layers.20.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=685), symbol:model.layers.20.self_attn.o_proj.weight])[symbol:model.layers.20.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%172:tensor<[6144, 2048], Float32, CPU>[@model.layers.20.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=688), symbol:model.layers.20.mlp.gate_proj.weight])[symbol:model.layers.20.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%299:tensor<[6144, 2048], Float32, CPU>[@model.layers.20.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=691), symbol:model.layers.20.mlp.up_proj.weight])[symbol:model.layers.20.mlp.up_proj.weight]
            tensor.CPU.register () -> (%123:tensor<[2048, 6144], Float32, CPU>[@model.layers.20.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=693), symbol:model.layers.20.mlp.down_proj.weight])[symbol:model.layers.20.mlp.down_proj.weight]
            tensor.CPU.register () -> (%295:tensor<[2048, 2048], Float32, CPU>[@model.layers.21.self_attn.q_proj.weight][symbol:model.layers.21.self_attn.q_proj.weight])[symbol:model.layers.21.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%139:tensor<[1024, 2048], Float32, CPU>[@model.layers.21.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=696), symbol:model.layers.21.self_attn.k_proj.weight])[symbol:model.layers.21.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%142:tensor<[1024, 2048], Float32, CPU>[@model.layers.21.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=698), symbol:model.layers.21.self_attn.v_proj.weight])[symbol:model.layers.21.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%115:tensor<[2048, 2048], Float32, CPU>[@model.layers.21.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=715), symbol:model.layers.21.self_attn.o_proj.weight])[symbol:model.layers.21.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%259:tensor<[6144, 2048], Float32, CPU>[@model.layers.21.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=718), symbol:model.layers.21.mlp.gate_proj.weight])[symbol:model.layers.21.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%162:tensor<[6144, 2048], Float32, CPU>[@model.layers.21.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=721), symbol:model.layers.21.mlp.up_proj.weight])[symbol:model.layers.21.mlp.up_proj.weight]
            tensor.CPU.register () -> (%183:tensor<[2048, 6144], Float32, CPU>[@model.layers.21.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=723), symbol:model.layers.21.mlp.down_proj.weight])[symbol:model.layers.21.mlp.down_proj.weight]
            tensor.CPU.register () -> (%89:tensor<[2048, 2048], Float32, CPU>[@model.layers.22.self_attn.q_proj.weight][symbol:model.layers.22.self_attn.q_proj.weight])[symbol:model.layers.22.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%36:tensor<[1024, 2048], Float32, CPU>[@model.layers.22.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=726), symbol:model.layers.22.self_attn.k_proj.weight])[symbol:model.layers.22.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%204:tensor<[1024, 2048], Float32, CPU>[@model.layers.22.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=728), symbol:model.layers.22.self_attn.v_proj.weight])[symbol:model.layers.22.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%234:tensor<[2048, 2048], Float32, CPU>[@model.layers.22.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=745), symbol:model.layers.22.self_attn.o_proj.weight])[symbol:model.layers.22.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%198:tensor<[6144, 2048], Float32, CPU>[@model.layers.22.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=748), symbol:model.layers.22.mlp.gate_proj.weight])[symbol:model.layers.22.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%254:tensor<[6144, 2048], Float32, CPU>[@model.layers.22.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=751), symbol:model.layers.22.mlp.up_proj.weight])[symbol:model.layers.22.mlp.up_proj.weight]
            tensor.CPU.register () -> (%31:tensor<[2048, 6144], Float32, CPU>[@model.layers.22.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=753), symbol:model.layers.22.mlp.down_proj.weight])[symbol:model.layers.22.mlp.down_proj.weight]
            tensor.CPU.register () -> (%109:tensor<[2048, 2048], Float32, CPU>[@model.layers.23.self_attn.q_proj.weight][symbol:model.layers.23.self_attn.q_proj.weight])[symbol:model.layers.23.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%39:tensor<[1024, 2048], Float32, CPU>[@model.layers.23.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=756), symbol:model.layers.23.self_attn.k_proj.weight])[symbol:model.layers.23.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%83:tensor<[1024, 2048], Float32, CPU>[@model.layers.23.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=758), symbol:model.layers.23.self_attn.v_proj.weight])[symbol:model.layers.23.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%176:tensor<[2048, 2048], Float32, CPU>[@model.layers.23.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=775), symbol:model.layers.23.self_attn.o_proj.weight])[symbol:model.layers.23.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%169:tensor<[6144, 2048], Float32, CPU>[@model.layers.23.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=778), symbol:model.layers.23.mlp.gate_proj.weight])[symbol:model.layers.23.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%243:tensor<[6144, 2048], Float32, CPU>[@model.layers.23.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=781), symbol:model.layers.23.mlp.up_proj.weight])[symbol:model.layers.23.mlp.up_proj.weight]
            tensor.CPU.register () -> (%149:tensor<[2048, 6144], Float32, CPU>[@model.layers.23.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=783), symbol:model.layers.23.mlp.down_proj.weight])[symbol:model.layers.23.mlp.down_proj.weight]
            tensor.CPU.register () -> (%11:tensor<[2048, 2048], Float32, CPU>[@model.layers.24.self_attn.q_proj.weight][symbol:model.layers.24.self_attn.q_proj.weight])[symbol:model.layers.24.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%61:tensor<[1024, 2048], Float32, CPU>[@model.layers.24.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=786), symbol:model.layers.24.self_attn.k_proj.weight])[symbol:model.layers.24.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%81:tensor<[1024, 2048], Float32, CPU>[@model.layers.24.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=788), symbol:model.layers.24.self_attn.v_proj.weight])[symbol:model.layers.24.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%127:tensor<[2048, 2048], Float32, CPU>[@model.layers.24.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=805), symbol:model.layers.24.self_attn.o_proj.weight])[symbol:model.layers.24.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%141:tensor<[6144, 2048], Float32, CPU>[@model.layers.24.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=808), symbol:model.layers.24.mlp.gate_proj.weight])[symbol:model.layers.24.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%126:tensor<[6144, 2048], Float32, CPU>[@model.layers.24.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=811), symbol:model.layers.24.mlp.up_proj.weight])[symbol:model.layers.24.mlp.up_proj.weight]
            tensor.CPU.register () -> (%34:tensor<[2048, 6144], Float32, CPU>[@model.layers.24.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=813), symbol:model.layers.24.mlp.down_proj.weight])[symbol:model.layers.24.mlp.down_proj.weight]
            tensor.CPU.register () -> (%206:tensor<[2048, 2048], Float32, CPU>[@model.layers.25.self_attn.q_proj.weight][symbol:model.layers.25.self_attn.q_proj.weight])[symbol:model.layers.25.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%27:tensor<[1024, 2048], Float32, CPU>[@model.layers.25.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=816), symbol:model.layers.25.self_attn.k_proj.weight])[symbol:model.layers.25.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%121:tensor<[1024, 2048], Float32, CPU>[@model.layers.25.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=818), symbol:model.layers.25.self_attn.v_proj.weight])[symbol:model.layers.25.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%150:tensor<[2048, 2048], Float32, CPU>[@model.layers.25.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=835), symbol:model.layers.25.self_attn.o_proj.weight])[symbol:model.layers.25.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%249:tensor<[6144, 2048], Float32, CPU>[@model.layers.25.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=838), symbol:model.layers.25.mlp.gate_proj.weight])[symbol:model.layers.25.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%159:tensor<[6144, 2048], Float32, CPU>[@model.layers.25.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=841), symbol:model.layers.25.mlp.up_proj.weight])[symbol:model.layers.25.mlp.up_proj.weight]
            tensor.CPU.register () -> (%267:tensor<[2048, 6144], Float32, CPU>[@model.layers.25.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=843), symbol:model.layers.25.mlp.down_proj.weight])[symbol:model.layers.25.mlp.down_proj.weight]
            tensor.CPU.register () -> (%265:tensor<[2048, 2048], Float32, CPU>[@model.layers.26.self_attn.q_proj.weight][symbol:model.layers.26.self_attn.q_proj.weight])[symbol:model.layers.26.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%190:tensor<[1024, 2048], Float32, CPU>[@model.layers.26.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=846), symbol:model.layers.26.self_attn.k_proj.weight])[symbol:model.layers.26.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%119:tensor<[1024, 2048], Float32, CPU>[@model.layers.26.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=848), symbol:model.layers.26.self_attn.v_proj.weight])[symbol:model.layers.26.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%88:tensor<[2048, 2048], Float32, CPU>[@model.layers.26.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=865), symbol:model.layers.26.self_attn.o_proj.weight])[symbol:model.layers.26.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%96:tensor<[6144, 2048], Float32, CPU>[@model.layers.26.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=868), symbol:model.layers.26.mlp.gate_proj.weight])[symbol:model.layers.26.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%62:tensor<[6144, 2048], Float32, CPU>[@model.layers.26.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=871), symbol:model.layers.26.mlp.up_proj.weight])[symbol:model.layers.26.mlp.up_proj.weight]
            tensor.CPU.register () -> (%220:tensor<[2048, 6144], Float32, CPU>[@model.layers.26.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=873), symbol:model.layers.26.mlp.down_proj.weight])[symbol:model.layers.26.mlp.down_proj.weight]
            tensor.CPU.register () -> (%185:tensor<[2048, 2048], Float32, CPU>[@model.layers.27.self_attn.q_proj.weight][symbol:model.layers.27.self_attn.q_proj.weight])[symbol:model.layers.27.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%12:tensor<[1024, 2048], Float32, CPU>[@model.layers.27.self_attn.k_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=876), symbol:model.layers.27.self_attn.k_proj.weight])[symbol:model.layers.27.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%54:tensor<[1024, 2048], Float32, CPU>[@model.layers.27.self_attn.v_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=878), symbol:model.layers.27.self_attn.v_proj.weight])[symbol:model.layers.27.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%60:tensor<[2048, 2048], Float32, CPU>[@model.layers.27.self_attn.o_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=895), symbol:model.layers.27.self_attn.o_proj.weight])[symbol:model.layers.27.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%144:tensor<[6144, 2048], Float32, CPU>[@model.layers.27.mlp.gate_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=898), symbol:model.layers.27.mlp.gate_proj.weight])[symbol:model.layers.27.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%146:tensor<[6144, 2048], Float32, CPU>[@model.layers.27.mlp.up_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=901), symbol:model.layers.27.mlp.up_proj.weight])[symbol:model.layers.27.mlp.up_proj.weight]
            tensor.CPU.register () -> (%195:tensor<[2048, 6144], Float32, CPU>[@model.layers.27.mlp.down_proj.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=903), symbol:model.layers.27.mlp.down_proj.weight])[symbol:model.layers.27.mlp.down_proj.weight]
            tensor.CPU.register () -> (%101:tensor<[151936, 2048], Float32, CPU>[@lm_head.weight][quant_recipe:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=906), symbol:lm_head.weight])[symbol:lm_head.weight]
        }
    }
    graph.SubGraphOp @deinit <notype> [symbol:deinit] {
        () -> () {
            
        }
    }
    graph.CallGraphOp @model (%318:tensor<[1, 32], Int64, CPU>[quant_recipe:QuantSpec(Raw(type: Int64), uuid=0)], %376:tensor<[1, 32], Int64, CPU>[quant_recipe:QuantSpec(Raw(type: Int64), uuid=1)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %320:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=3)], %322:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=4)], %324:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=5)], %326:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=6)], %328:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=7)], %330:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=8)], %332:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=9)], %334:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=10)], %336:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=11)], %338:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=12)], %340:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=13)], %342:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=14)], %344:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=15)], %346:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=16)], %348:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=17)], %350:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=18)], %352:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=19)], %354:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=20)], %356:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=21)], %358:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=22)], %360:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=23)], %362:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=24)], %364:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=25)], %366:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=26)], %368:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=27)], %370:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=28)], %372:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=29)], %374:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=30)], %321:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=31)], %323:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=32)], %325:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=33)], %327:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=34)], %329:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=35)], %331:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=36)], %333:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=37)], %335:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=38)], %337:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=39)], %339:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=40)], %341:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=41)], %343:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=42)], %345:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=43)], %347:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=44)], %349:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=45)], %351:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=46)], %353:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=47)], %355:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=48)], %357:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=49)], %359:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=50)], %361:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=51)], %363:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=52)], %365:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=53)], %367:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=54)], %369:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=55)], %371:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=56)], %373:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=57)], %375:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=58)]) -> (%1530:tensor<[1, 32, 151936], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=907)], %394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=74)], %435:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=104)], %476:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=134)], %517:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=164)], %558:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=194)], %599:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=224)], %640:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=254)], %681:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=284)], %722:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=314)], %763:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=344)], %804:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=374)], %845:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=404)], %886:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=434)], %927:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=464)], %968:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=494)], %1009:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=524)], %1050:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=554)], %1091:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=584)], %1132:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=614)], %1173:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=644)], %1214:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=674)], %1255:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=704)], %1296:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=734)], %1337:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=764)], %1378:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=794)], %1419:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=824)], %1460:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=854)], %1501:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=884)], %396:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=76)], %437:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=106)], %478:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=136)], %519:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=166)], %560:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=196)], %601:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=226)], %642:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=256)], %683:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=286)], %724:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=316)], %765:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=346)], %806:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=376)], %847:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=406)], %888:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=436)], %929:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=466)], %970:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=496)], %1011:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=526)], %1052:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=556)], %1093:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=586)], %1134:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=616)], %1175:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=646)], %1216:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=676)], %1257:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=706)], %1298:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=736)], %1339:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=766)], %1380:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=796)], %1421:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=826)], %1462:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=856)], %1503:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=886)])
    graph.SubGraphOp @model <CPU> [using_qnn:true, symbol:model] {
        (%318:tensor<[1, 32], Int64, CPU>[quant_recipe:QuantSpec(Raw(type: Int64), uuid=0)], %376:tensor<[1, 32], Int64, CPU>[quant_recipe:QuantSpec(Raw(type: Int64), uuid=1)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %320:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=3)], %322:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=4)], %324:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=5)], %326:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=6)], %328:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=7)], %330:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=8)], %332:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=9)], %334:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=10)], %336:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=11)], %338:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=12)], %340:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=13)], %342:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=14)], %344:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=15)], %346:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=16)], %348:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=17)], %350:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=18)], %352:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=19)], %354:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=20)], %356:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=21)], %358:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=22)], %360:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=23)], %362:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=24)], %364:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=25)], %366:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=26)], %368:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=27)], %370:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=28)], %372:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=29)], %374:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=30)], %321:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=31)], %323:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=32)], %325:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=33)], %327:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=34)], %329:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=35)], %331:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=36)], %333:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=37)], %335:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=38)], %337:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=39)], %339:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=40)], %341:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=41)], %343:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=42)], %345:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=43)], %347:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=44)], %349:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=45)], %351:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=46)], %353:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=47)], %355:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=48)], %357:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=49)], %359:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=50)], %361:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=51)], %363:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=52)], %365:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=53)], %367:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=54)], %369:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=55)], %371:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=56)], %373:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=57)], %375:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=58)]) -> (%1530:tensor<[1, 32, 151936], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=907)], %394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=74)], %435:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=104)], %476:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=134)], %517:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=164)], %558:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=194)], %599:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=224)], %640:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=254)], %681:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=284)], %722:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=314)], %763:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=344)], %804:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=374)], %845:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=404)], %886:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=434)], %927:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=464)], %968:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=494)], %1009:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=524)], %1050:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=554)], %1091:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=584)], %1132:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=614)], %1173:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=644)], %1214:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=674)], %1255:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=704)], %1296:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=734)], %1337:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=764)], %1378:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=794)], %1419:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=824)], %1460:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=854)], %1501:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=884)], %396:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=76)], %437:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=106)], %478:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=136)], %519:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=166)], %560:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=196)], %601:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=226)], %642:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=256)], %683:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=286)], %724:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=316)], %765:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=346)], %806:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=376)], %847:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=406)], %888:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=436)], %929:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=466)], %970:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=496)], %1011:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=526)], %1052:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=556)], %1093:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=586)], %1134:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=616)], %1175:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=646)], %1216:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=676)], %1257:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=706)], %1298:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=736)], %1339:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=766)], %1380:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=796)], %1421:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=826)], %1462:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=856)], %1503:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=886)]) {
            linalg.CPU.EmbeddingOp <name="model.embed_tokens">(%318:tensor<[1, 32], Int64, CPU>[quant_recipe:QuantSpec(Raw(type: Int64), uuid=0)]) -> (%377:tensor<[1, 32, 2048], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=59)])
            linalg.CPU.CastTypeOp <name="model.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float32), uuid=59), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=60), )] (%377:tensor<[1, 32, 2048], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=59)]) -> (%378:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=60)])
            linalg.CPU.ViewOp <name="model.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int64), uuid=1), outputs_0:QuantSpec(Raw(type: Int64), uuid=1), )] (%376:tensor<[1, 32], Int64, CPU>[quant_recipe:QuantSpec(Raw(type: Int64), uuid=1)]) -> (%376:tensor<[32], Int64, CPU>[quant_recipe:QuantSpec(Raw(type: Int64), uuid=1)])
            linalg.CPU.IndexOp <name="model.Index.0">(%316:tensor<[1, 1024, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=61)]) -> (%379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)])
            linalg.CPU.IndexOp <name="model.Index.1">(%317:tensor<[1, 1024, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=63)]) -> (%380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)])
            graph.CallGraphOp @model.layers.0 (%378:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=60)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %320:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=3)], %321:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=31)]) -> (%421:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=94)], %394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=74)], %396:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=76)])
            graph.CallGraphOp @model.layers.1 (%421:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=94)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %322:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=4)], %323:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=32)]) -> (%462:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=124)], %435:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=104)], %437:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=106)])
            graph.CallGraphOp @model.layers.2 (%462:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=124)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %324:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=5)], %325:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=33)]) -> (%503:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=154)], %476:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=134)], %478:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=136)])
            graph.CallGraphOp @model.layers.3 (%503:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=154)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %326:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=6)], %327:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=34)]) -> (%544:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=184)], %517:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=164)], %519:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=166)])
            graph.CallGraphOp @model.layers.4 (%544:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=184)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %328:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=7)], %329:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=35)]) -> (%585:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=214)], %558:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=194)], %560:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=196)])
            graph.CallGraphOp @model.layers.5 (%585:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=214)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %330:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=8)], %331:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=36)]) -> (%626:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=244)], %599:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=224)], %601:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=226)])
            graph.CallGraphOp @model.layers.6 (%626:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=244)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %332:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=9)], %333:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=37)]) -> (%667:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=274)], %640:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=254)], %642:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=256)])
            graph.CallGraphOp @model.layers.7 (%667:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=274)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %334:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=10)], %335:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=38)]) -> (%708:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=304)], %681:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=284)], %683:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=286)])
            graph.CallGraphOp @model.layers.8 (%708:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=304)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %336:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=11)], %337:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=39)]) -> (%749:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=334)], %722:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=314)], %724:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=316)])
            graph.CallGraphOp @model.layers.9 (%749:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=334)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %338:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=12)], %339:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=40)]) -> (%790:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=364)], %763:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=344)], %765:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=346)])
            graph.CallGraphOp @model.layers.10 (%790:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=364)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %340:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=13)], %341:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=41)]) -> (%831:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=394)], %804:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=374)], %806:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=376)])
            graph.CallGraphOp @model.layers.11 (%831:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=394)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %342:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=14)], %343:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=42)]) -> (%872:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=424)], %845:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=404)], %847:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=406)])
            graph.CallGraphOp @model.layers.12 (%872:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=424)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %344:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=15)], %345:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=43)]) -> (%913:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=454)], %886:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=434)], %888:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=436)])
            graph.CallGraphOp @model.layers.13 (%913:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=454)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %346:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=16)], %347:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=44)]) -> (%954:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=484)], %927:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=464)], %929:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=466)])
            graph.CallGraphOp @model.layers.14 (%954:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=484)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %348:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=17)], %349:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=45)]) -> (%995:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=514)], %968:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=494)], %970:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=496)])
            graph.CallGraphOp @model.layers.15 (%995:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=514)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %350:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=18)], %351:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=46)]) -> (%1036:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=544)], %1009:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=524)], %1011:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=526)])
            graph.CallGraphOp @model.layers.16 (%1036:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=544)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %352:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=19)], %353:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=47)]) -> (%1077:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=574)], %1050:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=554)], %1052:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=556)])
            graph.CallGraphOp @model.layers.17 (%1077:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=574)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %354:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=20)], %355:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=48)]) -> (%1118:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=604)], %1091:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=584)], %1093:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=586)])
            graph.CallGraphOp @model.layers.18 (%1118:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=604)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %356:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=21)], %357:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=49)]) -> (%1159:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=634)], %1132:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=614)], %1134:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=616)])
            graph.CallGraphOp @model.layers.19 (%1159:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=634)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %358:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=22)], %359:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=50)]) -> (%1200:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=664)], %1173:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=644)], %1175:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=646)])
            graph.CallGraphOp @model.layers.20 (%1200:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=664)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %360:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=23)], %361:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=51)]) -> (%1241:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=694)], %1214:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=674)], %1216:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=676)])
            graph.CallGraphOp @model.layers.21 (%1241:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=694)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %362:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=24)], %363:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=52)]) -> (%1282:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=724)], %1255:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=704)], %1257:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=706)])
            graph.CallGraphOp @model.layers.22 (%1282:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=724)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %364:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=25)], %365:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=53)]) -> (%1323:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=754)], %1296:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=734)], %1298:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=736)])
            graph.CallGraphOp @model.layers.23 (%1323:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=754)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %366:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=26)], %367:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=54)]) -> (%1364:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=784)], %1337:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=764)], %1339:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=766)])
            graph.CallGraphOp @model.layers.24 (%1364:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=784)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %368:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=27)], %369:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=55)]) -> (%1405:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=814)], %1378:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=794)], %1380:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=796)])
            graph.CallGraphOp @model.layers.25 (%1405:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=814)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %370:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=28)], %371:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=56)]) -> (%1446:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=844)], %1419:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=824)], %1421:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=826)])
            graph.CallGraphOp @model.layers.26 (%1446:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=844)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %372:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=29)], %373:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=57)]) -> (%1487:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=874)], %1460:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=854)], %1462:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=856)])
            graph.CallGraphOp @model.layers.27 (%1487:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=874)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %374:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=30)], %375:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=58)]) -> (%1528:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=904)], %1501:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=884)], %1503:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=886)])
            linalg.CPU.RMSNormOp <name="model.norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=904), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=905), )] (%1528:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=904)]) -> (%1529:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=905)])
            linalg.CPU.LinearOp <name="lm_head"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=905), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=907), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=906)), using_qnn:true] (%1529:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=905)]) -> (%1530:tensor<[1, 32, 151936], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=907)])
            cf.ReturnOp (%1530:tensor<[1, 32, 151936], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=907)], %394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=74)], %435:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=104)], %476:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=134)], %517:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=164)], %558:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=194)], %599:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=224)], %640:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=254)], %681:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=284)], %722:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=314)], %763:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=344)], %804:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=374)], %845:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=404)], %886:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=434)], %927:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=464)], %968:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=494)], %1009:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=524)], %1050:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=554)], %1091:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=584)], %1132:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=614)], %1173:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=644)], %1214:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=674)], %1255:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=704)], %1296:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=734)], %1337:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=764)], %1378:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=794)], %1419:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=824)], %1460:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=854)], %1501:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=884)], %396:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=76)], %437:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=106)], %478:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=136)], %519:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=166)], %560:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=196)], %601:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=226)], %642:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=256)], %683:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=286)], %724:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=316)], %765:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=346)], %806:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=376)], %847:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=406)], %888:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=436)], %929:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=466)], %970:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=496)], %1011:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=526)], %1052:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=556)], %1093:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=586)], %1134:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=616)], %1175:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=646)], %1216:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=676)], %1257:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=706)], %1298:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=736)], %1339:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=766)], %1380:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=796)], %1421:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=826)], %1462:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=856)], %1503:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=886)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.0 <CPU> [using_qnn:true, symbol:model.layers.0] {
        (%378:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=60)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %320:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=3)], %321:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=31)]) -> (%421:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=94)], %394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=74)], %396:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=76)]) {
            linalg.CPU.RMSNormOp <name="model.layers.0.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=60), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), )] (%378:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=60)]) -> (%381:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)])
            graph.CallGraphOp @model.layers.0.self_attn (%381:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %320:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=3)], %321:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=31)]) -> (%413:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=86)], %394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=74)], %396:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=76)])
            linalg.CPU.AddOp <name="model.layers.0.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=86), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=60), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=86), )] (%413:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=86)], %378:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=60)]) -> (%414:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=86)])
            linalg.CPU.RMSNormOp <name="model.layers.0.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=86), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=87), )] (%414:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=86)]) -> (%415:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=87)])
            graph.CallGraphOp @model.layers.0.mlp (%415:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=87)]) -> (%420:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=94)])
            linalg.CPU.AddOp <name="model.layers.0.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=94), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=86), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=94), )] (%420:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=94)], %414:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=86)]) -> (%421:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=94)])
            cf.ReturnOp (%421:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=94)], %394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=74)], %396:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=76)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.0.self_attn <CPU> [using_qnn:true, symbol:model.layers.0.self_attn] {
        (%381:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %320:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=3)], %321:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=31)]) -> (%413:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=86)], %394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=74)], %396:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=76)]) {
            linalg.CPU.LinearOp <name="model.layers.0.self_attn.q_proj">(%381:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%382:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=70)])
            linalg.CPU.LinearOp <name="model.layers.0.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=67), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=66))] (%381:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%383:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=67)])
            linalg.CPU.LinearOp <name="model.layers.0.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=69), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=68))] (%381:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=65)]) -> (%384:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=69)])
            linalg.CPU.ViewOp <name="model.layers.0.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=70), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=70), )] (%382:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=70)]) -> (%382:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=70)])
            linalg.CPU.TransposeOp <name="model.layers.0.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=70), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=70), )] (%382:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=70)]) -> (%385:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=70)])
            linalg.CPU.ViewOp <name="model.layers.0.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=67), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=67), )] (%383:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=67)]) -> (%383:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=67)])
            linalg.CPU.TransposeOp <name="model.layers.0.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=67), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=67), )] (%383:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=67)]) -> (%386:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=67)])
            linalg.CPU.ViewOp <name="model.layers.0.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=69), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=69), )] (%384:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=69)]) -> (%384:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=69)])
            linalg.CPU.TransposeOp <name="model.layers.0.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=69), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=69), )] (%384:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=69)]) -> (%387:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=69)])
            linalg.CPU.RMSNormOp <name="model.layers.0.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=70), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=71), )] (%385:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=70)]) -> (%388:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=71)])
            linalg.CPU.RMSNormOp <name="model.layers.0.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=67), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=72), )] (%386:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=67)]) -> (%389:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=72)])
            linalg.CPU.RoPEOp <name="model.layers.0.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=71), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=71), )] (%388:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=71)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%390:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=71)])
            linalg.CPU.RoPEOp <name="model.layers.0.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=72), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=72), )] (%389:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=72)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%391:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=72)])
            linalg.CPU.CastTypeOp <name="model.layers.0.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=72), outputs_0:QuantSpec(Raw(type: Float16), uuid=73), )] (%391:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=72)]) -> (%392:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=73)])
            linalg.CPU.CastTypeOp <name="model.layers.0.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=73), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=74), )] (%392:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=73)]) -> (%393:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=74)])
            linalg.CPU.TransposeOp <name="model.layers.0.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=74), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=74), )] (%393:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=74)]) -> (%394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=74)])
            linalg.CPU.CastTypeOp <name="model.layers.0.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=69), outputs_0:QuantSpec(Raw(type: Float16), uuid=75), )] (%387:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=69)]) -> (%395:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=75)])
            linalg.CPU.CastTypeOp <name="model.layers.0.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=75), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=76), )] (%395:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=75)]) -> (%396:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=76)])
            linalg.CPU.ConcatOp <name="model.layers.0.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=3), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=74), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=3), )] (%320:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=3)], %394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=74)]) -> (%397:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=3)])
            linalg.CPU.ConcatOp <name="model.layers.0.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=31), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=76), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=31), )] (%321:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=31)], %396:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=76)]) -> (%398:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=31)])
            linalg.CPU.RepeatOp <name="model.layers.0.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=3), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=3), )] (%397:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=3)]) -> (%399:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=3)])
            linalg.CPU.RepeatOp <name="model.layers.0.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=31), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=31), )] (%398:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=31)]) -> (%400:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=31)])
            linalg.CPU.MatMulOp <name="model.layers.0.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=71), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=3), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=77), )] (%390:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=71)], %399:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=3)]) -> (%401:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=77)])
            linalg.CPU.MulOp <name="model.layers.0.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=77), inputs_1:QuantSpec(Raw(type: Float32), uuid=78), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=77), )] (%401:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=77)], %402:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=78), constant:[0.088388346]]) -> (%403:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=77)])
            linalg.CPU.ReduceMinOp <name="model.layers.0.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=77), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=79), )] (%403:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=77)]) -> (%404:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=79)])
            linalg.CPU.AddOp <name="model.layers.0.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=79), inputs_1:QuantSpec(Raw(type: Int16), uuid=80), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=79), )] (%404:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=79)], %405:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=80), constant:[-20]]) -> (%406:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=79)])
            linalg.CPU.EqualOp <name="model.layers.0.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=81), outputs_0:QuantSpec(Raw(type: UInt8), uuid=82), )] (%319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %407:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=81), constant:[0]]) -> (%408:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=82)])
            linalg.CPU.WhereOp <name="model.layers.0.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=82), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=77), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=79), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=79), )] (%408:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=82)], %403:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=77)], %406:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=79)]) -> (%409:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=79)])
            linalg.CPU.SoftmaxOp <name="model.layers.0.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=79), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=83), )] (%409:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=79)]) -> (%410:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=83)])
            linalg.CPU.MatMulOp <name="model.layers.0.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=83), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=31), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=84), )] (%410:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=83)], %400:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=31)]) -> (%411:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=84)])
            linalg.CPU.TransposeOp <name="model.layers.0.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=84), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=84), )] (%411:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=84)]) -> (%412:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=84)])
            linalg.CPU.ViewOp <name="model.layers.0.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=84), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=84), )] (%412:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=84)]) -> (%412:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=84)])
            linalg.CPU.LinearOp <name="model.layers.0.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=84), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=86), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=85))] (%412:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=84)]) -> (%413:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=86)])
            cf.ReturnOp (%413:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=86)], %394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=74)], %396:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=76)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.0.mlp <CPU> [using_qnn:true, symbol:model.layers.0.mlp] {
        (%415:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=87)]) -> (%420:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=94)]) {
            linalg.CPU.LinearOp <name="model.layers.0.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=87), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=89), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=88))] (%415:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=87)]) -> (%416:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=89)])
            linalg.CPU.SiLUOp <name="model.layers.0.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=89), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=90), )] (%416:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=89)]) -> (%417:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=90)])
            linalg.CPU.LinearOp <name="model.layers.0.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=87), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=92), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=91))] (%415:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=87)]) -> (%418:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=92)])
            linalg.CPU.MulOp <name="model.layers.0.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=90), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=92), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=90), )] (%417:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=90)], %418:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=92)]) -> (%419:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=90)])
            linalg.CPU.LinearOp <name="model.layers.0.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=90), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=94), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=93))] (%419:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=90)]) -> (%420:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=94)])
            cf.ReturnOp (%420:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=94)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.1 <CPU> [using_qnn:true, symbol:model.layers.1] {
        (%421:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=94)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %322:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=4)], %323:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=32)]) -> (%462:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=124)], %435:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=104)], %437:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=106)]) {
            linalg.CPU.RMSNormOp <name="model.layers.1.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=94), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=95), )] (%421:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=94)]) -> (%422:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=95)])
            graph.CallGraphOp @model.layers.1.self_attn (%422:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=95)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %322:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=4)], %323:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=32)]) -> (%454:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=116)], %435:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=104)], %437:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=106)])
            linalg.CPU.AddOp <name="model.layers.1.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=116), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=94), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=116), )] (%454:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=116)], %421:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=94)]) -> (%455:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=116)])
            linalg.CPU.RMSNormOp <name="model.layers.1.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=116), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=117), )] (%455:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=116)]) -> (%456:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=117)])
            graph.CallGraphOp @model.layers.1.mlp (%456:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=117)]) -> (%461:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=124)])
            linalg.CPU.AddOp <name="model.layers.1.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=124), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=116), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=124), )] (%461:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=124)], %455:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=116)]) -> (%462:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=124)])
            cf.ReturnOp (%462:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=124)], %435:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=104)], %437:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=106)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.1.self_attn <CPU> [using_qnn:true, symbol:model.layers.1.self_attn] {
        (%422:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=95)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %322:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=4)], %323:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=32)]) -> (%454:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=116)], %435:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=104)], %437:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=106)]) {
            linalg.CPU.LinearOp <name="model.layers.1.self_attn.q_proj">(%422:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=95)]) -> (%423:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=100)])
            linalg.CPU.LinearOp <name="model.layers.1.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=95), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=97), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=96))] (%422:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=95)]) -> (%424:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=97)])
            linalg.CPU.LinearOp <name="model.layers.1.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=95), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=99), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=98))] (%422:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=95)]) -> (%425:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=99)])
            linalg.CPU.ViewOp <name="model.layers.1.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=100), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=100), )] (%423:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=100)]) -> (%423:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=100)])
            linalg.CPU.TransposeOp <name="model.layers.1.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=100), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=100), )] (%423:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=100)]) -> (%426:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=100)])
            linalg.CPU.ViewOp <name="model.layers.1.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=97), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=97), )] (%424:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=97)]) -> (%424:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=97)])
            linalg.CPU.TransposeOp <name="model.layers.1.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=97), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=97), )] (%424:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=97)]) -> (%427:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=97)])
            linalg.CPU.ViewOp <name="model.layers.1.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=99), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=99), )] (%425:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=99)]) -> (%425:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=99)])
            linalg.CPU.TransposeOp <name="model.layers.1.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=99), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=99), )] (%425:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=99)]) -> (%428:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=99)])
            linalg.CPU.RMSNormOp <name="model.layers.1.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=100), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=101), )] (%426:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=100)]) -> (%429:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=101)])
            linalg.CPU.RMSNormOp <name="model.layers.1.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=97), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=102), )] (%427:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=97)]) -> (%430:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=102)])
            linalg.CPU.RoPEOp <name="model.layers.1.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=101), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=101), )] (%429:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=101)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%431:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=101)])
            linalg.CPU.RoPEOp <name="model.layers.1.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=102), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=102), )] (%430:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=102)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%432:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=102)])
            linalg.CPU.CastTypeOp <name="model.layers.1.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=102), outputs_0:QuantSpec(Raw(type: Float16), uuid=103), )] (%432:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=102)]) -> (%433:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=103)])
            linalg.CPU.CastTypeOp <name="model.layers.1.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=103), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=104), )] (%433:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=103)]) -> (%434:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=104)])
            linalg.CPU.TransposeOp <name="model.layers.1.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=104), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=104), )] (%434:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=104)]) -> (%435:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=104)])
            linalg.CPU.CastTypeOp <name="model.layers.1.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=99), outputs_0:QuantSpec(Raw(type: Float16), uuid=105), )] (%428:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=99)]) -> (%436:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=105)])
            linalg.CPU.CastTypeOp <name="model.layers.1.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=105), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=106), )] (%436:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=105)]) -> (%437:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=106)])
            linalg.CPU.ConcatOp <name="model.layers.1.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=4), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=104), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=4), )] (%322:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=4)], %435:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=104)]) -> (%438:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=4)])
            linalg.CPU.ConcatOp <name="model.layers.1.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=32), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=106), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=32), )] (%323:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=32)], %437:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=106)]) -> (%439:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=32)])
            linalg.CPU.RepeatOp <name="model.layers.1.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=4), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=4), )] (%438:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=4)]) -> (%440:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=4)])
            linalg.CPU.RepeatOp <name="model.layers.1.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=32), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=32), )] (%439:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=32)]) -> (%441:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=32)])
            linalg.CPU.MatMulOp <name="model.layers.1.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=101), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=4), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=107), )] (%431:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=101)], %440:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=4)]) -> (%442:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=107)])
            linalg.CPU.MulOp <name="model.layers.1.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=107), inputs_1:QuantSpec(Raw(type: Float32), uuid=108), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=107), )] (%442:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=107)], %443:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=108), constant:[0.088388346]]) -> (%444:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=107)])
            linalg.CPU.ReduceMinOp <name="model.layers.1.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=107), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=109), )] (%444:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=107)]) -> (%445:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=109)])
            linalg.CPU.AddOp <name="model.layers.1.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=109), inputs_1:QuantSpec(Raw(type: Int16), uuid=110), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=109), )] (%445:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=109)], %446:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=110), constant:[-20]]) -> (%447:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=109)])
            linalg.CPU.EqualOp <name="model.layers.1.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=111), outputs_0:QuantSpec(Raw(type: UInt8), uuid=112), )] (%319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %448:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=111), constant:[0]]) -> (%449:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=112)])
            linalg.CPU.WhereOp <name="model.layers.1.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=112), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=107), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=109), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=109), )] (%449:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=112)], %444:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=107)], %447:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=109)]) -> (%450:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=109)])
            linalg.CPU.SoftmaxOp <name="model.layers.1.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=109), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=113), )] (%450:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=109)]) -> (%451:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=113)])
            linalg.CPU.MatMulOp <name="model.layers.1.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=113), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=32), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=114), )] (%451:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=113)], %441:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=32)]) -> (%452:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=114)])
            linalg.CPU.TransposeOp <name="model.layers.1.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=114), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=114), )] (%452:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=114)]) -> (%453:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=114)])
            linalg.CPU.ViewOp <name="model.layers.1.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=114), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=114), )] (%453:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=114)]) -> (%453:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=114)])
            linalg.CPU.LinearOp <name="model.layers.1.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=114), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=116), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=115))] (%453:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=114)]) -> (%454:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=116)])
            cf.ReturnOp (%454:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=116)], %435:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=104)], %437:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=106)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.1.mlp <CPU> [using_qnn:true, symbol:model.layers.1.mlp] {
        (%456:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=117)]) -> (%461:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=124)]) {
            linalg.CPU.LinearOp <name="model.layers.1.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=117), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=119), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=118))] (%456:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=117)]) -> (%457:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=119)])
            linalg.CPU.SiLUOp <name="model.layers.1.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=119), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=120), )] (%457:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=119)]) -> (%458:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=120)])
            linalg.CPU.LinearOp <name="model.layers.1.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=117), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=122), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=121))] (%456:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=117)]) -> (%459:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=122)])
            linalg.CPU.MulOp <name="model.layers.1.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=120), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=122), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=120), )] (%458:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=120)], %459:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=122)]) -> (%460:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=120)])
            linalg.CPU.LinearOp <name="model.layers.1.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=120), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=124), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=123))] (%460:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=120)]) -> (%461:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=124)])
            cf.ReturnOp (%461:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=124)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.2 <CPU> [using_qnn:true, symbol:model.layers.2] {
        (%462:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=124)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %324:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=5)], %325:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=33)]) -> (%503:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=154)], %476:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=134)], %478:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=136)]) {
            linalg.CPU.RMSNormOp <name="model.layers.2.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=124), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=125), )] (%462:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=124)]) -> (%463:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=125)])
            graph.CallGraphOp @model.layers.2.self_attn (%463:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=125)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %324:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=5)], %325:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=33)]) -> (%495:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=146)], %476:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=134)], %478:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=136)])
            linalg.CPU.AddOp <name="model.layers.2.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=146), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=124), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=146), )] (%495:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=146)], %462:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=124)]) -> (%496:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=146)])
            linalg.CPU.RMSNormOp <name="model.layers.2.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=146), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=147), )] (%496:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=146)]) -> (%497:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=147)])
            graph.CallGraphOp @model.layers.2.mlp (%497:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=147)]) -> (%502:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=154)])
            linalg.CPU.AddOp <name="model.layers.2.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=154), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=146), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=154), )] (%502:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=154)], %496:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=146)]) -> (%503:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=154)])
            cf.ReturnOp (%503:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=154)], %476:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=134)], %478:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=136)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.2.self_attn <CPU> [using_qnn:true, symbol:model.layers.2.self_attn] {
        (%463:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=125)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %324:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=5)], %325:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=33)]) -> (%495:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=146)], %476:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=134)], %478:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=136)]) {
            linalg.CPU.LinearOp <name="model.layers.2.self_attn.q_proj">(%463:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=125)]) -> (%464:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=130)])
            linalg.CPU.LinearOp <name="model.layers.2.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=125), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=127), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=126))] (%463:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=125)]) -> (%465:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=127)])
            linalg.CPU.LinearOp <name="model.layers.2.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=125), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=129), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=128))] (%463:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=125)]) -> (%466:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=129)])
            linalg.CPU.ViewOp <name="model.layers.2.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=130), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=130), )] (%464:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=130)]) -> (%464:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=130)])
            linalg.CPU.TransposeOp <name="model.layers.2.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=130), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=130), )] (%464:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=130)]) -> (%467:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=130)])
            linalg.CPU.ViewOp <name="model.layers.2.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=127), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=127), )] (%465:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=127)]) -> (%465:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=127)])
            linalg.CPU.TransposeOp <name="model.layers.2.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=127), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=127), )] (%465:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=127)]) -> (%468:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=127)])
            linalg.CPU.ViewOp <name="model.layers.2.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=129), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=129), )] (%466:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=129)]) -> (%466:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=129)])
            linalg.CPU.TransposeOp <name="model.layers.2.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=129), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=129), )] (%466:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=129)]) -> (%469:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=129)])
            linalg.CPU.RMSNormOp <name="model.layers.2.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=130), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=131), )] (%467:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=130)]) -> (%470:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=131)])
            linalg.CPU.RMSNormOp <name="model.layers.2.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=127), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=132), )] (%468:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=127)]) -> (%471:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=132)])
            linalg.CPU.RoPEOp <name="model.layers.2.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=131), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=131), )] (%470:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=131)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%472:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=131)])
            linalg.CPU.RoPEOp <name="model.layers.2.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=132), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=132), )] (%471:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=132)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%473:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=132)])
            linalg.CPU.CastTypeOp <name="model.layers.2.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=132), outputs_0:QuantSpec(Raw(type: Float16), uuid=133), )] (%473:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=132)]) -> (%474:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=133)])
            linalg.CPU.CastTypeOp <name="model.layers.2.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=133), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=134), )] (%474:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=133)]) -> (%475:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=134)])
            linalg.CPU.TransposeOp <name="model.layers.2.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=134), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=134), )] (%475:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=134)]) -> (%476:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=134)])
            linalg.CPU.CastTypeOp <name="model.layers.2.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=129), outputs_0:QuantSpec(Raw(type: Float16), uuid=135), )] (%469:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=129)]) -> (%477:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=135)])
            linalg.CPU.CastTypeOp <name="model.layers.2.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=135), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=136), )] (%477:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=135)]) -> (%478:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=136)])
            linalg.CPU.ConcatOp <name="model.layers.2.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=5), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=134), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=5), )] (%324:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=5)], %476:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=134)]) -> (%479:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=5)])
            linalg.CPU.ConcatOp <name="model.layers.2.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=33), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=136), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=33), )] (%325:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=33)], %478:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=136)]) -> (%480:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=33)])
            linalg.CPU.RepeatOp <name="model.layers.2.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=5), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=5), )] (%479:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=5)]) -> (%481:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=5)])
            linalg.CPU.RepeatOp <name="model.layers.2.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=33), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=33), )] (%480:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=33)]) -> (%482:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=33)])
            linalg.CPU.MatMulOp <name="model.layers.2.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=131), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=5), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=137), )] (%472:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=131)], %481:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=5)]) -> (%483:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=137)])
            linalg.CPU.MulOp <name="model.layers.2.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=137), inputs_1:QuantSpec(Raw(type: Float32), uuid=138), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=137), )] (%483:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=137)], %484:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=138), constant:[0.088388346]]) -> (%485:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=137)])
            linalg.CPU.ReduceMinOp <name="model.layers.2.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=137), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=139), )] (%485:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=137)]) -> (%486:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=139)])
            linalg.CPU.AddOp <name="model.layers.2.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=139), inputs_1:QuantSpec(Raw(type: Int16), uuid=140), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=139), )] (%486:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=139)], %487:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=140), constant:[-20]]) -> (%488:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=139)])
            linalg.CPU.EqualOp <name="model.layers.2.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=141), outputs_0:QuantSpec(Raw(type: UInt8), uuid=142), )] (%319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %489:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=141), constant:[0]]) -> (%490:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=142)])
            linalg.CPU.WhereOp <name="model.layers.2.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=142), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=137), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=139), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=139), )] (%490:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=142)], %485:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=137)], %488:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=139)]) -> (%491:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=139)])
            linalg.CPU.SoftmaxOp <name="model.layers.2.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=139), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=143), )] (%491:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=139)]) -> (%492:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=143)])
            linalg.CPU.MatMulOp <name="model.layers.2.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=143), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=33), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=144), )] (%492:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=143)], %482:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=33)]) -> (%493:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=144)])
            linalg.CPU.TransposeOp <name="model.layers.2.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=144), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=144), )] (%493:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=144)]) -> (%494:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=144)])
            linalg.CPU.ViewOp <name="model.layers.2.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=144), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=144), )] (%494:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=144)]) -> (%494:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=144)])
            linalg.CPU.LinearOp <name="model.layers.2.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=144), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=146), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=145))] (%494:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=144)]) -> (%495:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=146)])
            cf.ReturnOp (%495:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=146)], %476:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=134)], %478:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=136)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.2.mlp <CPU> [using_qnn:true, symbol:model.layers.2.mlp] {
        (%497:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=147)]) -> (%502:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=154)]) {
            linalg.CPU.LinearOp <name="model.layers.2.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=147), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=149), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=148))] (%497:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=147)]) -> (%498:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=149)])
            linalg.CPU.SiLUOp <name="model.layers.2.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=149), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=150), )] (%498:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=149)]) -> (%499:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=150)])
            linalg.CPU.LinearOp <name="model.layers.2.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=147), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=152), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=151))] (%497:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=147)]) -> (%500:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=152)])
            linalg.CPU.MulOp <name="model.layers.2.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=150), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=152), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=150), )] (%499:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=150)], %500:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=152)]) -> (%501:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=150)])
            linalg.CPU.LinearOp <name="model.layers.2.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=150), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=154), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=153))] (%501:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=150)]) -> (%502:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=154)])
            cf.ReturnOp (%502:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=154)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.3 <CPU> [using_qnn:true, symbol:model.layers.3] {
        (%503:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=154)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %326:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=6)], %327:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=34)]) -> (%544:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=184)], %517:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=164)], %519:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=166)]) {
            linalg.CPU.RMSNormOp <name="model.layers.3.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=154), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=155), )] (%503:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=154)]) -> (%504:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=155)])
            graph.CallGraphOp @model.layers.3.self_attn (%504:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=155)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %326:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=6)], %327:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=34)]) -> (%536:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=176)], %517:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=164)], %519:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=166)])
            linalg.CPU.AddOp <name="model.layers.3.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=176), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=154), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=176), )] (%536:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=176)], %503:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=154)]) -> (%537:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=176)])
            linalg.CPU.RMSNormOp <name="model.layers.3.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=176), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=177), )] (%537:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=176)]) -> (%538:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=177)])
            graph.CallGraphOp @model.layers.3.mlp (%538:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=177)]) -> (%543:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=184)])
            linalg.CPU.AddOp <name="model.layers.3.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=184), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=176), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=184), )] (%543:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=184)], %537:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=176)]) -> (%544:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=184)])
            cf.ReturnOp (%544:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=184)], %517:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=164)], %519:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=166)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.3.self_attn <CPU> [using_qnn:true, symbol:model.layers.3.self_attn] {
        (%504:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=155)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %326:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=6)], %327:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=34)]) -> (%536:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=176)], %517:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=164)], %519:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=166)]) {
            linalg.CPU.LinearOp <name="model.layers.3.self_attn.q_proj">(%504:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=155)]) -> (%505:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=160)])
            linalg.CPU.LinearOp <name="model.layers.3.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=155), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=157), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=156))] (%504:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=155)]) -> (%506:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=157)])
            linalg.CPU.LinearOp <name="model.layers.3.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=155), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=159), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=158))] (%504:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=155)]) -> (%507:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=159)])
            linalg.CPU.ViewOp <name="model.layers.3.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=160), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=160), )] (%505:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=160)]) -> (%505:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=160)])
            linalg.CPU.TransposeOp <name="model.layers.3.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=160), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=160), )] (%505:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=160)]) -> (%508:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=160)])
            linalg.CPU.ViewOp <name="model.layers.3.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=157), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=157), )] (%506:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=157)]) -> (%506:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=157)])
            linalg.CPU.TransposeOp <name="model.layers.3.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=157), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=157), )] (%506:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=157)]) -> (%509:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=157)])
            linalg.CPU.ViewOp <name="model.layers.3.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=159), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=159), )] (%507:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=159)]) -> (%507:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=159)])
            linalg.CPU.TransposeOp <name="model.layers.3.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=159), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=159), )] (%507:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=159)]) -> (%510:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=159)])
            linalg.CPU.RMSNormOp <name="model.layers.3.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=160), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=161), )] (%508:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=160)]) -> (%511:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=161)])
            linalg.CPU.RMSNormOp <name="model.layers.3.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=157), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=162), )] (%509:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=157)]) -> (%512:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=162)])
            linalg.CPU.RoPEOp <name="model.layers.3.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=161), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=161), )] (%511:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=161)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%513:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=161)])
            linalg.CPU.RoPEOp <name="model.layers.3.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=162), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=162), )] (%512:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=162)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%514:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=162)])
            linalg.CPU.CastTypeOp <name="model.layers.3.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=162), outputs_0:QuantSpec(Raw(type: Float16), uuid=163), )] (%514:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=162)]) -> (%515:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=163)])
            linalg.CPU.CastTypeOp <name="model.layers.3.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=163), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=164), )] (%515:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=163)]) -> (%516:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=164)])
            linalg.CPU.TransposeOp <name="model.layers.3.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=164), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=164), )] (%516:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=164)]) -> (%517:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=164)])
            linalg.CPU.CastTypeOp <name="model.layers.3.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=159), outputs_0:QuantSpec(Raw(type: Float16), uuid=165), )] (%510:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=159)]) -> (%518:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=165)])
            linalg.CPU.CastTypeOp <name="model.layers.3.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=165), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=166), )] (%518:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=165)]) -> (%519:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=166)])
            linalg.CPU.ConcatOp <name="model.layers.3.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=6), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=164), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=6), )] (%326:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=6)], %517:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=164)]) -> (%520:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=6)])
            linalg.CPU.ConcatOp <name="model.layers.3.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=34), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=166), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=34), )] (%327:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=34)], %519:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=166)]) -> (%521:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=34)])
            linalg.CPU.RepeatOp <name="model.layers.3.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=6), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=6), )] (%520:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=6)]) -> (%522:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=6)])
            linalg.CPU.RepeatOp <name="model.layers.3.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=34), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=34), )] (%521:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=34)]) -> (%523:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=34)])
            linalg.CPU.MatMulOp <name="model.layers.3.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=161), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=6), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=167), )] (%513:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=161)], %522:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=6)]) -> (%524:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=167)])
            linalg.CPU.MulOp <name="model.layers.3.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=167), inputs_1:QuantSpec(Raw(type: Float32), uuid=168), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=167), )] (%524:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=167)], %525:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=168), constant:[0.088388346]]) -> (%526:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=167)])
            linalg.CPU.ReduceMinOp <name="model.layers.3.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=167), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=169), )] (%526:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=167)]) -> (%527:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=169)])
            linalg.CPU.AddOp <name="model.layers.3.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=169), inputs_1:QuantSpec(Raw(type: Int16), uuid=170), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=169), )] (%527:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=169)], %528:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=170), constant:[-20]]) -> (%529:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=169)])
            linalg.CPU.EqualOp <name="model.layers.3.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=171), outputs_0:QuantSpec(Raw(type: UInt8), uuid=172), )] (%319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %530:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=171), constant:[0]]) -> (%531:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=172)])
            linalg.CPU.WhereOp <name="model.layers.3.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=172), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=167), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=169), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=169), )] (%531:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=172)], %526:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=167)], %529:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=169)]) -> (%532:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=169)])
            linalg.CPU.SoftmaxOp <name="model.layers.3.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=169), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=173), )] (%532:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=169)]) -> (%533:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=173)])
            linalg.CPU.MatMulOp <name="model.layers.3.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=173), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=34), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=174), )] (%533:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=173)], %523:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=34)]) -> (%534:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=174)])
            linalg.CPU.TransposeOp <name="model.layers.3.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=174), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=174), )] (%534:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=174)]) -> (%535:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=174)])
            linalg.CPU.ViewOp <name="model.layers.3.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=174), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=174), )] (%535:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=174)]) -> (%535:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=174)])
            linalg.CPU.LinearOp <name="model.layers.3.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=174), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=176), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=175))] (%535:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=174)]) -> (%536:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=176)])
            cf.ReturnOp (%536:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=176)], %517:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=164)], %519:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=166)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.3.mlp <CPU> [using_qnn:true, symbol:model.layers.3.mlp] {
        (%538:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=177)]) -> (%543:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=184)]) {
            linalg.CPU.LinearOp <name="model.layers.3.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=177), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=179), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=178))] (%538:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=177)]) -> (%539:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=179)])
            linalg.CPU.SiLUOp <name="model.layers.3.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=179), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=180), )] (%539:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=179)]) -> (%540:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=180)])
            linalg.CPU.LinearOp <name="model.layers.3.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=177), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=182), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=181))] (%538:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=177)]) -> (%541:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=182)])
            linalg.CPU.MulOp <name="model.layers.3.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=180), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=182), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=180), )] (%540:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=180)], %541:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=182)]) -> (%542:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=180)])
            linalg.CPU.LinearOp <name="model.layers.3.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=180), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=184), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=183))] (%542:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=180)]) -> (%543:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=184)])
            cf.ReturnOp (%543:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=184)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.4 <CPU> [using_qnn:true, symbol:model.layers.4] {
        (%544:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=184)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %328:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=7)], %329:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=35)]) -> (%585:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=214)], %558:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=194)], %560:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=196)]) {
            linalg.CPU.RMSNormOp <name="model.layers.4.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=184), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=185), )] (%544:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=184)]) -> (%545:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=185)])
            graph.CallGraphOp @model.layers.4.self_attn (%545:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=185)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %328:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=7)], %329:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=35)]) -> (%577:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=206)], %558:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=194)], %560:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=196)])
            linalg.CPU.AddOp <name="model.layers.4.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=206), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=184), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=206), )] (%577:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=206)], %544:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=184)]) -> (%578:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=206)])
            linalg.CPU.RMSNormOp <name="model.layers.4.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=206), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=207), )] (%578:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=206)]) -> (%579:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=207)])
            graph.CallGraphOp @model.layers.4.mlp (%579:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=207)]) -> (%584:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=214)])
            linalg.CPU.AddOp <name="model.layers.4.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=214), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=206), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=214), )] (%584:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=214)], %578:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=206)]) -> (%585:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=214)])
            cf.ReturnOp (%585:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=214)], %558:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=194)], %560:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=196)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.4.self_attn <CPU> [using_qnn:true, symbol:model.layers.4.self_attn] {
        (%545:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=185)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %328:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=7)], %329:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=35)]) -> (%577:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=206)], %558:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=194)], %560:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=196)]) {
            linalg.CPU.LinearOp <name="model.layers.4.self_attn.q_proj">(%545:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=185)]) -> (%546:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=190)])
            linalg.CPU.LinearOp <name="model.layers.4.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=185), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=187), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=186))] (%545:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=185)]) -> (%547:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=187)])
            linalg.CPU.LinearOp <name="model.layers.4.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=185), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=189), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=188))] (%545:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=185)]) -> (%548:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=189)])
            linalg.CPU.ViewOp <name="model.layers.4.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=190), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=190), )] (%546:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=190)]) -> (%546:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=190)])
            linalg.CPU.TransposeOp <name="model.layers.4.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=190), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=190), )] (%546:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=190)]) -> (%549:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=190)])
            linalg.CPU.ViewOp <name="model.layers.4.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=187), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=187), )] (%547:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=187)]) -> (%547:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=187)])
            linalg.CPU.TransposeOp <name="model.layers.4.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=187), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=187), )] (%547:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=187)]) -> (%550:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=187)])
            linalg.CPU.ViewOp <name="model.layers.4.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=189), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=189), )] (%548:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=189)]) -> (%548:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=189)])
            linalg.CPU.TransposeOp <name="model.layers.4.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=189), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=189), )] (%548:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=189)]) -> (%551:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=189)])
            linalg.CPU.RMSNormOp <name="model.layers.4.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=190), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=191), )] (%549:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=190)]) -> (%552:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=191)])
            linalg.CPU.RMSNormOp <name="model.layers.4.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=187), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=192), )] (%550:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=187)]) -> (%553:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=192)])
            linalg.CPU.RoPEOp <name="model.layers.4.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=191), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=191), )] (%552:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=191)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%554:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=191)])
            linalg.CPU.RoPEOp <name="model.layers.4.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=192), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=192), )] (%553:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=192)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%555:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=192)])
            linalg.CPU.CastTypeOp <name="model.layers.4.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=192), outputs_0:QuantSpec(Raw(type: Float16), uuid=193), )] (%555:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=192)]) -> (%556:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=193)])
            linalg.CPU.CastTypeOp <name="model.layers.4.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=193), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=194), )] (%556:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=193)]) -> (%557:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=194)])
            linalg.CPU.TransposeOp <name="model.layers.4.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=194), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=194), )] (%557:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=194)]) -> (%558:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=194)])
            linalg.CPU.CastTypeOp <name="model.layers.4.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=189), outputs_0:QuantSpec(Raw(type: Float16), uuid=195), )] (%551:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=189)]) -> (%559:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=195)])
            linalg.CPU.CastTypeOp <name="model.layers.4.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=195), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=196), )] (%559:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=195)]) -> (%560:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=196)])
            linalg.CPU.ConcatOp <name="model.layers.4.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=7), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=194), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=7), )] (%328:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=7)], %558:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=194)]) -> (%561:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=7)])
            linalg.CPU.ConcatOp <name="model.layers.4.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=35), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=196), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=35), )] (%329:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=35)], %560:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=196)]) -> (%562:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=35)])
            linalg.CPU.RepeatOp <name="model.layers.4.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=7), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=7), )] (%561:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=7)]) -> (%563:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=7)])
            linalg.CPU.RepeatOp <name="model.layers.4.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=35), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=35), )] (%562:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=35)]) -> (%564:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=35)])
            linalg.CPU.MatMulOp <name="model.layers.4.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=191), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=7), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=197), )] (%554:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=191)], %563:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=7)]) -> (%565:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=197)])
            linalg.CPU.MulOp <name="model.layers.4.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=197), inputs_1:QuantSpec(Raw(type: Float32), uuid=198), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=197), )] (%565:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=197)], %566:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=198), constant:[0.088388346]]) -> (%567:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=197)])
            linalg.CPU.ReduceMinOp <name="model.layers.4.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=197), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=199), )] (%567:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=197)]) -> (%568:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=199)])
            linalg.CPU.AddOp <name="model.layers.4.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=199), inputs_1:QuantSpec(Raw(type: Int16), uuid=200), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=199), )] (%568:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=199)], %569:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=200), constant:[-20]]) -> (%570:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=199)])
            linalg.CPU.EqualOp <name="model.layers.4.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=201), outputs_0:QuantSpec(Raw(type: UInt8), uuid=202), )] (%319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %571:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=201), constant:[0]]) -> (%572:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=202)])
            linalg.CPU.WhereOp <name="model.layers.4.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=202), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=197), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=199), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=199), )] (%572:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=202)], %567:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=197)], %570:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=199)]) -> (%573:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=199)])
            linalg.CPU.SoftmaxOp <name="model.layers.4.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=199), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=203), )] (%573:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=199)]) -> (%574:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=203)])
            linalg.CPU.MatMulOp <name="model.layers.4.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=203), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=35), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=204), )] (%574:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=203)], %564:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=35)]) -> (%575:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=204)])
            linalg.CPU.TransposeOp <name="model.layers.4.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=204), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=204), )] (%575:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=204)]) -> (%576:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=204)])
            linalg.CPU.ViewOp <name="model.layers.4.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=204), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=204), )] (%576:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=204)]) -> (%576:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=204)])
            linalg.CPU.LinearOp <name="model.layers.4.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=204), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=206), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=205))] (%576:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=204)]) -> (%577:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=206)])
            cf.ReturnOp (%577:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=206)], %558:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=194)], %560:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=196)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.4.mlp <CPU> [using_qnn:true, symbol:model.layers.4.mlp] {
        (%579:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=207)]) -> (%584:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=214)]) {
            linalg.CPU.LinearOp <name="model.layers.4.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=207), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=209), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=208))] (%579:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=207)]) -> (%580:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=209)])
            linalg.CPU.SiLUOp <name="model.layers.4.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=209), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=210), )] (%580:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=209)]) -> (%581:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=210)])
            linalg.CPU.LinearOp <name="model.layers.4.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=207), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=212), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=211))] (%579:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=207)]) -> (%582:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=212)])
            linalg.CPU.MulOp <name="model.layers.4.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=210), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=212), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=210), )] (%581:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=210)], %582:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=212)]) -> (%583:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=210)])
            linalg.CPU.LinearOp <name="model.layers.4.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=210), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=214), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=213))] (%583:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=210)]) -> (%584:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=214)])
            cf.ReturnOp (%584:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=214)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.5 <CPU> [using_qnn:true, symbol:model.layers.5] {
        (%585:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=214)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %330:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=8)], %331:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=36)]) -> (%626:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=244)], %599:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=224)], %601:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=226)]) {
            linalg.CPU.RMSNormOp <name="model.layers.5.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=214), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=215), )] (%585:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=214)]) -> (%586:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=215)])
            graph.CallGraphOp @model.layers.5.self_attn (%586:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=215)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %330:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=8)], %331:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=36)]) -> (%618:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=236)], %599:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=224)], %601:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=226)])
            linalg.CPU.AddOp <name="model.layers.5.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=236), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=214), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=236), )] (%618:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=236)], %585:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=214)]) -> (%619:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=236)])
            linalg.CPU.RMSNormOp <name="model.layers.5.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=236), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=237), )] (%619:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=236)]) -> (%620:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=237)])
            graph.CallGraphOp @model.layers.5.mlp (%620:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=237)]) -> (%625:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=244)])
            linalg.CPU.AddOp <name="model.layers.5.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=244), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=236), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=244), )] (%625:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=244)], %619:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=236)]) -> (%626:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=244)])
            cf.ReturnOp (%626:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=244)], %599:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=224)], %601:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=226)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.5.self_attn <CPU> [using_qnn:true, symbol:model.layers.5.self_attn] {
        (%586:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=215)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %330:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=8)], %331:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=36)]) -> (%618:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=236)], %599:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=224)], %601:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=226)]) {
            linalg.CPU.LinearOp <name="model.layers.5.self_attn.q_proj">(%586:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=215)]) -> (%587:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=220)])
            linalg.CPU.LinearOp <name="model.layers.5.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=215), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=217), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=216))] (%586:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=215)]) -> (%588:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=217)])
            linalg.CPU.LinearOp <name="model.layers.5.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=215), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=219), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=218))] (%586:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=215)]) -> (%589:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=219)])
            linalg.CPU.ViewOp <name="model.layers.5.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=220), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=220), )] (%587:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=220)]) -> (%587:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=220)])
            linalg.CPU.TransposeOp <name="model.layers.5.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=220), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=220), )] (%587:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=220)]) -> (%590:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=220)])
            linalg.CPU.ViewOp <name="model.layers.5.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=217), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=217), )] (%588:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=217)]) -> (%588:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=217)])
            linalg.CPU.TransposeOp <name="model.layers.5.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=217), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=217), )] (%588:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=217)]) -> (%591:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=217)])
            linalg.CPU.ViewOp <name="model.layers.5.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=219), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=219), )] (%589:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=219)]) -> (%589:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=219)])
            linalg.CPU.TransposeOp <name="model.layers.5.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=219), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=219), )] (%589:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=219)]) -> (%592:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=219)])
            linalg.CPU.RMSNormOp <name="model.layers.5.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=220), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=221), )] (%590:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=220)]) -> (%593:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=221)])
            linalg.CPU.RMSNormOp <name="model.layers.5.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=217), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=222), )] (%591:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=217)]) -> (%594:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=222)])
            linalg.CPU.RoPEOp <name="model.layers.5.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=221), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=221), )] (%593:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=221)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%595:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=221)])
            linalg.CPU.RoPEOp <name="model.layers.5.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=222), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=222), )] (%594:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=222)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%596:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=222)])
            linalg.CPU.CastTypeOp <name="model.layers.5.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=222), outputs_0:QuantSpec(Raw(type: Float16), uuid=223), )] (%596:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=222)]) -> (%597:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=223)])
            linalg.CPU.CastTypeOp <name="model.layers.5.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=223), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=224), )] (%597:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=223)]) -> (%598:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=224)])
            linalg.CPU.TransposeOp <name="model.layers.5.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=224), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=224), )] (%598:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=224)]) -> (%599:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=224)])
            linalg.CPU.CastTypeOp <name="model.layers.5.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=219), outputs_0:QuantSpec(Raw(type: Float16), uuid=225), )] (%592:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=219)]) -> (%600:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=225)])
            linalg.CPU.CastTypeOp <name="model.layers.5.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=225), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=226), )] (%600:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=225)]) -> (%601:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=226)])
            linalg.CPU.ConcatOp <name="model.layers.5.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=8), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=224), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=8), )] (%330:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=8)], %599:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=224)]) -> (%602:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=8)])
            linalg.CPU.ConcatOp <name="model.layers.5.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=36), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=226), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=36), )] (%331:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=36)], %601:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=226)]) -> (%603:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=36)])
            linalg.CPU.RepeatOp <name="model.layers.5.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=8), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=8), )] (%602:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=8)]) -> (%604:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=8)])
            linalg.CPU.RepeatOp <name="model.layers.5.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=36), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=36), )] (%603:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=36)]) -> (%605:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=36)])
            linalg.CPU.MatMulOp <name="model.layers.5.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=221), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=8), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=227), )] (%595:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=221)], %604:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=8)]) -> (%606:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=227)])
            linalg.CPU.MulOp <name="model.layers.5.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=227), inputs_1:QuantSpec(Raw(type: Float32), uuid=228), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=227), )] (%606:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=227)], %607:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=228), constant:[0.088388346]]) -> (%608:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=227)])
            linalg.CPU.ReduceMinOp <name="model.layers.5.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=227), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=229), )] (%608:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=227)]) -> (%609:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=229)])
            linalg.CPU.AddOp <name="model.layers.5.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=229), inputs_1:QuantSpec(Raw(type: Int16), uuid=230), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=229), )] (%609:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=229)], %610:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=230), constant:[-20]]) -> (%611:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=229)])
            linalg.CPU.EqualOp <name="model.layers.5.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=231), outputs_0:QuantSpec(Raw(type: UInt8), uuid=232), )] (%319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %612:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=231), constant:[0]]) -> (%613:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=232)])
            linalg.CPU.WhereOp <name="model.layers.5.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=232), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=227), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=229), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=229), )] (%613:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=232)], %608:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=227)], %611:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=229)]) -> (%614:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=229)])
            linalg.CPU.SoftmaxOp <name="model.layers.5.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=229), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=233), )] (%614:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=229)]) -> (%615:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=233)])
            linalg.CPU.MatMulOp <name="model.layers.5.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=233), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=36), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=234), )] (%615:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=233)], %605:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=36)]) -> (%616:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=234)])
            linalg.CPU.TransposeOp <name="model.layers.5.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=234), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=234), )] (%616:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=234)]) -> (%617:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=234)])
            linalg.CPU.ViewOp <name="model.layers.5.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=234), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=234), )] (%617:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=234)]) -> (%617:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=234)])
            linalg.CPU.LinearOp <name="model.layers.5.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=234), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=236), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=235))] (%617:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=234)]) -> (%618:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=236)])
            cf.ReturnOp (%618:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=236)], %599:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=224)], %601:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=226)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.5.mlp <CPU> [using_qnn:true, symbol:model.layers.5.mlp] {
        (%620:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=237)]) -> (%625:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=244)]) {
            linalg.CPU.LinearOp <name="model.layers.5.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=237), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=239), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=238))] (%620:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=237)]) -> (%621:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=239)])
            linalg.CPU.SiLUOp <name="model.layers.5.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=239), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=240), )] (%621:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=239)]) -> (%622:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=240)])
            linalg.CPU.LinearOp <name="model.layers.5.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=237), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=242), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=241))] (%620:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=237)]) -> (%623:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=242)])
            linalg.CPU.MulOp <name="model.layers.5.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=240), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=242), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=240), )] (%622:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=240)], %623:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=242)]) -> (%624:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=240)])
            linalg.CPU.LinearOp <name="model.layers.5.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=240), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=244), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=243))] (%624:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=240)]) -> (%625:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=244)])
            cf.ReturnOp (%625:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=244)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.6 <CPU> [using_qnn:true, symbol:model.layers.6] {
        (%626:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=244)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %332:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=9)], %333:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=37)]) -> (%667:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=274)], %640:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=254)], %642:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=256)]) {
            linalg.CPU.RMSNormOp <name="model.layers.6.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=244), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=245), )] (%626:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=244)]) -> (%627:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=245)])
            graph.CallGraphOp @model.layers.6.self_attn (%627:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=245)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %332:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=9)], %333:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=37)]) -> (%659:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=266)], %640:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=254)], %642:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=256)])
            linalg.CPU.AddOp <name="model.layers.6.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=266), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=244), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=266), )] (%659:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=266)], %626:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=244)]) -> (%660:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=266)])
            linalg.CPU.RMSNormOp <name="model.layers.6.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=266), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=267), )] (%660:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=266)]) -> (%661:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=267)])
            graph.CallGraphOp @model.layers.6.mlp (%661:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=267)]) -> (%666:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=274)])
            linalg.CPU.AddOp <name="model.layers.6.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=274), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=266), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=274), )] (%666:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=274)], %660:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=266)]) -> (%667:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=274)])
            cf.ReturnOp (%667:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=274)], %640:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=254)], %642:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=256)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.6.self_attn <CPU> [using_qnn:true, symbol:model.layers.6.self_attn] {
        (%627:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=245)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %332:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=9)], %333:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=37)]) -> (%659:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=266)], %640:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=254)], %642:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=256)]) {
            linalg.CPU.LinearOp <name="model.layers.6.self_attn.q_proj">(%627:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=245)]) -> (%628:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=250)])
            linalg.CPU.LinearOp <name="model.layers.6.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=245), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=247), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=246))] (%627:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=245)]) -> (%629:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=247)])
            linalg.CPU.LinearOp <name="model.layers.6.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=245), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=249), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=248))] (%627:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=245)]) -> (%630:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=249)])
            linalg.CPU.ViewOp <name="model.layers.6.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=250), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=250), )] (%628:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=250)]) -> (%628:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=250)])
            linalg.CPU.TransposeOp <name="model.layers.6.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=250), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=250), )] (%628:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=250)]) -> (%631:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=250)])
            linalg.CPU.ViewOp <name="model.layers.6.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=247), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=247), )] (%629:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=247)]) -> (%629:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=247)])
            linalg.CPU.TransposeOp <name="model.layers.6.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=247), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=247), )] (%629:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=247)]) -> (%632:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=247)])
            linalg.CPU.ViewOp <name="model.layers.6.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=249), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=249), )] (%630:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=249)]) -> (%630:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=249)])
            linalg.CPU.TransposeOp <name="model.layers.6.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=249), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=249), )] (%630:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=249)]) -> (%633:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=249)])
            linalg.CPU.RMSNormOp <name="model.layers.6.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=250), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=251), )] (%631:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=250)]) -> (%634:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=251)])
            linalg.CPU.RMSNormOp <name="model.layers.6.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=247), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=252), )] (%632:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=247)]) -> (%635:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=252)])
            linalg.CPU.RoPEOp <name="model.layers.6.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=251), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=251), )] (%634:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=251)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%636:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=251)])
            linalg.CPU.RoPEOp <name="model.layers.6.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=252), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=252), )] (%635:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=252)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%637:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=252)])
            linalg.CPU.CastTypeOp <name="model.layers.6.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=252), outputs_0:QuantSpec(Raw(type: Float16), uuid=253), )] (%637:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=252)]) -> (%638:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=253)])
            linalg.CPU.CastTypeOp <name="model.layers.6.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=253), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=254), )] (%638:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=253)]) -> (%639:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=254)])
            linalg.CPU.TransposeOp <name="model.layers.6.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=254), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=254), )] (%639:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=254)]) -> (%640:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=254)])
            linalg.CPU.CastTypeOp <name="model.layers.6.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=249), outputs_0:QuantSpec(Raw(type: Float16), uuid=255), )] (%633:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=249)]) -> (%641:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=255)])
            linalg.CPU.CastTypeOp <name="model.layers.6.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=255), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=256), )] (%641:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=255)]) -> (%642:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=256)])
            linalg.CPU.ConcatOp <name="model.layers.6.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=9), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=254), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=9), )] (%332:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=9)], %640:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=254)]) -> (%643:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=9)])
            linalg.CPU.ConcatOp <name="model.layers.6.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=37), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=256), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=37), )] (%333:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=37)], %642:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=256)]) -> (%644:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=37)])
            linalg.CPU.RepeatOp <name="model.layers.6.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=9), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=9), )] (%643:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=9)]) -> (%645:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=9)])
            linalg.CPU.RepeatOp <name="model.layers.6.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=37), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=37), )] (%644:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=37)]) -> (%646:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=37)])
            linalg.CPU.MatMulOp <name="model.layers.6.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=251), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=9), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=257), )] (%636:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=251)], %645:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=9)]) -> (%647:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=257)])
            linalg.CPU.MulOp <name="model.layers.6.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=257), inputs_1:QuantSpec(Raw(type: Float32), uuid=258), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=257), )] (%647:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=257)], %648:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=258), constant:[0.088388346]]) -> (%649:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=257)])
            linalg.CPU.ReduceMinOp <name="model.layers.6.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=257), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=259), )] (%649:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=257)]) -> (%650:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=259)])
            linalg.CPU.AddOp <name="model.layers.6.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=259), inputs_1:QuantSpec(Raw(type: Int16), uuid=260), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=259), )] (%650:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=259)], %651:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=260), constant:[-20]]) -> (%652:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=259)])
            linalg.CPU.EqualOp <name="model.layers.6.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=261), outputs_0:QuantSpec(Raw(type: UInt8), uuid=262), )] (%319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %653:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=261), constant:[0]]) -> (%654:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=262)])
            linalg.CPU.WhereOp <name="model.layers.6.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=262), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=257), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=259), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=259), )] (%654:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=262)], %649:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=257)], %652:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=259)]) -> (%655:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=259)])
            linalg.CPU.SoftmaxOp <name="model.layers.6.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=259), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=263), )] (%655:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=259)]) -> (%656:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=263)])
            linalg.CPU.MatMulOp <name="model.layers.6.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=263), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=37), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=264), )] (%656:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=263)], %646:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=37)]) -> (%657:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=264)])
            linalg.CPU.TransposeOp <name="model.layers.6.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=264), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=264), )] (%657:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=264)]) -> (%658:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=264)])
            linalg.CPU.ViewOp <name="model.layers.6.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=264), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=264), )] (%658:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=264)]) -> (%658:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=264)])
            linalg.CPU.LinearOp <name="model.layers.6.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=264), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=266), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=265))] (%658:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=264)]) -> (%659:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=266)])
            cf.ReturnOp (%659:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=266)], %640:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=254)], %642:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=256)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.6.mlp <CPU> [using_qnn:true, symbol:model.layers.6.mlp] {
        (%661:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=267)]) -> (%666:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=274)]) {
            linalg.CPU.LinearOp <name="model.layers.6.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=267), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=269), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=268))] (%661:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=267)]) -> (%662:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=269)])
            linalg.CPU.SiLUOp <name="model.layers.6.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=269), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=270), )] (%662:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=269)]) -> (%663:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=270)])
            linalg.CPU.LinearOp <name="model.layers.6.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=267), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=272), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=271))] (%661:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=267)]) -> (%664:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=272)])
            linalg.CPU.MulOp <name="model.layers.6.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=270), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=272), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=270), )] (%663:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=270)], %664:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=272)]) -> (%665:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=270)])
            linalg.CPU.LinearOp <name="model.layers.6.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=270), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=274), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=273))] (%665:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=270)]) -> (%666:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=274)])
            cf.ReturnOp (%666:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=274)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.7 <CPU> [using_qnn:true, symbol:model.layers.7] {
        (%667:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=274)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %334:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=10)], %335:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=38)]) -> (%708:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=304)], %681:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=284)], %683:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=286)]) {
            linalg.CPU.RMSNormOp <name="model.layers.7.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=274), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=275), )] (%667:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=274)]) -> (%668:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=275)])
            graph.CallGraphOp @model.layers.7.self_attn (%668:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=275)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %334:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=10)], %335:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=38)]) -> (%700:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=296)], %681:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=284)], %683:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=286)])
            linalg.CPU.AddOp <name="model.layers.7.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=296), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=274), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=296), )] (%700:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=296)], %667:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=274)]) -> (%701:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=296)])
            linalg.CPU.RMSNormOp <name="model.layers.7.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=296), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=297), )] (%701:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=296)]) -> (%702:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=297)])
            graph.CallGraphOp @model.layers.7.mlp (%702:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=297)]) -> (%707:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=304)])
            linalg.CPU.AddOp <name="model.layers.7.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=304), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=296), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=304), )] (%707:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=304)], %701:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=296)]) -> (%708:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=304)])
            cf.ReturnOp (%708:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=304)], %681:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=284)], %683:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=286)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.7.self_attn <CPU> [using_qnn:true, symbol:model.layers.7.self_attn] {
        (%668:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=275)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %334:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=10)], %335:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=38)]) -> (%700:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=296)], %681:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=284)], %683:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=286)]) {
            linalg.CPU.LinearOp <name="model.layers.7.self_attn.q_proj">(%668:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=275)]) -> (%669:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=280)])
            linalg.CPU.LinearOp <name="model.layers.7.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=275), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=277), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=276))] (%668:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=275)]) -> (%670:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=277)])
            linalg.CPU.LinearOp <name="model.layers.7.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=275), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=279), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=278))] (%668:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=275)]) -> (%671:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=279)])
            linalg.CPU.ViewOp <name="model.layers.7.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=280), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=280), )] (%669:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=280)]) -> (%669:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=280)])
            linalg.CPU.TransposeOp <name="model.layers.7.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=280), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=280), )] (%669:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=280)]) -> (%672:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=280)])
            linalg.CPU.ViewOp <name="model.layers.7.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=277), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=277), )] (%670:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=277)]) -> (%670:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=277)])
            linalg.CPU.TransposeOp <name="model.layers.7.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=277), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=277), )] (%670:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=277)]) -> (%673:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=277)])
            linalg.CPU.ViewOp <name="model.layers.7.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=279), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=279), )] (%671:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=279)]) -> (%671:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=279)])
            linalg.CPU.TransposeOp <name="model.layers.7.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=279), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=279), )] (%671:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=279)]) -> (%674:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=279)])
            linalg.CPU.RMSNormOp <name="model.layers.7.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=280), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=281), )] (%672:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=280)]) -> (%675:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=281)])
            linalg.CPU.RMSNormOp <name="model.layers.7.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=277), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=282), )] (%673:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=277)]) -> (%676:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=282)])
            linalg.CPU.RoPEOp <name="model.layers.7.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=281), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=281), )] (%675:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=281)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%677:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=281)])
            linalg.CPU.RoPEOp <name="model.layers.7.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=282), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=282), )] (%676:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=282)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%678:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=282)])
            linalg.CPU.CastTypeOp <name="model.layers.7.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=282), outputs_0:QuantSpec(Raw(type: Float16), uuid=283), )] (%678:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=282)]) -> (%679:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=283)])
            linalg.CPU.CastTypeOp <name="model.layers.7.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=283), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=284), )] (%679:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=283)]) -> (%680:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=284)])
            linalg.CPU.TransposeOp <name="model.layers.7.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=284), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=284), )] (%680:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=284)]) -> (%681:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=284)])
            linalg.CPU.CastTypeOp <name="model.layers.7.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=279), outputs_0:QuantSpec(Raw(type: Float16), uuid=285), )] (%674:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=279)]) -> (%682:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=285)])
            linalg.CPU.CastTypeOp <name="model.layers.7.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=285), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=286), )] (%682:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=285)]) -> (%683:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=286)])
            linalg.CPU.ConcatOp <name="model.layers.7.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=10), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=284), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=10), )] (%334:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=10)], %681:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=284)]) -> (%684:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=10)])
            linalg.CPU.ConcatOp <name="model.layers.7.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=38), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=286), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=38), )] (%335:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=38)], %683:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=286)]) -> (%685:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=38)])
            linalg.CPU.RepeatOp <name="model.layers.7.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=10), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=10), )] (%684:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=10)]) -> (%686:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=10)])
            linalg.CPU.RepeatOp <name="model.layers.7.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=38), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=38), )] (%685:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=38)]) -> (%687:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=38)])
            linalg.CPU.MatMulOp <name="model.layers.7.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=281), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=10), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=287), )] (%677:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=281)], %686:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=10)]) -> (%688:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=287)])
            linalg.CPU.MulOp <name="model.layers.7.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=287), inputs_1:QuantSpec(Raw(type: Float32), uuid=288), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=287), )] (%688:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=287)], %689:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=288), constant:[0.088388346]]) -> (%690:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=287)])
            linalg.CPU.ReduceMinOp <name="model.layers.7.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=287), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=289), )] (%690:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=287)]) -> (%691:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=289)])
            linalg.CPU.AddOp <name="model.layers.7.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=289), inputs_1:QuantSpec(Raw(type: Int16), uuid=290), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=289), )] (%691:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=289)], %692:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=290), constant:[-20]]) -> (%693:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=289)])
            linalg.CPU.EqualOp <name="model.layers.7.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=291), outputs_0:QuantSpec(Raw(type: UInt8), uuid=292), )] (%319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %694:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=291), constant:[0]]) -> (%695:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=292)])
            linalg.CPU.WhereOp <name="model.layers.7.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=292), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=287), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=289), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=289), )] (%695:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=292)], %690:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=287)], %693:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=289)]) -> (%696:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=289)])
            linalg.CPU.SoftmaxOp <name="model.layers.7.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=289), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=293), )] (%696:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=289)]) -> (%697:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=293)])
            linalg.CPU.MatMulOp <name="model.layers.7.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=293), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=38), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=294), )] (%697:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=293)], %687:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=38)]) -> (%698:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=294)])
            linalg.CPU.TransposeOp <name="model.layers.7.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=294), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=294), )] (%698:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=294)]) -> (%699:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=294)])
            linalg.CPU.ViewOp <name="model.layers.7.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=294), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=294), )] (%699:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=294)]) -> (%699:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=294)])
            linalg.CPU.LinearOp <name="model.layers.7.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=294), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=296), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=295))] (%699:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=294)]) -> (%700:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=296)])
            cf.ReturnOp (%700:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=296)], %681:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=284)], %683:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=286)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.7.mlp <CPU> [using_qnn:true, symbol:model.layers.7.mlp] {
        (%702:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=297)]) -> (%707:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=304)]) {
            linalg.CPU.LinearOp <name="model.layers.7.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=297), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=299), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=298))] (%702:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=297)]) -> (%703:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=299)])
            linalg.CPU.SiLUOp <name="model.layers.7.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=299), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=300), )] (%703:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=299)]) -> (%704:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=300)])
            linalg.CPU.LinearOp <name="model.layers.7.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=297), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=302), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=301))] (%702:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=297)]) -> (%705:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=302)])
            linalg.CPU.MulOp <name="model.layers.7.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=300), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=302), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=300), )] (%704:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=300)], %705:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=302)]) -> (%706:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=300)])
            linalg.CPU.LinearOp <name="model.layers.7.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=300), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=304), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=303))] (%706:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=300)]) -> (%707:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=304)])
            cf.ReturnOp (%707:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=304)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.8 <CPU> [using_qnn:true, symbol:model.layers.8] {
        (%708:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=304)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %336:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=11)], %337:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=39)]) -> (%749:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=334)], %722:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=314)], %724:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=316)]) {
            linalg.CPU.RMSNormOp <name="model.layers.8.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=304), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=305), )] (%708:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=304)]) -> (%709:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=305)])
            graph.CallGraphOp @model.layers.8.self_attn (%709:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=305)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %336:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=11)], %337:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=39)]) -> (%741:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=326)], %722:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=314)], %724:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=316)])
            linalg.CPU.AddOp <name="model.layers.8.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=326), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=304), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=326), )] (%741:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=326)], %708:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=304)]) -> (%742:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=326)])
            linalg.CPU.RMSNormOp <name="model.layers.8.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=326), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=327), )] (%742:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=326)]) -> (%743:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=327)])
            graph.CallGraphOp @model.layers.8.mlp (%743:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=327)]) -> (%748:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=334)])
            linalg.CPU.AddOp <name="model.layers.8.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=334), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=326), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=334), )] (%748:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=334)], %742:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=326)]) -> (%749:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=334)])
            cf.ReturnOp (%749:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=334)], %722:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=314)], %724:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=316)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.8.self_attn <CPU> [using_qnn:true, symbol:model.layers.8.self_attn] {
        (%709:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=305)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %336:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=11)], %337:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=39)]) -> (%741:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=326)], %722:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=314)], %724:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=316)]) {
            linalg.CPU.LinearOp <name="model.layers.8.self_attn.q_proj">(%709:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=305)]) -> (%710:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=310)])
            linalg.CPU.LinearOp <name="model.layers.8.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=305), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=307), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=306))] (%709:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=305)]) -> (%711:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=307)])
            linalg.CPU.LinearOp <name="model.layers.8.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=305), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=309), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=308))] (%709:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=305)]) -> (%712:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=309)])
            linalg.CPU.ViewOp <name="model.layers.8.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=310), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=310), )] (%710:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=310)]) -> (%710:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=310)])
            linalg.CPU.TransposeOp <name="model.layers.8.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=310), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=310), )] (%710:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=310)]) -> (%713:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=310)])
            linalg.CPU.ViewOp <name="model.layers.8.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=307), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=307), )] (%711:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=307)]) -> (%711:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=307)])
            linalg.CPU.TransposeOp <name="model.layers.8.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=307), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=307), )] (%711:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=307)]) -> (%714:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=307)])
            linalg.CPU.ViewOp <name="model.layers.8.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=309), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=309), )] (%712:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=309)]) -> (%712:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=309)])
            linalg.CPU.TransposeOp <name="model.layers.8.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=309), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=309), )] (%712:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=309)]) -> (%715:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=309)])
            linalg.CPU.RMSNormOp <name="model.layers.8.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=310), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=311), )] (%713:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=310)]) -> (%716:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=311)])
            linalg.CPU.RMSNormOp <name="model.layers.8.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=307), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=312), )] (%714:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=307)]) -> (%717:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=312)])
            linalg.CPU.RoPEOp <name="model.layers.8.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=311), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=311), )] (%716:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=311)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%718:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=311)])
            linalg.CPU.RoPEOp <name="model.layers.8.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=312), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=312), )] (%717:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=312)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%719:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=312)])
            linalg.CPU.CastTypeOp <name="model.layers.8.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=312), outputs_0:QuantSpec(Raw(type: Float16), uuid=313), )] (%719:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=312)]) -> (%720:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=313)])
            linalg.CPU.CastTypeOp <name="model.layers.8.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=313), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=314), )] (%720:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=313)]) -> (%721:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=314)])
            linalg.CPU.TransposeOp <name="model.layers.8.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=314), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=314), )] (%721:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=314)]) -> (%722:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=314)])
            linalg.CPU.CastTypeOp <name="model.layers.8.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=309), outputs_0:QuantSpec(Raw(type: Float16), uuid=315), )] (%715:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=309)]) -> (%723:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=315)])
            linalg.CPU.CastTypeOp <name="model.layers.8.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=315), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=316), )] (%723:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=315)]) -> (%724:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=316)])
            linalg.CPU.ConcatOp <name="model.layers.8.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=11), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=314), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=11), )] (%336:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=11)], %722:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=314)]) -> (%725:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=11)])
            linalg.CPU.ConcatOp <name="model.layers.8.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=39), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=316), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=39), )] (%337:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=39)], %724:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=316)]) -> (%726:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=39)])
            linalg.CPU.RepeatOp <name="model.layers.8.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=11), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=11), )] (%725:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=11)]) -> (%727:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=11)])
            linalg.CPU.RepeatOp <name="model.layers.8.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=39), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=39), )] (%726:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=39)]) -> (%728:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=39)])
            linalg.CPU.MatMulOp <name="model.layers.8.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=311), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=11), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=317), )] (%718:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=311)], %727:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=11)]) -> (%729:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=317)])
            linalg.CPU.MulOp <name="model.layers.8.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=317), inputs_1:QuantSpec(Raw(type: Float32), uuid=318), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=317), )] (%729:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=317)], %730:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=318), constant:[0.088388346]]) -> (%731:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=317)])
            linalg.CPU.ReduceMinOp <name="model.layers.8.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=317), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=319), )] (%731:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=317)]) -> (%732:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=319)])
            linalg.CPU.AddOp <name="model.layers.8.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=319), inputs_1:QuantSpec(Raw(type: Int16), uuid=320), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=319), )] (%732:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=319)], %733:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=320), constant:[-20]]) -> (%734:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=319)])
            linalg.CPU.EqualOp <name="model.layers.8.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=321), outputs_0:QuantSpec(Raw(type: UInt8), uuid=322), )] (%319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %735:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=321), constant:[1]]) -> (%736:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=322)])
            linalg.CPU.WhereOp <name="model.layers.8.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=322), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=317), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=319), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=319), )] (%736:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=322)], %731:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=317)], %734:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=319)]) -> (%737:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=319)])
            linalg.CPU.SoftmaxOp <name="model.layers.8.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=319), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=323), )] (%737:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=319)]) -> (%738:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=323)])
            linalg.CPU.MatMulOp <name="model.layers.8.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=323), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=39), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=324), )] (%738:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=323)], %728:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=39)]) -> (%739:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=324)])
            linalg.CPU.TransposeOp <name="model.layers.8.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=324), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=324), )] (%739:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=324)]) -> (%740:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=324)])
            linalg.CPU.ViewOp <name="model.layers.8.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=324), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=324), )] (%740:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=324)]) -> (%740:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=324)])
            linalg.CPU.LinearOp <name="model.layers.8.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=324), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=326), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=325))] (%740:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=324)]) -> (%741:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=326)])
            cf.ReturnOp (%741:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=326)], %722:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=314)], %724:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=316)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.8.mlp <CPU> [using_qnn:true, symbol:model.layers.8.mlp] {
        (%743:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=327)]) -> (%748:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=334)]) {
            linalg.CPU.LinearOp <name="model.layers.8.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=327), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=329), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=328))] (%743:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=327)]) -> (%744:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=329)])
            linalg.CPU.SiLUOp <name="model.layers.8.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=329), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=330), )] (%744:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=329)]) -> (%745:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=330)])
            linalg.CPU.LinearOp <name="model.layers.8.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=327), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=332), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=331))] (%743:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=327)]) -> (%746:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=332)])
            linalg.CPU.MulOp <name="model.layers.8.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=330), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=332), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=330), )] (%745:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=330)], %746:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=332)]) -> (%747:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=330)])
            linalg.CPU.LinearOp <name="model.layers.8.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=330), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=334), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=333))] (%747:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=330)]) -> (%748:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=334)])
            cf.ReturnOp (%748:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=334)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.9 <CPU> [using_qnn:true, symbol:model.layers.9] {
        (%749:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=334)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %338:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=12)], %339:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=40)]) -> (%790:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=364)], %763:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=344)], %765:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=346)]) {
            linalg.CPU.RMSNormOp <name="model.layers.9.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=334), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=335), )] (%749:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=334)]) -> (%750:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=335)])
            graph.CallGraphOp @model.layers.9.self_attn (%750:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=335)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %338:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=12)], %339:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=40)]) -> (%782:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=356)], %763:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=344)], %765:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=346)])
            linalg.CPU.AddOp <name="model.layers.9.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=356), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=334), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=356), )] (%782:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=356)], %749:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=334)]) -> (%783:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=356)])
            linalg.CPU.RMSNormOp <name="model.layers.9.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=356), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=357), )] (%783:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=356)]) -> (%784:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=357)])
            graph.CallGraphOp @model.layers.9.mlp (%784:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=357)]) -> (%789:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=364)])
            linalg.CPU.AddOp <name="model.layers.9.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=364), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=356), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=364), )] (%789:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=364)], %783:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=356)]) -> (%790:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=364)])
            cf.ReturnOp (%790:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=364)], %763:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=344)], %765:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=346)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.9.self_attn <CPU> [using_qnn:true, symbol:model.layers.9.self_attn] {
        (%750:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=335)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %338:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=12)], %339:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=40)]) -> (%782:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=356)], %763:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=344)], %765:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=346)]) {
            linalg.CPU.LinearOp <name="model.layers.9.self_attn.q_proj">(%750:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=335)]) -> (%751:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=340)])
            linalg.CPU.LinearOp <name="model.layers.9.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=335), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=337), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=336))] (%750:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=335)]) -> (%752:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=337)])
            linalg.CPU.LinearOp <name="model.layers.9.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=335), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=339), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=338))] (%750:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=335)]) -> (%753:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=339)])
            linalg.CPU.ViewOp <name="model.layers.9.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=340), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=340), )] (%751:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=340)]) -> (%751:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=340)])
            linalg.CPU.TransposeOp <name="model.layers.9.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=340), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=340), )] (%751:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=340)]) -> (%754:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=340)])
            linalg.CPU.ViewOp <name="model.layers.9.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=337), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=337), )] (%752:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=337)]) -> (%752:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=337)])
            linalg.CPU.TransposeOp <name="model.layers.9.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=337), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=337), )] (%752:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=337)]) -> (%755:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=337)])
            linalg.CPU.ViewOp <name="model.layers.9.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=339), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=339), )] (%753:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=339)]) -> (%753:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=339)])
            linalg.CPU.TransposeOp <name="model.layers.9.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=339), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=339), )] (%753:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=339)]) -> (%756:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=339)])
            linalg.CPU.RMSNormOp <name="model.layers.9.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=340), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=341), )] (%754:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=340)]) -> (%757:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=341)])
            linalg.CPU.RMSNormOp <name="model.layers.9.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=337), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=342), )] (%755:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=337)]) -> (%758:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=342)])
            linalg.CPU.RoPEOp <name="model.layers.9.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=341), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=341), )] (%757:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=341)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%759:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=341)])
            linalg.CPU.RoPEOp <name="model.layers.9.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=342), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=342), )] (%758:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=342)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%760:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=342)])
            linalg.CPU.CastTypeOp <name="model.layers.9.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=342), outputs_0:QuantSpec(Raw(type: Float16), uuid=343), )] (%760:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=342)]) -> (%761:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=343)])
            linalg.CPU.CastTypeOp <name="model.layers.9.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=343), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=344), )] (%761:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=343)]) -> (%762:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=344)])
            linalg.CPU.TransposeOp <name="model.layers.9.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=344), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=344), )] (%762:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=344)]) -> (%763:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=344)])
            linalg.CPU.CastTypeOp <name="model.layers.9.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=339), outputs_0:QuantSpec(Raw(type: Float16), uuid=345), )] (%756:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=339)]) -> (%764:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=345)])
            linalg.CPU.CastTypeOp <name="model.layers.9.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=345), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=346), )] (%764:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=345)]) -> (%765:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=346)])
            linalg.CPU.ConcatOp <name="model.layers.9.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=12), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=344), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=12), )] (%338:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=12)], %763:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=344)]) -> (%766:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=12)])
            linalg.CPU.ConcatOp <name="model.layers.9.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=40), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=346), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=40), )] (%339:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=40)], %765:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=346)]) -> (%767:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=40)])
            linalg.CPU.RepeatOp <name="model.layers.9.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=12), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=12), )] (%766:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=12)]) -> (%768:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=12)])
            linalg.CPU.RepeatOp <name="model.layers.9.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=40), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=40), )] (%767:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=40)]) -> (%769:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=40)])
            linalg.CPU.MatMulOp <name="model.layers.9.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=341), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=12), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=347), )] (%759:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=341)], %768:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=12)]) -> (%770:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=347)])
            linalg.CPU.MulOp <name="model.layers.9.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=347), inputs_1:QuantSpec(Raw(type: Float32), uuid=348), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=347), )] (%770:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=347)], %771:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=348), constant:[0.088388346]]) -> (%772:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=347)])
            linalg.CPU.ReduceMinOp <name="model.layers.9.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=347), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=349), )] (%772:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=347)]) -> (%773:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=349)])
            linalg.CPU.AddOp <name="model.layers.9.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=349), inputs_1:QuantSpec(Raw(type: Int16), uuid=350), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=349), )] (%773:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=349)], %774:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=350), constant:[-20]]) -> (%775:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=349)])
            linalg.CPU.EqualOp <name="model.layers.9.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=351), outputs_0:QuantSpec(Raw(type: UInt8), uuid=352), )] (%319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %776:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=351), constant:[-0.1796875]]) -> (%777:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=352)])
            linalg.CPU.WhereOp <name="model.layers.9.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=352), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=347), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=349), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=349), )] (%777:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=352)], %772:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=347)], %775:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=349)]) -> (%778:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=349)])
            linalg.CPU.SoftmaxOp <name="model.layers.9.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=349), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=353), )] (%778:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=349)]) -> (%779:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=353)])
            linalg.CPU.MatMulOp <name="model.layers.9.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=353), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=40), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=354), )] (%779:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=353)], %769:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=40)]) -> (%780:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=354)])
            linalg.CPU.TransposeOp <name="model.layers.9.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=354), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=354), )] (%780:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=354)]) -> (%781:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=354)])
            linalg.CPU.ViewOp <name="model.layers.9.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=354), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=354), )] (%781:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=354)]) -> (%781:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=354)])
            linalg.CPU.LinearOp <name="model.layers.9.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=354), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=356), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=355))] (%781:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=354)]) -> (%782:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=356)])
            cf.ReturnOp (%782:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=356)], %763:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=344)], %765:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=346)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.9.mlp <CPU> [using_qnn:true, symbol:model.layers.9.mlp] {
        (%784:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=357)]) -> (%789:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=364)]) {
            linalg.CPU.LinearOp <name="model.layers.9.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=357), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=359), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=358))] (%784:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=357)]) -> (%785:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=359)])
            linalg.CPU.SiLUOp <name="model.layers.9.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=359), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=360), )] (%785:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=359)]) -> (%786:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=360)])
            linalg.CPU.LinearOp <name="model.layers.9.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=357), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=362), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=361))] (%784:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=357)]) -> (%787:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=362)])
            linalg.CPU.MulOp <name="model.layers.9.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=360), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=362), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=360), )] (%786:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=360)], %787:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=362)]) -> (%788:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=360)])
            linalg.CPU.LinearOp <name="model.layers.9.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=360), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=364), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=363))] (%788:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=360)]) -> (%789:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=364)])
            cf.ReturnOp (%789:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=364)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.10 <CPU> [using_qnn:true, symbol:model.layers.10] {
        (%790:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=364)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %340:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=13)], %341:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=41)]) -> (%831:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=394)], %804:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=374)], %806:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=376)]) {
            linalg.CPU.RMSNormOp <name="model.layers.10.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=364), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=365), )] (%790:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=364)]) -> (%791:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=365)])
            graph.CallGraphOp @model.layers.10.self_attn (%791:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=365)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %340:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=13)], %341:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=41)]) -> (%823:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=386)], %804:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=374)], %806:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=376)])
            linalg.CPU.AddOp <name="model.layers.10.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=386), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=364), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=386), )] (%823:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=386)], %790:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=364)]) -> (%824:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=386)])
            linalg.CPU.RMSNormOp <name="model.layers.10.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=386), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=387), )] (%824:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=386)]) -> (%825:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=387)])
            graph.CallGraphOp @model.layers.10.mlp (%825:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=387)]) -> (%830:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=394)])
            linalg.CPU.AddOp <name="model.layers.10.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=394), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=386), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=394), )] (%830:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=394)], %824:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=386)]) -> (%831:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=394)])
            cf.ReturnOp (%831:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=394)], %804:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=374)], %806:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=376)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.10.self_attn <CPU> [using_qnn:true, symbol:model.layers.10.self_attn] {
        (%791:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=365)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %340:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=13)], %341:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=41)]) -> (%823:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=386)], %804:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=374)], %806:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=376)]) {
            linalg.CPU.LinearOp <name="model.layers.10.self_attn.q_proj">(%791:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=365)]) -> (%792:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=370)])
            linalg.CPU.LinearOp <name="model.layers.10.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=365), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=367), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=366))] (%791:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=365)]) -> (%793:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=367)])
            linalg.CPU.LinearOp <name="model.layers.10.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=365), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=369), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=368))] (%791:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=365)]) -> (%794:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=369)])
            linalg.CPU.ViewOp <name="model.layers.10.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=370), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=370), )] (%792:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=370)]) -> (%792:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=370)])
            linalg.CPU.TransposeOp <name="model.layers.10.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=370), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=370), )] (%792:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=370)]) -> (%795:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=370)])
            linalg.CPU.ViewOp <name="model.layers.10.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=367), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=367), )] (%793:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=367)]) -> (%793:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=367)])
            linalg.CPU.TransposeOp <name="model.layers.10.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=367), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=367), )] (%793:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=367)]) -> (%796:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=367)])
            linalg.CPU.ViewOp <name="model.layers.10.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=369), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=369), )] (%794:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=369)]) -> (%794:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=369)])
            linalg.CPU.TransposeOp <name="model.layers.10.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=369), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=369), )] (%794:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=369)]) -> (%797:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=369)])
            linalg.CPU.RMSNormOp <name="model.layers.10.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=370), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=371), )] (%795:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=370)]) -> (%798:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=371)])
            linalg.CPU.RMSNormOp <name="model.layers.10.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=367), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=372), )] (%796:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=367)]) -> (%799:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=372)])
            linalg.CPU.RoPEOp <name="model.layers.10.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=371), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=371), )] (%798:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=371)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%800:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=371)])
            linalg.CPU.RoPEOp <name="model.layers.10.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=372), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=372), )] (%799:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=372)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%801:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=372)])
            linalg.CPU.CastTypeOp <name="model.layers.10.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=372), outputs_0:QuantSpec(Raw(type: Float16), uuid=373), )] (%801:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=372)]) -> (%802:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=373)])
            linalg.CPU.CastTypeOp <name="model.layers.10.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=373), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=374), )] (%802:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=373)]) -> (%803:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=374)])
            linalg.CPU.TransposeOp <name="model.layers.10.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=374), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=374), )] (%803:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=374)]) -> (%804:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=374)])
            linalg.CPU.CastTypeOp <name="model.layers.10.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=369), outputs_0:QuantSpec(Raw(type: Float16), uuid=375), )] (%797:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=369)]) -> (%805:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=375)])
            linalg.CPU.CastTypeOp <name="model.layers.10.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=375), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=376), )] (%805:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=375)]) -> (%806:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=376)])
            linalg.CPU.ConcatOp <name="model.layers.10.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=13), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=374), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=13), )] (%340:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=13)], %804:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=374)]) -> (%807:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=13)])
            linalg.CPU.ConcatOp <name="model.layers.10.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=41), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=376), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=41), )] (%341:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=41)], %806:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=376)]) -> (%808:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=41)])
            linalg.CPU.RepeatOp <name="model.layers.10.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=13), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=13), )] (%807:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=13)]) -> (%809:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=13)])
            linalg.CPU.RepeatOp <name="model.layers.10.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=41), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=41), )] (%808:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=41)]) -> (%810:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=41)])
            linalg.CPU.MatMulOp <name="model.layers.10.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=371), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=13), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=377), )] (%800:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=371)], %809:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=13)]) -> (%811:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=377)])
            linalg.CPU.MulOp <name="model.layers.10.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=377), inputs_1:QuantSpec(Raw(type: Float32), uuid=378), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=377), )] (%811:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=377)], %812:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=378), constant:[0.088388346]]) -> (%813:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=377)])
            linalg.CPU.ReduceMinOp <name="model.layers.10.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=377), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=379), )] (%813:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=377)]) -> (%814:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=379)])
            linalg.CPU.AddOp <name="model.layers.10.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=379), inputs_1:QuantSpec(Raw(type: Int16), uuid=380), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=379), )] (%814:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=379)], %815:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=380), constant:[-20]]) -> (%816:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=379)])
            linalg.CPU.EqualOp <name="model.layers.10.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=381), outputs_0:QuantSpec(Raw(type: UInt8), uuid=382), )] (%319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %817:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=381), constant:[-0.93359375]]) -> (%818:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=382)])
            linalg.CPU.WhereOp <name="model.layers.10.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=382), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=377), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=379), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=379), )] (%818:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=382)], %813:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=377)], %816:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=379)]) -> (%819:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=379)])
            linalg.CPU.SoftmaxOp <name="model.layers.10.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=379), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=383), )] (%819:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=379)]) -> (%820:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=383)])
            linalg.CPU.MatMulOp <name="model.layers.10.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=383), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=41), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=384), )] (%820:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=383)], %810:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=41)]) -> (%821:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=384)])
            linalg.CPU.TransposeOp <name="model.layers.10.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=384), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=384), )] (%821:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=384)]) -> (%822:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=384)])
            linalg.CPU.ViewOp <name="model.layers.10.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=384), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=384), )] (%822:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=384)]) -> (%822:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=384)])
            linalg.CPU.LinearOp <name="model.layers.10.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=384), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=386), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=385))] (%822:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=384)]) -> (%823:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=386)])
            cf.ReturnOp (%823:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=386)], %804:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=374)], %806:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=376)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.10.mlp <CPU> [using_qnn:true, symbol:model.layers.10.mlp] {
        (%825:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=387)]) -> (%830:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=394)]) {
            linalg.CPU.LinearOp <name="model.layers.10.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=387), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=389), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=388))] (%825:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=387)]) -> (%826:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=389)])
            linalg.CPU.SiLUOp <name="model.layers.10.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=389), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=390), )] (%826:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=389)]) -> (%827:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=390)])
            linalg.CPU.LinearOp <name="model.layers.10.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=387), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=392), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=391))] (%825:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=387)]) -> (%828:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=392)])
            linalg.CPU.MulOp <name="model.layers.10.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=390), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=392), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=390), )] (%827:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=390)], %828:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=392)]) -> (%829:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=390)])
            linalg.CPU.LinearOp <name="model.layers.10.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=390), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=394), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=393))] (%829:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=390)]) -> (%830:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=394)])
            cf.ReturnOp (%830:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=394)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.11 <CPU> [using_qnn:true, symbol:model.layers.11] {
        (%831:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=394)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %342:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=14)], %343:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=42)]) -> (%872:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=424)], %845:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=404)], %847:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=406)]) {
            linalg.CPU.RMSNormOp <name="model.layers.11.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=394), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=395), )] (%831:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=394)]) -> (%832:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=395)])
            graph.CallGraphOp @model.layers.11.self_attn (%832:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=395)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %342:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=14)], %343:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=42)]) -> (%864:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=416)], %845:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=404)], %847:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=406)])
            linalg.CPU.AddOp <name="model.layers.11.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=416), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=394), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=416), )] (%864:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=416)], %831:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=394)]) -> (%865:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=416)])
            linalg.CPU.RMSNormOp <name="model.layers.11.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=416), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=417), )] (%865:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=416)]) -> (%866:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=417)])
            graph.CallGraphOp @model.layers.11.mlp (%866:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=417)]) -> (%871:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=424)])
            linalg.CPU.AddOp <name="model.layers.11.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=424), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=416), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=424), )] (%871:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=424)], %865:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=416)]) -> (%872:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=424)])
            cf.ReturnOp (%872:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=424)], %845:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=404)], %847:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=406)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.11.self_attn <CPU> [using_qnn:true, symbol:model.layers.11.self_attn] {
        (%832:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=395)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %342:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=14)], %343:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=42)]) -> (%864:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=416)], %845:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=404)], %847:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=406)]) {
            linalg.CPU.LinearOp <name="model.layers.11.self_attn.q_proj">(%832:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=395)]) -> (%833:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=400)])
            linalg.CPU.LinearOp <name="model.layers.11.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=395), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=397), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=396))] (%832:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=395)]) -> (%834:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=397)])
            linalg.CPU.LinearOp <name="model.layers.11.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=395), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=399), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=398))] (%832:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=395)]) -> (%835:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=399)])
            linalg.CPU.ViewOp <name="model.layers.11.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=400), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=400), )] (%833:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=400)]) -> (%833:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=400)])
            linalg.CPU.TransposeOp <name="model.layers.11.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=400), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=400), )] (%833:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=400)]) -> (%836:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=400)])
            linalg.CPU.ViewOp <name="model.layers.11.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=397), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=397), )] (%834:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=397)]) -> (%834:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=397)])
            linalg.CPU.TransposeOp <name="model.layers.11.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=397), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=397), )] (%834:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=397)]) -> (%837:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=397)])
            linalg.CPU.ViewOp <name="model.layers.11.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=399), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=399), )] (%835:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=399)]) -> (%835:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=399)])
            linalg.CPU.TransposeOp <name="model.layers.11.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=399), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=399), )] (%835:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=399)]) -> (%838:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=399)])
            linalg.CPU.RMSNormOp <name="model.layers.11.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=400), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=401), )] (%836:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=400)]) -> (%839:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=401)])
            linalg.CPU.RMSNormOp <name="model.layers.11.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=397), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=402), )] (%837:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=397)]) -> (%840:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=402)])
            linalg.CPU.RoPEOp <name="model.layers.11.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=401), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=401), )] (%839:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=401)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%841:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=401)])
            linalg.CPU.RoPEOp <name="model.layers.11.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=402), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=402), )] (%840:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=402)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%842:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=402)])
            linalg.CPU.CastTypeOp <name="model.layers.11.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=402), outputs_0:QuantSpec(Raw(type: Float16), uuid=403), )] (%842:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=402)]) -> (%843:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=403)])
            linalg.CPU.CastTypeOp <name="model.layers.11.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=403), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=404), )] (%843:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=403)]) -> (%844:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=404)])
            linalg.CPU.TransposeOp <name="model.layers.11.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=404), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=404), )] (%844:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=404)]) -> (%845:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=404)])
            linalg.CPU.CastTypeOp <name="model.layers.11.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=399), outputs_0:QuantSpec(Raw(type: Float16), uuid=405), )] (%838:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=399)]) -> (%846:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=405)])
            linalg.CPU.CastTypeOp <name="model.layers.11.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=405), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=406), )] (%846:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=405)]) -> (%847:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=406)])
            linalg.CPU.ConcatOp <name="model.layers.11.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=14), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=404), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=14), )] (%342:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=14)], %845:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=404)]) -> (%848:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=14)])
            linalg.CPU.ConcatOp <name="model.layers.11.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=42), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=406), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=42), )] (%343:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=42)], %847:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=406)]) -> (%849:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=42)])
            linalg.CPU.RepeatOp <name="model.layers.11.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=14), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=14), )] (%848:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=14)]) -> (%850:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=14)])
            linalg.CPU.RepeatOp <name="model.layers.11.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=42), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=42), )] (%849:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=42)]) -> (%851:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=42)])
            linalg.CPU.MatMulOp <name="model.layers.11.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=401), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=14), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=407), )] (%841:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=401)], %850:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=14)]) -> (%852:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=407)])
            linalg.CPU.MulOp <name="model.layers.11.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=407), inputs_1:QuantSpec(Raw(type: Float32), uuid=408), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=407), )] (%852:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=407)], %853:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=408), constant:[0.088388346]]) -> (%854:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=407)])
            linalg.CPU.ReduceMinOp <name="model.layers.11.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=407), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=409), )] (%854:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=407)]) -> (%855:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=409)])
            linalg.CPU.AddOp <name="model.layers.11.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=409), inputs_1:QuantSpec(Raw(type: Int16), uuid=410), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=409), )] (%855:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=409)], %856:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=410), constant:[-20]]) -> (%857:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=409)])
            linalg.CPU.EqualOp <name="model.layers.11.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=411), outputs_0:QuantSpec(Raw(type: UInt8), uuid=412), )] (%319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %858:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=411), constant:[0.515625]]) -> (%859:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=412)])
            linalg.CPU.WhereOp <name="model.layers.11.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=412), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=407), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=409), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=409), )] (%859:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=412)], %854:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=407)], %857:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=409)]) -> (%860:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=409)])
            linalg.CPU.SoftmaxOp <name="model.layers.11.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=409), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=413), )] (%860:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=409)]) -> (%861:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=413)])
            linalg.CPU.MatMulOp <name="model.layers.11.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=413), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=42), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=414), )] (%861:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=413)], %851:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=42)]) -> (%862:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=414)])
            linalg.CPU.TransposeOp <name="model.layers.11.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=414), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=414), )] (%862:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=414)]) -> (%863:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=414)])
            linalg.CPU.ViewOp <name="model.layers.11.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=414), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=414), )] (%863:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=414)]) -> (%863:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=414)])
            linalg.CPU.LinearOp <name="model.layers.11.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=414), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=416), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=415))] (%863:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=414)]) -> (%864:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=416)])
            cf.ReturnOp (%864:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=416)], %845:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=404)], %847:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=406)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.11.mlp <CPU> [using_qnn:true, symbol:model.layers.11.mlp] {
        (%866:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=417)]) -> (%871:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=424)]) {
            linalg.CPU.LinearOp <name="model.layers.11.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=417), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=419), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=418))] (%866:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=417)]) -> (%867:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=419)])
            linalg.CPU.SiLUOp <name="model.layers.11.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=419), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=420), )] (%867:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=419)]) -> (%868:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=420)])
            linalg.CPU.LinearOp <name="model.layers.11.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=417), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=422), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=421))] (%866:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=417)]) -> (%869:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=422)])
            linalg.CPU.MulOp <name="model.layers.11.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=420), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=422), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=420), )] (%868:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=420)], %869:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=422)]) -> (%870:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=420)])
            linalg.CPU.LinearOp <name="model.layers.11.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=420), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=424), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=423))] (%870:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=420)]) -> (%871:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=424)])
            cf.ReturnOp (%871:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=424)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.12 <CPU> [using_qnn:true, symbol:model.layers.12] {
        (%872:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=424)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %344:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=15)], %345:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=43)]) -> (%913:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=454)], %886:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=434)], %888:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=436)]) {
            linalg.CPU.RMSNormOp <name="model.layers.12.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=424), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=425), )] (%872:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=424)]) -> (%873:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=425)])
            graph.CallGraphOp @model.layers.12.self_attn (%873:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=425)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %344:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=15)], %345:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=43)]) -> (%905:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=446)], %886:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=434)], %888:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=436)])
            linalg.CPU.AddOp <name="model.layers.12.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=446), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=424), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=446), )] (%905:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=446)], %872:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=424)]) -> (%906:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=446)])
            linalg.CPU.RMSNormOp <name="model.layers.12.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=446), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=447), )] (%906:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=446)]) -> (%907:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=447)])
            graph.CallGraphOp @model.layers.12.mlp (%907:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=447)]) -> (%912:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=454)])
            linalg.CPU.AddOp <name="model.layers.12.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=454), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=446), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=454), )] (%912:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=454)], %906:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=446)]) -> (%913:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=454)])
            cf.ReturnOp (%913:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=454)], %886:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=434)], %888:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=436)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.12.self_attn <CPU> [using_qnn:true, symbol:model.layers.12.self_attn] {
        (%873:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=425)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %344:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=15)], %345:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=43)]) -> (%905:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=446)], %886:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=434)], %888:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=436)]) {
            linalg.CPU.LinearOp <name="model.layers.12.self_attn.q_proj">(%873:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=425)]) -> (%874:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=430)])
            linalg.CPU.LinearOp <name="model.layers.12.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=425), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=427), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=426))] (%873:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=425)]) -> (%875:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=427)])
            linalg.CPU.LinearOp <name="model.layers.12.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=425), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=429), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=428))] (%873:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=425)]) -> (%876:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=429)])
            linalg.CPU.ViewOp <name="model.layers.12.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=430), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=430), )] (%874:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=430)]) -> (%874:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=430)])
            linalg.CPU.TransposeOp <name="model.layers.12.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=430), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=430), )] (%874:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=430)]) -> (%877:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=430)])
            linalg.CPU.ViewOp <name="model.layers.12.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=427), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=427), )] (%875:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=427)]) -> (%875:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=427)])
            linalg.CPU.TransposeOp <name="model.layers.12.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=427), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=427), )] (%875:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=427)]) -> (%878:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=427)])
            linalg.CPU.ViewOp <name="model.layers.12.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=429), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=429), )] (%876:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=429)]) -> (%876:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=429)])
            linalg.CPU.TransposeOp <name="model.layers.12.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=429), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=429), )] (%876:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=429)]) -> (%879:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=429)])
            linalg.CPU.RMSNormOp <name="model.layers.12.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=430), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=431), )] (%877:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=430)]) -> (%880:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=431)])
            linalg.CPU.RMSNormOp <name="model.layers.12.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=427), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=432), )] (%878:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=427)]) -> (%881:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=432)])
            linalg.CPU.RoPEOp <name="model.layers.12.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=431), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=431), )] (%880:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=431)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%882:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=431)])
            linalg.CPU.RoPEOp <name="model.layers.12.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=432), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=432), )] (%881:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=432)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%883:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=432)])
            linalg.CPU.CastTypeOp <name="model.layers.12.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=432), outputs_0:QuantSpec(Raw(type: Float16), uuid=433), )] (%883:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=432)]) -> (%884:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=433)])
            linalg.CPU.CastTypeOp <name="model.layers.12.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=433), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=434), )] (%884:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=433)]) -> (%885:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=434)])
            linalg.CPU.TransposeOp <name="model.layers.12.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=434), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=434), )] (%885:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=434)]) -> (%886:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=434)])
            linalg.CPU.CastTypeOp <name="model.layers.12.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=429), outputs_0:QuantSpec(Raw(type: Float16), uuid=435), )] (%879:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=429)]) -> (%887:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=435)])
            linalg.CPU.CastTypeOp <name="model.layers.12.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=435), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=436), )] (%887:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=435)]) -> (%888:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=436)])
            linalg.CPU.ConcatOp <name="model.layers.12.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=15), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=434), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=15), )] (%344:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=15)], %886:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=434)]) -> (%889:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=15)])
            linalg.CPU.ConcatOp <name="model.layers.12.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=43), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=436), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=43), )] (%345:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=43)], %888:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=436)]) -> (%890:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=43)])
            linalg.CPU.RepeatOp <name="model.layers.12.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=15), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=15), )] (%889:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=15)]) -> (%891:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=15)])
            linalg.CPU.RepeatOp <name="model.layers.12.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=43), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=43), )] (%890:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=43)]) -> (%892:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=43)])
            linalg.CPU.MatMulOp <name="model.layers.12.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=431), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=15), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=437), )] (%882:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=431)], %891:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=15)]) -> (%893:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=437)])
            linalg.CPU.MulOp <name="model.layers.12.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=437), inputs_1:QuantSpec(Raw(type: Float32), uuid=438), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=437), )] (%893:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=437)], %894:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=438), constant:[0.088388346]]) -> (%895:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=437)])
            linalg.CPU.ReduceMinOp <name="model.layers.12.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=437), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=439), )] (%895:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=437)]) -> (%896:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=439)])
            linalg.CPU.AddOp <name="model.layers.12.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=439), inputs_1:QuantSpec(Raw(type: Int16), uuid=440), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=439), )] (%896:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=439)], %897:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=440), constant:[-20]]) -> (%898:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=439)])
            linalg.CPU.EqualOp <name="model.layers.12.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=441), outputs_0:QuantSpec(Raw(type: UInt8), uuid=442), )] (%319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %899:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=441), constant:[0.74609375]]) -> (%900:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=442)])
            linalg.CPU.WhereOp <name="model.layers.12.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=442), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=437), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=439), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=439), )] (%900:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=442)], %895:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=437)], %898:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=439)]) -> (%901:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=439)])
            linalg.CPU.SoftmaxOp <name="model.layers.12.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=439), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=443), )] (%901:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=439)]) -> (%902:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=443)])
            linalg.CPU.MatMulOp <name="model.layers.12.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=443), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=43), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=444), )] (%902:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=443)], %892:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=43)]) -> (%903:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=444)])
            linalg.CPU.TransposeOp <name="model.layers.12.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=444), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=444), )] (%903:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=444)]) -> (%904:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=444)])
            linalg.CPU.ViewOp <name="model.layers.12.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=444), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=444), )] (%904:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=444)]) -> (%904:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=444)])
            linalg.CPU.LinearOp <name="model.layers.12.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=444), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=446), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=445))] (%904:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=444)]) -> (%905:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=446)])
            cf.ReturnOp (%905:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=446)], %886:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=434)], %888:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=436)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.12.mlp <CPU> [using_qnn:true, symbol:model.layers.12.mlp] {
        (%907:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=447)]) -> (%912:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=454)]) {
            linalg.CPU.LinearOp <name="model.layers.12.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=447), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=449), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=448))] (%907:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=447)]) -> (%908:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=449)])
            linalg.CPU.SiLUOp <name="model.layers.12.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=449), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=450), )] (%908:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=449)]) -> (%909:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=450)])
            linalg.CPU.LinearOp <name="model.layers.12.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=447), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=452), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=451))] (%907:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=447)]) -> (%910:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=452)])
            linalg.CPU.MulOp <name="model.layers.12.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=450), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=452), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=450), )] (%909:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=450)], %910:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=452)]) -> (%911:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=450)])
            linalg.CPU.LinearOp <name="model.layers.12.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=450), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=454), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=453))] (%911:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=450)]) -> (%912:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=454)])
            cf.ReturnOp (%912:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=454)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.13 <CPU> [using_qnn:true, symbol:model.layers.13] {
        (%913:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=454)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %346:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=16)], %347:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=44)]) -> (%954:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=484)], %927:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=464)], %929:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=466)]) {
            linalg.CPU.RMSNormOp <name="model.layers.13.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=454), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=455), )] (%913:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=454)]) -> (%914:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=455)])
            graph.CallGraphOp @model.layers.13.self_attn (%914:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=455)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %346:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=16)], %347:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=44)]) -> (%946:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=476)], %927:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=464)], %929:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=466)])
            linalg.CPU.AddOp <name="model.layers.13.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=476), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=454), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=476), )] (%946:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=476)], %913:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=454)]) -> (%947:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=476)])
            linalg.CPU.RMSNormOp <name="model.layers.13.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=476), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=477), )] (%947:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=476)]) -> (%948:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=477)])
            graph.CallGraphOp @model.layers.13.mlp (%948:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=477)]) -> (%953:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=484)])
            linalg.CPU.AddOp <name="model.layers.13.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=484), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=476), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=484), )] (%953:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=484)], %947:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=476)]) -> (%954:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=484)])
            cf.ReturnOp (%954:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=484)], %927:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=464)], %929:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=466)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.13.self_attn <CPU> [using_qnn:true, symbol:model.layers.13.self_attn] {
        (%914:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=455)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %346:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=16)], %347:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=44)]) -> (%946:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=476)], %927:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=464)], %929:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=466)]) {
            linalg.CPU.LinearOp <name="model.layers.13.self_attn.q_proj">(%914:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=455)]) -> (%915:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=460)])
            linalg.CPU.LinearOp <name="model.layers.13.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=455), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=457), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=456))] (%914:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=455)]) -> (%916:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=457)])
            linalg.CPU.LinearOp <name="model.layers.13.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=455), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=459), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=458))] (%914:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=455)]) -> (%917:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=459)])
            linalg.CPU.ViewOp <name="model.layers.13.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=460), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=460), )] (%915:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=460)]) -> (%915:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=460)])
            linalg.CPU.TransposeOp <name="model.layers.13.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=460), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=460), )] (%915:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=460)]) -> (%918:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=460)])
            linalg.CPU.ViewOp <name="model.layers.13.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=457), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=457), )] (%916:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=457)]) -> (%916:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=457)])
            linalg.CPU.TransposeOp <name="model.layers.13.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=457), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=457), )] (%916:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=457)]) -> (%919:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=457)])
            linalg.CPU.ViewOp <name="model.layers.13.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=459), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=459), )] (%917:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=459)]) -> (%917:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=459)])
            linalg.CPU.TransposeOp <name="model.layers.13.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=459), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=459), )] (%917:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=459)]) -> (%920:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=459)])
            linalg.CPU.RMSNormOp <name="model.layers.13.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=460), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=461), )] (%918:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=460)]) -> (%921:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=461)])
            linalg.CPU.RMSNormOp <name="model.layers.13.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=457), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=462), )] (%919:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=457)]) -> (%922:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=462)])
            linalg.CPU.RoPEOp <name="model.layers.13.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=461), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=461), )] (%921:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=461)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%923:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=461)])
            linalg.CPU.RoPEOp <name="model.layers.13.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=462), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=462), )] (%922:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=462)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%924:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=462)])
            linalg.CPU.CastTypeOp <name="model.layers.13.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=462), outputs_0:QuantSpec(Raw(type: Float16), uuid=463), )] (%924:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=462)]) -> (%925:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=463)])
            linalg.CPU.CastTypeOp <name="model.layers.13.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=463), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=464), )] (%925:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=463)]) -> (%926:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=464)])
            linalg.CPU.TransposeOp <name="model.layers.13.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=464), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=464), )] (%926:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=464)]) -> (%927:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=464)])
            linalg.CPU.CastTypeOp <name="model.layers.13.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=459), outputs_0:QuantSpec(Raw(type: Float16), uuid=465), )] (%920:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=459)]) -> (%928:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=465)])
            linalg.CPU.CastTypeOp <name="model.layers.13.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=465), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=466), )] (%928:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=465)]) -> (%929:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=466)])
            linalg.CPU.ConcatOp <name="model.layers.13.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=16), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=464), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=16), )] (%346:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=16)], %927:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=464)]) -> (%930:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=16)])
            linalg.CPU.ConcatOp <name="model.layers.13.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=44), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=466), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=44), )] (%347:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=44)], %929:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=466)]) -> (%931:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=44)])
            linalg.CPU.RepeatOp <name="model.layers.13.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=16), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=16), )] (%930:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=16)]) -> (%932:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=16)])
            linalg.CPU.RepeatOp <name="model.layers.13.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=44), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=44), )] (%931:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=44)]) -> (%933:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=44)])
            linalg.CPU.MatMulOp <name="model.layers.13.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=461), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=16), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=467), )] (%923:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=461)], %932:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=16)]) -> (%934:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=467)])
            linalg.CPU.MulOp <name="model.layers.13.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=467), inputs_1:QuantSpec(Raw(type: Float32), uuid=468), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=467), )] (%934:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=467)], %935:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=468), constant:[0.088388346]]) -> (%936:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=467)])
            linalg.CPU.ReduceMinOp <name="model.layers.13.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=467), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=469), )] (%936:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=467)]) -> (%937:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=469)])
            linalg.CPU.AddOp <name="model.layers.13.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=469), inputs_1:QuantSpec(Raw(type: Int16), uuid=470), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=469), )] (%937:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=469)], %938:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=470), constant:[-20]]) -> (%939:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=469)])
            linalg.CPU.EqualOp <name="model.layers.13.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=471), outputs_0:QuantSpec(Raw(type: UInt8), uuid=472), )] (%319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %940:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=471), constant:[-0.78515625]]) -> (%941:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=472)])
            linalg.CPU.WhereOp <name="model.layers.13.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=472), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=467), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=469), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=469), )] (%941:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=472)], %936:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=467)], %939:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=469)]) -> (%942:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=469)])
            linalg.CPU.SoftmaxOp <name="model.layers.13.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=469), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=473), )] (%942:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=469)]) -> (%943:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=473)])
            linalg.CPU.MatMulOp <name="model.layers.13.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=473), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=44), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=474), )] (%943:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=473)], %933:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=44)]) -> (%944:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=474)])
            linalg.CPU.TransposeOp <name="model.layers.13.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=474), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=474), )] (%944:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=474)]) -> (%945:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=474)])
            linalg.CPU.ViewOp <name="model.layers.13.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=474), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=474), )] (%945:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=474)]) -> (%945:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=474)])
            linalg.CPU.LinearOp <name="model.layers.13.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=474), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=476), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=475))] (%945:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=474)]) -> (%946:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=476)])
            cf.ReturnOp (%946:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=476)], %927:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=464)], %929:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=466)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.13.mlp <CPU> [using_qnn:true, symbol:model.layers.13.mlp] {
        (%948:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=477)]) -> (%953:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=484)]) {
            linalg.CPU.LinearOp <name="model.layers.13.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=477), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=479), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=478))] (%948:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=477)]) -> (%949:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=479)])
            linalg.CPU.SiLUOp <name="model.layers.13.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=479), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=480), )] (%949:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=479)]) -> (%950:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=480)])
            linalg.CPU.LinearOp <name="model.layers.13.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=477), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=482), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=481))] (%948:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=477)]) -> (%951:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=482)])
            linalg.CPU.MulOp <name="model.layers.13.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=480), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=482), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=480), )] (%950:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=480)], %951:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=482)]) -> (%952:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=480)])
            linalg.CPU.LinearOp <name="model.layers.13.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=480), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=484), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=483))] (%952:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=480)]) -> (%953:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=484)])
            cf.ReturnOp (%953:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=484)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.14 <CPU> [using_qnn:true, symbol:model.layers.14] {
        (%954:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=484)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %348:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=17)], %349:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=45)]) -> (%995:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=514)], %968:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=494)], %970:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=496)]) {
            linalg.CPU.RMSNormOp <name="model.layers.14.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=484), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=485), )] (%954:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=484)]) -> (%955:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=485)])
            graph.CallGraphOp @model.layers.14.self_attn (%955:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=485)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %348:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=17)], %349:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=45)]) -> (%987:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=506)], %968:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=494)], %970:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=496)])
            linalg.CPU.AddOp <name="model.layers.14.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=506), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=484), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=506), )] (%987:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=506)], %954:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=484)]) -> (%988:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=506)])
            linalg.CPU.RMSNormOp <name="model.layers.14.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=506), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=507), )] (%988:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=506)]) -> (%989:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=507)])
            graph.CallGraphOp @model.layers.14.mlp (%989:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=507)]) -> (%994:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=514)])
            linalg.CPU.AddOp <name="model.layers.14.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=514), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=506), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=514), )] (%994:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=514)], %988:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=506)]) -> (%995:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=514)])
            cf.ReturnOp (%995:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=514)], %968:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=494)], %970:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=496)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.14.self_attn <CPU> [using_qnn:true, symbol:model.layers.14.self_attn] {
        (%955:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=485)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %348:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=17)], %349:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=45)]) -> (%987:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=506)], %968:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=494)], %970:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=496)]) {
            linalg.CPU.LinearOp <name="model.layers.14.self_attn.q_proj">(%955:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=485)]) -> (%956:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=490)])
            linalg.CPU.LinearOp <name="model.layers.14.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=485), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=487), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=486))] (%955:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=485)]) -> (%957:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=487)])
            linalg.CPU.LinearOp <name="model.layers.14.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=485), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=489), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=488))] (%955:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=485)]) -> (%958:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=489)])
            linalg.CPU.ViewOp <name="model.layers.14.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=490), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=490), )] (%956:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=490)]) -> (%956:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=490)])
            linalg.CPU.TransposeOp <name="model.layers.14.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=490), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=490), )] (%956:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=490)]) -> (%959:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=490)])
            linalg.CPU.ViewOp <name="model.layers.14.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=487), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=487), )] (%957:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=487)]) -> (%957:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=487)])
            linalg.CPU.TransposeOp <name="model.layers.14.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=487), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=487), )] (%957:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=487)]) -> (%960:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=487)])
            linalg.CPU.ViewOp <name="model.layers.14.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=489), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=489), )] (%958:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=489)]) -> (%958:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=489)])
            linalg.CPU.TransposeOp <name="model.layers.14.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=489), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=489), )] (%958:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=489)]) -> (%961:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=489)])
            linalg.CPU.RMSNormOp <name="model.layers.14.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=490), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=491), )] (%959:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=490)]) -> (%962:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=491)])
            linalg.CPU.RMSNormOp <name="model.layers.14.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=487), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=492), )] (%960:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=487)]) -> (%963:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=492)])
            linalg.CPU.RoPEOp <name="model.layers.14.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=491), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=491), )] (%962:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=491)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%964:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=491)])
            linalg.CPU.RoPEOp <name="model.layers.14.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=492), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=492), )] (%963:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=492)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%965:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=492)])
            linalg.CPU.CastTypeOp <name="model.layers.14.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=492), outputs_0:QuantSpec(Raw(type: Float16), uuid=493), )] (%965:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=492)]) -> (%966:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=493)])
            linalg.CPU.CastTypeOp <name="model.layers.14.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=493), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=494), )] (%966:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=493)]) -> (%967:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=494)])
            linalg.CPU.TransposeOp <name="model.layers.14.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=494), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=494), )] (%967:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=494)]) -> (%968:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=494)])
            linalg.CPU.CastTypeOp <name="model.layers.14.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=489), outputs_0:QuantSpec(Raw(type: Float16), uuid=495), )] (%961:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=489)]) -> (%969:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=495)])
            linalg.CPU.CastTypeOp <name="model.layers.14.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=495), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=496), )] (%969:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=495)]) -> (%970:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=496)])
            linalg.CPU.ConcatOp <name="model.layers.14.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=17), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=494), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=17), )] (%348:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=17)], %968:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=494)]) -> (%971:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=17)])
            linalg.CPU.ConcatOp <name="model.layers.14.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=45), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=496), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=45), )] (%349:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=45)], %970:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=496)]) -> (%972:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=45)])
            linalg.CPU.RepeatOp <name="model.layers.14.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=17), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=17), )] (%971:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=17)]) -> (%973:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=17)])
            linalg.CPU.RepeatOp <name="model.layers.14.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=45), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=45), )] (%972:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=45)]) -> (%974:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=45)])
            linalg.CPU.MatMulOp <name="model.layers.14.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=491), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=17), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=497), )] (%964:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=491)], %973:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=17)]) -> (%975:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=497)])
            linalg.CPU.MulOp <name="model.layers.14.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=497), inputs_1:QuantSpec(Raw(type: Float32), uuid=498), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=497), )] (%975:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=497)], %976:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=498), constant:[0.088388346]]) -> (%977:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=497)])
            linalg.CPU.ReduceMinOp <name="model.layers.14.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=497), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=499), )] (%977:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=497)]) -> (%978:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=499)])
            linalg.CPU.AddOp <name="model.layers.14.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=499), inputs_1:QuantSpec(Raw(type: Int16), uuid=500), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=499), )] (%978:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=499)], %979:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=500), constant:[-20]]) -> (%980:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=499)])
            linalg.CPU.EqualOp <name="model.layers.14.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=501), outputs_0:QuantSpec(Raw(type: UInt8), uuid=502), )] (%319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %981:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=501), constant:[-0.46289062]]) -> (%982:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=502)])
            linalg.CPU.WhereOp <name="model.layers.14.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=502), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=497), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=499), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=499), )] (%982:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=502)], %977:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=497)], %980:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=499)]) -> (%983:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=499)])
            linalg.CPU.SoftmaxOp <name="model.layers.14.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=499), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=503), )] (%983:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=499)]) -> (%984:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=503)])
            linalg.CPU.MatMulOp <name="model.layers.14.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=503), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=45), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=504), )] (%984:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=503)], %974:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=45)]) -> (%985:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=504)])
            linalg.CPU.TransposeOp <name="model.layers.14.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=504), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=504), )] (%985:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=504)]) -> (%986:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=504)])
            linalg.CPU.ViewOp <name="model.layers.14.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=504), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=504), )] (%986:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=504)]) -> (%986:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=504)])
            linalg.CPU.LinearOp <name="model.layers.14.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=504), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=506), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=505))] (%986:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=504)]) -> (%987:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=506)])
            cf.ReturnOp (%987:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=506)], %968:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=494)], %970:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=496)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.14.mlp <CPU> [using_qnn:true, symbol:model.layers.14.mlp] {
        (%989:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=507)]) -> (%994:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=514)]) {
            linalg.CPU.LinearOp <name="model.layers.14.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=507), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=509), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=508))] (%989:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=507)]) -> (%990:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=509)])
            linalg.CPU.SiLUOp <name="model.layers.14.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=509), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=510), )] (%990:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=509)]) -> (%991:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=510)])
            linalg.CPU.LinearOp <name="model.layers.14.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=507), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=512), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=511))] (%989:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=507)]) -> (%992:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=512)])
            linalg.CPU.MulOp <name="model.layers.14.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=510), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=512), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=510), )] (%991:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=510)], %992:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=512)]) -> (%993:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=510)])
            linalg.CPU.LinearOp <name="model.layers.14.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=510), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=514), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=513))] (%993:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=510)]) -> (%994:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=514)])
            cf.ReturnOp (%994:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=514)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.15 <CPU> [using_qnn:true, symbol:model.layers.15] {
        (%995:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=514)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %350:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=18)], %351:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=46)]) -> (%1036:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=544)], %1009:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=524)], %1011:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=526)]) {
            linalg.CPU.RMSNormOp <name="model.layers.15.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=514), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=515), )] (%995:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=514)]) -> (%996:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=515)])
            graph.CallGraphOp @model.layers.15.self_attn (%996:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=515)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %350:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=18)], %351:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=46)]) -> (%1028:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=536)], %1009:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=524)], %1011:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=526)])
            linalg.CPU.AddOp <name="model.layers.15.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=536), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=514), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=536), )] (%1028:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=536)], %995:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=514)]) -> (%1029:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=536)])
            linalg.CPU.RMSNormOp <name="model.layers.15.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=536), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=537), )] (%1029:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=536)]) -> (%1030:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=537)])
            graph.CallGraphOp @model.layers.15.mlp (%1030:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=537)]) -> (%1035:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=544)])
            linalg.CPU.AddOp <name="model.layers.15.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=544), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=536), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=544), )] (%1035:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=544)], %1029:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=536)]) -> (%1036:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=544)])
            cf.ReturnOp (%1036:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=544)], %1009:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=524)], %1011:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=526)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.15.self_attn <CPU> [using_qnn:true, symbol:model.layers.15.self_attn] {
        (%996:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=515)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %350:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=18)], %351:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=46)]) -> (%1028:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=536)], %1009:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=524)], %1011:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=526)]) {
            linalg.CPU.LinearOp <name="model.layers.15.self_attn.q_proj">(%996:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=515)]) -> (%997:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=520)])
            linalg.CPU.LinearOp <name="model.layers.15.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=515), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=517), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=516))] (%996:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=515)]) -> (%998:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=517)])
            linalg.CPU.LinearOp <name="model.layers.15.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=515), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=519), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=518))] (%996:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=515)]) -> (%999:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=519)])
            linalg.CPU.ViewOp <name="model.layers.15.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=520), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=520), )] (%997:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=520)]) -> (%997:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=520)])
            linalg.CPU.TransposeOp <name="model.layers.15.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=520), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=520), )] (%997:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=520)]) -> (%1000:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=520)])
            linalg.CPU.ViewOp <name="model.layers.15.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=517), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=517), )] (%998:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=517)]) -> (%998:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=517)])
            linalg.CPU.TransposeOp <name="model.layers.15.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=517), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=517), )] (%998:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=517)]) -> (%1001:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=517)])
            linalg.CPU.ViewOp <name="model.layers.15.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=519), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=519), )] (%999:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=519)]) -> (%999:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=519)])
            linalg.CPU.TransposeOp <name="model.layers.15.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=519), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=519), )] (%999:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=519)]) -> (%1002:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=519)])
            linalg.CPU.RMSNormOp <name="model.layers.15.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=520), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=521), )] (%1000:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=520)]) -> (%1003:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=521)])
            linalg.CPU.RMSNormOp <name="model.layers.15.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=517), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=522), )] (%1001:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=517)]) -> (%1004:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=522)])
            linalg.CPU.RoPEOp <name="model.layers.15.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=521), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=521), )] (%1003:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=521)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%1005:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=521)])
            linalg.CPU.RoPEOp <name="model.layers.15.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=522), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=522), )] (%1004:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=522)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%1006:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=522)])
            linalg.CPU.CastTypeOp <name="model.layers.15.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=522), outputs_0:QuantSpec(Raw(type: Float16), uuid=523), )] (%1006:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=522)]) -> (%1007:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=523)])
            linalg.CPU.CastTypeOp <name="model.layers.15.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=523), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=524), )] (%1007:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=523)]) -> (%1008:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=524)])
            linalg.CPU.TransposeOp <name="model.layers.15.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=524), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=524), )] (%1008:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=524)]) -> (%1009:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=524)])
            linalg.CPU.CastTypeOp <name="model.layers.15.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=519), outputs_0:QuantSpec(Raw(type: Float16), uuid=525), )] (%1002:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=519)]) -> (%1010:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=525)])
            linalg.CPU.CastTypeOp <name="model.layers.15.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=525), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=526), )] (%1010:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=525)]) -> (%1011:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=526)])
            linalg.CPU.ConcatOp <name="model.layers.15.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=18), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=524), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=18), )] (%350:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=18)], %1009:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=524)]) -> (%1012:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=18)])
            linalg.CPU.ConcatOp <name="model.layers.15.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=46), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=526), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=46), )] (%351:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=46)], %1011:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=526)]) -> (%1013:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=46)])
            linalg.CPU.RepeatOp <name="model.layers.15.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=18), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=18), )] (%1012:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=18)]) -> (%1014:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=18)])
            linalg.CPU.RepeatOp <name="model.layers.15.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=46), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=46), )] (%1013:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=46)]) -> (%1015:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=46)])
            linalg.CPU.MatMulOp <name="model.layers.15.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=521), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=18), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=527), )] (%1005:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=521)], %1014:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=18)]) -> (%1016:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=527)])
            linalg.CPU.MulOp <name="model.layers.15.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=527), inputs_1:QuantSpec(Raw(type: Float32), uuid=528), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=527), )] (%1016:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=527)], %1017:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=528), constant:[0.088388346]]) -> (%1018:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=527)])
            linalg.CPU.ReduceMinOp <name="model.layers.15.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=527), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=529), )] (%1018:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=527)]) -> (%1019:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=529)])
            linalg.CPU.AddOp <name="model.layers.15.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=529), inputs_1:QuantSpec(Raw(type: Int16), uuid=530), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=529), )] (%1019:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=529)], %1020:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=530), constant:[-20]]) -> (%1021:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=529)])
            linalg.CPU.EqualOp <name="model.layers.15.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=531), outputs_0:QuantSpec(Raw(type: UInt8), uuid=532), )] (%319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %1022:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=531), constant:[0.953125]]) -> (%1023:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=532)])
            linalg.CPU.WhereOp <name="model.layers.15.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=532), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=527), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=529), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=529), )] (%1023:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=532)], %1018:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=527)], %1021:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=529)]) -> (%1024:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=529)])
            linalg.CPU.SoftmaxOp <name="model.layers.15.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=529), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=533), )] (%1024:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=529)]) -> (%1025:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=533)])
            linalg.CPU.MatMulOp <name="model.layers.15.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=533), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=46), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=534), )] (%1025:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=533)], %1015:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=46)]) -> (%1026:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=534)])
            linalg.CPU.TransposeOp <name="model.layers.15.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=534), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=534), )] (%1026:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=534)]) -> (%1027:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=534)])
            linalg.CPU.ViewOp <name="model.layers.15.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=534), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=534), )] (%1027:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=534)]) -> (%1027:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=534)])
            linalg.CPU.LinearOp <name="model.layers.15.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=534), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=536), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=535))] (%1027:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=534)]) -> (%1028:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=536)])
            cf.ReturnOp (%1028:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=536)], %1009:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=524)], %1011:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=526)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.15.mlp <CPU> [using_qnn:true, symbol:model.layers.15.mlp] {
        (%1030:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=537)]) -> (%1035:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=544)]) {
            linalg.CPU.LinearOp <name="model.layers.15.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=537), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=539), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=538))] (%1030:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=537)]) -> (%1031:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=539)])
            linalg.CPU.SiLUOp <name="model.layers.15.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=539), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=540), )] (%1031:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=539)]) -> (%1032:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=540)])
            linalg.CPU.LinearOp <name="model.layers.15.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=537), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=542), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=541))] (%1030:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=537)]) -> (%1033:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=542)])
            linalg.CPU.MulOp <name="model.layers.15.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=540), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=542), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=540), )] (%1032:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=540)], %1033:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=542)]) -> (%1034:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=540)])
            linalg.CPU.LinearOp <name="model.layers.15.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=540), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=544), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=543))] (%1034:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=540)]) -> (%1035:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=544)])
            cf.ReturnOp (%1035:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=544)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.16 <CPU> [using_qnn:true, symbol:model.layers.16] {
        (%1036:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=544)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %352:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=19)], %353:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=47)]) -> (%1077:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=574)], %1050:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=554)], %1052:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=556)]) {
            linalg.CPU.RMSNormOp <name="model.layers.16.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=544), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=545), )] (%1036:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=544)]) -> (%1037:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=545)])
            graph.CallGraphOp @model.layers.16.self_attn (%1037:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=545)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %352:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=19)], %353:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=47)]) -> (%1069:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=566)], %1050:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=554)], %1052:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=556)])
            linalg.CPU.AddOp <name="model.layers.16.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=566), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=544), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=566), )] (%1069:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=566)], %1036:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=544)]) -> (%1070:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=566)])
            linalg.CPU.RMSNormOp <name="model.layers.16.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=566), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=567), )] (%1070:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=566)]) -> (%1071:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=567)])
            graph.CallGraphOp @model.layers.16.mlp (%1071:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=567)]) -> (%1076:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=574)])
            linalg.CPU.AddOp <name="model.layers.16.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=574), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=566), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=574), )] (%1076:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=574)], %1070:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=566)]) -> (%1077:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=574)])
            cf.ReturnOp (%1077:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=574)], %1050:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=554)], %1052:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=556)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.16.self_attn <CPU> [using_qnn:true, symbol:model.layers.16.self_attn] {
        (%1037:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=545)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %352:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=19)], %353:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=47)]) -> (%1069:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=566)], %1050:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=554)], %1052:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=556)]) {
            linalg.CPU.LinearOp <name="model.layers.16.self_attn.q_proj">(%1037:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=545)]) -> (%1038:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=550)])
            linalg.CPU.LinearOp <name="model.layers.16.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=545), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=547), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=546))] (%1037:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=545)]) -> (%1039:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=547)])
            linalg.CPU.LinearOp <name="model.layers.16.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=545), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=549), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=548))] (%1037:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=545)]) -> (%1040:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=549)])
            linalg.CPU.ViewOp <name="model.layers.16.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=550), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=550), )] (%1038:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=550)]) -> (%1038:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=550)])
            linalg.CPU.TransposeOp <name="model.layers.16.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=550), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=550), )] (%1038:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=550)]) -> (%1041:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=550)])
            linalg.CPU.ViewOp <name="model.layers.16.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=547), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=547), )] (%1039:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=547)]) -> (%1039:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=547)])
            linalg.CPU.TransposeOp <name="model.layers.16.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=547), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=547), )] (%1039:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=547)]) -> (%1042:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=547)])
            linalg.CPU.ViewOp <name="model.layers.16.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=549), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=549), )] (%1040:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=549)]) -> (%1040:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=549)])
            linalg.CPU.TransposeOp <name="model.layers.16.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=549), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=549), )] (%1040:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=549)]) -> (%1043:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=549)])
            linalg.CPU.RMSNormOp <name="model.layers.16.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=550), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=551), )] (%1041:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=550)]) -> (%1044:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=551)])
            linalg.CPU.RMSNormOp <name="model.layers.16.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=547), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=552), )] (%1042:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=547)]) -> (%1045:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=552)])
            linalg.CPU.RoPEOp <name="model.layers.16.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=551), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=551), )] (%1044:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=551)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%1046:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=551)])
            linalg.CPU.RoPEOp <name="model.layers.16.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=552), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=552), )] (%1045:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=552)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%1047:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=552)])
            linalg.CPU.CastTypeOp <name="model.layers.16.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=552), outputs_0:QuantSpec(Raw(type: Float16), uuid=553), )] (%1047:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=552)]) -> (%1048:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=553)])
            linalg.CPU.CastTypeOp <name="model.layers.16.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=553), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=554), )] (%1048:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=553)]) -> (%1049:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=554)])
            linalg.CPU.TransposeOp <name="model.layers.16.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=554), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=554), )] (%1049:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=554)]) -> (%1050:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=554)])
            linalg.CPU.CastTypeOp <name="model.layers.16.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=549), outputs_0:QuantSpec(Raw(type: Float16), uuid=555), )] (%1043:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=549)]) -> (%1051:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=555)])
            linalg.CPU.CastTypeOp <name="model.layers.16.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=555), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=556), )] (%1051:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=555)]) -> (%1052:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=556)])
            linalg.CPU.ConcatOp <name="model.layers.16.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=19), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=554), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=19), )] (%352:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=19)], %1050:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=554)]) -> (%1053:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=19)])
            linalg.CPU.ConcatOp <name="model.layers.16.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=47), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=556), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=47), )] (%353:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=47)], %1052:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=556)]) -> (%1054:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=47)])
            linalg.CPU.RepeatOp <name="model.layers.16.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=19), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=19), )] (%1053:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=19)]) -> (%1055:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=19)])
            linalg.CPU.RepeatOp <name="model.layers.16.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=47), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=47), )] (%1054:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=47)]) -> (%1056:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=47)])
            linalg.CPU.MatMulOp <name="model.layers.16.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=551), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=19), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=557), )] (%1046:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=551)], %1055:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=19)]) -> (%1057:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=557)])
            linalg.CPU.MulOp <name="model.layers.16.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=557), inputs_1:QuantSpec(Raw(type: Float32), uuid=558), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=557), )] (%1057:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=557)], %1058:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=558), constant:[0.088388346]]) -> (%1059:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=557)])
            linalg.CPU.ReduceMinOp <name="model.layers.16.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=557), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=559), )] (%1059:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=557)]) -> (%1060:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=559)])
            linalg.CPU.AddOp <name="model.layers.16.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=559), inputs_1:QuantSpec(Raw(type: Int16), uuid=560), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=559), )] (%1060:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=559)], %1061:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=560), constant:[-20]]) -> (%1062:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=559)])
            linalg.CPU.EqualOp <name="model.layers.16.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=561), outputs_0:QuantSpec(Raw(type: UInt8), uuid=562), )] (%319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %1063:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=561), constant:[0.118652344]]) -> (%1064:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=562)])
            linalg.CPU.WhereOp <name="model.layers.16.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=562), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=557), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=559), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=559), )] (%1064:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=562)], %1059:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=557)], %1062:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=559)]) -> (%1065:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=559)])
            linalg.CPU.SoftmaxOp <name="model.layers.16.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=559), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=563), )] (%1065:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=559)]) -> (%1066:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=563)])
            linalg.CPU.MatMulOp <name="model.layers.16.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=563), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=47), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=564), )] (%1066:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=563)], %1056:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=47)]) -> (%1067:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=564)])
            linalg.CPU.TransposeOp <name="model.layers.16.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=564), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=564), )] (%1067:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=564)]) -> (%1068:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=564)])
            linalg.CPU.ViewOp <name="model.layers.16.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=564), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=564), )] (%1068:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=564)]) -> (%1068:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=564)])
            linalg.CPU.LinearOp <name="model.layers.16.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=564), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=566), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=565))] (%1068:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=564)]) -> (%1069:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=566)])
            cf.ReturnOp (%1069:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=566)], %1050:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=554)], %1052:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=556)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.16.mlp <CPU> [using_qnn:true, symbol:model.layers.16.mlp] {
        (%1071:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=567)]) -> (%1076:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=574)]) {
            linalg.CPU.LinearOp <name="model.layers.16.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=567), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=569), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=568))] (%1071:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=567)]) -> (%1072:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=569)])
            linalg.CPU.SiLUOp <name="model.layers.16.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=569), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=570), )] (%1072:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=569)]) -> (%1073:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=570)])
            linalg.CPU.LinearOp <name="model.layers.16.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=567), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=572), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=571))] (%1071:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=567)]) -> (%1074:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=572)])
            linalg.CPU.MulOp <name="model.layers.16.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=570), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=572), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=570), )] (%1073:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=570)], %1074:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=572)]) -> (%1075:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=570)])
            linalg.CPU.LinearOp <name="model.layers.16.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=570), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=574), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=573))] (%1075:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=570)]) -> (%1076:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=574)])
            cf.ReturnOp (%1076:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=574)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.17 <CPU> [using_qnn:true, symbol:model.layers.17] {
        (%1077:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=574)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %354:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=20)], %355:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=48)]) -> (%1118:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=604)], %1091:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=584)], %1093:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=586)]) {
            linalg.CPU.RMSNormOp <name="model.layers.17.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=574), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=575), )] (%1077:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=574)]) -> (%1078:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=575)])
            graph.CallGraphOp @model.layers.17.self_attn (%1078:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=575)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %354:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=20)], %355:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=48)]) -> (%1110:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=596)], %1091:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=584)], %1093:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=586)])
            linalg.CPU.AddOp <name="model.layers.17.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=596), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=574), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=596), )] (%1110:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=596)], %1077:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=574)]) -> (%1111:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=596)])
            linalg.CPU.RMSNormOp <name="model.layers.17.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=596), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=597), )] (%1111:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=596)]) -> (%1112:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=597)])
            graph.CallGraphOp @model.layers.17.mlp (%1112:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=597)]) -> (%1117:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=604)])
            linalg.CPU.AddOp <name="model.layers.17.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=604), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=596), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=604), )] (%1117:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=604)], %1111:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=596)]) -> (%1118:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=604)])
            cf.ReturnOp (%1118:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=604)], %1091:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=584)], %1093:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=586)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.17.self_attn <CPU> [using_qnn:true, symbol:model.layers.17.self_attn] {
        (%1078:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=575)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %354:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=20)], %355:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=48)]) -> (%1110:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=596)], %1091:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=584)], %1093:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=586)]) {
            linalg.CPU.LinearOp <name="model.layers.17.self_attn.q_proj">(%1078:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=575)]) -> (%1079:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=580)])
            linalg.CPU.LinearOp <name="model.layers.17.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=575), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=577), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=576))] (%1078:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=575)]) -> (%1080:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=577)])
            linalg.CPU.LinearOp <name="model.layers.17.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=575), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=579), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=578))] (%1078:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=575)]) -> (%1081:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=579)])
            linalg.CPU.ViewOp <name="model.layers.17.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=580), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=580), )] (%1079:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=580)]) -> (%1079:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=580)])
            linalg.CPU.TransposeOp <name="model.layers.17.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=580), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=580), )] (%1079:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=580)]) -> (%1082:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=580)])
            linalg.CPU.ViewOp <name="model.layers.17.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=577), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=577), )] (%1080:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=577)]) -> (%1080:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=577)])
            linalg.CPU.TransposeOp <name="model.layers.17.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=577), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=577), )] (%1080:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=577)]) -> (%1083:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=577)])
            linalg.CPU.ViewOp <name="model.layers.17.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=579), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=579), )] (%1081:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=579)]) -> (%1081:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=579)])
            linalg.CPU.TransposeOp <name="model.layers.17.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=579), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=579), )] (%1081:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=579)]) -> (%1084:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=579)])
            linalg.CPU.RMSNormOp <name="model.layers.17.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=580), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=581), )] (%1082:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=580)]) -> (%1085:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=581)])
            linalg.CPU.RMSNormOp <name="model.layers.17.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=577), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=582), )] (%1083:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=577)]) -> (%1086:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=582)])
            linalg.CPU.RoPEOp <name="model.layers.17.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=581), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=581), )] (%1085:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=581)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%1087:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=581)])
            linalg.CPU.RoPEOp <name="model.layers.17.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=582), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=582), )] (%1086:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=582)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%1088:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=582)])
            linalg.CPU.CastTypeOp <name="model.layers.17.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=582), outputs_0:QuantSpec(Raw(type: Float16), uuid=583), )] (%1088:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=582)]) -> (%1089:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=583)])
            linalg.CPU.CastTypeOp <name="model.layers.17.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=583), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=584), )] (%1089:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=583)]) -> (%1090:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=584)])
            linalg.CPU.TransposeOp <name="model.layers.17.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=584), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=584), )] (%1090:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=584)]) -> (%1091:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=584)])
            linalg.CPU.CastTypeOp <name="model.layers.17.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=579), outputs_0:QuantSpec(Raw(type: Float16), uuid=585), )] (%1084:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=579)]) -> (%1092:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=585)])
            linalg.CPU.CastTypeOp <name="model.layers.17.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=585), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=586), )] (%1092:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=585)]) -> (%1093:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=586)])
            linalg.CPU.ConcatOp <name="model.layers.17.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=20), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=584), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=20), )] (%354:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=20)], %1091:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=584)]) -> (%1094:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=20)])
            linalg.CPU.ConcatOp <name="model.layers.17.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=48), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=586), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=48), )] (%355:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=48)], %1093:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=586)]) -> (%1095:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=48)])
            linalg.CPU.RepeatOp <name="model.layers.17.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=20), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=20), )] (%1094:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=20)]) -> (%1096:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=20)])
            linalg.CPU.RepeatOp <name="model.layers.17.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=48), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=48), )] (%1095:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=48)]) -> (%1097:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=48)])
            linalg.CPU.MatMulOp <name="model.layers.17.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=581), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=20), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=587), )] (%1087:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=581)], %1096:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=20)]) -> (%1098:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=587)])
            linalg.CPU.MulOp <name="model.layers.17.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=587), inputs_1:QuantSpec(Raw(type: Float32), uuid=588), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=587), )] (%1098:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=587)], %1099:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=588), constant:[0.088388346]]) -> (%1100:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=587)])
            linalg.CPU.ReduceMinOp <name="model.layers.17.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=587), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=589), )] (%1100:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=587)]) -> (%1101:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=589)])
            linalg.CPU.AddOp <name="model.layers.17.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=589), inputs_1:QuantSpec(Raw(type: Int16), uuid=590), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=589), )] (%1101:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=589)], %1102:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=590), constant:[-20]]) -> (%1103:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=589)])
            linalg.CPU.EqualOp <name="model.layers.17.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=591), outputs_0:QuantSpec(Raw(type: UInt8), uuid=592), )] (%319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %1104:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=591), constant:[-0.99609375]]) -> (%1105:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=592)])
            linalg.CPU.WhereOp <name="model.layers.17.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=592), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=587), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=589), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=589), )] (%1105:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=592)], %1100:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=587)], %1103:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=589)]) -> (%1106:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=589)])
            linalg.CPU.SoftmaxOp <name="model.layers.17.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=589), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=593), )] (%1106:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=589)]) -> (%1107:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=593)])
            linalg.CPU.MatMulOp <name="model.layers.17.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=593), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=48), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=594), )] (%1107:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=593)], %1097:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=48)]) -> (%1108:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=594)])
            linalg.CPU.TransposeOp <name="model.layers.17.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=594), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=594), )] (%1108:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=594)]) -> (%1109:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=594)])
            linalg.CPU.ViewOp <name="model.layers.17.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=594), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=594), )] (%1109:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=594)]) -> (%1109:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=594)])
            linalg.CPU.LinearOp <name="model.layers.17.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=594), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=596), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=595))] (%1109:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=594)]) -> (%1110:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=596)])
            cf.ReturnOp (%1110:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=596)], %1091:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=584)], %1093:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=586)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.17.mlp <CPU> [using_qnn:true, symbol:model.layers.17.mlp] {
        (%1112:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=597)]) -> (%1117:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=604)]) {
            linalg.CPU.LinearOp <name="model.layers.17.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=597), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=599), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=598))] (%1112:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=597)]) -> (%1113:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=599)])
            linalg.CPU.SiLUOp <name="model.layers.17.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=599), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=600), )] (%1113:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=599)]) -> (%1114:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=600)])
            linalg.CPU.LinearOp <name="model.layers.17.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=597), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=602), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=601))] (%1112:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=597)]) -> (%1115:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=602)])
            linalg.CPU.MulOp <name="model.layers.17.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=600), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=602), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=600), )] (%1114:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=600)], %1115:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=602)]) -> (%1116:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=600)])
            linalg.CPU.LinearOp <name="model.layers.17.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=600), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=604), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=603))] (%1116:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=600)]) -> (%1117:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=604)])
            cf.ReturnOp (%1117:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=604)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.18 <CPU> [using_qnn:true, symbol:model.layers.18] {
        (%1118:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=604)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %356:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=21)], %357:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=49)]) -> (%1159:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=634)], %1132:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=614)], %1134:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=616)]) {
            linalg.CPU.RMSNormOp <name="model.layers.18.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=604), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=605), )] (%1118:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=604)]) -> (%1119:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=605)])
            graph.CallGraphOp @model.layers.18.self_attn (%1119:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=605)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %356:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=21)], %357:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=49)]) -> (%1151:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=626)], %1132:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=614)], %1134:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=616)])
            linalg.CPU.AddOp <name="model.layers.18.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=626), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=604), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=626), )] (%1151:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=626)], %1118:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=604)]) -> (%1152:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=626)])
            linalg.CPU.RMSNormOp <name="model.layers.18.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=626), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=627), )] (%1152:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=626)]) -> (%1153:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=627)])
            graph.CallGraphOp @model.layers.18.mlp (%1153:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=627)]) -> (%1158:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=634)])
            linalg.CPU.AddOp <name="model.layers.18.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=634), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=626), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=634), )] (%1158:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=634)], %1152:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=626)]) -> (%1159:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=634)])
            cf.ReturnOp (%1159:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=634)], %1132:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=614)], %1134:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=616)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.18.self_attn <CPU> [using_qnn:true, symbol:model.layers.18.self_attn] {
        (%1119:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=605)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %356:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=21)], %357:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=49)]) -> (%1151:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=626)], %1132:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=614)], %1134:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=616)]) {
            linalg.CPU.LinearOp <name="model.layers.18.self_attn.q_proj">(%1119:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=605)]) -> (%1120:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=610)])
            linalg.CPU.LinearOp <name="model.layers.18.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=605), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=607), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=606))] (%1119:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=605)]) -> (%1121:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=607)])
            linalg.CPU.LinearOp <name="model.layers.18.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=605), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=609), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=608))] (%1119:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=605)]) -> (%1122:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=609)])
            linalg.CPU.ViewOp <name="model.layers.18.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=610), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=610), )] (%1120:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=610)]) -> (%1120:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=610)])
            linalg.CPU.TransposeOp <name="model.layers.18.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=610), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=610), )] (%1120:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=610)]) -> (%1123:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=610)])
            linalg.CPU.ViewOp <name="model.layers.18.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=607), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=607), )] (%1121:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=607)]) -> (%1121:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=607)])
            linalg.CPU.TransposeOp <name="model.layers.18.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=607), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=607), )] (%1121:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=607)]) -> (%1124:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=607)])
            linalg.CPU.ViewOp <name="model.layers.18.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=609), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=609), )] (%1122:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=609)]) -> (%1122:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=609)])
            linalg.CPU.TransposeOp <name="model.layers.18.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=609), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=609), )] (%1122:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=609)]) -> (%1125:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=609)])
            linalg.CPU.RMSNormOp <name="model.layers.18.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=610), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=611), )] (%1123:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=610)]) -> (%1126:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=611)])
            linalg.CPU.RMSNormOp <name="model.layers.18.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=607), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=612), )] (%1124:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=607)]) -> (%1127:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=612)])
            linalg.CPU.RoPEOp <name="model.layers.18.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=611), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=611), )] (%1126:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=611)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%1128:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=611)])
            linalg.CPU.RoPEOp <name="model.layers.18.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=612), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=612), )] (%1127:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=612)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%1129:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=612)])
            linalg.CPU.CastTypeOp <name="model.layers.18.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=612), outputs_0:QuantSpec(Raw(type: Float16), uuid=613), )] (%1129:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=612)]) -> (%1130:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=613)])
            linalg.CPU.CastTypeOp <name="model.layers.18.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=613), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=614), )] (%1130:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=613)]) -> (%1131:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=614)])
            linalg.CPU.TransposeOp <name="model.layers.18.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=614), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=614), )] (%1131:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=614)]) -> (%1132:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=614)])
            linalg.CPU.CastTypeOp <name="model.layers.18.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=609), outputs_0:QuantSpec(Raw(type: Float16), uuid=615), )] (%1125:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=609)]) -> (%1133:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=615)])
            linalg.CPU.CastTypeOp <name="model.layers.18.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=615), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=616), )] (%1133:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=615)]) -> (%1134:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=616)])
            linalg.CPU.ConcatOp <name="model.layers.18.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=21), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=614), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=21), )] (%356:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=21)], %1132:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=614)]) -> (%1135:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=21)])
            linalg.CPU.ConcatOp <name="model.layers.18.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=49), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=616), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=49), )] (%357:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=49)], %1134:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=616)]) -> (%1136:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=49)])
            linalg.CPU.RepeatOp <name="model.layers.18.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=21), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=21), )] (%1135:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=21)]) -> (%1137:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=21)])
            linalg.CPU.RepeatOp <name="model.layers.18.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=49), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=49), )] (%1136:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=49)]) -> (%1138:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=49)])
            linalg.CPU.MatMulOp <name="model.layers.18.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=611), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=21), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=617), )] (%1128:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=611)], %1137:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=21)]) -> (%1139:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=617)])
            linalg.CPU.MulOp <name="model.layers.18.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=617), inputs_1:QuantSpec(Raw(type: Float32), uuid=618), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=617), )] (%1139:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=617)], %1140:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=618), constant:[0.088388346]]) -> (%1141:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=617)])
            linalg.CPU.ReduceMinOp <name="model.layers.18.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=617), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=619), )] (%1141:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=617)]) -> (%1142:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=619)])
            linalg.CPU.AddOp <name="model.layers.18.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=619), inputs_1:QuantSpec(Raw(type: Int16), uuid=620), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=619), )] (%1142:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=619)], %1143:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=620), constant:[-20]]) -> (%1144:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=619)])
            linalg.CPU.EqualOp <name="model.layers.18.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=621), outputs_0:QuantSpec(Raw(type: UInt8), uuid=622), )] (%319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %1145:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=621), constant:[0.24023438]]) -> (%1146:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=622)])
            linalg.CPU.WhereOp <name="model.layers.18.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=622), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=617), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=619), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=619), )] (%1146:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=622)], %1141:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=617)], %1144:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=619)]) -> (%1147:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=619)])
            linalg.CPU.SoftmaxOp <name="model.layers.18.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=619), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=623), )] (%1147:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=619)]) -> (%1148:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=623)])
            linalg.CPU.MatMulOp <name="model.layers.18.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=623), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=49), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=624), )] (%1148:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=623)], %1138:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=49)]) -> (%1149:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=624)])
            linalg.CPU.TransposeOp <name="model.layers.18.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=624), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=624), )] (%1149:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=624)]) -> (%1150:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=624)])
            linalg.CPU.ViewOp <name="model.layers.18.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=624), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=624), )] (%1150:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=624)]) -> (%1150:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=624)])
            linalg.CPU.LinearOp <name="model.layers.18.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=624), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=626), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=625))] (%1150:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=624)]) -> (%1151:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=626)])
            cf.ReturnOp (%1151:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=626)], %1132:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=614)], %1134:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=616)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.18.mlp <CPU> [using_qnn:true, symbol:model.layers.18.mlp] {
        (%1153:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=627)]) -> (%1158:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=634)]) {
            linalg.CPU.LinearOp <name="model.layers.18.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=627), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=629), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=628))] (%1153:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=627)]) -> (%1154:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=629)])
            linalg.CPU.SiLUOp <name="model.layers.18.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=629), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=630), )] (%1154:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=629)]) -> (%1155:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=630)])
            linalg.CPU.LinearOp <name="model.layers.18.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=627), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=632), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=631))] (%1153:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=627)]) -> (%1156:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=632)])
            linalg.CPU.MulOp <name="model.layers.18.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=630), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=632), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=630), )] (%1155:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=630)], %1156:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=632)]) -> (%1157:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=630)])
            linalg.CPU.LinearOp <name="model.layers.18.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=630), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=634), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=633))] (%1157:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=630)]) -> (%1158:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=634)])
            cf.ReturnOp (%1158:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=634)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.19 <CPU> [using_qnn:true, symbol:model.layers.19] {
        (%1159:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=634)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %358:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=22)], %359:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=50)]) -> (%1200:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=664)], %1173:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=644)], %1175:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=646)]) {
            linalg.CPU.RMSNormOp <name="model.layers.19.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=634), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=635), )] (%1159:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=634)]) -> (%1160:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=635)])
            graph.CallGraphOp @model.layers.19.self_attn (%1160:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=635)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %358:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=22)], %359:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=50)]) -> (%1192:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=656)], %1173:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=644)], %1175:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=646)])
            linalg.CPU.AddOp <name="model.layers.19.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=656), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=634), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=656), )] (%1192:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=656)], %1159:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=634)]) -> (%1193:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=656)])
            linalg.CPU.RMSNormOp <name="model.layers.19.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=656), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=657), )] (%1193:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=656)]) -> (%1194:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=657)])
            graph.CallGraphOp @model.layers.19.mlp (%1194:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=657)]) -> (%1199:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=664)])
            linalg.CPU.AddOp <name="model.layers.19.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=664), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=656), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=664), )] (%1199:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=664)], %1193:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=656)]) -> (%1200:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=664)])
            cf.ReturnOp (%1200:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=664)], %1173:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=644)], %1175:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=646)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.19.self_attn <CPU> [using_qnn:true, symbol:model.layers.19.self_attn] {
        (%1160:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=635)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %358:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=22)], %359:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=50)]) -> (%1192:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=656)], %1173:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=644)], %1175:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=646)]) {
            linalg.CPU.LinearOp <name="model.layers.19.self_attn.q_proj">(%1160:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=635)]) -> (%1161:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=640)])
            linalg.CPU.LinearOp <name="model.layers.19.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=635), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=637), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=636))] (%1160:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=635)]) -> (%1162:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=637)])
            linalg.CPU.LinearOp <name="model.layers.19.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=635), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=639), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=638))] (%1160:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=635)]) -> (%1163:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=639)])
            linalg.CPU.ViewOp <name="model.layers.19.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=640), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=640), )] (%1161:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=640)]) -> (%1161:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=640)])
            linalg.CPU.TransposeOp <name="model.layers.19.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=640), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=640), )] (%1161:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=640)]) -> (%1164:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=640)])
            linalg.CPU.ViewOp <name="model.layers.19.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=637), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=637), )] (%1162:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=637)]) -> (%1162:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=637)])
            linalg.CPU.TransposeOp <name="model.layers.19.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=637), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=637), )] (%1162:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=637)]) -> (%1165:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=637)])
            linalg.CPU.ViewOp <name="model.layers.19.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=639), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=639), )] (%1163:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=639)]) -> (%1163:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=639)])
            linalg.CPU.TransposeOp <name="model.layers.19.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=639), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=639), )] (%1163:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=639)]) -> (%1166:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=639)])
            linalg.CPU.RMSNormOp <name="model.layers.19.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=640), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=641), )] (%1164:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=640)]) -> (%1167:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=641)])
            linalg.CPU.RMSNormOp <name="model.layers.19.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=637), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=642), )] (%1165:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=637)]) -> (%1168:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=642)])
            linalg.CPU.RoPEOp <name="model.layers.19.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=641), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=641), )] (%1167:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=641)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%1169:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=641)])
            linalg.CPU.RoPEOp <name="model.layers.19.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=642), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=642), )] (%1168:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=642)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%1170:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=642)])
            linalg.CPU.CastTypeOp <name="model.layers.19.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=642), outputs_0:QuantSpec(Raw(type: Float16), uuid=643), )] (%1170:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=642)]) -> (%1171:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=643)])
            linalg.CPU.CastTypeOp <name="model.layers.19.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=643), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=644), )] (%1171:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=643)]) -> (%1172:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=644)])
            linalg.CPU.TransposeOp <name="model.layers.19.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=644), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=644), )] (%1172:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=644)]) -> (%1173:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=644)])
            linalg.CPU.CastTypeOp <name="model.layers.19.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=639), outputs_0:QuantSpec(Raw(type: Float16), uuid=645), )] (%1166:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=639)]) -> (%1174:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=645)])
            linalg.CPU.CastTypeOp <name="model.layers.19.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=645), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=646), )] (%1174:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=645)]) -> (%1175:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=646)])
            linalg.CPU.ConcatOp <name="model.layers.19.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=22), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=644), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=22), )] (%358:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=22)], %1173:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=644)]) -> (%1176:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=22)])
            linalg.CPU.ConcatOp <name="model.layers.19.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=50), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=646), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=50), )] (%359:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=50)], %1175:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=646)]) -> (%1177:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=50)])
            linalg.CPU.RepeatOp <name="model.layers.19.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=22), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=22), )] (%1176:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=22)]) -> (%1178:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=22)])
            linalg.CPU.RepeatOp <name="model.layers.19.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=50), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=50), )] (%1177:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=50)]) -> (%1179:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=50)])
            linalg.CPU.MatMulOp <name="model.layers.19.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=641), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=22), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=647), )] (%1169:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=641)], %1178:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=22)]) -> (%1180:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=647)])
            linalg.CPU.MulOp <name="model.layers.19.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=647), inputs_1:QuantSpec(Raw(type: Float32), uuid=648), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=647), )] (%1180:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=647)], %1181:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=648), constant:[0.088388346]]) -> (%1182:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=647)])
            linalg.CPU.ReduceMinOp <name="model.layers.19.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=647), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=649), )] (%1182:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=647)]) -> (%1183:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=649)])
            linalg.CPU.AddOp <name="model.layers.19.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=649), inputs_1:QuantSpec(Raw(type: Int16), uuid=650), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=649), )] (%1183:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=649)], %1184:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=650), constant:[-20]]) -> (%1185:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=649)])
            linalg.CPU.EqualOp <name="model.layers.19.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=651), outputs_0:QuantSpec(Raw(type: UInt8), uuid=652), )] (%319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %1186:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=651), constant:[0.55078125]]) -> (%1187:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=652)])
            linalg.CPU.WhereOp <name="model.layers.19.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=652), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=647), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=649), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=649), )] (%1187:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=652)], %1182:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=647)], %1185:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=649)]) -> (%1188:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=649)])
            linalg.CPU.SoftmaxOp <name="model.layers.19.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=649), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=653), )] (%1188:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=649)]) -> (%1189:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=653)])
            linalg.CPU.MatMulOp <name="model.layers.19.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=653), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=50), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=654), )] (%1189:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=653)], %1179:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=50)]) -> (%1190:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=654)])
            linalg.CPU.TransposeOp <name="model.layers.19.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=654), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=654), )] (%1190:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=654)]) -> (%1191:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=654)])
            linalg.CPU.ViewOp <name="model.layers.19.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=654), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=654), )] (%1191:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=654)]) -> (%1191:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=654)])
            linalg.CPU.LinearOp <name="model.layers.19.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=654), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=656), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=655))] (%1191:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=654)]) -> (%1192:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=656)])
            cf.ReturnOp (%1192:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=656)], %1173:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=644)], %1175:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=646)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.19.mlp <CPU> [using_qnn:true, symbol:model.layers.19.mlp] {
        (%1194:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=657)]) -> (%1199:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=664)]) {
            linalg.CPU.LinearOp <name="model.layers.19.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=657), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=659), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=658))] (%1194:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=657)]) -> (%1195:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=659)])
            linalg.CPU.SiLUOp <name="model.layers.19.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=659), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=660), )] (%1195:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=659)]) -> (%1196:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=660)])
            linalg.CPU.LinearOp <name="model.layers.19.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=657), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=662), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=661))] (%1194:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=657)]) -> (%1197:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=662)])
            linalg.CPU.MulOp <name="model.layers.19.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=660), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=662), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=660), )] (%1196:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=660)], %1197:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=662)]) -> (%1198:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=660)])
            linalg.CPU.LinearOp <name="model.layers.19.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=660), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=664), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=663))] (%1198:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=660)]) -> (%1199:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=664)])
            cf.ReturnOp (%1199:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=664)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.20 <CPU> [using_qnn:true, symbol:model.layers.20] {
        (%1200:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=664)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %360:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=23)], %361:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=51)]) -> (%1241:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=694)], %1214:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=674)], %1216:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=676)]) {
            linalg.CPU.RMSNormOp <name="model.layers.20.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=664), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=665), )] (%1200:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=664)]) -> (%1201:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=665)])
            graph.CallGraphOp @model.layers.20.self_attn (%1201:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=665)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %360:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=23)], %361:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=51)]) -> (%1233:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=686)], %1214:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=674)], %1216:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=676)])
            linalg.CPU.AddOp <name="model.layers.20.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=686), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=664), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=686), )] (%1233:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=686)], %1200:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=664)]) -> (%1234:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=686)])
            linalg.CPU.RMSNormOp <name="model.layers.20.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=686), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=687), )] (%1234:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=686)]) -> (%1235:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=687)])
            graph.CallGraphOp @model.layers.20.mlp (%1235:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=687)]) -> (%1240:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=694)])
            linalg.CPU.AddOp <name="model.layers.20.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=694), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=686), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=694), )] (%1240:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=694)], %1234:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=686)]) -> (%1241:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=694)])
            cf.ReturnOp (%1241:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=694)], %1214:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=674)], %1216:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=676)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.20.self_attn <CPU> [using_qnn:true, symbol:model.layers.20.self_attn] {
        (%1201:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=665)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %360:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=23)], %361:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=51)]) -> (%1233:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=686)], %1214:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=674)], %1216:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=676)]) {
            linalg.CPU.LinearOp <name="model.layers.20.self_attn.q_proj">(%1201:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=665)]) -> (%1202:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=670)])
            linalg.CPU.LinearOp <name="model.layers.20.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=665), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=667), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=666))] (%1201:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=665)]) -> (%1203:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=667)])
            linalg.CPU.LinearOp <name="model.layers.20.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=665), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=669), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=668))] (%1201:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=665)]) -> (%1204:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=669)])
            linalg.CPU.ViewOp <name="model.layers.20.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=670), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=670), )] (%1202:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=670)]) -> (%1202:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=670)])
            linalg.CPU.TransposeOp <name="model.layers.20.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=670), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=670), )] (%1202:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=670)]) -> (%1205:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=670)])
            linalg.CPU.ViewOp <name="model.layers.20.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=667), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=667), )] (%1203:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=667)]) -> (%1203:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=667)])
            linalg.CPU.TransposeOp <name="model.layers.20.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=667), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=667), )] (%1203:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=667)]) -> (%1206:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=667)])
            linalg.CPU.ViewOp <name="model.layers.20.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=669), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=669), )] (%1204:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=669)]) -> (%1204:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=669)])
            linalg.CPU.TransposeOp <name="model.layers.20.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=669), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=669), )] (%1204:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=669)]) -> (%1207:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=669)])
            linalg.CPU.RMSNormOp <name="model.layers.20.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=670), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=671), )] (%1205:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=670)]) -> (%1208:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=671)])
            linalg.CPU.RMSNormOp <name="model.layers.20.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=667), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=672), )] (%1206:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=667)]) -> (%1209:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=672)])
            linalg.CPU.RoPEOp <name="model.layers.20.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=671), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=671), )] (%1208:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=671)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%1210:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=671)])
            linalg.CPU.RoPEOp <name="model.layers.20.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=672), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=672), )] (%1209:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=672)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%1211:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=672)])
            linalg.CPU.CastTypeOp <name="model.layers.20.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=672), outputs_0:QuantSpec(Raw(type: Float16), uuid=673), )] (%1211:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=672)]) -> (%1212:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=673)])
            linalg.CPU.CastTypeOp <name="model.layers.20.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=673), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=674), )] (%1212:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=673)]) -> (%1213:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=674)])
            linalg.CPU.TransposeOp <name="model.layers.20.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=674), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=674), )] (%1213:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=674)]) -> (%1214:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=674)])
            linalg.CPU.CastTypeOp <name="model.layers.20.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=669), outputs_0:QuantSpec(Raw(type: Float16), uuid=675), )] (%1207:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=669)]) -> (%1215:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=675)])
            linalg.CPU.CastTypeOp <name="model.layers.20.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=675), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=676), )] (%1215:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=675)]) -> (%1216:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=676)])
            linalg.CPU.ConcatOp <name="model.layers.20.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=23), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=674), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=23), )] (%360:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=23)], %1214:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=674)]) -> (%1217:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=23)])
            linalg.CPU.ConcatOp <name="model.layers.20.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=51), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=676), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=51), )] (%361:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=51)], %1216:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=676)]) -> (%1218:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=51)])
            linalg.CPU.RepeatOp <name="model.layers.20.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=23), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=23), )] (%1217:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=23)]) -> (%1219:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=23)])
            linalg.CPU.RepeatOp <name="model.layers.20.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=51), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=51), )] (%1218:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=51)]) -> (%1220:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=51)])
            linalg.CPU.MatMulOp <name="model.layers.20.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=671), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=23), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=677), )] (%1210:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=671)], %1219:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=23)]) -> (%1221:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=677)])
            linalg.CPU.MulOp <name="model.layers.20.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=677), inputs_1:QuantSpec(Raw(type: Float32), uuid=678), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=677), )] (%1221:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=677)], %1222:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=678), constant:[0.088388346]]) -> (%1223:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=677)])
            linalg.CPU.ReduceMinOp <name="model.layers.20.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=677), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=679), )] (%1223:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=677)]) -> (%1224:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=679)])
            linalg.CPU.AddOp <name="model.layers.20.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=679), inputs_1:QuantSpec(Raw(type: Int16), uuid=680), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=679), )] (%1224:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=679)], %1225:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=680), constant:[-20]]) -> (%1226:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=679)])
            linalg.CPU.EqualOp <name="model.layers.20.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=681), outputs_0:QuantSpec(Raw(type: UInt8), uuid=682), )] (%319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %1227:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=681), constant:[0.71875]]) -> (%1228:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=682)])
            linalg.CPU.WhereOp <name="model.layers.20.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=682), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=677), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=679), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=679), )] (%1228:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=682)], %1223:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=677)], %1226:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=679)]) -> (%1229:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=679)])
            linalg.CPU.SoftmaxOp <name="model.layers.20.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=679), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=683), )] (%1229:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=679)]) -> (%1230:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=683)])
            linalg.CPU.MatMulOp <name="model.layers.20.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=683), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=51), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=684), )] (%1230:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=683)], %1220:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=51)]) -> (%1231:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=684)])
            linalg.CPU.TransposeOp <name="model.layers.20.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=684), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=684), )] (%1231:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=684)]) -> (%1232:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=684)])
            linalg.CPU.ViewOp <name="model.layers.20.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=684), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=684), )] (%1232:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=684)]) -> (%1232:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=684)])
            linalg.CPU.LinearOp <name="model.layers.20.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=684), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=686), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=685))] (%1232:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=684)]) -> (%1233:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=686)])
            cf.ReturnOp (%1233:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=686)], %1214:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=674)], %1216:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=676)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.20.mlp <CPU> [using_qnn:true, symbol:model.layers.20.mlp] {
        (%1235:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=687)]) -> (%1240:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=694)]) {
            linalg.CPU.LinearOp <name="model.layers.20.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=687), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=689), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=688))] (%1235:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=687)]) -> (%1236:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=689)])
            linalg.CPU.SiLUOp <name="model.layers.20.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=689), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=690), )] (%1236:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=689)]) -> (%1237:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=690)])
            linalg.CPU.LinearOp <name="model.layers.20.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=687), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=692), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=691))] (%1235:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=687)]) -> (%1238:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=692)])
            linalg.CPU.MulOp <name="model.layers.20.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=690), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=692), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=690), )] (%1237:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=690)], %1238:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=692)]) -> (%1239:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=690)])
            linalg.CPU.LinearOp <name="model.layers.20.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=690), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=694), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=693))] (%1239:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=690)]) -> (%1240:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=694)])
            cf.ReturnOp (%1240:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=694)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.21 <CPU> [using_qnn:true, symbol:model.layers.21] {
        (%1241:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=694)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %362:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=24)], %363:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=52)]) -> (%1282:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=724)], %1255:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=704)], %1257:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=706)]) {
            linalg.CPU.RMSNormOp <name="model.layers.21.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=694), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=695), )] (%1241:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=694)]) -> (%1242:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=695)])
            graph.CallGraphOp @model.layers.21.self_attn (%1242:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=695)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %362:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=24)], %363:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=52)]) -> (%1274:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=716)], %1255:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=704)], %1257:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=706)])
            linalg.CPU.AddOp <name="model.layers.21.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=716), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=694), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=716), )] (%1274:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=716)], %1241:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=694)]) -> (%1275:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=716)])
            linalg.CPU.RMSNormOp <name="model.layers.21.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=716), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=717), )] (%1275:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=716)]) -> (%1276:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=717)])
            graph.CallGraphOp @model.layers.21.mlp (%1276:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=717)]) -> (%1281:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=724)])
            linalg.CPU.AddOp <name="model.layers.21.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=724), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=716), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=724), )] (%1281:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=724)], %1275:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=716)]) -> (%1282:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=724)])
            cf.ReturnOp (%1282:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=724)], %1255:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=704)], %1257:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=706)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.21.self_attn <CPU> [using_qnn:true, symbol:model.layers.21.self_attn] {
        (%1242:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=695)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %362:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=24)], %363:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=52)]) -> (%1274:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=716)], %1255:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=704)], %1257:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=706)]) {
            linalg.CPU.LinearOp <name="model.layers.21.self_attn.q_proj">(%1242:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=695)]) -> (%1243:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=700)])
            linalg.CPU.LinearOp <name="model.layers.21.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=695), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=697), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=696))] (%1242:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=695)]) -> (%1244:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=697)])
            linalg.CPU.LinearOp <name="model.layers.21.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=695), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=699), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=698))] (%1242:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=695)]) -> (%1245:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=699)])
            linalg.CPU.ViewOp <name="model.layers.21.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=700), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=700), )] (%1243:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=700)]) -> (%1243:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=700)])
            linalg.CPU.TransposeOp <name="model.layers.21.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=700), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=700), )] (%1243:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=700)]) -> (%1246:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=700)])
            linalg.CPU.ViewOp <name="model.layers.21.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=697), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=697), )] (%1244:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=697)]) -> (%1244:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=697)])
            linalg.CPU.TransposeOp <name="model.layers.21.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=697), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=697), )] (%1244:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=697)]) -> (%1247:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=697)])
            linalg.CPU.ViewOp <name="model.layers.21.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=699), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=699), )] (%1245:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=699)]) -> (%1245:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=699)])
            linalg.CPU.TransposeOp <name="model.layers.21.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=699), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=699), )] (%1245:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=699)]) -> (%1248:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=699)])
            linalg.CPU.RMSNormOp <name="model.layers.21.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=700), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=701), )] (%1246:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=700)]) -> (%1249:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=701)])
            linalg.CPU.RMSNormOp <name="model.layers.21.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=697), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=702), )] (%1247:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=697)]) -> (%1250:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=702)])
            linalg.CPU.RoPEOp <name="model.layers.21.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=701), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=701), )] (%1249:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=701)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%1251:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=701)])
            linalg.CPU.RoPEOp <name="model.layers.21.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=702), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=702), )] (%1250:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=702)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%1252:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=702)])
            linalg.CPU.CastTypeOp <name="model.layers.21.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=702), outputs_0:QuantSpec(Raw(type: Float16), uuid=703), )] (%1252:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=702)]) -> (%1253:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=703)])
            linalg.CPU.CastTypeOp <name="model.layers.21.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=703), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=704), )] (%1253:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=703)]) -> (%1254:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=704)])
            linalg.CPU.TransposeOp <name="model.layers.21.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=704), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=704), )] (%1254:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=704)]) -> (%1255:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=704)])
            linalg.CPU.CastTypeOp <name="model.layers.21.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=699), outputs_0:QuantSpec(Raw(type: Float16), uuid=705), )] (%1248:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=699)]) -> (%1256:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=705)])
            linalg.CPU.CastTypeOp <name="model.layers.21.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=705), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=706), )] (%1256:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=705)]) -> (%1257:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=706)])
            linalg.CPU.ConcatOp <name="model.layers.21.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=24), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=704), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=24), )] (%362:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=24)], %1255:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=704)]) -> (%1258:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=24)])
            linalg.CPU.ConcatOp <name="model.layers.21.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=52), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=706), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=52), )] (%363:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=52)], %1257:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=706)]) -> (%1259:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=52)])
            linalg.CPU.RepeatOp <name="model.layers.21.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=24), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=24), )] (%1258:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=24)]) -> (%1260:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=24)])
            linalg.CPU.RepeatOp <name="model.layers.21.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=52), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=52), )] (%1259:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=52)]) -> (%1261:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=52)])
            linalg.CPU.MatMulOp <name="model.layers.21.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=701), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=24), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=707), )] (%1251:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=701)], %1260:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=24)]) -> (%1262:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=707)])
            linalg.CPU.MulOp <name="model.layers.21.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=707), inputs_1:QuantSpec(Raw(type: Float32), uuid=708), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=707), )] (%1262:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=707)], %1263:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=708), constant:[0.088388346]]) -> (%1264:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=707)])
            linalg.CPU.ReduceMinOp <name="model.layers.21.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=707), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=709), )] (%1264:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=707)]) -> (%1265:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=709)])
            linalg.CPU.AddOp <name="model.layers.21.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=709), inputs_1:QuantSpec(Raw(type: Int16), uuid=710), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=709), )] (%1265:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=709)], %1266:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=710), constant:[-20]]) -> (%1267:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=709)])
            linalg.CPU.EqualOp <name="model.layers.21.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=711), outputs_0:QuantSpec(Raw(type: UInt8), uuid=712), )] (%319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %1268:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=711), constant:[-0.80859375]]) -> (%1269:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=712)])
            linalg.CPU.WhereOp <name="model.layers.21.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=712), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=707), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=709), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=709), )] (%1269:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=712)], %1264:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=707)], %1267:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=709)]) -> (%1270:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=709)])
            linalg.CPU.SoftmaxOp <name="model.layers.21.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=709), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=713), )] (%1270:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=709)]) -> (%1271:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=713)])
            linalg.CPU.MatMulOp <name="model.layers.21.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=713), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=52), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=714), )] (%1271:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=713)], %1261:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=52)]) -> (%1272:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=714)])
            linalg.CPU.TransposeOp <name="model.layers.21.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=714), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=714), )] (%1272:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=714)]) -> (%1273:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=714)])
            linalg.CPU.ViewOp <name="model.layers.21.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=714), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=714), )] (%1273:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=714)]) -> (%1273:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=714)])
            linalg.CPU.LinearOp <name="model.layers.21.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=714), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=716), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=715))] (%1273:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=714)]) -> (%1274:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=716)])
            cf.ReturnOp (%1274:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=716)], %1255:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=704)], %1257:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=706)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.21.mlp <CPU> [using_qnn:true, symbol:model.layers.21.mlp] {
        (%1276:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=717)]) -> (%1281:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=724)]) {
            linalg.CPU.LinearOp <name="model.layers.21.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=717), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=719), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=718))] (%1276:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=717)]) -> (%1277:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=719)])
            linalg.CPU.SiLUOp <name="model.layers.21.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=719), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=720), )] (%1277:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=719)]) -> (%1278:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=720)])
            linalg.CPU.LinearOp <name="model.layers.21.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=717), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=722), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=721))] (%1276:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=717)]) -> (%1279:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=722)])
            linalg.CPU.MulOp <name="model.layers.21.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=720), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=722), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=720), )] (%1278:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=720)], %1279:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=722)]) -> (%1280:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=720)])
            linalg.CPU.LinearOp <name="model.layers.21.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=720), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=724), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=723))] (%1280:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=720)]) -> (%1281:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=724)])
            cf.ReturnOp (%1281:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=724)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.22 <CPU> [using_qnn:true, symbol:model.layers.22] {
        (%1282:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=724)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %364:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=25)], %365:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=53)]) -> (%1323:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=754)], %1296:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=734)], %1298:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=736)]) {
            linalg.CPU.RMSNormOp <name="model.layers.22.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=724), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=725), )] (%1282:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=724)]) -> (%1283:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=725)])
            graph.CallGraphOp @model.layers.22.self_attn (%1283:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=725)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %364:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=25)], %365:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=53)]) -> (%1315:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=746)], %1296:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=734)], %1298:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=736)])
            linalg.CPU.AddOp <name="model.layers.22.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=746), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=724), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=746), )] (%1315:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=746)], %1282:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=724)]) -> (%1316:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=746)])
            linalg.CPU.RMSNormOp <name="model.layers.22.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=746), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=747), )] (%1316:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=746)]) -> (%1317:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=747)])
            graph.CallGraphOp @model.layers.22.mlp (%1317:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=747)]) -> (%1322:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=754)])
            linalg.CPU.AddOp <name="model.layers.22.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=754), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=746), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=754), )] (%1322:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=754)], %1316:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=746)]) -> (%1323:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=754)])
            cf.ReturnOp (%1323:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=754)], %1296:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=734)], %1298:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=736)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.22.self_attn <CPU> [using_qnn:true, symbol:model.layers.22.self_attn] {
        (%1283:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=725)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %364:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=25)], %365:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=53)]) -> (%1315:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=746)], %1296:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=734)], %1298:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=736)]) {
            linalg.CPU.LinearOp <name="model.layers.22.self_attn.q_proj">(%1283:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=725)]) -> (%1284:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=730)])
            linalg.CPU.LinearOp <name="model.layers.22.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=725), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=727), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=726))] (%1283:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=725)]) -> (%1285:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=727)])
            linalg.CPU.LinearOp <name="model.layers.22.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=725), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=729), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=728))] (%1283:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=725)]) -> (%1286:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=729)])
            linalg.CPU.ViewOp <name="model.layers.22.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=730), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=730), )] (%1284:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=730)]) -> (%1284:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=730)])
            linalg.CPU.TransposeOp <name="model.layers.22.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=730), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=730), )] (%1284:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=730)]) -> (%1287:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=730)])
            linalg.CPU.ViewOp <name="model.layers.22.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=727), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=727), )] (%1285:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=727)]) -> (%1285:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=727)])
            linalg.CPU.TransposeOp <name="model.layers.22.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=727), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=727), )] (%1285:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=727)]) -> (%1288:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=727)])
            linalg.CPU.ViewOp <name="model.layers.22.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=729), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=729), )] (%1286:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=729)]) -> (%1286:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=729)])
            linalg.CPU.TransposeOp <name="model.layers.22.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=729), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=729), )] (%1286:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=729)]) -> (%1289:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=729)])
            linalg.CPU.RMSNormOp <name="model.layers.22.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=730), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=731), )] (%1287:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=730)]) -> (%1290:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=731)])
            linalg.CPU.RMSNormOp <name="model.layers.22.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=727), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=732), )] (%1288:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=727)]) -> (%1291:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=732)])
            linalg.CPU.RoPEOp <name="model.layers.22.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=731), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=731), )] (%1290:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=731)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%1292:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=731)])
            linalg.CPU.RoPEOp <name="model.layers.22.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=732), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=732), )] (%1291:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=732)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%1293:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=732)])
            linalg.CPU.CastTypeOp <name="model.layers.22.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=732), outputs_0:QuantSpec(Raw(type: Float16), uuid=733), )] (%1293:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=732)]) -> (%1294:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=733)])
            linalg.CPU.CastTypeOp <name="model.layers.22.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=733), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=734), )] (%1294:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=733)]) -> (%1295:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=734)])
            linalg.CPU.TransposeOp <name="model.layers.22.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=734), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=734), )] (%1295:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=734)]) -> (%1296:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=734)])
            linalg.CPU.CastTypeOp <name="model.layers.22.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=729), outputs_0:QuantSpec(Raw(type: Float16), uuid=735), )] (%1289:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=729)]) -> (%1297:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=735)])
            linalg.CPU.CastTypeOp <name="model.layers.22.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=735), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=736), )] (%1297:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=735)]) -> (%1298:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=736)])
            linalg.CPU.ConcatOp <name="model.layers.22.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=25), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=734), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=25), )] (%364:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=25)], %1296:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=734)]) -> (%1299:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=25)])
            linalg.CPU.ConcatOp <name="model.layers.22.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=53), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=736), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=53), )] (%365:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=53)], %1298:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=736)]) -> (%1300:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=53)])
            linalg.CPU.RepeatOp <name="model.layers.22.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=25), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=25), )] (%1299:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=25)]) -> (%1301:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=25)])
            linalg.CPU.RepeatOp <name="model.layers.22.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=53), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=53), )] (%1300:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=53)]) -> (%1302:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=53)])
            linalg.CPU.MatMulOp <name="model.layers.22.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=731), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=25), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=737), )] (%1292:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=731)], %1301:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=25)]) -> (%1303:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=737)])
            linalg.CPU.MulOp <name="model.layers.22.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=737), inputs_1:QuantSpec(Raw(type: Float32), uuid=738), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=737), )] (%1303:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=737)], %1304:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=738), constant:[0.088388346]]) -> (%1305:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=737)])
            linalg.CPU.ReduceMinOp <name="model.layers.22.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=737), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=739), )] (%1305:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=737)]) -> (%1306:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=739)])
            linalg.CPU.AddOp <name="model.layers.22.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=739), inputs_1:QuantSpec(Raw(type: Int16), uuid=740), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=739), )] (%1306:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=739)], %1307:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=740), constant:[-20]]) -> (%1308:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=739)])
            linalg.CPU.EqualOp <name="model.layers.22.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=741), outputs_0:QuantSpec(Raw(type: UInt8), uuid=742), )] (%319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %1309:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=741), constant:[-0.42773438]]) -> (%1310:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=742)])
            linalg.CPU.WhereOp <name="model.layers.22.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=742), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=737), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=739), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=739), )] (%1310:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=742)], %1305:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=737)], %1308:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=739)]) -> (%1311:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=739)])
            linalg.CPU.SoftmaxOp <name="model.layers.22.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=739), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=743), )] (%1311:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=739)]) -> (%1312:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=743)])
            linalg.CPU.MatMulOp <name="model.layers.22.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=743), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=53), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=744), )] (%1312:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=743)], %1302:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=53)]) -> (%1313:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=744)])
            linalg.CPU.TransposeOp <name="model.layers.22.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=744), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=744), )] (%1313:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=744)]) -> (%1314:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=744)])
            linalg.CPU.ViewOp <name="model.layers.22.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=744), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=744), )] (%1314:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=744)]) -> (%1314:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=744)])
            linalg.CPU.LinearOp <name="model.layers.22.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=744), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=746), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=745))] (%1314:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=744)]) -> (%1315:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=746)])
            cf.ReturnOp (%1315:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=746)], %1296:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=734)], %1298:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=736)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.22.mlp <CPU> [using_qnn:true, symbol:model.layers.22.mlp] {
        (%1317:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=747)]) -> (%1322:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=754)]) {
            linalg.CPU.LinearOp <name="model.layers.22.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=747), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=749), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=748))] (%1317:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=747)]) -> (%1318:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=749)])
            linalg.CPU.SiLUOp <name="model.layers.22.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=749), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=750), )] (%1318:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=749)]) -> (%1319:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=750)])
            linalg.CPU.LinearOp <name="model.layers.22.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=747), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=752), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=751))] (%1317:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=747)]) -> (%1320:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=752)])
            linalg.CPU.MulOp <name="model.layers.22.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=750), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=752), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=750), )] (%1319:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=750)], %1320:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=752)]) -> (%1321:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=750)])
            linalg.CPU.LinearOp <name="model.layers.22.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=750), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=754), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=753))] (%1321:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=750)]) -> (%1322:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=754)])
            cf.ReturnOp (%1322:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=754)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.23 <CPU> [using_qnn:true, symbol:model.layers.23] {
        (%1323:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=754)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %366:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=26)], %367:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=54)]) -> (%1364:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=784)], %1337:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=764)], %1339:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=766)]) {
            linalg.CPU.RMSNormOp <name="model.layers.23.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=754), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=755), )] (%1323:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=754)]) -> (%1324:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=755)])
            graph.CallGraphOp @model.layers.23.self_attn (%1324:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=755)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %366:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=26)], %367:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=54)]) -> (%1356:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=776)], %1337:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=764)], %1339:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=766)])
            linalg.CPU.AddOp <name="model.layers.23.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=776), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=754), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=776), )] (%1356:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=776)], %1323:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=754)]) -> (%1357:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=776)])
            linalg.CPU.RMSNormOp <name="model.layers.23.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=776), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=777), )] (%1357:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=776)]) -> (%1358:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=777)])
            graph.CallGraphOp @model.layers.23.mlp (%1358:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=777)]) -> (%1363:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=784)])
            linalg.CPU.AddOp <name="model.layers.23.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=784), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=776), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=784), )] (%1363:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=784)], %1357:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=776)]) -> (%1364:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=784)])
            cf.ReturnOp (%1364:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=784)], %1337:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=764)], %1339:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=766)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.23.self_attn <CPU> [using_qnn:true, symbol:model.layers.23.self_attn] {
        (%1324:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=755)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %366:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=26)], %367:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=54)]) -> (%1356:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=776)], %1337:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=764)], %1339:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=766)]) {
            linalg.CPU.LinearOp <name="model.layers.23.self_attn.q_proj">(%1324:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=755)]) -> (%1325:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=760)])
            linalg.CPU.LinearOp <name="model.layers.23.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=755), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=757), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=756))] (%1324:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=755)]) -> (%1326:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=757)])
            linalg.CPU.LinearOp <name="model.layers.23.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=755), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=759), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=758))] (%1324:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=755)]) -> (%1327:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=759)])
            linalg.CPU.ViewOp <name="model.layers.23.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=760), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=760), )] (%1325:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=760)]) -> (%1325:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=760)])
            linalg.CPU.TransposeOp <name="model.layers.23.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=760), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=760), )] (%1325:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=760)]) -> (%1328:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=760)])
            linalg.CPU.ViewOp <name="model.layers.23.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=757), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=757), )] (%1326:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=757)]) -> (%1326:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=757)])
            linalg.CPU.TransposeOp <name="model.layers.23.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=757), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=757), )] (%1326:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=757)]) -> (%1329:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=757)])
            linalg.CPU.ViewOp <name="model.layers.23.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=759), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=759), )] (%1327:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=759)]) -> (%1327:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=759)])
            linalg.CPU.TransposeOp <name="model.layers.23.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=759), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=759), )] (%1327:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=759)]) -> (%1330:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=759)])
            linalg.CPU.RMSNormOp <name="model.layers.23.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=760), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=761), )] (%1328:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=760)]) -> (%1331:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=761)])
            linalg.CPU.RMSNormOp <name="model.layers.23.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=757), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=762), )] (%1329:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=757)]) -> (%1332:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=762)])
            linalg.CPU.RoPEOp <name="model.layers.23.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=761), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=761), )] (%1331:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=761)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%1333:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=761)])
            linalg.CPU.RoPEOp <name="model.layers.23.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=762), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=762), )] (%1332:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=762)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%1334:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=762)])
            linalg.CPU.CastTypeOp <name="model.layers.23.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=762), outputs_0:QuantSpec(Raw(type: Float16), uuid=763), )] (%1334:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=762)]) -> (%1335:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=763)])
            linalg.CPU.CastTypeOp <name="model.layers.23.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=763), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=764), )] (%1335:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=763)]) -> (%1336:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=764)])
            linalg.CPU.TransposeOp <name="model.layers.23.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=764), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=764), )] (%1336:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=764)]) -> (%1337:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=764)])
            linalg.CPU.CastTypeOp <name="model.layers.23.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=759), outputs_0:QuantSpec(Raw(type: Float16), uuid=765), )] (%1330:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=759)]) -> (%1338:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=765)])
            linalg.CPU.CastTypeOp <name="model.layers.23.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=765), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=766), )] (%1338:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=765)]) -> (%1339:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=766)])
            linalg.CPU.ConcatOp <name="model.layers.23.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=26), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=764), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=26), )] (%366:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=26)], %1337:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=764)]) -> (%1340:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=26)])
            linalg.CPU.ConcatOp <name="model.layers.23.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=54), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=766), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=54), )] (%367:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=54)], %1339:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=766)]) -> (%1341:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=54)])
            linalg.CPU.RepeatOp <name="model.layers.23.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=26), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=26), )] (%1340:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=26)]) -> (%1342:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=26)])
            linalg.CPU.RepeatOp <name="model.layers.23.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=54), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=54), )] (%1341:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=54)]) -> (%1343:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=54)])
            linalg.CPU.MatMulOp <name="model.layers.23.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=761), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=26), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=767), )] (%1333:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=761)], %1342:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=26)]) -> (%1344:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=767)])
            linalg.CPU.MulOp <name="model.layers.23.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=767), inputs_1:QuantSpec(Raw(type: Float32), uuid=768), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=767), )] (%1344:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=767)], %1345:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=768), constant:[0.088388346]]) -> (%1346:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=767)])
            linalg.CPU.ReduceMinOp <name="model.layers.23.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=767), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=769), )] (%1346:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=767)]) -> (%1347:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=769)])
            linalg.CPU.AddOp <name="model.layers.23.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=769), inputs_1:QuantSpec(Raw(type: Int16), uuid=770), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=769), )] (%1347:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=769)], %1348:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=770), constant:[-20]]) -> (%1349:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=769)])
            linalg.CPU.EqualOp <name="model.layers.23.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=771), outputs_0:QuantSpec(Raw(type: UInt8), uuid=772), )] (%319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %1350:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=771), constant:[0.96484375]]) -> (%1351:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=772)])
            linalg.CPU.WhereOp <name="model.layers.23.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=772), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=767), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=769), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=769), )] (%1351:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=772)], %1346:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=767)], %1349:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=769)]) -> (%1352:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=769)])
            linalg.CPU.SoftmaxOp <name="model.layers.23.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=769), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=773), )] (%1352:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=769)]) -> (%1353:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=773)])
            linalg.CPU.MatMulOp <name="model.layers.23.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=773), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=54), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=774), )] (%1353:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=773)], %1343:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=54)]) -> (%1354:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=774)])
            linalg.CPU.TransposeOp <name="model.layers.23.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=774), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=774), )] (%1354:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=774)]) -> (%1355:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=774)])
            linalg.CPU.ViewOp <name="model.layers.23.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=774), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=774), )] (%1355:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=774)]) -> (%1355:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=774)])
            linalg.CPU.LinearOp <name="model.layers.23.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=774), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=776), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=775))] (%1355:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=774)]) -> (%1356:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=776)])
            cf.ReturnOp (%1356:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=776)], %1337:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=764)], %1339:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=766)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.23.mlp <CPU> [using_qnn:true, symbol:model.layers.23.mlp] {
        (%1358:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=777)]) -> (%1363:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=784)]) {
            linalg.CPU.LinearOp <name="model.layers.23.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=777), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=779), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=778))] (%1358:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=777)]) -> (%1359:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=779)])
            linalg.CPU.SiLUOp <name="model.layers.23.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=779), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=780), )] (%1359:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=779)]) -> (%1360:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=780)])
            linalg.CPU.LinearOp <name="model.layers.23.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=777), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=782), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=781))] (%1358:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=777)]) -> (%1361:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=782)])
            linalg.CPU.MulOp <name="model.layers.23.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=780), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=782), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=780), )] (%1360:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=780)], %1361:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=782)]) -> (%1362:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=780)])
            linalg.CPU.LinearOp <name="model.layers.23.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=780), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=784), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=783))] (%1362:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=780)]) -> (%1363:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=784)])
            cf.ReturnOp (%1363:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=784)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.24 <CPU> [using_qnn:true, symbol:model.layers.24] {
        (%1364:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=784)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %368:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=27)], %369:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=55)]) -> (%1405:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=814)], %1378:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=794)], %1380:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=796)]) {
            linalg.CPU.RMSNormOp <name="model.layers.24.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=784), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=785), )] (%1364:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=784)]) -> (%1365:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=785)])
            graph.CallGraphOp @model.layers.24.self_attn (%1365:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=785)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %368:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=27)], %369:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=55)]) -> (%1397:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=806)], %1378:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=794)], %1380:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=796)])
            linalg.CPU.AddOp <name="model.layers.24.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=806), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=784), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=806), )] (%1397:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=806)], %1364:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=784)]) -> (%1398:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=806)])
            linalg.CPU.RMSNormOp <name="model.layers.24.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=806), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=807), )] (%1398:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=806)]) -> (%1399:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=807)])
            graph.CallGraphOp @model.layers.24.mlp (%1399:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=807)]) -> (%1404:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=814)])
            linalg.CPU.AddOp <name="model.layers.24.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=814), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=806), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=814), )] (%1404:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=814)], %1398:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=806)]) -> (%1405:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=814)])
            cf.ReturnOp (%1405:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=814)], %1378:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=794)], %1380:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=796)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.24.self_attn <CPU> [using_qnn:true, symbol:model.layers.24.self_attn] {
        (%1365:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=785)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %368:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=27)], %369:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=55)]) -> (%1397:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=806)], %1378:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=794)], %1380:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=796)]) {
            linalg.CPU.LinearOp <name="model.layers.24.self_attn.q_proj">(%1365:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=785)]) -> (%1366:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=790)])
            linalg.CPU.LinearOp <name="model.layers.24.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=785), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=787), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=786))] (%1365:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=785)]) -> (%1367:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=787)])
            linalg.CPU.LinearOp <name="model.layers.24.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=785), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=789), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=788))] (%1365:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=785)]) -> (%1368:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=789)])
            linalg.CPU.ViewOp <name="model.layers.24.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=790), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=790), )] (%1366:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=790)]) -> (%1366:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=790)])
            linalg.CPU.TransposeOp <name="model.layers.24.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=790), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=790), )] (%1366:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=790)]) -> (%1369:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=790)])
            linalg.CPU.ViewOp <name="model.layers.24.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=787), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=787), )] (%1367:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=787)]) -> (%1367:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=787)])
            linalg.CPU.TransposeOp <name="model.layers.24.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=787), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=787), )] (%1367:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=787)]) -> (%1370:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=787)])
            linalg.CPU.ViewOp <name="model.layers.24.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=789), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=789), )] (%1368:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=789)]) -> (%1368:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=789)])
            linalg.CPU.TransposeOp <name="model.layers.24.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=789), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=789), )] (%1368:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=789)]) -> (%1371:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=789)])
            linalg.CPU.RMSNormOp <name="model.layers.24.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=790), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=791), )] (%1369:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=790)]) -> (%1372:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=791)])
            linalg.CPU.RMSNormOp <name="model.layers.24.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=787), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=792), )] (%1370:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=787)]) -> (%1373:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=792)])
            linalg.CPU.RoPEOp <name="model.layers.24.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=791), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=791), )] (%1372:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=791)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%1374:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=791)])
            linalg.CPU.RoPEOp <name="model.layers.24.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=792), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=792), )] (%1373:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=792)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%1375:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=792)])
            linalg.CPU.CastTypeOp <name="model.layers.24.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=792), outputs_0:QuantSpec(Raw(type: Float16), uuid=793), )] (%1375:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=792)]) -> (%1376:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=793)])
            linalg.CPU.CastTypeOp <name="model.layers.24.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=793), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=794), )] (%1376:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=793)]) -> (%1377:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=794)])
            linalg.CPU.TransposeOp <name="model.layers.24.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=794), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=794), )] (%1377:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=794)]) -> (%1378:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=794)])
            linalg.CPU.CastTypeOp <name="model.layers.24.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=789), outputs_0:QuantSpec(Raw(type: Float16), uuid=795), )] (%1371:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=789)]) -> (%1379:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=795)])
            linalg.CPU.CastTypeOp <name="model.layers.24.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=795), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=796), )] (%1379:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=795)]) -> (%1380:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=796)])
            linalg.CPU.ConcatOp <name="model.layers.24.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=27), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=794), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=27), )] (%368:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=27)], %1378:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=794)]) -> (%1381:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=27)])
            linalg.CPU.ConcatOp <name="model.layers.24.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=55), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=796), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=55), )] (%369:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=55)], %1380:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=796)]) -> (%1382:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=55)])
            linalg.CPU.RepeatOp <name="model.layers.24.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=27), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=27), )] (%1381:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=27)]) -> (%1383:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=27)])
            linalg.CPU.RepeatOp <name="model.layers.24.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=55), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=55), )] (%1382:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=55)]) -> (%1384:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=55)])
            linalg.CPU.MatMulOp <name="model.layers.24.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=791), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=27), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=797), )] (%1374:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=791)], %1383:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=27)]) -> (%1385:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=797)])
            linalg.CPU.MulOp <name="model.layers.24.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=797), inputs_1:QuantSpec(Raw(type: Float32), uuid=798), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=797), )] (%1385:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=797)], %1386:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=798), constant:[0.088388346]]) -> (%1387:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=797)])
            linalg.CPU.ReduceMinOp <name="model.layers.24.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=797), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=799), )] (%1387:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=797)]) -> (%1388:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=799)])
            linalg.CPU.AddOp <name="model.layers.24.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=799), inputs_1:QuantSpec(Raw(type: Int16), uuid=800), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=799), )] (%1388:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=799)], %1389:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=800), constant:[-20]]) -> (%1390:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=799)])
            linalg.CPU.EqualOp <name="model.layers.24.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=801), outputs_0:QuantSpec(Raw(type: UInt8), uuid=802), )] (%319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %1391:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=801), constant:[0.07910156]]) -> (%1392:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=802)])
            linalg.CPU.WhereOp <name="model.layers.24.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=802), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=797), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=799), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=799), )] (%1392:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=802)], %1387:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=797)], %1390:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=799)]) -> (%1393:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=799)])
            linalg.CPU.SoftmaxOp <name="model.layers.24.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=799), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=803), )] (%1393:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=799)]) -> (%1394:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=803)])
            linalg.CPU.MatMulOp <name="model.layers.24.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=803), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=55), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=804), )] (%1394:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=803)], %1384:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=55)]) -> (%1395:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=804)])
            linalg.CPU.TransposeOp <name="model.layers.24.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=804), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=804), )] (%1395:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=804)]) -> (%1396:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=804)])
            linalg.CPU.ViewOp <name="model.layers.24.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=804), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=804), )] (%1396:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=804)]) -> (%1396:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=804)])
            linalg.CPU.LinearOp <name="model.layers.24.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=804), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=806), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=805))] (%1396:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=804)]) -> (%1397:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=806)])
            cf.ReturnOp (%1397:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=806)], %1378:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=794)], %1380:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=796)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.24.mlp <CPU> [using_qnn:true, symbol:model.layers.24.mlp] {
        (%1399:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=807)]) -> (%1404:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=814)]) {
            linalg.CPU.LinearOp <name="model.layers.24.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=807), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=809), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=808))] (%1399:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=807)]) -> (%1400:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=809)])
            linalg.CPU.SiLUOp <name="model.layers.24.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=809), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=810), )] (%1400:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=809)]) -> (%1401:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=810)])
            linalg.CPU.LinearOp <name="model.layers.24.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=807), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=812), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=811))] (%1399:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=807)]) -> (%1402:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=812)])
            linalg.CPU.MulOp <name="model.layers.24.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=810), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=812), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=810), )] (%1401:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=810)], %1402:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=812)]) -> (%1403:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=810)])
            linalg.CPU.LinearOp <name="model.layers.24.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=810), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=814), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=813))] (%1403:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=810)]) -> (%1404:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=814)])
            cf.ReturnOp (%1404:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=814)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.25 <CPU> [using_qnn:true, symbol:model.layers.25] {
        (%1405:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=814)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %370:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=28)], %371:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=56)]) -> (%1446:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=844)], %1419:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=824)], %1421:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=826)]) {
            linalg.CPU.RMSNormOp <name="model.layers.25.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=814), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=815), )] (%1405:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=814)]) -> (%1406:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=815)])
            graph.CallGraphOp @model.layers.25.self_attn (%1406:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=815)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %370:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=28)], %371:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=56)]) -> (%1438:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=836)], %1419:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=824)], %1421:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=826)])
            linalg.CPU.AddOp <name="model.layers.25.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=836), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=814), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=836), )] (%1438:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=836)], %1405:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=814)]) -> (%1439:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=836)])
            linalg.CPU.RMSNormOp <name="model.layers.25.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=836), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=837), )] (%1439:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=836)]) -> (%1440:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=837)])
            graph.CallGraphOp @model.layers.25.mlp (%1440:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=837)]) -> (%1445:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=844)])
            linalg.CPU.AddOp <name="model.layers.25.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=844), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=836), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=844), )] (%1445:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=844)], %1439:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=836)]) -> (%1446:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=844)])
            cf.ReturnOp (%1446:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=844)], %1419:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=824)], %1421:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=826)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.25.self_attn <CPU> [using_qnn:true, symbol:model.layers.25.self_attn] {
        (%1406:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=815)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %370:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=28)], %371:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=56)]) -> (%1438:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=836)], %1419:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=824)], %1421:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=826)]) {
            linalg.CPU.LinearOp <name="model.layers.25.self_attn.q_proj">(%1406:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=815)]) -> (%1407:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=820)])
            linalg.CPU.LinearOp <name="model.layers.25.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=815), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=817), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=816))] (%1406:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=815)]) -> (%1408:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=817)])
            linalg.CPU.LinearOp <name="model.layers.25.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=815), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=819), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=818))] (%1406:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=815)]) -> (%1409:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=819)])
            linalg.CPU.ViewOp <name="model.layers.25.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=820), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=820), )] (%1407:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=820)]) -> (%1407:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=820)])
            linalg.CPU.TransposeOp <name="model.layers.25.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=820), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=820), )] (%1407:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=820)]) -> (%1410:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=820)])
            linalg.CPU.ViewOp <name="model.layers.25.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=817), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=817), )] (%1408:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=817)]) -> (%1408:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=817)])
            linalg.CPU.TransposeOp <name="model.layers.25.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=817), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=817), )] (%1408:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=817)]) -> (%1411:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=817)])
            linalg.CPU.ViewOp <name="model.layers.25.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=819), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=819), )] (%1409:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=819)]) -> (%1409:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=819)])
            linalg.CPU.TransposeOp <name="model.layers.25.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=819), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=819), )] (%1409:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=819)]) -> (%1412:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=819)])
            linalg.CPU.RMSNormOp <name="model.layers.25.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=820), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=821), )] (%1410:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=820)]) -> (%1413:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=821)])
            linalg.CPU.RMSNormOp <name="model.layers.25.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=817), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=822), )] (%1411:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=817)]) -> (%1414:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=822)])
            linalg.CPU.RoPEOp <name="model.layers.25.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=821), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=821), )] (%1413:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=821)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%1415:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=821)])
            linalg.CPU.RoPEOp <name="model.layers.25.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=822), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=822), )] (%1414:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=822)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%1416:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=822)])
            linalg.CPU.CastTypeOp <name="model.layers.25.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=822), outputs_0:QuantSpec(Raw(type: Float16), uuid=823), )] (%1416:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=822)]) -> (%1417:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=823)])
            linalg.CPU.CastTypeOp <name="model.layers.25.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=823), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=824), )] (%1417:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=823)]) -> (%1418:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=824)])
            linalg.CPU.TransposeOp <name="model.layers.25.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=824), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=824), )] (%1418:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=824)]) -> (%1419:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=824)])
            linalg.CPU.CastTypeOp <name="model.layers.25.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=819), outputs_0:QuantSpec(Raw(type: Float16), uuid=825), )] (%1412:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=819)]) -> (%1420:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=825)])
            linalg.CPU.CastTypeOp <name="model.layers.25.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=825), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=826), )] (%1420:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=825)]) -> (%1421:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=826)])
            linalg.CPU.ConcatOp <name="model.layers.25.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=28), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=824), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=28), )] (%370:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=28)], %1419:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=824)]) -> (%1422:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=28)])
            linalg.CPU.ConcatOp <name="model.layers.25.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=56), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=826), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=56), )] (%371:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=56)], %1421:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=826)]) -> (%1423:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=56)])
            linalg.CPU.RepeatOp <name="model.layers.25.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=28), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=28), )] (%1422:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=28)]) -> (%1424:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=28)])
            linalg.CPU.RepeatOp <name="model.layers.25.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=56), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=56), )] (%1423:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=56)]) -> (%1425:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=56)])
            linalg.CPU.MatMulOp <name="model.layers.25.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=821), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=28), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=827), )] (%1415:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=821)], %1424:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=28)]) -> (%1426:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=827)])
            linalg.CPU.MulOp <name="model.layers.25.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=827), inputs_1:QuantSpec(Raw(type: Float32), uuid=828), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=827), )] (%1426:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=827)], %1427:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=828), constant:[0.088388346]]) -> (%1428:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=827)])
            linalg.CPU.ReduceMinOp <name="model.layers.25.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=827), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=829), )] (%1428:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=827)]) -> (%1429:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=829)])
            linalg.CPU.AddOp <name="model.layers.25.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=829), inputs_1:QuantSpec(Raw(type: Int16), uuid=830), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=829), )] (%1429:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=829)], %1430:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=830), constant:[-20]]) -> (%1431:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=829)])
            linalg.CPU.EqualOp <name="model.layers.25.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=831), outputs_0:QuantSpec(Raw(type: UInt8), uuid=832), )] (%319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %1432:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=831), constant:[-0.9921875]]) -> (%1433:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=832)])
            linalg.CPU.WhereOp <name="model.layers.25.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=832), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=827), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=829), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=829), )] (%1433:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=832)], %1428:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=827)], %1431:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=829)]) -> (%1434:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=829)])
            linalg.CPU.SoftmaxOp <name="model.layers.25.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=829), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=833), )] (%1434:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=829)]) -> (%1435:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=833)])
            linalg.CPU.MatMulOp <name="model.layers.25.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=833), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=56), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=834), )] (%1435:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=833)], %1425:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=56)]) -> (%1436:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=834)])
            linalg.CPU.TransposeOp <name="model.layers.25.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=834), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=834), )] (%1436:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=834)]) -> (%1437:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=834)])
            linalg.CPU.ViewOp <name="model.layers.25.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=834), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=834), )] (%1437:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=834)]) -> (%1437:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=834)])
            linalg.CPU.LinearOp <name="model.layers.25.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=834), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=836), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=835))] (%1437:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=834)]) -> (%1438:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=836)])
            cf.ReturnOp (%1438:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=836)], %1419:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=824)], %1421:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=826)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.25.mlp <CPU> [using_qnn:true, symbol:model.layers.25.mlp] {
        (%1440:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=837)]) -> (%1445:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=844)]) {
            linalg.CPU.LinearOp <name="model.layers.25.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=837), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=839), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=838))] (%1440:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=837)]) -> (%1441:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=839)])
            linalg.CPU.SiLUOp <name="model.layers.25.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=839), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=840), )] (%1441:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=839)]) -> (%1442:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=840)])
            linalg.CPU.LinearOp <name="model.layers.25.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=837), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=842), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=841))] (%1440:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=837)]) -> (%1443:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=842)])
            linalg.CPU.MulOp <name="model.layers.25.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=840), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=842), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=840), )] (%1442:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=840)], %1443:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=842)]) -> (%1444:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=840)])
            linalg.CPU.LinearOp <name="model.layers.25.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=840), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=844), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=843))] (%1444:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=840)]) -> (%1445:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=844)])
            cf.ReturnOp (%1445:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=844)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.26 <CPU> [using_qnn:true, symbol:model.layers.26] {
        (%1446:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=844)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %372:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=29)], %373:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=57)]) -> (%1487:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=874)], %1460:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=854)], %1462:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=856)]) {
            linalg.CPU.RMSNormOp <name="model.layers.26.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=844), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=845), )] (%1446:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=844)]) -> (%1447:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=845)])
            graph.CallGraphOp @model.layers.26.self_attn (%1447:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=845)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %372:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=29)], %373:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=57)]) -> (%1479:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=866)], %1460:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=854)], %1462:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=856)])
            linalg.CPU.AddOp <name="model.layers.26.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=866), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=844), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=866), )] (%1479:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=866)], %1446:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=844)]) -> (%1480:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=866)])
            linalg.CPU.RMSNormOp <name="model.layers.26.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=866), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=867), )] (%1480:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=866)]) -> (%1481:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=867)])
            graph.CallGraphOp @model.layers.26.mlp (%1481:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=867)]) -> (%1486:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=874)])
            linalg.CPU.AddOp <name="model.layers.26.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=874), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=866), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=874), )] (%1486:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=874)], %1480:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=866)]) -> (%1487:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=874)])
            cf.ReturnOp (%1487:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=874)], %1460:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=854)], %1462:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=856)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.26.self_attn <CPU> [using_qnn:true, symbol:model.layers.26.self_attn] {
        (%1447:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=845)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %372:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=29)], %373:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=57)]) -> (%1479:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=866)], %1460:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=854)], %1462:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=856)]) {
            linalg.CPU.LinearOp <name="model.layers.26.self_attn.q_proj">(%1447:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=845)]) -> (%1448:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=850)])
            linalg.CPU.LinearOp <name="model.layers.26.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=845), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=847), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=846))] (%1447:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=845)]) -> (%1449:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=847)])
            linalg.CPU.LinearOp <name="model.layers.26.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=845), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=849), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=848))] (%1447:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=845)]) -> (%1450:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=849)])
            linalg.CPU.ViewOp <name="model.layers.26.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=850), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=850), )] (%1448:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=850)]) -> (%1448:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=850)])
            linalg.CPU.TransposeOp <name="model.layers.26.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=850), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=850), )] (%1448:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=850)]) -> (%1451:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=850)])
            linalg.CPU.ViewOp <name="model.layers.26.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=847), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=847), )] (%1449:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=847)]) -> (%1449:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=847)])
            linalg.CPU.TransposeOp <name="model.layers.26.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=847), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=847), )] (%1449:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=847)]) -> (%1452:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=847)])
            linalg.CPU.ViewOp <name="model.layers.26.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=849), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=849), )] (%1450:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=849)]) -> (%1450:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=849)])
            linalg.CPU.TransposeOp <name="model.layers.26.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=849), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=849), )] (%1450:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=849)]) -> (%1453:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=849)])
            linalg.CPU.RMSNormOp <name="model.layers.26.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=850), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=851), )] (%1451:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=850)]) -> (%1454:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=851)])
            linalg.CPU.RMSNormOp <name="model.layers.26.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=847), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=852), )] (%1452:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=847)]) -> (%1455:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=852)])
            linalg.CPU.RoPEOp <name="model.layers.26.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=851), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=851), )] (%1454:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=851)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%1456:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=851)])
            linalg.CPU.RoPEOp <name="model.layers.26.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=852), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=852), )] (%1455:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=852)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%1457:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=852)])
            linalg.CPU.CastTypeOp <name="model.layers.26.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=852), outputs_0:QuantSpec(Raw(type: Float16), uuid=853), )] (%1457:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=852)]) -> (%1458:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=853)])
            linalg.CPU.CastTypeOp <name="model.layers.26.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=853), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=854), )] (%1458:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=853)]) -> (%1459:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=854)])
            linalg.CPU.TransposeOp <name="model.layers.26.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=854), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=854), )] (%1459:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=854)]) -> (%1460:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=854)])
            linalg.CPU.CastTypeOp <name="model.layers.26.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=849), outputs_0:QuantSpec(Raw(type: Float16), uuid=855), )] (%1453:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=849)]) -> (%1461:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=855)])
            linalg.CPU.CastTypeOp <name="model.layers.26.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=855), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=856), )] (%1461:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=855)]) -> (%1462:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=856)])
            linalg.CPU.ConcatOp <name="model.layers.26.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=29), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=854), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=29), )] (%372:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=29)], %1460:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=854)]) -> (%1463:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=29)])
            linalg.CPU.ConcatOp <name="model.layers.26.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=57), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=856), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=57), )] (%373:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=57)], %1462:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=856)]) -> (%1464:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=57)])
            linalg.CPU.RepeatOp <name="model.layers.26.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=29), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=29), )] (%1463:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=29)]) -> (%1465:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=29)])
            linalg.CPU.RepeatOp <name="model.layers.26.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=57), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=57), )] (%1464:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=57)]) -> (%1466:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=57)])
            linalg.CPU.MatMulOp <name="model.layers.26.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=851), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=29), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=857), )] (%1456:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=851)], %1465:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=29)]) -> (%1467:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=857)])
            linalg.CPU.MulOp <name="model.layers.26.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=857), inputs_1:QuantSpec(Raw(type: Float32), uuid=858), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=857), )] (%1467:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=857)], %1468:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=858), constant:[0.088388346]]) -> (%1469:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=857)])
            linalg.CPU.ReduceMinOp <name="model.layers.26.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=857), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=859), )] (%1469:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=857)]) -> (%1470:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=859)])
            linalg.CPU.AddOp <name="model.layers.26.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=859), inputs_1:QuantSpec(Raw(type: Int16), uuid=860), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=859), )] (%1470:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=859)], %1471:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=860), constant:[-20]]) -> (%1472:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=859)])
            linalg.CPU.EqualOp <name="model.layers.26.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=861), outputs_0:QuantSpec(Raw(type: UInt8), uuid=862), )] (%319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %1473:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=861), constant:[0.27929688]]) -> (%1474:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=862)])
            linalg.CPU.WhereOp <name="model.layers.26.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=862), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=857), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=859), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=859), )] (%1474:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=862)], %1469:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=857)], %1472:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=859)]) -> (%1475:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=859)])
            linalg.CPU.SoftmaxOp <name="model.layers.26.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=859), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=863), )] (%1475:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=859)]) -> (%1476:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=863)])
            linalg.CPU.MatMulOp <name="model.layers.26.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=863), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=57), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=864), )] (%1476:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=863)], %1466:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=57)]) -> (%1477:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=864)])
            linalg.CPU.TransposeOp <name="model.layers.26.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=864), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=864), )] (%1477:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=864)]) -> (%1478:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=864)])
            linalg.CPU.ViewOp <name="model.layers.26.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=864), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=864), )] (%1478:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=864)]) -> (%1478:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=864)])
            linalg.CPU.LinearOp <name="model.layers.26.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=864), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=866), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=865))] (%1478:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=864)]) -> (%1479:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=866)])
            cf.ReturnOp (%1479:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=866)], %1460:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=854)], %1462:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=856)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.26.mlp <CPU> [using_qnn:true, symbol:model.layers.26.mlp] {
        (%1481:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=867)]) -> (%1486:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=874)]) {
            linalg.CPU.LinearOp <name="model.layers.26.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=867), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=869), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=868))] (%1481:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=867)]) -> (%1482:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=869)])
            linalg.CPU.SiLUOp <name="model.layers.26.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=869), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=870), )] (%1482:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=869)]) -> (%1483:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=870)])
            linalg.CPU.LinearOp <name="model.layers.26.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=867), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=872), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=871))] (%1481:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=867)]) -> (%1484:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=872)])
            linalg.CPU.MulOp <name="model.layers.26.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=870), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=872), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=870), )] (%1483:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=870)], %1484:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=872)]) -> (%1485:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=870)])
            linalg.CPU.LinearOp <name="model.layers.26.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=870), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=874), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=873))] (%1485:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=870)]) -> (%1486:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=874)])
            cf.ReturnOp (%1486:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=874)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.27 <CPU> [using_qnn:true, symbol:model.layers.27] {
        (%1487:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=874)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %374:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=30)], %375:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=58)]) -> (%1528:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=904)], %1501:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=884)], %1503:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=886)]) {
            linalg.CPU.RMSNormOp <name="model.layers.27.input_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=874), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=875), )] (%1487:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=874)]) -> (%1488:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=875)])
            graph.CallGraphOp @model.layers.27.self_attn (%1488:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=875)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %374:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=30)], %375:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=58)]) -> (%1520:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=896)], %1501:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=884)], %1503:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=886)])
            linalg.CPU.AddOp <name="model.layers.27.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=896), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=874), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=896), )] (%1520:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=896)], %1487:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=874)]) -> (%1521:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=896)])
            linalg.CPU.RMSNormOp <name="model.layers.27.post_attention_layernorm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=896), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=897), )] (%1521:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=896)]) -> (%1522:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=897)])
            graph.CallGraphOp @model.layers.27.mlp (%1522:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=897)]) -> (%1527:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=904)])
            linalg.CPU.AddOp <name="model.layers.27.Add.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=904), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=896), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=904), )] (%1527:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=904)], %1521:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=896)]) -> (%1528:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=904)])
            cf.ReturnOp (%1528:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=904)], %1501:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=884)], %1503:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=886)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.27.self_attn <CPU> [using_qnn:true, symbol:model.layers.27.self_attn] {
        (%1488:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=875)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)], %319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %374:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=30)], %375:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=58)]) -> (%1520:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=896)], %1501:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=884)], %1503:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=886)]) {
            linalg.CPU.LinearOp <name="model.layers.27.self_attn.q_proj">(%1488:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=875)]) -> (%1489:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=880)])
            linalg.CPU.LinearOp <name="model.layers.27.self_attn.k_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=875), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=877), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=876))] (%1488:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=875)]) -> (%1490:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=877)])
            linalg.CPU.LinearOp <name="model.layers.27.self_attn.v_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=875), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=879), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=878))] (%1488:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=875)]) -> (%1491:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=879)])
            linalg.CPU.ViewOp <name="model.layers.27.self_attn.View.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=880), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=880), )] (%1489:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=880)]) -> (%1489:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=880)])
            linalg.CPU.TransposeOp <name="model.layers.27.self_attn.Transpose.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=880), outputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=880), )] (%1489:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=880)]) -> (%1492:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=880)])
            linalg.CPU.ViewOp <name="model.layers.27.self_attn.View.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=877), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=877), )] (%1490:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=877)]) -> (%1490:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=877)])
            linalg.CPU.TransposeOp <name="model.layers.27.self_attn.Transpose.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=877), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=877), )] (%1490:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=877)]) -> (%1493:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=877)])
            linalg.CPU.ViewOp <name="model.layers.27.self_attn.View.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=879), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=879), )] (%1491:tensor<[1, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=879)]) -> (%1491:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=879)])
            linalg.CPU.TransposeOp <name="model.layers.27.self_attn.Transpose.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=879), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=879), )] (%1491:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=879)]) -> (%1494:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=879)])
            linalg.CPU.RMSNormOp <name="model.layers.27.self_attn.q_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Int16PerTensor), uuid=880), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=881), )] (%1492:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(Raw(type: Int16PerTensor), uuid=880)]) -> (%1495:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=881)])
            linalg.CPU.RMSNormOp <name="model.layers.27.self_attn.k_norm"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=877), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=882), )] (%1493:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=877)]) -> (%1496:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=882)])
            linalg.CPU.RoPEOp <name="model.layers.27.self_attn.q_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=881), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=881), )] (%1495:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=881)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%1497:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=881)])
            linalg.CPU.RoPEOp <name="model.layers.27.self_attn.k_rope"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=882), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=882), )] (%1496:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=882)], %379:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=62)], %380:tensor<[1, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=64)]) -> (%1498:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=882)])
            linalg.CPU.CastTypeOp <name="model.layers.27.self_attn.CastType.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=882), outputs_0:QuantSpec(Raw(type: Float16), uuid=883), )] (%1498:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=882)]) -> (%1499:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=883)])
            linalg.CPU.CastTypeOp <name="model.layers.27.self_attn.CastType.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=883), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=884), )] (%1499:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=883)]) -> (%1500:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=884)])
            linalg.CPU.TransposeOp <name="model.layers.27.self_attn.Transpose.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=884), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=884), )] (%1500:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=884)]) -> (%1501:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=884)])
            linalg.CPU.CastTypeOp <name="model.layers.27.self_attn.CastType.2"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=879), outputs_0:QuantSpec(Raw(type: Float16), uuid=885), )] (%1494:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=879)]) -> (%1502:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=885)])
            linalg.CPU.CastTypeOp <name="model.layers.27.self_attn.CastType.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: Float16), uuid=885), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=886), )] (%1502:tensor<[1, 8, 32, 128], Float16, CPU>[quant_recipe:QuantSpec(Raw(type: Float16), uuid=885)]) -> (%1503:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=886)])
            linalg.CPU.ConcatOp <name="model.layers.27.self_attn.Concat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=30), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=884), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=30), )] (%374:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=30)], %1501:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=884)]) -> (%1504:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=30)])
            linalg.CPU.ConcatOp <name="model.layers.27.self_attn.Concat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=58), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=886), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=58), )] (%375:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=58)], %1503:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=886)]) -> (%1505:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=58)])
            linalg.CPU.RepeatOp <name="model.layers.27.self_attn.Repeat.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=30), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=30), )] (%1504:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=30)]) -> (%1506:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=30)])
            linalg.CPU.RepeatOp <name="model.layers.27.self_attn.Repeat.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=58), outputs_0:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=58), )] (%1505:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=58)]) -> (%1507:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=58)])
            linalg.CPU.MatMulOp <name="model.layers.27.self_attn.MatMul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=881), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=30), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=887), )] (%1497:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=881)], %1506:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=30)]) -> (%1508:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=887)])
            linalg.CPU.MulOp <name="model.layers.27.self_attn.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=887), inputs_1:QuantSpec(Raw(type: Float32), uuid=888), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=887), )] (%1508:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=887)], %1509:tensor<[1], Float32, CPU>[quant_recipe:QuantSpec(Raw(type: Float32), uuid=888), constant:[0.088388346]]) -> (%1510:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=887)])
            linalg.CPU.ReduceMinOp <name="model.layers.27.self_attn.ReduceMin.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=887), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=889), )] (%1510:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=887)]) -> (%1511:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=889)])
            linalg.CPU.AddOp <name="model.layers.27.self_attn.Add.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=889), inputs_1:QuantSpec(Raw(type: Int16), uuid=890), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=889), )] (%1511:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=889)], %1512:tensor<[1], Int16, CPU>[quant_recipe:QuantSpec(Raw(type: Int16), uuid=890), constant:[-20]]) -> (%1513:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=889)])
            linalg.CPU.EqualOp <name="model.layers.27.self_attn.Equal.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt16), uuid=2), inputs_1:QuantSpec(Raw(type: UInt16), uuid=891), outputs_0:QuantSpec(Raw(type: UInt8), uuid=892), )] (%319:tensor<[1, 1, 32, 1024], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=2)], %1514:tensor<[1], UInt16, CPU>[quant_recipe:QuantSpec(Raw(type: UInt16), uuid=891), constant:[0.890625]]) -> (%1515:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=892)])
            linalg.CPU.WhereOp <name="model.layers.27.self_attn.Where.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(Raw(type: UInt8), uuid=892), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=887), inputs_2:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=889), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=889), )] (%1515:tensor<[1, 1, 32, 1024], UInt8, CPU>[quant_recipe:QuantSpec(Raw(type: UInt8), uuid=892)], %1510:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=887)], %1513:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=889)]) -> (%1516:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=889)])
            linalg.CPU.SoftmaxOp <name="model.layers.27.self_attn.Softmax.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=889), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=893), )] (%1516:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=889)]) -> (%1517:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=893)])
            linalg.CPU.MatMulOp <name="model.layers.27.self_attn.MatMul.1"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=893), inputs_1:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=58), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=894), )] (%1517:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=893)], %1507:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=58)]) -> (%1518:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=894)])
            linalg.CPU.TransposeOp <name="model.layers.27.self_attn.Transpose.4"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=894), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=894), )] (%1518:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=894)]) -> (%1519:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=894)])
            linalg.CPU.ViewOp <name="model.layers.27.self_attn.View.3"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=894), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=894), )] (%1519:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=894)]) -> (%1519:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=894)])
            linalg.CPU.LinearOp <name="model.layers.27.self_attn.o_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=894), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=896), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=895))] (%1519:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=894)]) -> (%1520:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=896)])
            cf.ReturnOp (%1520:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=896)], %1501:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=884)], %1503:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -128, quant_max: 127, quant_to_type: Int8, scale_type: Float32), uuid=886)]) -> ()
        }
    }
    graph.SubGraphOp @model.layers.27.mlp <CPU> [using_qnn:true, symbol:model.layers.27.mlp] {
        (%1522:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=897)]) -> (%1527:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=904)]) {
            linalg.CPU.LinearOp <name="model.layers.27.mlp.gate_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=897), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=899), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=898))] (%1522:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=897)]) -> (%1523:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=899)])
            linalg.CPU.SiLUOp <name="model.layers.27.mlp.act"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=899), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=900), )] (%1523:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=899)]) -> (%1524:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=900)])
            linalg.CPU.LinearOp <name="model.layers.27.mlp.up_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=897), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=902), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=901))] (%1522:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=897)]) -> (%1525:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=902)])
            linalg.CPU.MulOp <name="model.layers.27.mlp.Mul.0"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=900), inputs_1:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=902), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=900), )] (%1524:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=900)], %1525:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=902)]) -> (%1526:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=900)])
            linalg.CPU.LinearOp <name="model.layers.27.mlp.down_proj"> [quant_recipe:QuantAnnotation(inputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=900), outputs_0:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=904), weight_weight:QuantSpec(LPBQ(quant_min: -8, quant_max: 7, block_size: 32, ch_axis: -1, scale_level_0_bitwidth: 4, quant_to_type: UInt4, scale_1_type: Float32), uuid=903))] (%1526:tensor<[1, 32, 6144], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=900)]) -> (%1527:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=904)])
            cf.ReturnOp (%1527:tensor<[1, 32, 2048], Int16PerTensor, CPU>[quant_recipe:QuantSpec(SymPerTensor(quant_min: -32768, quant_max: 32767, quant_to_type: Int16, scale_type: Float32), uuid=904)]) -> ()
        }
    }
    //        
    //      o o    
    //            
    //       
    //             
    //        
}
 
