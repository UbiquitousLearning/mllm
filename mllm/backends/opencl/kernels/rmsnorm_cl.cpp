#include "a_opencl_source_map.hpp"
namespace mllm::opencl {
const char* rmsnorm = "// Filename: rmsnorm.cl\n"
                      "#pragma OPENCL EXTENSION cl_khr_fp16 : enable\n"
                      "// Define the number of threads used for parallel reduction within a workgroup\n"
                      "// (must be a power of 2)\n"
                      "#define RMSNORM_WG_SIZE 256\n"
                      "// Quantized block structure matching definition in C++ DataType.hpp\n"
                      "typedef struct {\n"
                      " half d;\n"
                      " uchar qs[16]; // QK4_0/2=16\n"
                      "} block_q4_0;\n"
                      "// Kernel helper function: Dequantize an element from Q4_0 block on-the-fly\n"
                      "inline float dequantize_q4_0(const __global block_q4_0 *blocks,int index) {\n"
                      " const int block_idx=index/32; // QK4_0 is 32\n"
                      " const int quant_idx_in_block=index % 32;\n"
                      " // Locate the corresponding Q4_0 block\n"
                      " const __global block_q4_0 *b=&blocks[block_idx];\n"
                      " // Extract 4-bit quantized value from uchar\n"
                      " const uchar quant_pair=b->qs[quant_idx_in_block/2];\n"
                      " const int nibble =\n"
                      " (quant_idx_in_block % 2 == 0) ? (quant_pair & 0x0F) : (quant_pair >> 4);\n"
                      " // Apply dequantization formula\n"
                      " return (float)b->d*(float)(nibble-8);\n"
                      "}\n"
                      "__kernel void rmsnorm_f32_q4(\n"
                      " __global const float *src,// Input tensor (fp32)\n"
                      " __global float *dst,// Output tensor (fp32)\n"
                      " __global const void *weights,// Weight tensor (can be fp32 or q4_0)\n"
                      " const int weight_is_q4,// Flag: 0 means weights are fp32,1 means q4_0\n"
                      " const int D,// Dimension,i.e.,length of each row\n"
                      " const float epsilon,// Epsilon value to prevent division by zero\n"
                      " const int\n"
                      " add_unit_offset // Flag: whether to perform +1 operation on weights\n"
                      ") {\n"
                      " // 1. Get IDs\n"
                      " const int row_id=get_group_id(\n"
                      " 0); // Each workgroup processes one row,row ID determined by workgroup ID\n"
                      " const int local_id=get_local_id(0); // Thread ID within workgroup\n"
                      " // 2. Declare shared array in local memory\n"
                      " __local float local_sum_sq[RMSNORM_WG_SIZE];\n"
                      " // 3. Parallel computation of sum of squares\n"
                      " float thread_sum_sq =\n"
                      " 0.0f; // Each thread calculates sum of squares for part of elements\n"
                      " for (int i=local_id; i<D; i += RMSNORM_WG_SIZE) {\n"
                      " float val=src[row_id*D+i];\n"
                      " thread_sum_sq += val*val;\n"
                      " }\n"
                      " local_sum_sq[local_id]=thread_sum_sq;\n"
                      " // 4. Workgroup reduction to compute total sum of squares for the entire row\n"
                      " barrier(CLK_LOCAL_MEM_FENCE);\n"
                      " for (int s=RMSNORM_WG_SIZE/2; s>0; s >>= 1) {\n"
                      " if (local_id<s) {\n"
                      " local_sum_sq[local_id] += local_sum_sq[local_id+s];\n"
                      " }\n"
                      " barrier(CLK_LOCAL_MEM_FENCE);\n"
                      " }\n"
                      " // At this point,local_sum_sq[0] contains the sum of squares for the entire\n"
                      " // row\n"
                      " // 5. Calculate RMS scaling factor and broadcast safely\n"
                      " float rms_val;\n"
                      " // Only the first thread in the workgroup performs this scalar calculation\n"
                      " if (local_id == 0) {\n"
                      " float variance=local_sum_sq[0]/D;\n"
                      " rms_val=rsqrt(variance+epsilon);\n"
                      " local_sum_sq[0] =\n"
                      " rms_val; // Thread 0 calculates result and stores in shared memory\n"
                      " }\n"
                      " // Synchronization point: Ensure all threads wait for thread 0 to write\n"
                      " // rms_val to shared memory\n"
                      " barrier(CLK_LOCAL_MEM_FENCE);\n"
                      " // All threads (including thread 0) read the broadcast value from shared\n"
                      " // memory\n"
                      " rms_val=local_sum_sq[0];\n"
                      " // 6. Parallel normalization and apply weights\n"
                      " for (int i=local_id; i<D; i += RMSNORM_WG_SIZE) {\n"
                      " // a. Get weight value\n"
                      " float weight_val;\n"
                      " if (weight_is_q4) {\n"
                      " weight_val=dequantize_q4_0((const __global block_q4_0 *)weights,i);\n"
                      " } else {\n"
                      " weight_val=((const __global float *)weights)[i];\n"
                      " }\n"
                      " // b. Decide whether to add 1 based on flag\n"
                      " if (add_unit_offset) {\n"
                      " weight_val += 1.0f;\n"
                      " }\n"
                      " // c. Calculate final result and write back to global memory\n"
                      " size_t index=row_id*D+i;\n"
                      " dst[index]=src[index]*rms_val*weight_val;\n"
                      " }\n"
                      "}\n"
                      "// ==================================================================\n"
                      "// 2. FP16 Input Kernel (rmsnorm_f16_q4)\n"
                      "// ==================================================================\n"
                      "__kernel void rmsnorm_f16_q4(\n"
                      " __global const half *src,// Input tensor (fp16)\n"
                      " __global half *dst,// Output tensor (fp16)\n"
                      " __global const void *weights,// Weight tensor (can be fp32 or q4_0)\n"
                      " const int weight_is_q4,// Flag\n"
                      " const int D,// Dimension\n"
                      " const float epsilon,// Epsilon (still float)\n"
                      " const int add_unit_offset // Flag\n"
                      ") {\n"
                      " const int row_id=get_group_id(0);\n"
                      " const int local_id=get_local_id(0);\n"
                      " __local float local_sum_sq[RMSNORM_WG_SIZE];\n"
                      " // Use float accumulator to ensure precision\n"
                      " float thread_sum_sq=0.0f;\n"
                      " for (int i=local_id; i<D; i += RMSNORM_WG_SIZE) {\n"
                      " // Convert from half to float for calculation\n"
                      " float val=(float)src[row_id*D+i];\n"
                      " thread_sum_sq += val*val;\n"
                      " }\n"
                      " local_sum_sq[local_id]=thread_sum_sq;\n"
                      " // Workgroup reduction (identical to fp32 version)\n"
                      " barrier(CLK_LOCAL_MEM_FENCE);\n"
                      " for (int s=RMSNORM_WG_SIZE/2; s>0; s >>= 1) {\n"
                      " if (local_id<s) {\n"
                      " local_sum_sq[local_id] += local_sum_sq[local_id+s];\n"
                      " }\n"
                      " barrier(CLK_LOCAL_MEM_FENCE);\n"
                      " }\n"
                      " // Calculate RMS scaling factor (identical to fp32 version)\n"
                      " float rms_val;\n"
                      " if (local_id == 0) {\n"
                      " float variance=local_sum_sq[0]/D;\n"
                      " rms_val=rsqrt(variance+epsilon);\n"
                      " local_sum_sq[0]=rms_val;\n"
                      " }\n"
                      " barrier(CLK_LOCAL_MEM_FENCE);\n"
                      " rms_val=local_sum_sq[0];\n"
                      " // Normalization and apply weights\n"
                      " for (int i=local_id; i<D; i += RMSNORM_WG_SIZE) {\n"
                      " float weight_val;\n"
                      " if (weight_is_q4) {\n"
                      " weight_val=dequantize_q4_0((const __global block_q4_0 *)weights,i);\n"
                      " } else {\n"
                      " weight_val=((const __global float *)weights)[i];\n"
                      " }\n"
                      " if (add_unit_offset) {\n"
                      " weight_val += 1.0f;\n"
                      " }\n"
                      " size_t index=row_id*D+i;\n"
                      " // Calculation result is float,finally convert back to half and store in\n"
                      " // dst\n"
                      " float src_val=(float)src[index];\n"
                      " dst[index]=(half)(src_val*rms_val*weight_val);\n"
                      " }\n"
                      "}\n";
}  // namespace mllm::opencl
