#include "a_opencl_source_map.hpp"
namespace mllm::opencl {
const char* matmul_transb_bias = "// kernel/matmul_transb_bias.cl\n"
                                 "#pragma OPENCL EXTENSION cl_khr_fp16 : enable\n"
                                 "#define TILE_SIZE 16\n"
                                 "#define QK4_0 32\n"
                                 "#define QK8_0 32\n"
                                 "typedef struct {\n"
                                 " half d;\n"
                                 " uchar qs[QK4_0/2];\n"
                                 "} block_q4_0;\n"
                                 "// ==================================================================\n"
                                 "// 1. FP32 Fused GEMM+Bias Kernel\n"
                                 "// ==================================================================\n"
                                 "__kernel void gemm_fp32_transb_bias(__global const float *A,\n"
                                 " __global const float *B,\n"
                                 " __global const float *bias,\n"
                                 " __global float *C,const int M,const int K,\n"
                                 " const int N,const int has_bias) {\n"
                                 " const int s=get_global_id(1);\n"
                                 " const int n=get_global_id(0);\n"
                                 " const int batch_idx=get_global_id(2);\n"
                                 " const int local_row=get_local_id(1);\n"
                                 " const int local_col=get_local_id(0);\n"
                                 " __local float a_tile[TILE_SIZE][TILE_SIZE];\n"
                                 " __local float b_tile[TILE_SIZE][TILE_SIZE];\n"
                                 " float acc=0.0f;\n"
                                 " const int num_tiles=(K+TILE_SIZE-1)/TILE_SIZE;\n"
                                 " for (int t=0; t<num_tiles; ++t) {\n"
                                 " const int k_start=t*TILE_SIZE;\n"
                                 " const int a_k_idx=k_start+local_col;\n"
                                 " if (s<M && a_k_idx<K) {\n"
                                 " a_tile[local_row][local_col] =\n"
                                 " A[(long)batch_idx*M*K+(long)s*K+a_k_idx];\n"
                                 " } else {\n"
                                 " a_tile[local_row][local_col]=0.0f;\n"
                                 " }\n"
                                 " const int b_n_idx=get_group_id(0)*TILE_SIZE+local_row;\n"
                                 " const int b_k_idx=k_start+local_col;\n"
                                 " if (b_n_idx<N && b_k_idx<K) {\n"
                                 " b_tile[local_row][local_col]=B[(long)b_n_idx*K+b_k_idx];\n"
                                 " } else {\n"
                                 " b_tile[local_row][local_col]=0.0f;\n"
                                 " }\n"
                                 " barrier(CLK_LOCAL_MEM_FENCE);\n"
                                 " for (int k_tile=0; k_tile<TILE_SIZE; ++k_tile) {\n"
                                 " acc += a_tile[local_row][k_tile]*b_tile[local_col][k_tile];\n"
                                 " }\n"
                                 " barrier(CLK_LOCAL_MEM_FENCE);\n"
                                 " }\n"
                                 " if (s<M && n<N) {\n"
                                 " if (has_bias != 0) {\n"
                                 " acc += bias[n];\n"
                                 " }\n"
                                 " C[(long)batch_idx*M*N+(long)s*N+n]=acc;\n"
                                 " }\n"
                                 "}\n"
                                 "__kernel void gemm_fp32_fp16_transb_bias(__global const float *A,\n"
                                 " __global const half *B,\n"
                                 " __global const float *bias,\n"
                                 " __global float *C,const int M,\n"
                                 " const int K,const int N,\n"
                                 " const int has_bias) {\n"
                                 " const int s=get_global_id(1);\n"
                                 " const int n=get_global_id(0);\n"
                                 " const int batch_idx=get_global_id(2);\n"
                                 " const int local_row=get_local_id(1);\n"
                                 " const int local_col=get_local_id(0);\n"
                                 " __local float a_tile[TILE_SIZE][TILE_SIZE];\n"
                                 " __local float b_tile[TILE_SIZE][TILE_SIZE];\n"
                                 " float acc=0.0f;\n"
                                 " const int num_tiles=(K+TILE_SIZE-1)/TILE_SIZE;\n"
                                 " for (int t=0; t<num_tiles; ++t) {\n"
                                 " const int k_start=t*TILE_SIZE;\n"
                                 " const int a_k_idx=k_start+local_col;\n"
                                 " if (s<M && a_k_idx<K) {\n"
                                 " a_tile[local_row][local_col] =\n"
                                 " A[(long)batch_idx*M*K+(long)s*K+a_k_idx];\n"
                                 " } else {\n"
                                 " a_tile[local_row][local_col]=0.0f;\n"
                                 " }\n"
                                 " const int b_n_idx=get_group_id(0)*TILE_SIZE+local_row;\n"
                                 " const int b_k_idx=k_start+local_col;\n"
                                 " if (b_n_idx<N && b_k_idx<K) {\n"
                                 " b_tile[local_row][local_col]=(float)B[(long)b_n_idx*K+b_k_idx];\n"
                                 " } else {\n"
                                 " b_tile[local_row][local_col]=0.0f;\n"
                                 " }\n"
                                 " barrier(CLK_LOCAL_MEM_FENCE);\n"
                                 " for (int k_tile=0; k_tile<TILE_SIZE; ++k_tile) {\n"
                                 " acc += a_tile[local_row][k_tile]*b_tile[local_col][k_tile];\n"
                                 " }\n"
                                 " barrier(CLK_LOCAL_MEM_FENCE);\n"
                                 " }\n"
                                 " if (s<M && n<N) {\n"
                                 " if (has_bias != 0) {\n"
                                 " acc += bias[n];\n"
                                 " }\n"
                                 " C[(long)batch_idx*M*N+(long)s*N+n]=acc;\n"
                                 " }\n"
                                 "}\n"
                                 "// ==================================================================\n"
                                 "// 2. FP16 Fused GEMM+Bias Kernel\n"
                                 "// ==================================================================\n"
                                 "#if defined(SUPPORTS_FP16)\n"
                                 "__kernel void gemm_fp16_transb_bias(__global const half *A,\n"
                                 " __global const half *B,\n"
                                 " __global const float *bias,\n"
                                 " __global half *C,const int M,const int K,\n"
                                 " const int N,const int has_bias) {\n"
                                 " const int s=get_global_id(1);\n"
                                 " const int n=get_global_id(0);\n"
                                 " const int batch_idx=get_global_id(2);\n"
                                 " const int local_row=get_local_id(1);\n"
                                 " const int local_col=get_local_id(0);\n"
                                 " __local half a_tile[TILE_SIZE][TILE_SIZE];\n"
                                 " __local half b_tile[TILE_SIZE][TILE_SIZE];\n"
                                 " half acc=0.0h;\n"
                                 " const int num_tiles=(K+TILE_SIZE-1)/TILE_SIZE;\n"
                                 " for (int t=0; t<num_tiles; ++t) {\n"
                                 " const int k_start=t*TILE_SIZE;\n"
                                 " const int a_k_idx=k_start+local_col;\n"
                                 " if (s<M && a_k_idx<K) {\n"
                                 " a_tile[local_row][local_col] =\n"
                                 " A[(long)batch_idx*M*K+(long)s*K+a_k_idx];\n"
                                 " } else {\n"
                                 " a_tile[local_row][local_col]=0.0h;\n"
                                 " }\n"
                                 " const int b_n_idx=get_group_id(0)*TILE_SIZE+local_row;\n"
                                 " const int b_k_idx=k_start+local_col;\n"
                                 " if (b_n_idx<N && b_k_idx<K) {\n"
                                 " b_tile[local_row][local_col]=B[(long)b_n_idx*K+b_k_idx];\n"
                                 " } else {\n"
                                 " b_tile[local_row][local_col]=0.0h;\n"
                                 " }\n"
                                 " barrier(CLK_LOCAL_MEM_FENCE);\n"
                                 " for (int k_tile=0; k_tile<TILE_SIZE; ++k_tile) {\n"
                                 " acc += a_tile[local_row][k_tile]*b_tile[local_col][k_tile];\n"
                                 " }\n"
                                 " barrier(CLK_LOCAL_MEM_FENCE);\n"
                                 " }\n"
                                 " if (s<M && n<N) {\n"
                                 " if (has_bias != 0) {\n"
                                 " acc += bias[n];\n"
                                 " }\n"
                                 " C[(long)batch_idx*M*N+(long)s*N+n]=acc;\n"
                                 " }\n"
                                 "}\n"
                                 "#else\n"
                                 "__kernel void gemm_fp16_transb_bias(__global const half *A,\n"
                                 " __global const half *B,\n"
                                 " __global const float *bias,\n"
                                 " __global half *C,const int M,const int K,\n"
                                 " const int N,const int has_bias) {\n"
                                 " const int s=get_global_id(1);\n"
                                 " const int n=get_global_id(0);\n"
                                 " const int batch_idx=get_global_id(2);\n"
                                 " if (s >= M || n >= N)\n"
                                 " return;\n"
                                 " float acc=0.0f;\n"
                                 " for (int k=0; k<K; ++k) {\n"
                                 " long a_idx=(long)batch_idx*M*K+(long)s*K+k;\n"
                                 " long b_idx=(long)n*K+k;\n"
                                 " acc += (float)A[a_idx]*(float)B[b_idx];\n"
                                 " }\n"
                                 " if (has_bias != 0) {\n"
                                 " acc += bias[n];\n"
                                 " }\n"
                                 " long c_idx=(long)batch_idx*M*N+(long)s*N+n;\n"
                                 " C[c_idx]=(half)acc;\n"
                                 "}\n"
                                 "#endif // SUPPORTS_FP16\n"
                                 "// ==================================================================\n"
                                 "// 3. FP32*Q4_0 Fused GEMV+Bias Kernels (for M=1,Decoding)\n"
                                 "// ==================================================================\n"
                                 "__kernel void gemv_fp32_q4_0_transb_bias(__global const float *A,\n"
                                 " __global const block_q4_0 *B,\n"
                                 " __global const float *bias,\n"
                                 " __global float *C,const int K,\n"
                                 " const int N,const int has_bias) {\n"
                                 " const int n=get_group_id(0);\n"
                                 " const int batch_idx=get_group_id(1);\n"
                                 " if (n >= N)\n"
                                 " return;\n"
                                 " const int local_id=get_local_id(0);\n"
                                 " const int wg_size=get_local_size(0);\n"
                                 " __local float partial_sums[256];\n"
                                 " float private_acc=0.0f;\n"
                                 " const long a_base_idx=(long)batch_idx*K;\n"
                                 " const long b_row_offset_blocks=(long)n*(K/QK4_0);\n"
                                 " for (int k=local_id; k<K; k += wg_size) {\n"
                                 " const int k_block_idx=k/QK4_0;\n"
                                 " const int k_in_block=k % QK4_0;\n"
                                 " const __global block_q4_0 *b_block_ptr =\n"
                                 " &B[b_row_offset_blocks+k_block_idx];\n"
                                 "#if defined(SUPPORTS_FP16)\n"
                                 " const float d_b=vload_half(0,(__global half *)(&(b_block_ptr->d)));\n"
                                 "#else\n"
                                 " const float d_b=(float)(b_block_ptr->d); // TODO Change here [gemini]\n"
                                 "#endif\n"
                                 " const uchar q_packed=b_block_ptr->qs[k_in_block % 16];\n"
                                 " char q_nibble=(k_in_block<16) ? (q_packed & 0x0F) : (q_packed >> 4);\n"
                                 " const float b_val=(float)(q_nibble-8)*d_b;\n"
                                 " private_acc += A[a_base_idx+k]*b_val;\n"
                                 " }\n"
                                 " partial_sums[local_id]=private_acc;\n"
                                 " barrier(CLK_LOCAL_MEM_FENCE);\n"
                                 " for (int offset=wg_size/2; offset>0; offset >>= 1) {\n"
                                 " if (local_id<offset) {\n"
                                 " partial_sums[local_id] += partial_sums[local_id+offset];\n"
                                 " }\n"
                                 " barrier(CLK_LOCAL_MEM_FENCE);\n"
                                 " }\n"
                                 " if (local_id == 0) {\n"
                                 " float final_val=partial_sums[0];\n"
                                 " if (has_bias != 0) {\n"
                                 " final_val += bias[n];\n"
                                 " }\n"
                                 " const long c_idx=(long)batch_idx*N+n;\n"
                                 " C[c_idx]=final_val;\n"
                                 " }\n"
                                 "}\n"
                                 "// ==================================================================\n"
                                 "// 4. FP32*Q4_0 Fused GEMM+Bias Kernels (for M>1,Training)\n"
                                 "// ==================================================================\n"
                                 "__kernel void gemm_fp32_q4_0_transb_bias(__global const float *A,\n"
                                 " __global const block_q4_0 *B,\n"
                                 " __global const float *bias,\n"
                                 " __global float *C,const int M,\n"
                                 " const int K,const int N,\n"
                                 " const int has_bias) {\n"
                                 " const int s=get_global_id(1);\n"
                                 " const int n=get_global_id(0);\n"
                                 " const int batch_idx=get_global_id(2);\n"
                                 " if (s >= M || n >= N) {\n"
                                 " return;\n"
                                 " }\n"
                                 " float acc=0.0f;\n"
                                 " const long a_row_offset=(long)batch_idx*M*K+(long)s*K;\n"
                                 " const long b_row_offset_blocks=(long)n*(K/QK4_0);\n"
                                 " for (int k_block_idx=0; k_block_idx<K/QK4_0; ++k_block_idx) {\n"
                                 " const __global block_q4_0 *b_block_ptr =\n"
                                 " &B[b_row_offset_blocks+k_block_idx];\n"
                                 "#if defined(SUPPORTS_FP16)\n"
                                 " const float d_b=vload_half(0,(__global half *)(&(b_block_ptr->d)));\n"
                                 "#else\n"
                                 " const float d_b=(float)(b_block_ptr->d); // TODO Change here [gemini]\n"
                                 "#endif\n"
                                 " const __global float *a_ptr=A+a_row_offset+k_block_idx*QK4_0;\n"
                                 " for (int j=0; j<QK4_0/2; j += 4) { // QK4_0/2=16\n"
                                 " const uchar q_packed0=b_block_ptr->qs[j+0];\n"
                                 " const uchar q_packed1=b_block_ptr->qs[j+1];\n"
                                 " const uchar q_packed2=b_block_ptr->qs[j+2];\n"
                                 " const uchar q_packed3=b_block_ptr->qs[j+3];\n"
                                 " const float4 a_vals_lo=vload4(0,a_ptr+j);\n"
                                 " const float4 a_vals_hi =\n"
                                 " vload4(0,a_ptr+j+(QK4_0/2)); // (QK4_0/2)=16\n"
                                 " float4 b_dequant_lo;\n"
                                 " b_dequant_lo.x=(float)((q_packed0 & 0x0F)-8)*d_b;\n"
                                 " b_dequant_lo.y=(float)((q_packed1 & 0x0F)-8)*d_b;\n"
                                 " b_dequant_lo.z=(float)((q_packed2 & 0x0F)-8)*d_b;\n"
                                 " b_dequant_lo.w=(float)((q_packed3 & 0x0F)-8)*d_b;\n"
                                 " float4 b_dequant_hi;\n"
                                 " b_dequant_hi.x=(float)((q_packed0 >> 4)-8)*d_b;\n"
                                 " b_dequant_hi.y=(float)((q_packed1 >> 4)-8)*d_b;\n"
                                 " b_dequant_hi.z=(float)((q_packed2 >> 4)-8)*d_b;\n"
                                 " b_dequant_hi.w=(float)((q_packed3 >> 4)-8)*d_b;\n"
                                 " acc += dot(a_vals_lo,b_dequant_lo);\n"
                                 " acc += dot(a_vals_hi,b_dequant_hi);\n"
                                 " }\n"
                                 " }\n"
                                 " if (has_bias != 0) {\n"
                                 " acc += bias[n];\n"
                                 " }\n"
                                 " const long c_idx=(long)batch_idx*M*N+(long)s*N+n;\n"
                                 " C[c_idx]=acc;\n"
                                 "}\n"
                                 "// ==================================================================\n"
                                 "// 5. FP16*Q4_0 Fused GEMV+Bias Kernel (for M=1,Decoding)\n"
                                 "// ==================================================================\n"
                                 "#if defined(SUPPORTS_FP16)\n"
                                 "__kernel void gemv_fp16_q4_0_transb_bias(__global const half *A,\n"
                                 " __global const block_q4_0 *B,\n"
                                 " __global const float *bias,\n"
                                 " __global half *C,const int K,\n"
                                 " const int N,const int has_bias) {\n"
                                 " const int n=get_group_id(0);\n"
                                 " const int batch_idx=get_group_id(1);\n"
                                 " if (n >= N)\n"
                                 " return;\n"
                                 " const int local_id=get_local_id(0);\n"
                                 " const int wg_size=get_local_size(0);\n"
                                 " __local float partial_sums[256];\n"
                                 " float private_acc=0.0f;\n"
                                 " const long a_base_idx=(long)batch_idx*K;\n"
                                 " const long b_row_offset_blocks=(long)n*(K/QK4_0);\n"
                                 " const int num_k_blocks=K/QK4_0;\n"
                                 " for (int k_block_idx=local_id; k_block_idx<num_k_blocks;\n"
                                 " k_block_idx += wg_size) {\n"
                                 " const __global block_q4_0 *b_block_ptr =\n"
                                 " &B[b_row_offset_blocks+k_block_idx];\n"
                                 " const float d_b=vload_half(0,(__global half *)(&(b_block_ptr->d)));\n"
                                 " const __global half *a_ptr=A+a_base_idx+k_block_idx*QK4_0;\n"
                                 "#pragma unroll\n"
                                 " for (int j=0; j<QK4_0/2; j += 4) {\n"
                                 " const uchar q_packed0=b_block_ptr->qs[j+0];\n"
                                 " const uchar q_packed1=b_block_ptr->qs[j+1];\n"
                                 " const uchar q_packed2=b_block_ptr->qs[j+2];\n"
                                 " const uchar q_packed3=b_block_ptr->qs[j+3];\n"
                                 " float4 a_vals_lo,a_vals_hi;\n"
                                 " a_vals_lo.x=(float)a_ptr[j+0];\n"
                                 " a_vals_lo.y=(float)a_ptr[j+1];\n"
                                 " a_vals_lo.z=(float)a_ptr[j+2];\n"
                                 " a_vals_lo.w=(float)a_ptr[j+3];\n"
                                 " a_vals_hi.x=(float)a_ptr[j+16+0];\n"
                                 " a_vals_hi.y=(float)a_ptr[j+16+1];\n"
                                 " a_vals_hi.z=(float)a_ptr[j+16+2];\n"
                                 " a_vals_hi.w=(float)a_ptr[j+16+3];\n"
                                 " float4 b_dequant_lo,b_dequant_hi;\n"
                                 " b_dequant_lo.x=(float)((q_packed0 & 0x0F)-8)*d_b;\n"
                                 " b_dequant_lo.y=(float)((q_packed1 & 0x0F)-8)*d_b;\n"
                                 " b_dequant_lo.z=(float)((q_packed2 & 0x0F)-8)*d_b;\n"
                                 " b_dequant_lo.w=(float)((q_packed3 & 0x0F)-8)*d_b;\n"
                                 " b_dequant_hi.x=(float)((q_packed0 >> 4)-8)*d_b;\n"
                                 " b_dequant_hi.y=(float)((q_packed1 >> 4)-8)*d_b;\n"
                                 " b_dequant_hi.z=(float)((q_packed2 >> 4)-8)*d_b;\n"
                                 " b_dequant_hi.w=(float)((q_packed3 >> 4)-8)*d_b;\n"
                                 " private_acc += dot(a_vals_lo,b_dequant_lo);\n"
                                 " private_acc += dot(a_vals_hi,b_dequant_hi);\n"
                                 " }\n"
                                 " }\n"
                                 " partial_sums[local_id]=private_acc;\n"
                                 " barrier(CLK_LOCAL_MEM_FENCE);\n"
                                 " for (int offset=wg_size/2; offset>0; offset >>= 1) {\n"
                                 " if (local_id<offset) {\n"
                                 " partial_sums[local_id] += partial_sums[local_id+offset];\n"
                                 " }\n"
                                 " barrier(CLK_LOCAL_MEM_FENCE);\n"
                                 " }\n"
                                 " if (local_id == 0) {\n"
                                 " float final_val=partial_sums[0];\n"
                                 " if (has_bias != 0) {\n"
                                 " final_val += bias[n];\n"
                                 " }\n"
                                 " const long c_idx=(long)batch_idx*N+n;\n"
                                 " vstore_half_rte(final_val,0,&C[c_idx]);\n"
                                 " }\n"
                                 "}\n"
                                 "inline float hsum_half16(half16 v) {\n"
                                 " half8 r1=v.lo+v.hi;\n"
                                 " half4 r2=r1.lo+r1.hi;\n"
                                 " half2 r3=r2.lo+r2.hi;\n"
                                 " return (float)(r3.x+r3.y);\n"
                                 "}\n"
                                 "__kernel void gemv_fp16_q4_0_transb_bias_half16(__global const half *A,\n"
                                 " __global const block_q4_0 *B,\n"
                                 " __global const float *bias,\n"
                                 " __global half *C,const int K,\n"
                                 " const int N,\n"
                                 " const int has_bias) {\n"
                                 " const int n=get_group_id(0);\n"
                                 " const int bh_idx=get_group_id(1);\n"
                                 " const int batch_idx=get_group_id(2);\n"
                                 " if (n >= N)\n"
                                 " return;\n"
                                 " const int local_id=get_local_id(0);\n"
                                 " const int wg_size=get_local_size(0);\n"
                                 " float private_acc=0.0f;\n"
                                 " __local float partial_sums[256];\n"
                                 " const long a_base_idx=(long)batch_idx*K;\n"
                                 " const long b_row_offset_blocks=(long)n*(K/QK4_0);\n"
                                 " const int num_k_blocks=K/QK4_0;\n"
                                 " for (int k_block_idx=local_id; k_block_idx<num_k_blocks;\n"
                                 " k_block_idx += wg_size) {\n"
                                 " const __global block_q4_0 *b_block_ptr =\n"
                                 " &B[b_row_offset_blocks+k_block_idx];\n"
                                 " const half d_b=b_block_ptr->d;\n"
                                 " const __global half *a_ptr=A+a_base_idx+k_block_idx*QK4_0;\n"
                                 " const half16 a_vals_lo=vload16(0,a_ptr);\n"
                                 " const uchar8 q_packed_lo=vload8(0,b_block_ptr->qs);\n"
                                 " char16 b_s_lo;\n"
                                 " b_s_lo.s0=(q_packed_lo.s0 & 0x0F)-8;\n"
                                 " b_s_lo.s1=(q_packed_lo.s0 >> 4)-8;\n"
                                 " b_s_lo.s2=(q_packed_lo.s1 & 0x0F)-8;\n"
                                 " b_s_lo.s3=(q_packed_lo.s1 >> 4)-8;\n"
                                 " b_s_lo.s4=(q_packed_lo.s2 & 0x0F)-8;\n"
                                 " b_s_lo.s5=(q_packed_lo.s2 >> 4)-8;\n"
                                 " b_s_lo.s6=(q_packed_lo.s3 & 0x0F)-8;\n"
                                 " b_s_lo.s7=(q_packed_lo.s3 >> 4)-8;\n"
                                 " b_s_lo.s8=(q_packed_lo.s4 & 0x0F)-8;\n"
                                 " b_s_lo.s9=(q_packed_lo.s4 >> 4)-8;\n"
                                 " b_s_lo.sa=(q_packed_lo.s5 & 0x0F)-8;\n"
                                 " b_s_lo.sb=(q_packed_lo.s5 >> 4)-8;\n"
                                 " b_s_lo.sc=(q_packed_lo.s6 & 0x0F)-8;\n"
                                 " b_s_lo.sd=(q_packed_lo.s6 >> 4)-8;\n"
                                 " b_s_lo.se=(q_packed_lo.s7 & 0x0F)-8;\n"
                                 " b_s_lo.sf=(q_packed_lo.s7 >> 4)-8;\n"
                                 " const half16 b_vals_dequant_lo=convert_half16(b_s_lo)*d_b;\n"
                                 " private_acc += hsum_half16(a_vals_lo*b_vals_dequant_lo);\n"
                                 " const half16 a_vals_hi=vload16(0,a_ptr+16);\n"
                                 " const uchar8 q_packed_hi=vload8(0,b_block_ptr->qs+8);\n"
                                 " char16 b_s_hi;\n"
                                 " b_s_hi.s0=(q_packed_hi.s0 & 0x0F)-8;\n"
                                 " b_s_hi.s1=(q_packed_hi.s0 >> 4)-8;\n"
                                 " b_s_hi.s2=(q_packed_hi.s1 & 0x0F)-8;\n"
                                 " b_s_hi.s3=(q_packed_hi.s1 >> 4)-8;\n"
                                 " b_s_hi.s4=(q_packed_hi.s2 & 0x0F)-8;\n"
                                 " b_s_hi.s5=(q_packed_hi.s2 >> 4)-8;\n"
                                 " b_s_hi.s6=(q_packed_hi.s3 & 0x0F)-8;\n"
                                 " b_s_hi.s7=(q_packed_hi.s3 >> 4)-8;\n"
                                 " b_s_hi.s8=(q_packed_hi.s4 & 0x0F)-8;\n"
                                 " b_s_hi.s9=(q_packed_hi.s4 >> 4)-8;\n"
                                 " b_s_hi.sa=(q_packed_hi.s5 & 0x0F)-8;\n"
                                 " b_s_hi.sb=(q_packed_hi.s5 >> 4)-8;\n"
                                 " b_s_hi.sc=(q_packed_hi.s6 & 0x0F)-8;\n"
                                 " b_s_hi.sd=(q_packed_hi.s6 >> 4)-8;\n"
                                 " b_s_hi.se=(q_packed_hi.s7 & 0x0F)-8;\n"
                                 " b_s_hi.sf=(q_packed_hi.s7 >> 4)-8;\n"
                                 " const half16 b_vals_dequant_hi=convert_half16(b_s_hi)*d_b;\n"
                                 " private_acc += hsum_half16(a_vals_hi*b_vals_dequant_hi);\n"
                                 " }\n"
                                 " partial_sums[local_id]=private_acc;\n"
                                 " barrier(CLK_LOCAL_MEM_FENCE);\n"
                                 " for (int offset=wg_size/2; offset>0; offset >>= 1) {\n"
                                 " if (local_id<offset) {\n"
                                 " partial_sums[local_id] += partial_sums[local_id+offset];\n"
                                 " }\n"
                                 " barrier(CLK_LOCAL_MEM_FENCE);\n"
                                 " }\n"
                                 " if (local_id == 0) {\n"
                                 " float final_val=partial_sums[0];\n"
                                 " if (has_bias != 0) {\n"
                                 " final_val += bias[n];\n"
                                 " }\n"
                                 " const long c_idx=(long)batch_idx*N+n;\n"
                                 " vstore_half_rte(final_val,0,&C[c_idx]);\n"
                                 " }\n"
                                 "}\n"
                                 "#else\n"
                                 "__kernel void gemv_fp16_q4_0_transb_bias(__global const half *A,\n"
                                 " __global const block_q4_0 *B,\n"
                                 " __global const float *bias,\n"
                                 " __global half *C,const int K,\n"
                                 " const int N,const int has_bias) {\n"
                                 " const int n=get_group_id(0);\n"
                                 " const int bh_idx=get_group_id(1);\n"
                                 " const int batch_idx=bh_idx;\n"
                                 " if (n >= N)\n"
                                 " return;\n"
                                 " const int local_id=get_local_id(0);\n"
                                 " const int wg_size=get_local_size(0);\n"
                                 " __local float partial_sums[256];\n"
                                 " float private_acc=0.0f;\n"
                                 " const long a_base_idx=(long)batch_idx*K;\n"
                                 " const long b_row_offset_blocks=(long)n*(K/QK4_0);\n"
                                 " const int num_k_blocks=K/QK4_0;\n"
                                 " for (int k_block_idx=local_id; k_block_idx<num_k_blocks;\n"
                                 " k_block_idx += wg_size) {\n"
                                 " const __global block_q4_0 *b_block_ptr =\n"
                                 " &B[b_row_offset_blocks+k_block_idx];\n"
                                 " const float d_b=vload_half(0,(__global half *)(&(b_block_ptr->d)));\n"
                                 " const __global half *a_ptr=A+a_base_idx+k_block_idx*QK4_0;\n"
                                 "#pragma unroll\n"
                                 " for (int j=0; j<QK4_0/2; j += 4) {\n"
                                 " const uchar q_packed0=b_block_ptr->qs[j+0];\n"
                                 " const uchar q_packed1=b_block_ptr->qs[j+1];\n"
                                 " const uchar q_packed2=b_block_ptr->qs[j+2];\n"
                                 " const uchar q_packed3=b_block_ptr->qs[j+3];\n"
                                 " float4 a_vals_lo,a_vals_hi;\n"
                                 " a_vals_lo.x=(float)a_ptr[j+0];\n"
                                 " a_vals_lo.y=(float)a_ptr[j+1];\n"
                                 " a_vals_lo.z=(float)a_ptr[j+2];\n"
                                 " a_vals_lo.w=(float)a_ptr[j+3];\n"
                                 " a_vals_hi.x=(float)a_ptr[j+16+0];\n"
                                 " a_vals_hi.y=(float)a_ptr[j+16+1];\n"
                                 " a_vals_hi.z=(float)a_ptr[j+16+2];\n"
                                 " a_vals_hi.w=(float)a_ptr[j+16+3];\n"
                                 " float4 b_dequant_lo,b_dequant_hi;\n"
                                 " b_dequant_lo.x=(float)((q_packed0 & 0x0F)-8)*d_b;\n"
                                 " b_dequant_lo.y=(float)((q_packed1 & 0x0F)-8)*d_b;\n"
                                 " b_dequant_lo.z=(float)((q_packed2 & 0x0F)-8)*d_b;\n"
                                 " b_dequant_lo.w=(float)((q_packed3 & 0x0F)-8)*d_b;\n"
                                 " b_dequant_hi.x=(float)((q_packed0 >> 4)-8)*d_b;\n"
                                 " b_dequant_hi.y=(float)((q_packed1 >> 4)-8)*d_b;\n"
                                 " b_dequant_hi.z=(float)((q_packed2 >> 4)-8)*d_b;\n"
                                 " b_dequant_hi.w=(float)((q_packed3 >> 4)-8)*d_b;\n"
                                 " private_acc += dot(a_vals_lo,b_dequant_lo);\n"
                                 " private_acc += dot(a_vals_hi,b_dequant_hi);\n"
                                 " }\n"
                                 " }\n"
                                 " partial_sums[local_id]=private_acc;\n"
                                 " barrier(CLK_LOCAL_MEM_FENCE);\n"
                                 " for (int offset=wg_size/2; offset>0; offset >>= 1) {\n"
                                 " if (local_id<offset) {\n"
                                 " partial_sums[local_id] += partial_sums[local_id+offset];\n"
                                 " }\n"
                                 " barrier(CLK_LOCAL_MEM_FENCE);\n"
                                 " }\n"
                                 " if (local_id == 0) {\n"
                                 " float final_val=partial_sums[0];\n"
                                 " if (has_bias != 0) {\n"
                                 " final_val += bias[n];\n"
                                 " }\n"
                                 " const long c_idx=(long)batch_idx*N+n;\n"
                                 " vstore_half_rte(final_val,0,&C[c_idx]);\n"
                                 " }\n"
                                 "}\n"
                                 "#endif // SUPPORTS_FP16\n"
                                 "// ==================================================================\n"
                                 "// 6. FP16*Q4_0 Fused GEMM+Bias Kernel (for M>1,Prefill)\n"
                                 "// ==================================================================\n"
                                 "#if defined(SUPPORTS_FP16)\n"
                                 "#define TILE_M 64\n"
                                 "#define TILE_N 64\n"
                                 "#define TILE_K 16\n"
                                 "#define WPT_M 8\n"
                                 "#define WPT_N 8\n"
                                 "#define THREADS_X (TILE_N/WPT_N) // 8\n"
                                 "#define THREADS_Y (TILE_M/WPT_M) // 8\n"
                                 "__kernel void gemm_fp16_q4_0_transb_bias(__global const half *A,\n"
                                 " __global const block_q4_0 *B,\n"
                                 " __global const float *bias,\n"
                                 " __global half *C,const int M,\n"
                                 " const int K,const int N,\n"
                                 " const int has_bias) {\n"
                                 " const int group_m_idx=get_group_id(1);\n"
                                 " const int group_n_idx=get_group_id(0);\n"
                                 " const int local_m_idx=get_local_id(1);\n"
                                 " const int local_n_idx=get_local_id(0);\n"
                                 " const int batch_idx=get_global_id(2);\n"
                                 " __local half a_tile[TILE_M][TILE_K+1];\n"
                                 " __local half b_tile[TILE_K][TILE_N+1];\n"
                                 " float acc[WPT_M][WPT_N];\n"
                                 "#pragma unroll\n"
                                 " for (int i=0; i<WPT_M; ++i) {\n"
                                 "#pragma unroll\n"
                                 " for (int j=0; j<WPT_N; ++j) {\n"
                                 " acc[i][j]=0.0f;\n"
                                 " }\n"
                                 " }\n"
                                 " const long base_a_offset=(long)batch_idx*M*K;\n"
                                 " const int num_k_tiles=(K+TILE_K-1)/TILE_K;\n"
                                 " for (int t=0; t<num_k_tiles; ++t) {\n"
                                 " const int k_start=t*TILE_K;\n"
                                 "#pragma unroll\n"
                                 " for (int i=0; i<WPT_M; ++i) {\n"
                                 " const int m_local=local_m_idx*WPT_M+i;\n"
                                 " const int k_local=local_n_idx;\n"
                                 " const int m_global=group_m_idx*TILE_M+m_local;\n"
                                 " if (m_global<M) {\n"
                                 " for (int k_load_step=0; k_load_step<TILE_K/THREADS_X;\n"
                                 " ++k_load_step) {\n"
                                 " int k_global=k_start+k_local+k_load_step*THREADS_X;\n"
                                 " if (k_global<K) {\n"
                                 " a_tile[m_local][k_local+k_load_step*THREADS_X] =\n"
                                 " A[base_a_offset+m_global*K+k_global];\n"
                                 " } else {\n"
                                 " a_tile[m_local][k_local+k_load_step*THREADS_X]=0.0h;\n"
                                 " }\n"
                                 " }\n"
                                 " } else {\n"
                                 " for (int k_load_step=0; k_load_step<TILE_K/THREADS_X;\n"
                                 " ++k_load_step) {\n"
                                 " a_tile[m_local][k_local+k_load_step*THREADS_X]=0.0h;\n"
                                 " }\n"
                                 " }\n"
                                 " }\n"
                                 "#pragma unroll\n"
                                 " for (int i=0; i<WPT_N; ++i) {\n"
                                 " const int n_local=local_n_idx*WPT_N+i;\n"
                                 " const int k_local=local_m_idx;\n"
                                 " const int n_global=group_n_idx*TILE_N+n_local;\n"
                                 " if (n_global<N) {\n"
                                 " for (int k_load_step=0; k_load_step<TILE_K/THREADS_Y;\n"
                                 " ++k_load_step) {\n"
                                 " int k_global=k_start+k_local+k_load_step*THREADS_Y;\n"
                                 " if (k_global<K) {\n"
                                 " const int k_block_idx=k_global/QK4_0;\n"
                                 " const int k_in_block=k_global % QK4_0;\n"
                                 " const __global block_q4_0 *b_block_ptr =\n"
                                 " &B[n_global*(K/QK4_0)+k_block_idx];\n"
                                 " const float d_b =\n"
                                 " vload_half(0,(__global half *)(&(b_block_ptr->d)));\n"
                                 " const uchar qs_sub_idx=k_in_block % 16;\n"
                                 " const uchar q_packed=b_block_ptr->qs[qs_sub_idx];\n"
                                 " const bool is_low_nibble=(k_in_block<16);\n"
                                 " char q_nibble =\n"
                                 " is_low_nibble ? ((q_packed & 0x0F)-8) : ((q_packed >> 4)-8);\n"
                                 " b_tile[k_local+k_load_step*THREADS_Y][n_local] =\n"
                                 " (half)((float)q_nibble*d_b);\n"
                                 " } else {\n"
                                 " b_tile[k_local+k_load_step*THREADS_Y][n_local]=0.0h;\n"
                                 " }\n"
                                 " }\n"
                                 " } else {\n"
                                 " for (int k_load_step=0; k_load_step<TILE_K/THREADS_Y;\n"
                                 " ++k_load_step) {\n"
                                 " b_tile[k_local+k_load_step*THREADS_Y][n_local]=0.0h;\n"
                                 " }\n"
                                 " }\n"
                                 " }\n"
                                 " barrier(CLK_LOCAL_MEM_FENCE);\n"
                                 "#pragma unroll\n"
                                 " for (int k_tile=0; k_tile<TILE_K; ++k_tile) {\n"
                                 "#pragma unroll\n"
                                 " for (int m=0; m<WPT_M; ++m) {\n"
                                 " half a_val=a_tile[local_m_idx*WPT_M+m][k_tile];\n"
                                 "#pragma unroll\n"
                                 " for (int n=0; n<WPT_N; ++n) {\n"
                                 " half b_val=b_tile[k_tile][local_n_idx*WPT_N+n];\n"
                                 " acc[m][n]=mad((float)a_val,(float)b_val,acc[m][n]);\n"
                                 " }\n"
                                 " }\n"
                                 " }\n"
                                 " barrier(CLK_LOCAL_MEM_FENCE);\n"
                                 " }\n"
                                 " long c_offset=(long)batch_idx*M*N;\n"
                                 "#pragma unroll\n"
                                 " for (int m=0; m<WPT_M; ++m) {\n"
                                 " int m_global=group_m_idx*TILE_M+local_m_idx*WPT_M+m;\n"
                                 " if (m_global<M) {\n"
                                 "#pragma unroll\n"
                                 " for (int n=0; n<WPT_N; ++n) {\n"
                                 " int n_global=group_n_idx*TILE_N+local_n_idx*WPT_N+n;\n"
                                 " if (n_global<N) {\n"
                                 " float result=acc[m][n];\n"
                                 " if (has_bias) {\n"
                                 " result += bias[n_global];\n"
                                 " }\n"
                                 " C[c_offset+m_global*N+n_global]=(half)result;\n"
                                 " }\n"
                                 " }\n"
                                 " }\n"
                                 " }\n"
                                 "}\n"
                                 "#else\n"
                                 "// ---------- [Fallback] ----------\n"
                                 "__kernel void gemm_fp16_q4_0_transb_bias(__global const half *A,\n"
                                 " __global const block_q4_0 *B,\n"
                                 " __global const float *bias,\n"
                                 " __global half *C,const int M,\n"
                                 " const int K,const int N,\n"
                                 " const int has_bias) {\n"
                                 " const int s=get_global_id(1);\n"
                                 " const int n=get_global_id(0);\n"
                                 " const int batch_idx=get_global_id(2);\n"
                                 " if (s >= M || n >= N) {\n"
                                 " return;\n"
                                 " }\n"
                                 " float acc=0.0f;\n"
                                 " const long a_row_offset=(long)batch_idx*M*K+(long)s*K;\n"
                                 " const long b_row_offset_blocks=(long)n*(K/QK4_0);\n"
                                 " for (int k_block_idx=0; k_block_idx<K/QK4_0; ++k_block_idx) {\n"
                                 " const __global block_q4_0 *b_block_ptr =\n"
                                 " &B[b_row_offset_blocks+k_block_idx];\n"
                                 " const float d_b=vload_half(0,(__global half *)(&(b_block_ptr->d)));\n"
                                 " const __global half *a_ptr=A+a_row_offset+k_block_idx*QK4_0;\n"
                                 " for (int j=0; j<QK4_0/2; ++j) {\n"
                                 " const uchar q_packed=b_block_ptr->qs[j];\n"
                                 " const char q_lo=(q_packed & 0x0F)-8;\n"
                                 " const char q_hi=(q_packed >> 4)-8;\n"
                                 " acc += vload_half(j,a_ptr)*(float)q_lo*d_b;\n"
                                 " acc += vload_half(j+QK4_0/2,a_ptr)*(float)q_hi*d_b;\n"
                                 " }\n"
                                 " }\n"
                                 " if (has_bias != 0) {\n"
                                 " acc += bias[n];\n"
                                 " }\n"
                                 " const long c_idx=(long)batch_idx*M*N+(long)s*N+n;\n"
                                 " vstore_half_rte(acc,0,&C[c_idx]);\n"
                                 "}\n"
                                 "#endif // SUPPORTS_FP16\n";
}  // namespace mllm::opencl
