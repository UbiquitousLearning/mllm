@main () -> () {
    graph.SubGraphOp @init <notype> [symbol:init] {
        () -> () {
            tensor.CPU.register () -> (%105:tensor<[151936, 2048], Float32, CPU>[@model.embed_tokens.weight][symbol:model.embed_tokens.weight])[symbol:model.embed_tokens.weight]
            tensor.CPU.register () -> (%76:tensor<[2048, 2048], Float32, CPU>[@model.layers.0.self_attn.q_proj.weight][symbol:model.layers.0.self_attn.q_proj.weight])[symbol:model.layers.0.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%133:tensor<[1024, 2048], Float32, CPU>[@model.layers.0.self_attn.k_proj.weight][symbol:model.layers.0.self_attn.k_proj.weight])[symbol:model.layers.0.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%179:tensor<[1024, 2048], Float32, CPU>[@model.layers.0.self_attn.v_proj.weight][symbol:model.layers.0.self_attn.v_proj.weight])[symbol:model.layers.0.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%269:tensor<[2048, 2048], Float32, CPU>[@model.layers.0.self_attn.o_proj.weight][symbol:model.layers.0.self_attn.o_proj.weight])[symbol:model.layers.0.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%9:tensor<[6144, 2048], Float32, CPU>[@model.layers.0.mlp.gate_proj.weight][symbol:model.layers.0.mlp.gate_proj.weight])[symbol:model.layers.0.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%111:tensor<[6144, 2048], Float32, CPU>[@model.layers.0.mlp.up_proj.weight][symbol:model.layers.0.mlp.up_proj.weight])[symbol:model.layers.0.mlp.up_proj.weight]
            tensor.CPU.register () -> (%184:tensor<[2048, 6144], Float32, CPU>[@model.layers.0.mlp.down_proj.weight][symbol:model.layers.0.mlp.down_proj.weight])[symbol:model.layers.0.mlp.down_proj.weight]
            tensor.CPU.register () -> (%285:tensor<[2048, 2048], Float32, CPU>[@model.layers.1.self_attn.q_proj.weight][symbol:model.layers.1.self_attn.q_proj.weight])[symbol:model.layers.1.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%32:tensor<[1024, 2048], Float32, CPU>[@model.layers.1.self_attn.k_proj.weight][symbol:model.layers.1.self_attn.k_proj.weight])[symbol:model.layers.1.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%154:tensor<[1024, 2048], Float32, CPU>[@model.layers.1.self_attn.v_proj.weight][symbol:model.layers.1.self_attn.v_proj.weight])[symbol:model.layers.1.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%20:tensor<[2048, 2048], Float32, CPU>[@model.layers.1.self_attn.o_proj.weight][symbol:model.layers.1.self_attn.o_proj.weight])[symbol:model.layers.1.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%245:tensor<[6144, 2048], Float32, CPU>[@model.layers.1.mlp.gate_proj.weight][symbol:model.layers.1.mlp.gate_proj.weight])[symbol:model.layers.1.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%230:tensor<[6144, 2048], Float32, CPU>[@model.layers.1.mlp.up_proj.weight][symbol:model.layers.1.mlp.up_proj.weight])[symbol:model.layers.1.mlp.up_proj.weight]
            tensor.CPU.register () -> (%43:tensor<[2048, 6144], Float32, CPU>[@model.layers.1.mlp.down_proj.weight][symbol:model.layers.1.mlp.down_proj.weight])[symbol:model.layers.1.mlp.down_proj.weight]
            tensor.CPU.register () -> (%221:tensor<[2048, 2048], Float32, CPU>[@model.layers.2.self_attn.q_proj.weight][symbol:model.layers.2.self_attn.q_proj.weight])[symbol:model.layers.2.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%103:tensor<[1024, 2048], Float32, CPU>[@model.layers.2.self_attn.k_proj.weight][symbol:model.layers.2.self_attn.k_proj.weight])[symbol:model.layers.2.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%47:tensor<[1024, 2048], Float32, CPU>[@model.layers.2.self_attn.v_proj.weight][symbol:model.layers.2.self_attn.v_proj.weight])[symbol:model.layers.2.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%85:tensor<[2048, 2048], Float32, CPU>[@model.layers.2.self_attn.o_proj.weight][symbol:model.layers.2.self_attn.o_proj.weight])[symbol:model.layers.2.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%252:tensor<[6144, 2048], Float32, CPU>[@model.layers.2.mlp.gate_proj.weight][symbol:model.layers.2.mlp.gate_proj.weight])[symbol:model.layers.2.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%24:tensor<[6144, 2048], Float32, CPU>[@model.layers.2.mlp.up_proj.weight][symbol:model.layers.2.mlp.up_proj.weight])[symbol:model.layers.2.mlp.up_proj.weight]
            tensor.CPU.register () -> (%28:tensor<[2048, 6144], Float32, CPU>[@model.layers.2.mlp.down_proj.weight][symbol:model.layers.2.mlp.down_proj.weight])[symbol:model.layers.2.mlp.down_proj.weight]
            tensor.CPU.register () -> (%283:tensor<[2048, 2048], Float32, CPU>[@model.layers.3.self_attn.q_proj.weight][symbol:model.layers.3.self_attn.q_proj.weight])[symbol:model.layers.3.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%48:tensor<[1024, 2048], Float32, CPU>[@model.layers.3.self_attn.k_proj.weight][symbol:model.layers.3.self_attn.k_proj.weight])[symbol:model.layers.3.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%244:tensor<[1024, 2048], Float32, CPU>[@model.layers.3.self_attn.v_proj.weight][symbol:model.layers.3.self_attn.v_proj.weight])[symbol:model.layers.3.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%301:tensor<[2048, 2048], Float32, CPU>[@model.layers.3.self_attn.o_proj.weight][symbol:model.layers.3.self_attn.o_proj.weight])[symbol:model.layers.3.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%129:tensor<[6144, 2048], Float32, CPU>[@model.layers.3.mlp.gate_proj.weight][symbol:model.layers.3.mlp.gate_proj.weight])[symbol:model.layers.3.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%188:tensor<[6144, 2048], Float32, CPU>[@model.layers.3.mlp.up_proj.weight][symbol:model.layers.3.mlp.up_proj.weight])[symbol:model.layers.3.mlp.up_proj.weight]
            tensor.CPU.register () -> (%97:tensor<[2048, 6144], Float32, CPU>[@model.layers.3.mlp.down_proj.weight][symbol:model.layers.3.mlp.down_proj.weight])[symbol:model.layers.3.mlp.down_proj.weight]
            tensor.CPU.register () -> (%164:tensor<[2048, 2048], Float32, CPU>[@model.layers.4.self_attn.q_proj.weight][symbol:model.layers.4.self_attn.q_proj.weight])[symbol:model.layers.4.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%148:tensor<[1024, 2048], Float32, CPU>[@model.layers.4.self_attn.k_proj.weight][symbol:model.layers.4.self_attn.k_proj.weight])[symbol:model.layers.4.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%279:tensor<[1024, 2048], Float32, CPU>[@model.layers.4.self_attn.v_proj.weight][symbol:model.layers.4.self_attn.v_proj.weight])[symbol:model.layers.4.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%91:tensor<[2048, 2048], Float32, CPU>[@model.layers.4.self_attn.o_proj.weight][symbol:model.layers.4.self_attn.o_proj.weight])[symbol:model.layers.4.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%189:tensor<[6144, 2048], Float32, CPU>[@model.layers.4.mlp.gate_proj.weight][symbol:model.layers.4.mlp.gate_proj.weight])[symbol:model.layers.4.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%156:tensor<[6144, 2048], Float32, CPU>[@model.layers.4.mlp.up_proj.weight][symbol:model.layers.4.mlp.up_proj.weight])[symbol:model.layers.4.mlp.up_proj.weight]
            tensor.CPU.register () -> (%153:tensor<[2048, 6144], Float32, CPU>[@model.layers.4.mlp.down_proj.weight][symbol:model.layers.4.mlp.down_proj.weight])[symbol:model.layers.4.mlp.down_proj.weight]
            tensor.CPU.register () -> (%78:tensor<[2048, 2048], Float32, CPU>[@model.layers.5.self_attn.q_proj.weight][symbol:model.layers.5.self_attn.q_proj.weight])[symbol:model.layers.5.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%72:tensor<[1024, 2048], Float32, CPU>[@model.layers.5.self_attn.k_proj.weight][symbol:model.layers.5.self_attn.k_proj.weight])[symbol:model.layers.5.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%289:tensor<[1024, 2048], Float32, CPU>[@model.layers.5.self_attn.v_proj.weight][symbol:model.layers.5.self_attn.v_proj.weight])[symbol:model.layers.5.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%264:tensor<[2048, 2048], Float32, CPU>[@model.layers.5.self_attn.o_proj.weight][symbol:model.layers.5.self_attn.o_proj.weight])[symbol:model.layers.5.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%4:tensor<[6144, 2048], Float32, CPU>[@model.layers.5.mlp.gate_proj.weight][symbol:model.layers.5.mlp.gate_proj.weight])[symbol:model.layers.5.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%308:tensor<[6144, 2048], Float32, CPU>[@model.layers.5.mlp.up_proj.weight][symbol:model.layers.5.mlp.up_proj.weight])[symbol:model.layers.5.mlp.up_proj.weight]
            tensor.CPU.register () -> (%74:tensor<[2048, 6144], Float32, CPU>[@model.layers.5.mlp.down_proj.weight][symbol:model.layers.5.mlp.down_proj.weight])[symbol:model.layers.5.mlp.down_proj.weight]
            tensor.CPU.register () -> (%59:tensor<[2048, 2048], Float32, CPU>[@model.layers.6.self_attn.q_proj.weight][symbol:model.layers.6.self_attn.q_proj.weight])[symbol:model.layers.6.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%208:tensor<[1024, 2048], Float32, CPU>[@model.layers.6.self_attn.k_proj.weight][symbol:model.layers.6.self_attn.k_proj.weight])[symbol:model.layers.6.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%238:tensor<[1024, 2048], Float32, CPU>[@model.layers.6.self_attn.v_proj.weight][symbol:model.layers.6.self_attn.v_proj.weight])[symbol:model.layers.6.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%52:tensor<[2048, 2048], Float32, CPU>[@model.layers.6.self_attn.o_proj.weight][symbol:model.layers.6.self_attn.o_proj.weight])[symbol:model.layers.6.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%80:tensor<[6144, 2048], Float32, CPU>[@model.layers.6.mlp.gate_proj.weight][symbol:model.layers.6.mlp.gate_proj.weight])[symbol:model.layers.6.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%276:tensor<[6144, 2048], Float32, CPU>[@model.layers.6.mlp.up_proj.weight][symbol:model.layers.6.mlp.up_proj.weight])[symbol:model.layers.6.mlp.up_proj.weight]
            tensor.CPU.register () -> (%227:tensor<[2048, 6144], Float32, CPU>[@model.layers.6.mlp.down_proj.weight][symbol:model.layers.6.mlp.down_proj.weight])[symbol:model.layers.6.mlp.down_proj.weight]
            tensor.CPU.register () -> (%287:tensor<[2048, 2048], Float32, CPU>[@model.layers.7.self_attn.q_proj.weight][symbol:model.layers.7.self_attn.q_proj.weight])[symbol:model.layers.7.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%135:tensor<[1024, 2048], Float32, CPU>[@model.layers.7.self_attn.k_proj.weight][symbol:model.layers.7.self_attn.k_proj.weight])[symbol:model.layers.7.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%300:tensor<[1024, 2048], Float32, CPU>[@model.layers.7.self_attn.v_proj.weight][symbol:model.layers.7.self_attn.v_proj.weight])[symbol:model.layers.7.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%251:tensor<[2048, 2048], Float32, CPU>[@model.layers.7.self_attn.o_proj.weight][symbol:model.layers.7.self_attn.o_proj.weight])[symbol:model.layers.7.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%155:tensor<[6144, 2048], Float32, CPU>[@model.layers.7.mlp.gate_proj.weight][symbol:model.layers.7.mlp.gate_proj.weight])[symbol:model.layers.7.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%218:tensor<[6144, 2048], Float32, CPU>[@model.layers.7.mlp.up_proj.weight][symbol:model.layers.7.mlp.up_proj.weight])[symbol:model.layers.7.mlp.up_proj.weight]
            tensor.CPU.register () -> (%275:tensor<[2048, 6144], Float32, CPU>[@model.layers.7.mlp.down_proj.weight][symbol:model.layers.7.mlp.down_proj.weight])[symbol:model.layers.7.mlp.down_proj.weight]
            tensor.CPU.register () -> (%165:tensor<[2048, 2048], Float32, CPU>[@model.layers.8.self_attn.q_proj.weight][symbol:model.layers.8.self_attn.q_proj.weight])[symbol:model.layers.8.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%194:tensor<[1024, 2048], Float32, CPU>[@model.layers.8.self_attn.k_proj.weight][symbol:model.layers.8.self_attn.k_proj.weight])[symbol:model.layers.8.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%181:tensor<[1024, 2048], Float32, CPU>[@model.layers.8.self_attn.v_proj.weight][symbol:model.layers.8.self_attn.v_proj.weight])[symbol:model.layers.8.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%197:tensor<[2048, 2048], Float32, CPU>[@model.layers.8.self_attn.o_proj.weight][symbol:model.layers.8.self_attn.o_proj.weight])[symbol:model.layers.8.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%110:tensor<[6144, 2048], Float32, CPU>[@model.layers.8.mlp.gate_proj.weight][symbol:model.layers.8.mlp.gate_proj.weight])[symbol:model.layers.8.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%236:tensor<[6144, 2048], Float32, CPU>[@model.layers.8.mlp.up_proj.weight][symbol:model.layers.8.mlp.up_proj.weight])[symbol:model.layers.8.mlp.up_proj.weight]
            tensor.CPU.register () -> (%106:tensor<[2048, 6144], Float32, CPU>[@model.layers.8.mlp.down_proj.weight][symbol:model.layers.8.mlp.down_proj.weight])[symbol:model.layers.8.mlp.down_proj.weight]
            tensor.CPU.register () -> (%235:tensor<[2048, 2048], Float32, CPU>[@model.layers.9.self_attn.q_proj.weight][symbol:model.layers.9.self_attn.q_proj.weight])[symbol:model.layers.9.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%69:tensor<[1024, 2048], Float32, CPU>[@model.layers.9.self_attn.k_proj.weight][symbol:model.layers.9.self_attn.k_proj.weight])[symbol:model.layers.9.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%120:tensor<[1024, 2048], Float32, CPU>[@model.layers.9.self_attn.v_proj.weight][symbol:model.layers.9.self_attn.v_proj.weight])[symbol:model.layers.9.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%205:tensor<[2048, 2048], Float32, CPU>[@model.layers.9.self_attn.o_proj.weight][symbol:model.layers.9.self_attn.o_proj.weight])[symbol:model.layers.9.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%263:tensor<[6144, 2048], Float32, CPU>[@model.layers.9.mlp.gate_proj.weight][symbol:model.layers.9.mlp.gate_proj.weight])[symbol:model.layers.9.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%102:tensor<[6144, 2048], Float32, CPU>[@model.layers.9.mlp.up_proj.weight][symbol:model.layers.9.mlp.up_proj.weight])[symbol:model.layers.9.mlp.up_proj.weight]
            tensor.CPU.register () -> (%136:tensor<[2048, 6144], Float32, CPU>[@model.layers.9.mlp.down_proj.weight][symbol:model.layers.9.mlp.down_proj.weight])[symbol:model.layers.9.mlp.down_proj.weight]
            tensor.CPU.register () -> (%278:tensor<[2048, 2048], Float32, CPU>[@model.layers.10.self_attn.q_proj.weight][symbol:model.layers.10.self_attn.q_proj.weight])[symbol:model.layers.10.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%182:tensor<[1024, 2048], Float32, CPU>[@model.layers.10.self_attn.k_proj.weight][symbol:model.layers.10.self_attn.k_proj.weight])[symbol:model.layers.10.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%138:tensor<[1024, 2048], Float32, CPU>[@model.layers.10.self_attn.v_proj.weight][symbol:model.layers.10.self_attn.v_proj.weight])[symbol:model.layers.10.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%233:tensor<[2048, 2048], Float32, CPU>[@model.layers.10.self_attn.o_proj.weight][symbol:model.layers.10.self_attn.o_proj.weight])[symbol:model.layers.10.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%124:tensor<[6144, 2048], Float32, CPU>[@model.layers.10.mlp.gate_proj.weight][symbol:model.layers.10.mlp.gate_proj.weight])[symbol:model.layers.10.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%261:tensor<[6144, 2048], Float32, CPU>[@model.layers.10.mlp.up_proj.weight][symbol:model.layers.10.mlp.up_proj.weight])[symbol:model.layers.10.mlp.up_proj.weight]
            tensor.CPU.register () -> (%45:tensor<[2048, 6144], Float32, CPU>[@model.layers.10.mlp.down_proj.weight][symbol:model.layers.10.mlp.down_proj.weight])[symbol:model.layers.10.mlp.down_proj.weight]
            tensor.CPU.register () -> (%274:tensor<[2048, 2048], Float32, CPU>[@model.layers.11.self_attn.q_proj.weight][symbol:model.layers.11.self_attn.q_proj.weight])[symbol:model.layers.11.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%157:tensor<[1024, 2048], Float32, CPU>[@model.layers.11.self_attn.k_proj.weight][symbol:model.layers.11.self_attn.k_proj.weight])[symbol:model.layers.11.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%63:tensor<[1024, 2048], Float32, CPU>[@model.layers.11.self_attn.v_proj.weight][symbol:model.layers.11.self_attn.v_proj.weight])[symbol:model.layers.11.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%118:tensor<[2048, 2048], Float32, CPU>[@model.layers.11.self_attn.o_proj.weight][symbol:model.layers.11.self_attn.o_proj.weight])[symbol:model.layers.11.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%207:tensor<[6144, 2048], Float32, CPU>[@model.layers.11.mlp.gate_proj.weight][symbol:model.layers.11.mlp.gate_proj.weight])[symbol:model.layers.11.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%226:tensor<[6144, 2048], Float32, CPU>[@model.layers.11.mlp.up_proj.weight][symbol:model.layers.11.mlp.up_proj.weight])[symbol:model.layers.11.mlp.up_proj.weight]
            tensor.CPU.register () -> (%224:tensor<[2048, 6144], Float32, CPU>[@model.layers.11.mlp.down_proj.weight][symbol:model.layers.11.mlp.down_proj.weight])[symbol:model.layers.11.mlp.down_proj.weight]
            tensor.CPU.register () -> (%217:tensor<[2048, 2048], Float32, CPU>[@model.layers.12.self_attn.q_proj.weight][symbol:model.layers.12.self_attn.q_proj.weight])[symbol:model.layers.12.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%297:tensor<[1024, 2048], Float32, CPU>[@model.layers.12.self_attn.k_proj.weight][symbol:model.layers.12.self_attn.k_proj.weight])[symbol:model.layers.12.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%94:tensor<[1024, 2048], Float32, CPU>[@model.layers.12.self_attn.v_proj.weight][symbol:model.layers.12.self_attn.v_proj.weight])[symbol:model.layers.12.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%49:tensor<[2048, 2048], Float32, CPU>[@model.layers.12.self_attn.o_proj.weight][symbol:model.layers.12.self_attn.o_proj.weight])[symbol:model.layers.12.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%262:tensor<[6144, 2048], Float32, CPU>[@model.layers.12.mlp.gate_proj.weight][symbol:model.layers.12.mlp.gate_proj.weight])[symbol:model.layers.12.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%255:tensor<[6144, 2048], Float32, CPU>[@model.layers.12.mlp.up_proj.weight][symbol:model.layers.12.mlp.up_proj.weight])[symbol:model.layers.12.mlp.up_proj.weight]
            tensor.CPU.register () -> (%22:tensor<[2048, 6144], Float32, CPU>[@model.layers.12.mlp.down_proj.weight][symbol:model.layers.12.mlp.down_proj.weight])[symbol:model.layers.12.mlp.down_proj.weight]
            tensor.CPU.register () -> (%114:tensor<[2048, 2048], Float32, CPU>[@model.layers.13.self_attn.q_proj.weight][symbol:model.layers.13.self_attn.q_proj.weight])[symbol:model.layers.13.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%152:tensor<[1024, 2048], Float32, CPU>[@model.layers.13.self_attn.k_proj.weight][symbol:model.layers.13.self_attn.k_proj.weight])[symbol:model.layers.13.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%15:tensor<[1024, 2048], Float32, CPU>[@model.layers.13.self_attn.v_proj.weight][symbol:model.layers.13.self_attn.v_proj.weight])[symbol:model.layers.13.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%250:tensor<[2048, 2048], Float32, CPU>[@model.layers.13.self_attn.o_proj.weight][symbol:model.layers.13.self_attn.o_proj.weight])[symbol:model.layers.13.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%247:tensor<[6144, 2048], Float32, CPU>[@model.layers.13.mlp.gate_proj.weight][symbol:model.layers.13.mlp.gate_proj.weight])[symbol:model.layers.13.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%98:tensor<[6144, 2048], Float32, CPU>[@model.layers.13.mlp.up_proj.weight][symbol:model.layers.13.mlp.up_proj.weight])[symbol:model.layers.13.mlp.up_proj.weight]
            tensor.CPU.register () -> (%193:tensor<[2048, 6144], Float32, CPU>[@model.layers.13.mlp.down_proj.weight][symbol:model.layers.13.mlp.down_proj.weight])[symbol:model.layers.13.mlp.down_proj.weight]
            tensor.CPU.register () -> (%209:tensor<[2048, 2048], Float32, CPU>[@model.layers.14.self_attn.q_proj.weight][symbol:model.layers.14.self_attn.q_proj.weight])[symbol:model.layers.14.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%38:tensor<[1024, 2048], Float32, CPU>[@model.layers.14.self_attn.k_proj.weight][symbol:model.layers.14.self_attn.k_proj.weight])[symbol:model.layers.14.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%232:tensor<[1024, 2048], Float32, CPU>[@model.layers.14.self_attn.v_proj.weight][symbol:model.layers.14.self_attn.v_proj.weight])[symbol:model.layers.14.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%168:tensor<[2048, 2048], Float32, CPU>[@model.layers.14.self_attn.o_proj.weight][symbol:model.layers.14.self_attn.o_proj.weight])[symbol:model.layers.14.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%37:tensor<[6144, 2048], Float32, CPU>[@model.layers.14.mlp.gate_proj.weight][symbol:model.layers.14.mlp.gate_proj.weight])[symbol:model.layers.14.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%147:tensor<[6144, 2048], Float32, CPU>[@model.layers.14.mlp.up_proj.weight][symbol:model.layers.14.mlp.up_proj.weight])[symbol:model.layers.14.mlp.up_proj.weight]
            tensor.CPU.register () -> (%163:tensor<[2048, 6144], Float32, CPU>[@model.layers.14.mlp.down_proj.weight][symbol:model.layers.14.mlp.down_proj.weight])[symbol:model.layers.14.mlp.down_proj.weight]
            tensor.CPU.register () -> (%46:tensor<[2048, 2048], Float32, CPU>[@model.layers.15.self_attn.q_proj.weight][symbol:model.layers.15.self_attn.q_proj.weight])[symbol:model.layers.15.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%268:tensor<[1024, 2048], Float32, CPU>[@model.layers.15.self_attn.k_proj.weight][symbol:model.layers.15.self_attn.k_proj.weight])[symbol:model.layers.15.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%117:tensor<[1024, 2048], Float32, CPU>[@model.layers.15.self_attn.v_proj.weight][symbol:model.layers.15.self_attn.v_proj.weight])[symbol:model.layers.15.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%303:tensor<[2048, 2048], Float32, CPU>[@model.layers.15.self_attn.o_proj.weight][symbol:model.layers.15.self_attn.o_proj.weight])[symbol:model.layers.15.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%260:tensor<[6144, 2048], Float32, CPU>[@model.layers.15.mlp.gate_proj.weight][symbol:model.layers.15.mlp.gate_proj.weight])[symbol:model.layers.15.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%42:tensor<[6144, 2048], Float32, CPU>[@model.layers.15.mlp.up_proj.weight][symbol:model.layers.15.mlp.up_proj.weight])[symbol:model.layers.15.mlp.up_proj.weight]
            tensor.CPU.register () -> (%290:tensor<[2048, 6144], Float32, CPU>[@model.layers.15.mlp.down_proj.weight][symbol:model.layers.15.mlp.down_proj.weight])[symbol:model.layers.15.mlp.down_proj.weight]
            tensor.CPU.register () -> (%17:tensor<[2048, 2048], Float32, CPU>[@model.layers.16.self_attn.q_proj.weight][symbol:model.layers.16.self_attn.q_proj.weight])[symbol:model.layers.16.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%228:tensor<[1024, 2048], Float32, CPU>[@model.layers.16.self_attn.k_proj.weight][symbol:model.layers.16.self_attn.k_proj.weight])[symbol:model.layers.16.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%66:tensor<[1024, 2048], Float32, CPU>[@model.layers.16.self_attn.v_proj.weight][symbol:model.layers.16.self_attn.v_proj.weight])[symbol:model.layers.16.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%211:tensor<[2048, 2048], Float32, CPU>[@model.layers.16.self_attn.o_proj.weight][symbol:model.layers.16.self_attn.o_proj.weight])[symbol:model.layers.16.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%130:tensor<[6144, 2048], Float32, CPU>[@model.layers.16.mlp.gate_proj.weight][symbol:model.layers.16.mlp.gate_proj.weight])[symbol:model.layers.16.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%79:tensor<[6144, 2048], Float32, CPU>[@model.layers.16.mlp.up_proj.weight][symbol:model.layers.16.mlp.up_proj.weight])[symbol:model.layers.16.mlp.up_proj.weight]
            tensor.CPU.register () -> (%248:tensor<[2048, 6144], Float32, CPU>[@model.layers.16.mlp.down_proj.weight][symbol:model.layers.16.mlp.down_proj.weight])[symbol:model.layers.16.mlp.down_proj.weight]
            tensor.CPU.register () -> (%64:tensor<[2048, 2048], Float32, CPU>[@model.layers.17.self_attn.q_proj.weight][symbol:model.layers.17.self_attn.q_proj.weight])[symbol:model.layers.17.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%237:tensor<[1024, 2048], Float32, CPU>[@model.layers.17.self_attn.k_proj.weight][symbol:model.layers.17.self_attn.k_proj.weight])[symbol:model.layers.17.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%6:tensor<[1024, 2048], Float32, CPU>[@model.layers.17.self_attn.v_proj.weight][symbol:model.layers.17.self_attn.v_proj.weight])[symbol:model.layers.17.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%125:tensor<[2048, 2048], Float32, CPU>[@model.layers.17.self_attn.o_proj.weight][symbol:model.layers.17.self_attn.o_proj.weight])[symbol:model.layers.17.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%177:tensor<[6144, 2048], Float32, CPU>[@model.layers.17.mlp.gate_proj.weight][symbol:model.layers.17.mlp.gate_proj.weight])[symbol:model.layers.17.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%26:tensor<[6144, 2048], Float32, CPU>[@model.layers.17.mlp.up_proj.weight][symbol:model.layers.17.mlp.up_proj.weight])[symbol:model.layers.17.mlp.up_proj.weight]
            tensor.CPU.register () -> (%25:tensor<[2048, 6144], Float32, CPU>[@model.layers.17.mlp.down_proj.weight][symbol:model.layers.17.mlp.down_proj.weight])[symbol:model.layers.17.mlp.down_proj.weight]
            tensor.CPU.register () -> (%273:tensor<[2048, 2048], Float32, CPU>[@model.layers.18.self_attn.q_proj.weight][symbol:model.layers.18.self_attn.q_proj.weight])[symbol:model.layers.18.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%284:tensor<[1024, 2048], Float32, CPU>[@model.layers.18.self_attn.k_proj.weight][symbol:model.layers.18.self_attn.k_proj.weight])[symbol:model.layers.18.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%18:tensor<[1024, 2048], Float32, CPU>[@model.layers.18.self_attn.v_proj.weight][symbol:model.layers.18.self_attn.v_proj.weight])[symbol:model.layers.18.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%2:tensor<[2048, 2048], Float32, CPU>[@model.layers.18.self_attn.o_proj.weight][symbol:model.layers.18.self_attn.o_proj.weight])[symbol:model.layers.18.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%166:tensor<[6144, 2048], Float32, CPU>[@model.layers.18.mlp.gate_proj.weight][symbol:model.layers.18.mlp.gate_proj.weight])[symbol:model.layers.18.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%271:tensor<[6144, 2048], Float32, CPU>[@model.layers.18.mlp.up_proj.weight][symbol:model.layers.18.mlp.up_proj.weight])[symbol:model.layers.18.mlp.up_proj.weight]
            tensor.CPU.register () -> (%112:tensor<[2048, 6144], Float32, CPU>[@model.layers.18.mlp.down_proj.weight][symbol:model.layers.18.mlp.down_proj.weight])[symbol:model.layers.18.mlp.down_proj.weight]
            tensor.CPU.register () -> (%8:tensor<[2048, 2048], Float32, CPU>[@model.layers.19.self_attn.q_proj.weight][symbol:model.layers.19.self_attn.q_proj.weight])[symbol:model.layers.19.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%286:tensor<[1024, 2048], Float32, CPU>[@model.layers.19.self_attn.k_proj.weight][symbol:model.layers.19.self_attn.k_proj.weight])[symbol:model.layers.19.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%50:tensor<[1024, 2048], Float32, CPU>[@model.layers.19.self_attn.v_proj.weight][symbol:model.layers.19.self_attn.v_proj.weight])[symbol:model.layers.19.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%58:tensor<[2048, 2048], Float32, CPU>[@model.layers.19.self_attn.o_proj.weight][symbol:model.layers.19.self_attn.o_proj.weight])[symbol:model.layers.19.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%281:tensor<[6144, 2048], Float32, CPU>[@model.layers.19.mlp.gate_proj.weight][symbol:model.layers.19.mlp.gate_proj.weight])[symbol:model.layers.19.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%82:tensor<[6144, 2048], Float32, CPU>[@model.layers.19.mlp.up_proj.weight][symbol:model.layers.19.mlp.up_proj.weight])[symbol:model.layers.19.mlp.up_proj.weight]
            tensor.CPU.register () -> (%173:tensor<[2048, 6144], Float32, CPU>[@model.layers.19.mlp.down_proj.weight][symbol:model.layers.19.mlp.down_proj.weight])[symbol:model.layers.19.mlp.down_proj.weight]
            tensor.CPU.register () -> (%280:tensor<[2048, 2048], Float32, CPU>[@model.layers.20.self_attn.q_proj.weight][symbol:model.layers.20.self_attn.q_proj.weight])[symbol:model.layers.20.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%253:tensor<[1024, 2048], Float32, CPU>[@model.layers.20.self_attn.k_proj.weight][symbol:model.layers.20.self_attn.k_proj.weight])[symbol:model.layers.20.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%239:tensor<[1024, 2048], Float32, CPU>[@model.layers.20.self_attn.v_proj.weight][symbol:model.layers.20.self_attn.v_proj.weight])[symbol:model.layers.20.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%41:tensor<[2048, 2048], Float32, CPU>[@model.layers.20.self_attn.o_proj.weight][symbol:model.layers.20.self_attn.o_proj.weight])[symbol:model.layers.20.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%172:tensor<[6144, 2048], Float32, CPU>[@model.layers.20.mlp.gate_proj.weight][symbol:model.layers.20.mlp.gate_proj.weight])[symbol:model.layers.20.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%299:tensor<[6144, 2048], Float32, CPU>[@model.layers.20.mlp.up_proj.weight][symbol:model.layers.20.mlp.up_proj.weight])[symbol:model.layers.20.mlp.up_proj.weight]
            tensor.CPU.register () -> (%123:tensor<[2048, 6144], Float32, CPU>[@model.layers.20.mlp.down_proj.weight][symbol:model.layers.20.mlp.down_proj.weight])[symbol:model.layers.20.mlp.down_proj.weight]
            tensor.CPU.register () -> (%295:tensor<[2048, 2048], Float32, CPU>[@model.layers.21.self_attn.q_proj.weight][symbol:model.layers.21.self_attn.q_proj.weight])[symbol:model.layers.21.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%139:tensor<[1024, 2048], Float32, CPU>[@model.layers.21.self_attn.k_proj.weight][symbol:model.layers.21.self_attn.k_proj.weight])[symbol:model.layers.21.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%142:tensor<[1024, 2048], Float32, CPU>[@model.layers.21.self_attn.v_proj.weight][symbol:model.layers.21.self_attn.v_proj.weight])[symbol:model.layers.21.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%115:tensor<[2048, 2048], Float32, CPU>[@model.layers.21.self_attn.o_proj.weight][symbol:model.layers.21.self_attn.o_proj.weight])[symbol:model.layers.21.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%259:tensor<[6144, 2048], Float32, CPU>[@model.layers.21.mlp.gate_proj.weight][symbol:model.layers.21.mlp.gate_proj.weight])[symbol:model.layers.21.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%162:tensor<[6144, 2048], Float32, CPU>[@model.layers.21.mlp.up_proj.weight][symbol:model.layers.21.mlp.up_proj.weight])[symbol:model.layers.21.mlp.up_proj.weight]
            tensor.CPU.register () -> (%183:tensor<[2048, 6144], Float32, CPU>[@model.layers.21.mlp.down_proj.weight][symbol:model.layers.21.mlp.down_proj.weight])[symbol:model.layers.21.mlp.down_proj.weight]
            tensor.CPU.register () -> (%89:tensor<[2048, 2048], Float32, CPU>[@model.layers.22.self_attn.q_proj.weight][symbol:model.layers.22.self_attn.q_proj.weight])[symbol:model.layers.22.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%36:tensor<[1024, 2048], Float32, CPU>[@model.layers.22.self_attn.k_proj.weight][symbol:model.layers.22.self_attn.k_proj.weight])[symbol:model.layers.22.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%204:tensor<[1024, 2048], Float32, CPU>[@model.layers.22.self_attn.v_proj.weight][symbol:model.layers.22.self_attn.v_proj.weight])[symbol:model.layers.22.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%234:tensor<[2048, 2048], Float32, CPU>[@model.layers.22.self_attn.o_proj.weight][symbol:model.layers.22.self_attn.o_proj.weight])[symbol:model.layers.22.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%198:tensor<[6144, 2048], Float32, CPU>[@model.layers.22.mlp.gate_proj.weight][symbol:model.layers.22.mlp.gate_proj.weight])[symbol:model.layers.22.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%254:tensor<[6144, 2048], Float32, CPU>[@model.layers.22.mlp.up_proj.weight][symbol:model.layers.22.mlp.up_proj.weight])[symbol:model.layers.22.mlp.up_proj.weight]
            tensor.CPU.register () -> (%31:tensor<[2048, 6144], Float32, CPU>[@model.layers.22.mlp.down_proj.weight][symbol:model.layers.22.mlp.down_proj.weight])[symbol:model.layers.22.mlp.down_proj.weight]
            tensor.CPU.register () -> (%109:tensor<[2048, 2048], Float32, CPU>[@model.layers.23.self_attn.q_proj.weight][symbol:model.layers.23.self_attn.q_proj.weight])[symbol:model.layers.23.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%39:tensor<[1024, 2048], Float32, CPU>[@model.layers.23.self_attn.k_proj.weight][symbol:model.layers.23.self_attn.k_proj.weight])[symbol:model.layers.23.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%83:tensor<[1024, 2048], Float32, CPU>[@model.layers.23.self_attn.v_proj.weight][symbol:model.layers.23.self_attn.v_proj.weight])[symbol:model.layers.23.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%176:tensor<[2048, 2048], Float32, CPU>[@model.layers.23.self_attn.o_proj.weight][symbol:model.layers.23.self_attn.o_proj.weight])[symbol:model.layers.23.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%169:tensor<[6144, 2048], Float32, CPU>[@model.layers.23.mlp.gate_proj.weight][symbol:model.layers.23.mlp.gate_proj.weight])[symbol:model.layers.23.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%243:tensor<[6144, 2048], Float32, CPU>[@model.layers.23.mlp.up_proj.weight][symbol:model.layers.23.mlp.up_proj.weight])[symbol:model.layers.23.mlp.up_proj.weight]
            tensor.CPU.register () -> (%149:tensor<[2048, 6144], Float32, CPU>[@model.layers.23.mlp.down_proj.weight][symbol:model.layers.23.mlp.down_proj.weight])[symbol:model.layers.23.mlp.down_proj.weight]
            tensor.CPU.register () -> (%11:tensor<[2048, 2048], Float32, CPU>[@model.layers.24.self_attn.q_proj.weight][symbol:model.layers.24.self_attn.q_proj.weight])[symbol:model.layers.24.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%61:tensor<[1024, 2048], Float32, CPU>[@model.layers.24.self_attn.k_proj.weight][symbol:model.layers.24.self_attn.k_proj.weight])[symbol:model.layers.24.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%81:tensor<[1024, 2048], Float32, CPU>[@model.layers.24.self_attn.v_proj.weight][symbol:model.layers.24.self_attn.v_proj.weight])[symbol:model.layers.24.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%127:tensor<[2048, 2048], Float32, CPU>[@model.layers.24.self_attn.o_proj.weight][symbol:model.layers.24.self_attn.o_proj.weight])[symbol:model.layers.24.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%141:tensor<[6144, 2048], Float32, CPU>[@model.layers.24.mlp.gate_proj.weight][symbol:model.layers.24.mlp.gate_proj.weight])[symbol:model.layers.24.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%126:tensor<[6144, 2048], Float32, CPU>[@model.layers.24.mlp.up_proj.weight][symbol:model.layers.24.mlp.up_proj.weight])[symbol:model.layers.24.mlp.up_proj.weight]
            tensor.CPU.register () -> (%34:tensor<[2048, 6144], Float32, CPU>[@model.layers.24.mlp.down_proj.weight][symbol:model.layers.24.mlp.down_proj.weight])[symbol:model.layers.24.mlp.down_proj.weight]
            tensor.CPU.register () -> (%206:tensor<[2048, 2048], Float32, CPU>[@model.layers.25.self_attn.q_proj.weight][symbol:model.layers.25.self_attn.q_proj.weight])[symbol:model.layers.25.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%27:tensor<[1024, 2048], Float32, CPU>[@model.layers.25.self_attn.k_proj.weight][symbol:model.layers.25.self_attn.k_proj.weight])[symbol:model.layers.25.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%121:tensor<[1024, 2048], Float32, CPU>[@model.layers.25.self_attn.v_proj.weight][symbol:model.layers.25.self_attn.v_proj.weight])[symbol:model.layers.25.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%150:tensor<[2048, 2048], Float32, CPU>[@model.layers.25.self_attn.o_proj.weight][symbol:model.layers.25.self_attn.o_proj.weight])[symbol:model.layers.25.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%249:tensor<[6144, 2048], Float32, CPU>[@model.layers.25.mlp.gate_proj.weight][symbol:model.layers.25.mlp.gate_proj.weight])[symbol:model.layers.25.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%159:tensor<[6144, 2048], Float32, CPU>[@model.layers.25.mlp.up_proj.weight][symbol:model.layers.25.mlp.up_proj.weight])[symbol:model.layers.25.mlp.up_proj.weight]
            tensor.CPU.register () -> (%267:tensor<[2048, 6144], Float32, CPU>[@model.layers.25.mlp.down_proj.weight][symbol:model.layers.25.mlp.down_proj.weight])[symbol:model.layers.25.mlp.down_proj.weight]
            tensor.CPU.register () -> (%265:tensor<[2048, 2048], Float32, CPU>[@model.layers.26.self_attn.q_proj.weight][symbol:model.layers.26.self_attn.q_proj.weight])[symbol:model.layers.26.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%190:tensor<[1024, 2048], Float32, CPU>[@model.layers.26.self_attn.k_proj.weight][symbol:model.layers.26.self_attn.k_proj.weight])[symbol:model.layers.26.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%119:tensor<[1024, 2048], Float32, CPU>[@model.layers.26.self_attn.v_proj.weight][symbol:model.layers.26.self_attn.v_proj.weight])[symbol:model.layers.26.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%88:tensor<[2048, 2048], Float32, CPU>[@model.layers.26.self_attn.o_proj.weight][symbol:model.layers.26.self_attn.o_proj.weight])[symbol:model.layers.26.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%96:tensor<[6144, 2048], Float32, CPU>[@model.layers.26.mlp.gate_proj.weight][symbol:model.layers.26.mlp.gate_proj.weight])[symbol:model.layers.26.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%62:tensor<[6144, 2048], Float32, CPU>[@model.layers.26.mlp.up_proj.weight][symbol:model.layers.26.mlp.up_proj.weight])[symbol:model.layers.26.mlp.up_proj.weight]
            tensor.CPU.register () -> (%220:tensor<[2048, 6144], Float32, CPU>[@model.layers.26.mlp.down_proj.weight][symbol:model.layers.26.mlp.down_proj.weight])[symbol:model.layers.26.mlp.down_proj.weight]
            tensor.CPU.register () -> (%185:tensor<[2048, 2048], Float32, CPU>[@model.layers.27.self_attn.q_proj.weight][symbol:model.layers.27.self_attn.q_proj.weight])[symbol:model.layers.27.self_attn.q_proj.weight]
            tensor.CPU.register () -> (%12:tensor<[1024, 2048], Float32, CPU>[@model.layers.27.self_attn.k_proj.weight][symbol:model.layers.27.self_attn.k_proj.weight])[symbol:model.layers.27.self_attn.k_proj.weight]
            tensor.CPU.register () -> (%54:tensor<[1024, 2048], Float32, CPU>[@model.layers.27.self_attn.v_proj.weight][symbol:model.layers.27.self_attn.v_proj.weight])[symbol:model.layers.27.self_attn.v_proj.weight]
            tensor.CPU.register () -> (%60:tensor<[2048, 2048], Float32, CPU>[@model.layers.27.self_attn.o_proj.weight][symbol:model.layers.27.self_attn.o_proj.weight])[symbol:model.layers.27.self_attn.o_proj.weight]
            tensor.CPU.register () -> (%144:tensor<[6144, 2048], Float32, CPU>[@model.layers.27.mlp.gate_proj.weight][symbol:model.layers.27.mlp.gate_proj.weight])[symbol:model.layers.27.mlp.gate_proj.weight]
            tensor.CPU.register () -> (%146:tensor<[6144, 2048], Float32, CPU>[@model.layers.27.mlp.up_proj.weight][symbol:model.layers.27.mlp.up_proj.weight])[symbol:model.layers.27.mlp.up_proj.weight]
            tensor.CPU.register () -> (%195:tensor<[2048, 6144], Float32, CPU>[@model.layers.27.mlp.down_proj.weight][symbol:model.layers.27.mlp.down_proj.weight])[symbol:model.layers.27.mlp.down_proj.weight]
            tensor.CPU.register () -> (%101:tensor<[151936, 2048], Float32, CPU>[@lm_head.weight][symbol:lm_head.weight])[symbol:lm_head.weight]
        }
    }
    graph.SubGraphOp @deinit <notype> [symbol:deinit] {
        () -> () {
            
        }
    }
    graph.CallGraphOp @model (%318:tensor<[1, 32], Float32, CPU>, %376:tensor<[1, 32], Int64, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %320:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %322:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %324:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %326:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %328:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %330:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %332:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %334:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %336:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %338:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %340:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %342:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %344:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %346:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %348:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %350:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %352:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %354:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %356:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %358:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %360:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %362:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %364:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %366:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %368:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %370:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %372:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %374:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %321:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %323:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %325:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %327:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %329:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %331:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %333:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %335:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %337:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %339:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %341:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %343:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %345:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %347:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %349:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %351:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %353:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %355:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %357:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %359:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %361:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %363:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %365:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %367:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %369:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %371:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %373:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %375:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1473:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %433:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %472:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %511:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %550:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %589:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %628:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %667:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %706:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %745:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %784:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %823:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %862:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %901:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %940:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %979:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1018:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1057:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1096:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1135:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1174:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1213:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1252:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1291:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1330:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1369:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1408:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1447:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %387:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %426:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %465:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %504:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %543:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %582:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %621:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %660:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %699:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %738:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %777:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %816:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %855:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %894:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %933:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %972:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1011:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1050:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1089:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1128:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1167:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1206:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1245:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1284:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1323:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1362:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1401:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1440:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
    graph.SubGraphOp @model <CPU> [using_qnn:true, symbol:model] {
        (%318:tensor<[1, 32], Float32, CPU>, %376:tensor<[1, 32], Int64, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %320:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %322:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %324:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %326:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %328:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %330:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %332:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %334:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %336:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %338:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %340:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %342:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %344:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %346:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %348:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %350:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %352:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %354:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %356:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %358:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %360:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %362:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %364:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %366:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %368:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %370:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %372:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %374:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %321:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %323:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %325:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %327:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %329:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %331:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %333:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %335:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %337:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %339:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %341:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %343:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %345:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %347:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %349:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %351:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %353:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %355:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %357:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %359:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %361:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %363:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %365:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %367:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %369:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %371:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %373:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %375:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1473:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %433:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %472:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %511:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %550:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %589:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %628:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %667:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %706:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %745:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %784:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %823:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %862:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %901:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %940:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %979:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1018:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1057:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1096:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1135:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1174:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1213:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1252:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1291:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1330:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1369:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1408:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1447:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %387:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %426:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %465:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %504:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %543:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %582:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %621:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %660:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %699:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %738:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %777:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %816:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %855:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %894:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %933:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %972:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1011:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1050:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1089:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1128:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1167:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1206:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1245:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1284:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1323:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1362:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1401:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1440:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.EmbeddingOp <name="model.embed_tokens">(%318:tensor<[1, 32], Float32, CPU>) -> (%377:tensor<[1, 32, 2048], Float32, CPU>)
            linalg.CPU.CastTypeOp <name="model.CastType.0">(%377:tensor<[1, 32, 2048], Float32, CPU>) -> (%378:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.View.0">(%376:tensor<[1, 32], Int64, CPU>) -> (%376:tensor<[32], Int64, CPU>)
            linalg.CPU.IndexOp <name="model.Index.0">(%316:tensor<[1, 1024, 128], Int16PerTensor, CPU>) -> (%379:tensor<[1, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.IndexOp <name="model.Index.1">(%317:tensor<[1, 1024, 128], Int16PerTensor, CPU>) -> (%380:tensor<[1, 32, 128], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.0 (%378:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %320:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %321:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%419:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %387:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.1 (%419:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %322:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %323:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%458:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %433:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %426:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.2 (%458:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %324:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %325:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%497:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %472:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %465:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.3 (%497:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %326:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %327:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%536:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %511:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %504:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.4 (%536:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %328:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %329:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%575:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %550:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %543:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.5 (%575:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %330:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %331:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%614:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %589:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %582:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.6 (%614:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %332:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %333:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%653:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %628:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %621:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.7 (%653:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %334:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %335:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%692:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %667:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %660:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.8 (%692:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %336:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %337:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%731:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %706:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %699:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.9 (%731:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %338:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %339:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%770:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %745:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %738:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.10 (%770:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %340:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %341:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%809:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %784:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %777:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.11 (%809:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %342:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %343:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%848:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %823:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %816:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.12 (%848:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %344:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %345:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%887:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %862:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %855:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.13 (%887:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %346:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %347:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%926:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %901:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %894:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.14 (%926:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %348:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %349:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%965:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %940:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %933:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.15 (%965:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %350:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %351:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1004:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %979:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %972:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.16 (%1004:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %352:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %353:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1043:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1018:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1011:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.17 (%1043:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %354:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %355:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1082:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1057:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1050:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.18 (%1082:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %356:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %357:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1121:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1096:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1089:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.19 (%1121:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %358:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %359:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1160:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1135:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1128:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.20 (%1160:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %360:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %361:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1199:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1174:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1167:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.21 (%1199:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %362:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %363:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1238:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1213:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1206:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.22 (%1238:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %364:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %365:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1277:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1252:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1245:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.23 (%1277:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %366:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %367:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1316:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1291:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1284:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.24 (%1316:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %368:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %369:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1355:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1330:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1323:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.25 (%1355:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %370:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %371:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1394:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1369:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1362:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.26 (%1394:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %372:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %373:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1433:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1408:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1401:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.27 (%1433:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %374:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %375:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1472:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1447:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1440:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.norm">(%1472:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1473:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1473:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %433:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %472:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %511:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %550:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %589:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %628:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %667:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %706:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %745:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %784:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %823:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %862:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %901:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %940:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %979:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1018:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1057:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1096:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1135:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1174:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1213:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1252:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1291:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1330:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1369:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1408:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1447:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %387:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %426:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %465:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %504:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %543:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %582:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %621:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %660:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %699:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %738:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %777:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %816:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %855:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %894:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %933:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %972:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1011:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1050:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1089:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1128:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1167:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1206:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1245:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1284:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1323:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1362:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1401:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %1440:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.0 <CPU> [using_qnn:true, symbol:model.layers.0] {
        (%378:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %320:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %321:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%419:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %387:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.0.input_layernorm">(%378:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%381:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.0.self_attn (%381:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %320:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %321:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%411:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %387:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.0.Add.0">(%411:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %378:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%412:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.0.post_attention_layernorm">(%412:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%413:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.0.mlp (%413:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%418:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.0.Add.1">(%418:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %412:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%419:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%419:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %387:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.0.self_attn <CPU> [using_qnn:true, symbol:model.layers.0.self_attn] {
        (%381:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %320:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %321:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%411:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %387:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.0.self_attn.q_proj">(%381:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%382:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.0.self_attn.k_proj">(%381:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%383:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.0.self_attn.v_proj">(%381:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%384:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.0.self_attn.View.0">(%382:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%382:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.0.self_attn.Transpose.0">(%382:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%385:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.0.self_attn.View.1">(%383:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%383:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.0.self_attn.Transpose.1">(%383:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%386:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.0.self_attn.View.2">(%384:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%384:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.0.self_attn.Transpose.2">(%384:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%387:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.0.self_attn.q_norm">(%385:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%388:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.0.self_attn.k_norm">(%386:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%389:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.0.self_attn.q_rope">(%388:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%390:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.0.self_attn.k_rope">(%389:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%391:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.0.self_attn.CastType.0">(%391:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%392:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.0.self_attn.CastType.1">(%392:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%393:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.0.self_attn.Transpose.3">(%393:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.0.self_attn.Concat.0">(%320:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%395:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.0.self_attn.Concat.1">(%321:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %387:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%396:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.0.self_attn.Repeat.0">(%395:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%397:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.0.self_attn.Repeat.1">(%396:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%398:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.0.self_attn.MatMul.0">(%390:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %397:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%399:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.0.self_attn.Mul.0">(%399:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %400:tensor<[1], Int16PerTensor, CPU>[constant: [0]][constant:[0]]) -> (%401:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.0.self_attn.ReduceMin.0">(%401:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%402:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.0.self_attn.Add.0">(%402:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %403:tensor<[1], Int16PerTensor, CPU>[constant: [9.1807e-41]][constant:[9.1807e-41]]) -> (%404:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.0.self_attn.Equal.0">(%319:tensor<[1, 1, 32, 1024], Float32, CPU>, %405:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%406:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.0.self_attn.Where.0">(%406:tensor<[1, 1, 32, 1024], UInt8, CPU>, %401:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %404:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%407:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.0.self_attn.Softmax.0">(%407:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%408:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.0.self_attn.MatMul.1">(%408:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %398:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%409:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.0.self_attn.Transpose.4">(%409:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%410:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.0.self_attn.View.3">(%410:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%410:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.0.self_attn.o_proj">(%410:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%411:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%411:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %394:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %387:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.0.mlp <CPU> [using_qnn:true, symbol:model.layers.0.mlp] {
        (%413:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%418:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.0.mlp.gate_proj">(%413:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%414:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.0.mlp.act">(%414:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%415:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.0.mlp.up_proj">(%413:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%416:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.0.mlp.Mul.0">(%415:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %416:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%417:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.0.mlp.down_proj">(%417:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%418:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%418:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.1 <CPU> [using_qnn:true, symbol:model.layers.1] {
        (%419:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %322:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %323:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%458:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %433:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %426:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.1.input_layernorm">(%419:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%420:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.1.self_attn (%420:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %322:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %323:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%450:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %433:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %426:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.1.Add.0">(%450:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %419:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%451:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.1.post_attention_layernorm">(%451:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%452:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.1.mlp (%452:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%457:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.1.Add.1">(%457:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %451:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%458:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%458:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %433:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %426:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.1.self_attn <CPU> [using_qnn:true, symbol:model.layers.1.self_attn] {
        (%420:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %322:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %323:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%450:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %433:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %426:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.1.self_attn.q_proj">(%420:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%421:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.1.self_attn.k_proj">(%420:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%422:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.1.self_attn.v_proj">(%420:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%423:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.1.self_attn.View.0">(%421:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%421:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.1.self_attn.Transpose.0">(%421:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%424:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.1.self_attn.View.1">(%422:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%422:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.1.self_attn.Transpose.1">(%422:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%425:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.1.self_attn.View.2">(%423:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%423:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.1.self_attn.Transpose.2">(%423:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%426:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.1.self_attn.q_norm">(%424:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%427:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.1.self_attn.k_norm">(%425:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%428:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.1.self_attn.q_rope">(%427:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%429:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.1.self_attn.k_rope">(%428:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%430:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.1.self_attn.CastType.0">(%430:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%431:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.1.self_attn.CastType.1">(%431:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%432:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.1.self_attn.Transpose.3">(%432:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%433:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.1.self_attn.Concat.0">(%322:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %433:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%434:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.1.self_attn.Concat.1">(%323:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %426:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%435:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.1.self_attn.Repeat.0">(%434:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%436:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.1.self_attn.Repeat.1">(%435:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%437:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.1.self_attn.MatMul.0">(%429:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %436:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%438:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.1.self_attn.Mul.0">(%438:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %439:tensor<[1], Int16PerTensor, CPU>[constant: [0]][constant:[0]]) -> (%440:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.1.self_attn.ReduceMin.0">(%440:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%441:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.1.self_attn.Add.0">(%441:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %442:tensor<[1], Int16PerTensor, CPU>[constant: [9.1807e-41]][constant:[9.1807e-41]]) -> (%443:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.1.self_attn.Equal.0">(%319:tensor<[1, 1, 32, 1024], Float32, CPU>, %444:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%445:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.1.self_attn.Where.0">(%445:tensor<[1, 1, 32, 1024], UInt8, CPU>, %440:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %443:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%446:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.1.self_attn.Softmax.0">(%446:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%447:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.1.self_attn.MatMul.1">(%447:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %437:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%448:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.1.self_attn.Transpose.4">(%448:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%449:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.1.self_attn.View.3">(%449:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%449:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.1.self_attn.o_proj">(%449:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%450:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%450:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %433:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %426:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.1.mlp <CPU> [using_qnn:true, symbol:model.layers.1.mlp] {
        (%452:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%457:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.1.mlp.gate_proj">(%452:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%453:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.1.mlp.act">(%453:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%454:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.1.mlp.up_proj">(%452:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%455:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.1.mlp.Mul.0">(%454:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %455:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%456:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.1.mlp.down_proj">(%456:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%457:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%457:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.2 <CPU> [using_qnn:true, symbol:model.layers.2] {
        (%458:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %324:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %325:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%497:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %472:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %465:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.2.input_layernorm">(%458:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%459:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.2.self_attn (%459:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %324:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %325:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%489:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %472:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %465:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.2.Add.0">(%489:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %458:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%490:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.2.post_attention_layernorm">(%490:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%491:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.2.mlp (%491:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%496:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.2.Add.1">(%496:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %490:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%497:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%497:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %472:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %465:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.2.self_attn <CPU> [using_qnn:true, symbol:model.layers.2.self_attn] {
        (%459:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %324:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %325:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%489:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %472:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %465:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.2.self_attn.q_proj">(%459:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%460:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.2.self_attn.k_proj">(%459:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%461:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.2.self_attn.v_proj">(%459:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%462:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.2.self_attn.View.0">(%460:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%460:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.2.self_attn.Transpose.0">(%460:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%463:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.2.self_attn.View.1">(%461:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%461:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.2.self_attn.Transpose.1">(%461:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%464:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.2.self_attn.View.2">(%462:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%462:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.2.self_attn.Transpose.2">(%462:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%465:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.2.self_attn.q_norm">(%463:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%466:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.2.self_attn.k_norm">(%464:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%467:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.2.self_attn.q_rope">(%466:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%468:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.2.self_attn.k_rope">(%467:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%469:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.2.self_attn.CastType.0">(%469:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%470:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.2.self_attn.CastType.1">(%470:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%471:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.2.self_attn.Transpose.3">(%471:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%472:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.2.self_attn.Concat.0">(%324:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %472:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%473:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.2.self_attn.Concat.1">(%325:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %465:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%474:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.2.self_attn.Repeat.0">(%473:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%475:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.2.self_attn.Repeat.1">(%474:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%476:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.2.self_attn.MatMul.0">(%468:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %475:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%477:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.2.self_attn.Mul.0">(%477:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %478:tensor<[1], Int16PerTensor, CPU>[constant: [1]][constant:[1]]) -> (%479:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.2.self_attn.ReduceMin.0">(%479:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%480:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.2.self_attn.Add.0">(%480:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %481:tensor<[1], Int16PerTensor, CPU>[constant: [9.1807e-41]][constant:[9.1807e-41]]) -> (%482:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.2.self_attn.Equal.0">(%319:tensor<[1, 1, 32, 1024], Float32, CPU>, %483:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%484:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.2.self_attn.Where.0">(%484:tensor<[1, 1, 32, 1024], UInt8, CPU>, %479:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %482:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%485:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.2.self_attn.Softmax.0">(%485:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%486:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.2.self_attn.MatMul.1">(%486:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %476:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%487:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.2.self_attn.Transpose.4">(%487:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%488:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.2.self_attn.View.3">(%488:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%488:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.2.self_attn.o_proj">(%488:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%489:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%489:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %472:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %465:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.2.mlp <CPU> [using_qnn:true, symbol:model.layers.2.mlp] {
        (%491:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%496:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.2.mlp.gate_proj">(%491:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%492:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.2.mlp.act">(%492:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%493:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.2.mlp.up_proj">(%491:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%494:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.2.mlp.Mul.0">(%493:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %494:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%495:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.2.mlp.down_proj">(%495:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%496:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%496:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.3 <CPU> [using_qnn:true, symbol:model.layers.3] {
        (%497:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %326:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %327:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%536:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %511:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %504:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.3.input_layernorm">(%497:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%498:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.3.self_attn (%498:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %326:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %327:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%528:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %511:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %504:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.3.Add.0">(%528:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %497:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%529:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.3.post_attention_layernorm">(%529:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%530:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.3.mlp (%530:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%535:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.3.Add.1">(%535:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %529:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%536:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%536:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %511:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %504:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.3.self_attn <CPU> [using_qnn:true, symbol:model.layers.3.self_attn] {
        (%498:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %326:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %327:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%528:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %511:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %504:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.3.self_attn.q_proj">(%498:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%499:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.3.self_attn.k_proj">(%498:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%500:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.3.self_attn.v_proj">(%498:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%501:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.3.self_attn.View.0">(%499:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%499:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.3.self_attn.Transpose.0">(%499:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%502:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.3.self_attn.View.1">(%500:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%500:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.3.self_attn.Transpose.1">(%500:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%503:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.3.self_attn.View.2">(%501:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%501:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.3.self_attn.Transpose.2">(%501:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%504:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.3.self_attn.q_norm">(%502:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%505:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.3.self_attn.k_norm">(%503:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%506:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.3.self_attn.q_rope">(%505:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%507:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.3.self_attn.k_rope">(%506:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%508:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.3.self_attn.CastType.0">(%508:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%509:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.3.self_attn.CastType.1">(%509:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%510:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.3.self_attn.Transpose.3">(%510:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%511:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.3.self_attn.Concat.0">(%326:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %511:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%512:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.3.self_attn.Concat.1">(%327:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %504:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%513:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.3.self_attn.Repeat.0">(%512:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%514:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.3.self_attn.Repeat.1">(%513:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%515:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.3.self_attn.MatMul.0">(%507:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %514:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%516:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.3.self_attn.Mul.0">(%516:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %517:tensor<[1], Int16PerTensor, CPU>[constant: [0]][constant:[0]]) -> (%518:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.3.self_attn.ReduceMin.0">(%518:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%519:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.3.self_attn.Add.0">(%519:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %520:tensor<[1], Int16PerTensor, CPU>[constant: [9.1807e-41]][constant:[9.1807e-41]]) -> (%521:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.3.self_attn.Equal.0">(%319:tensor<[1, 1, 32, 1024], Float32, CPU>, %522:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%523:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.3.self_attn.Where.0">(%523:tensor<[1, 1, 32, 1024], UInt8, CPU>, %518:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %521:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%524:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.3.self_attn.Softmax.0">(%524:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%525:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.3.self_attn.MatMul.1">(%525:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %515:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%526:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.3.self_attn.Transpose.4">(%526:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%527:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.3.self_attn.View.3">(%527:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%527:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.3.self_attn.o_proj">(%527:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%528:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%528:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %511:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %504:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.3.mlp <CPU> [using_qnn:true, symbol:model.layers.3.mlp] {
        (%530:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%535:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.3.mlp.gate_proj">(%530:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%531:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.3.mlp.act">(%531:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%532:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.3.mlp.up_proj">(%530:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%533:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.3.mlp.Mul.0">(%532:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %533:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%534:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.3.mlp.down_proj">(%534:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%535:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%535:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.4 <CPU> [using_qnn:true, symbol:model.layers.4] {
        (%536:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %328:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %329:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%575:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %550:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %543:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.4.input_layernorm">(%536:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%537:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.4.self_attn (%537:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %328:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %329:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%567:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %550:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %543:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.4.Add.0">(%567:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %536:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%568:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.4.post_attention_layernorm">(%568:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%569:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.4.mlp (%569:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%574:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.4.Add.1">(%574:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %568:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%575:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%575:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %550:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %543:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.4.self_attn <CPU> [using_qnn:true, symbol:model.layers.4.self_attn] {
        (%537:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %328:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %329:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%567:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %550:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %543:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.4.self_attn.q_proj">(%537:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%538:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.4.self_attn.k_proj">(%537:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%539:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.4.self_attn.v_proj">(%537:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%540:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.4.self_attn.View.0">(%538:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%538:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.4.self_attn.Transpose.0">(%538:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%541:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.4.self_attn.View.1">(%539:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%539:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.4.self_attn.Transpose.1">(%539:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%542:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.4.self_attn.View.2">(%540:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%540:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.4.self_attn.Transpose.2">(%540:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%543:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.4.self_attn.q_norm">(%541:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%544:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.4.self_attn.k_norm">(%542:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%545:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.4.self_attn.q_rope">(%544:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%546:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.4.self_attn.k_rope">(%545:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%547:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.4.self_attn.CastType.0">(%547:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%548:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.4.self_attn.CastType.1">(%548:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%549:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.4.self_attn.Transpose.3">(%549:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%550:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.4.self_attn.Concat.0">(%328:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %550:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%551:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.4.self_attn.Concat.1">(%329:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %543:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%552:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.4.self_attn.Repeat.0">(%551:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%553:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.4.self_attn.Repeat.1">(%552:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%554:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.4.self_attn.MatMul.0">(%546:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %553:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%555:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.4.self_attn.Mul.0">(%555:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %556:tensor<[1], Int16PerTensor, CPU>[constant: [0]][constant:[0]]) -> (%557:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.4.self_attn.ReduceMin.0">(%557:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%558:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.4.self_attn.Add.0">(%558:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %559:tensor<[1], Int16PerTensor, CPU>[constant: [9.1807e-41]][constant:[9.1807e-41]]) -> (%560:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.4.self_attn.Equal.0">(%319:tensor<[1, 1, 32, 1024], Float32, CPU>, %561:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%562:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.4.self_attn.Where.0">(%562:tensor<[1, 1, 32, 1024], UInt8, CPU>, %557:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %560:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%563:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.4.self_attn.Softmax.0">(%563:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%564:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.4.self_attn.MatMul.1">(%564:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %554:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%565:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.4.self_attn.Transpose.4">(%565:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%566:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.4.self_attn.View.3">(%566:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%566:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.4.self_attn.o_proj">(%566:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%567:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%567:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %550:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %543:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.4.mlp <CPU> [using_qnn:true, symbol:model.layers.4.mlp] {
        (%569:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%574:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.4.mlp.gate_proj">(%569:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%570:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.4.mlp.act">(%570:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%571:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.4.mlp.up_proj">(%569:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%572:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.4.mlp.Mul.0">(%571:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %572:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%573:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.4.mlp.down_proj">(%573:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%574:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%574:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.5 <CPU> [using_qnn:true, symbol:model.layers.5] {
        (%575:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %330:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %331:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%614:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %589:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %582:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.5.input_layernorm">(%575:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%576:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.5.self_attn (%576:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %330:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %331:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%606:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %589:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %582:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.5.Add.0">(%606:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %575:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%607:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.5.post_attention_layernorm">(%607:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%608:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.5.mlp (%608:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%613:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.5.Add.1">(%613:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %607:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%614:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%614:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %589:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %582:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.5.self_attn <CPU> [using_qnn:true, symbol:model.layers.5.self_attn] {
        (%576:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %330:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %331:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%606:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %589:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %582:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.5.self_attn.q_proj">(%576:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%577:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.5.self_attn.k_proj">(%576:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%578:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.5.self_attn.v_proj">(%576:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%579:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.5.self_attn.View.0">(%577:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%577:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.5.self_attn.Transpose.0">(%577:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%580:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.5.self_attn.View.1">(%578:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%578:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.5.self_attn.Transpose.1">(%578:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%581:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.5.self_attn.View.2">(%579:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%579:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.5.self_attn.Transpose.2">(%579:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%582:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.5.self_attn.q_norm">(%580:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%583:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.5.self_attn.k_norm">(%581:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%584:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.5.self_attn.q_rope">(%583:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%585:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.5.self_attn.k_rope">(%584:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%586:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.5.self_attn.CastType.0">(%586:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%587:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.5.self_attn.CastType.1">(%587:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%588:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.5.self_attn.Transpose.3">(%588:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%589:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.5.self_attn.Concat.0">(%330:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %589:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%590:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.5.self_attn.Concat.1">(%331:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %582:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%591:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.5.self_attn.Repeat.0">(%590:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%592:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.5.self_attn.Repeat.1">(%591:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%593:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.5.self_attn.MatMul.0">(%585:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %592:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%594:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.5.self_attn.Mul.0">(%594:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %595:tensor<[1], Int16PerTensor, CPU>[constant: [0]][constant:[0]]) -> (%596:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.5.self_attn.ReduceMin.0">(%596:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%597:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.5.self_attn.Add.0">(%597:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %598:tensor<[1], Int16PerTensor, CPU>[constant: [9.1807e-41]][constant:[9.1807e-41]]) -> (%599:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.5.self_attn.Equal.0">(%319:tensor<[1, 1, 32, 1024], Float32, CPU>, %600:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%601:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.5.self_attn.Where.0">(%601:tensor<[1, 1, 32, 1024], UInt8, CPU>, %596:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %599:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%602:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.5.self_attn.Softmax.0">(%602:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%603:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.5.self_attn.MatMul.1">(%603:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %593:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%604:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.5.self_attn.Transpose.4">(%604:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%605:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.5.self_attn.View.3">(%605:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%605:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.5.self_attn.o_proj">(%605:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%606:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%606:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %589:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %582:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.5.mlp <CPU> [using_qnn:true, symbol:model.layers.5.mlp] {
        (%608:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%613:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.5.mlp.gate_proj">(%608:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%609:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.5.mlp.act">(%609:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%610:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.5.mlp.up_proj">(%608:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%611:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.5.mlp.Mul.0">(%610:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %611:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%612:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.5.mlp.down_proj">(%612:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%613:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%613:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.6 <CPU> [using_qnn:true, symbol:model.layers.6] {
        (%614:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %332:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %333:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%653:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %628:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %621:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.6.input_layernorm">(%614:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%615:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.6.self_attn (%615:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %332:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %333:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%645:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %628:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %621:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.6.Add.0">(%645:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %614:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%646:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.6.post_attention_layernorm">(%646:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%647:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.6.mlp (%647:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%652:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.6.Add.1">(%652:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %646:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%653:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%653:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %628:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %621:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.6.self_attn <CPU> [using_qnn:true, symbol:model.layers.6.self_attn] {
        (%615:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %332:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %333:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%645:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %628:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %621:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.6.self_attn.q_proj">(%615:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%616:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.6.self_attn.k_proj">(%615:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%617:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.6.self_attn.v_proj">(%615:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%618:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.6.self_attn.View.0">(%616:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%616:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.6.self_attn.Transpose.0">(%616:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%619:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.6.self_attn.View.1">(%617:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%617:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.6.self_attn.Transpose.1">(%617:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%620:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.6.self_attn.View.2">(%618:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%618:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.6.self_attn.Transpose.2">(%618:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%621:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.6.self_attn.q_norm">(%619:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%622:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.6.self_attn.k_norm">(%620:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%623:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.6.self_attn.q_rope">(%622:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%624:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.6.self_attn.k_rope">(%623:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%625:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.6.self_attn.CastType.0">(%625:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%626:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.6.self_attn.CastType.1">(%626:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%627:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.6.self_attn.Transpose.3">(%627:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%628:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.6.self_attn.Concat.0">(%332:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %628:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%629:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.6.self_attn.Concat.1">(%333:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %621:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%630:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.6.self_attn.Repeat.0">(%629:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%631:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.6.self_attn.Repeat.1">(%630:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%632:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.6.self_attn.MatMul.0">(%624:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %631:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%633:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.6.self_attn.Mul.0">(%633:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %634:tensor<[1], Int16PerTensor, CPU>[constant: [0]][constant:[0]]) -> (%635:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.6.self_attn.ReduceMin.0">(%635:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%636:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.6.self_attn.Add.0">(%636:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %637:tensor<[1], Int16PerTensor, CPU>[constant: [9.1807e-41]][constant:[9.1807e-41]]) -> (%638:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.6.self_attn.Equal.0">(%319:tensor<[1, 1, 32, 1024], Float32, CPU>, %639:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%640:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.6.self_attn.Where.0">(%640:tensor<[1, 1, 32, 1024], UInt8, CPU>, %635:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %638:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%641:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.6.self_attn.Softmax.0">(%641:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%642:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.6.self_attn.MatMul.1">(%642:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %632:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%643:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.6.self_attn.Transpose.4">(%643:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%644:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.6.self_attn.View.3">(%644:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%644:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.6.self_attn.o_proj">(%644:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%645:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%645:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %628:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %621:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.6.mlp <CPU> [using_qnn:true, symbol:model.layers.6.mlp] {
        (%647:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%652:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.6.mlp.gate_proj">(%647:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%648:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.6.mlp.act">(%648:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%649:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.6.mlp.up_proj">(%647:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%650:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.6.mlp.Mul.0">(%649:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %650:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%651:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.6.mlp.down_proj">(%651:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%652:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%652:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.7 <CPU> [using_qnn:true, symbol:model.layers.7] {
        (%653:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %334:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %335:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%692:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %667:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %660:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.7.input_layernorm">(%653:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%654:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.7.self_attn (%654:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %334:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %335:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%684:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %667:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %660:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.7.Add.0">(%684:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %653:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%685:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.7.post_attention_layernorm">(%685:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%686:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.7.mlp (%686:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%691:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.7.Add.1">(%691:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %685:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%692:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%692:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %667:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %660:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.7.self_attn <CPU> [using_qnn:true, symbol:model.layers.7.self_attn] {
        (%654:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %334:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %335:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%684:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %667:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %660:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.7.self_attn.q_proj">(%654:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%655:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.7.self_attn.k_proj">(%654:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%656:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.7.self_attn.v_proj">(%654:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%657:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.7.self_attn.View.0">(%655:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%655:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.7.self_attn.Transpose.0">(%655:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%658:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.7.self_attn.View.1">(%656:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%656:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.7.self_attn.Transpose.1">(%656:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%659:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.7.self_attn.View.2">(%657:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%657:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.7.self_attn.Transpose.2">(%657:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%660:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.7.self_attn.q_norm">(%658:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%661:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.7.self_attn.k_norm">(%659:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%662:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.7.self_attn.q_rope">(%661:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%663:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.7.self_attn.k_rope">(%662:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%664:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.7.self_attn.CastType.0">(%664:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%665:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.7.self_attn.CastType.1">(%665:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%666:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.7.self_attn.Transpose.3">(%666:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%667:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.7.self_attn.Concat.0">(%334:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %667:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%668:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.7.self_attn.Concat.1">(%335:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %660:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%669:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.7.self_attn.Repeat.0">(%668:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%670:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.7.self_attn.Repeat.1">(%669:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%671:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.7.self_attn.MatMul.0">(%663:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %670:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%672:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.7.self_attn.Mul.0">(%672:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %673:tensor<[1], Int16PerTensor, CPU>[constant: [0]][constant:[0]]) -> (%674:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.7.self_attn.ReduceMin.0">(%674:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%675:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.7.self_attn.Add.0">(%675:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %676:tensor<[1], Int16PerTensor, CPU>[constant: [1.0078101]][constant:[1.0078101]]) -> (%677:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.7.self_attn.Equal.0">(%319:tensor<[1, 1, 32, 1024], Float32, CPU>, %678:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%679:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.7.self_attn.Where.0">(%679:tensor<[1, 1, 32, 1024], UInt8, CPU>, %674:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %677:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%680:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.7.self_attn.Softmax.0">(%680:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%681:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.7.self_attn.MatMul.1">(%681:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %671:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%682:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.7.self_attn.Transpose.4">(%682:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%683:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.7.self_attn.View.3">(%683:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%683:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.7.self_attn.o_proj">(%683:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%684:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%684:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %667:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %660:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.7.mlp <CPU> [using_qnn:true, symbol:model.layers.7.mlp] {
        (%686:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%691:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.7.mlp.gate_proj">(%686:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%687:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.7.mlp.act">(%687:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%688:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.7.mlp.up_proj">(%686:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%689:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.7.mlp.Mul.0">(%688:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %689:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%690:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.7.mlp.down_proj">(%690:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%691:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%691:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.8 <CPU> [using_qnn:true, symbol:model.layers.8] {
        (%692:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %336:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %337:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%731:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %706:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %699:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.8.input_layernorm">(%692:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%693:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.8.self_attn (%693:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %336:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %337:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%723:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %706:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %699:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.8.Add.0">(%723:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %692:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%724:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.8.post_attention_layernorm">(%724:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%725:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.8.mlp (%725:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%730:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.8.Add.1">(%730:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %724:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%731:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%731:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %706:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %699:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.8.self_attn <CPU> [using_qnn:true, symbol:model.layers.8.self_attn] {
        (%693:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %336:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %337:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%723:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %706:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %699:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.8.self_attn.q_proj">(%693:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%694:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.8.self_attn.k_proj">(%693:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%695:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.8.self_attn.v_proj">(%693:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%696:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.8.self_attn.View.0">(%694:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%694:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.8.self_attn.Transpose.0">(%694:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%697:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.8.self_attn.View.1">(%695:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%695:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.8.self_attn.Transpose.1">(%695:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%698:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.8.self_attn.View.2">(%696:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%696:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.8.self_attn.Transpose.2">(%696:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%699:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.8.self_attn.q_norm">(%697:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%700:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.8.self_attn.k_norm">(%698:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%701:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.8.self_attn.q_rope">(%700:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%702:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.8.self_attn.k_rope">(%701:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%703:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.8.self_attn.CastType.0">(%703:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%704:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.8.self_attn.CastType.1">(%704:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%705:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.8.self_attn.Transpose.3">(%705:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%706:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.8.self_attn.Concat.0">(%336:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %706:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%707:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.8.self_attn.Concat.1">(%337:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %699:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%708:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.8.self_attn.Repeat.0">(%707:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%709:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.8.self_attn.Repeat.1">(%708:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%710:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.8.self_attn.MatMul.0">(%702:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %709:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%711:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.8.self_attn.Mul.0">(%711:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %712:tensor<[1], Int16PerTensor, CPU>[constant: [0.390625]][constant:[0.390625]]) -> (%713:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.8.self_attn.ReduceMin.0">(%713:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%714:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.8.self_attn.Add.0">(%714:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %715:tensor<[1], Int16PerTensor, CPU>[constant: [-0.18066376]][constant:[-0.18066376]]) -> (%716:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.8.self_attn.Equal.0">(%319:tensor<[1, 1, 32, 1024], Float32, CPU>, %717:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%718:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.8.self_attn.Where.0">(%718:tensor<[1, 1, 32, 1024], UInt8, CPU>, %713:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %716:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%719:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.8.self_attn.Softmax.0">(%719:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%720:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.8.self_attn.MatMul.1">(%720:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %710:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%721:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.8.self_attn.Transpose.4">(%721:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%722:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.8.self_attn.View.3">(%722:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%722:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.8.self_attn.o_proj">(%722:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%723:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%723:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %706:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %699:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.8.mlp <CPU> [using_qnn:true, symbol:model.layers.8.mlp] {
        (%725:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%730:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.8.mlp.gate_proj">(%725:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%726:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.8.mlp.act">(%726:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%727:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.8.mlp.up_proj">(%725:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%728:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.8.mlp.Mul.0">(%727:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %728:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%729:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.8.mlp.down_proj">(%729:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%730:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%730:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.9 <CPU> [using_qnn:true, symbol:model.layers.9] {
        (%731:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %338:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %339:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%770:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %745:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %738:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.9.input_layernorm">(%731:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%732:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.9.self_attn (%732:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %338:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %339:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%762:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %745:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %738:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.9.Add.0">(%762:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %731:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%763:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.9.post_attention_layernorm">(%763:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%764:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.9.mlp (%764:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%769:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.9.Add.1">(%769:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %763:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%770:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%770:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %745:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %738:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.9.self_attn <CPU> [using_qnn:true, symbol:model.layers.9.self_attn] {
        (%732:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %338:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %339:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%762:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %745:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %738:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.9.self_attn.q_proj">(%732:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%733:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.9.self_attn.k_proj">(%732:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%734:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.9.self_attn.v_proj">(%732:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%735:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.9.self_attn.View.0">(%733:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%733:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.9.self_attn.Transpose.0">(%733:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%736:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.9.self_attn.View.1">(%734:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%734:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.9.self_attn.Transpose.1">(%734:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%737:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.9.self_attn.View.2">(%735:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%735:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.9.self_attn.Transpose.2">(%735:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%738:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.9.self_attn.q_norm">(%736:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%739:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.9.self_attn.k_norm">(%737:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%740:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.9.self_attn.q_rope">(%739:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%741:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.9.self_attn.k_rope">(%740:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%742:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.9.self_attn.CastType.0">(%742:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%743:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.9.self_attn.CastType.1">(%743:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%744:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.9.self_attn.Transpose.3">(%744:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%745:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.9.self_attn.Concat.0">(%338:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %745:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%746:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.9.self_attn.Concat.1">(%339:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %738:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%747:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.9.self_attn.Repeat.0">(%746:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%748:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.9.self_attn.Repeat.1">(%747:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%749:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.9.self_attn.MatMul.0">(%741:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %748:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%750:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.9.self_attn.Mul.0">(%750:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %751:tensor<[1], Int16PerTensor, CPU>[constant: [-0.97265625]][constant:[-0.97265625]]) -> (%752:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.9.self_attn.ReduceMin.0">(%752:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%753:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.9.self_attn.Add.0">(%753:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %754:tensor<[1], Int16PerTensor, CPU>[constant: [-0.9374988]][constant:[-0.9374988]]) -> (%755:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.9.self_attn.Equal.0">(%319:tensor<[1, 1, 32, 1024], Float32, CPU>, %756:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%757:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.9.self_attn.Where.0">(%757:tensor<[1, 1, 32, 1024], UInt8, CPU>, %752:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %755:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%758:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.9.self_attn.Softmax.0">(%758:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%759:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.9.self_attn.MatMul.1">(%759:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %749:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%760:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.9.self_attn.Transpose.4">(%760:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%761:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.9.self_attn.View.3">(%761:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%761:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.9.self_attn.o_proj">(%761:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%762:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%762:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %745:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %738:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.9.mlp <CPU> [using_qnn:true, symbol:model.layers.9.mlp] {
        (%764:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%769:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.9.mlp.gate_proj">(%764:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%765:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.9.mlp.act">(%765:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%766:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.9.mlp.up_proj">(%764:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%767:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.9.mlp.Mul.0">(%766:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %767:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%768:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.9.mlp.down_proj">(%768:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%769:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%769:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.10 <CPU> [using_qnn:true, symbol:model.layers.10] {
        (%770:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %340:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %341:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%809:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %784:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %777:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.10.input_layernorm">(%770:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%771:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.10.self_attn (%771:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %340:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %341:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%801:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %784:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %777:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.10.Add.0">(%801:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %770:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%802:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.10.post_attention_layernorm">(%802:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%803:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.10.mlp (%803:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%808:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.10.Add.1">(%808:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %802:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%809:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%809:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %784:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %777:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.10.self_attn <CPU> [using_qnn:true, symbol:model.layers.10.self_attn] {
        (%771:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %340:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %341:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%801:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %784:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %777:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.10.self_attn.q_proj">(%771:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%772:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.10.self_attn.k_proj">(%771:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%773:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.10.self_attn.v_proj">(%771:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%774:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.10.self_attn.View.0">(%772:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%772:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.10.self_attn.Transpose.0">(%772:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%775:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.10.self_attn.View.1">(%773:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%773:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.10.self_attn.Transpose.1">(%773:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%776:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.10.self_attn.View.2">(%774:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%774:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.10.self_attn.Transpose.2">(%774:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%777:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.10.self_attn.q_norm">(%775:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%778:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.10.self_attn.k_norm">(%776:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%779:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.10.self_attn.q_rope">(%778:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%780:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.10.self_attn.k_rope">(%779:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%781:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.10.self_attn.CastType.0">(%781:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%782:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.10.self_attn.CastType.1">(%782:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%783:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.10.self_attn.Transpose.3">(%783:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%784:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.10.self_attn.Concat.0">(%340:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %784:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%785:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.10.self_attn.Concat.1">(%341:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %777:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%786:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.10.self_attn.Repeat.0">(%785:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%787:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.10.self_attn.Repeat.1">(%786:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%788:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.10.self_attn.MatMul.0">(%780:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %787:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%789:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.10.self_attn.Mul.0">(%789:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %790:tensor<[1], Int16PerTensor, CPU>[constant: [-0.03955078]][constant:[-0.03955078]]) -> (%791:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.10.self_attn.ReduceMin.0">(%791:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%792:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.10.self_attn.Add.0">(%792:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %793:tensor<[1], Int16PerTensor, CPU>[constant: [0.51953006]][constant:[0.51953006]]) -> (%794:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.10.self_attn.Equal.0">(%319:tensor<[1, 1, 32, 1024], Float32, CPU>, %795:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%796:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.10.self_attn.Where.0">(%796:tensor<[1, 1, 32, 1024], UInt8, CPU>, %791:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %794:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%797:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.10.self_attn.Softmax.0">(%797:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%798:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.10.self_attn.MatMul.1">(%798:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %788:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%799:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.10.self_attn.Transpose.4">(%799:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%800:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.10.self_attn.View.3">(%800:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%800:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.10.self_attn.o_proj">(%800:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%801:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%801:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %784:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %777:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.10.mlp <CPU> [using_qnn:true, symbol:model.layers.10.mlp] {
        (%803:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%808:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.10.mlp.gate_proj">(%803:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%804:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.10.mlp.act">(%804:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%805:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.10.mlp.up_proj">(%803:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%806:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.10.mlp.Mul.0">(%805:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %806:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%807:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.10.mlp.down_proj">(%807:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%808:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%808:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.11 <CPU> [using_qnn:true, symbol:model.layers.11] {
        (%809:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %342:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %343:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%848:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %823:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %816:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.11.input_layernorm">(%809:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%810:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.11.self_attn (%810:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %342:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %343:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%840:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %823:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %816:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.11.Add.0">(%840:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %809:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%841:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.11.post_attention_layernorm">(%841:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%842:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.11.mlp (%842:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%847:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.11.Add.1">(%847:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %841:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%848:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%848:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %823:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %816:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.11.self_attn <CPU> [using_qnn:true, symbol:model.layers.11.self_attn] {
        (%810:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %342:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %343:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%840:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %823:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %816:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.11.self_attn.q_proj">(%810:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%811:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.11.self_attn.k_proj">(%810:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%812:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.11.self_attn.v_proj">(%810:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%813:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.11.self_attn.View.0">(%811:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%811:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.11.self_attn.Transpose.0">(%811:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%814:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.11.self_attn.View.1">(%812:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%812:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.11.self_attn.Transpose.1">(%812:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%815:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.11.self_attn.View.2">(%813:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%813:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.11.self_attn.Transpose.2">(%813:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%816:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.11.self_attn.q_norm">(%814:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%817:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.11.self_attn.k_norm">(%815:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%818:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.11.self_attn.q_rope">(%817:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%819:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.11.self_attn.k_rope">(%818:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%820:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.11.self_attn.CastType.0">(%820:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%821:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.11.self_attn.CastType.1">(%821:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%822:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.11.self_attn.Transpose.3">(%822:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%823:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.11.self_attn.Concat.0">(%342:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %823:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%824:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.11.self_attn.Concat.1">(%343:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %816:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%825:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.11.self_attn.Repeat.0">(%824:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%826:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.11.self_attn.Repeat.1">(%825:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%827:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.11.self_attn.MatMul.0">(%819:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %826:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%828:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.11.self_attn.Mul.0">(%828:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %829:tensor<[1], Int16PerTensor, CPU>[constant: [0.98828125]][constant:[0.98828125]]) -> (%830:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.11.self_attn.ReduceMin.0">(%830:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%831:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.11.self_attn.Add.0">(%831:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %832:tensor<[1], Int16PerTensor, CPU>[constant: [0.7499988]][constant:[0.7499988]]) -> (%833:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.11.self_attn.Equal.0">(%319:tensor<[1, 1, 32, 1024], Float32, CPU>, %834:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%835:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.11.self_attn.Where.0">(%835:tensor<[1, 1, 32, 1024], UInt8, CPU>, %830:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %833:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%836:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.11.self_attn.Softmax.0">(%836:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%837:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.11.self_attn.MatMul.1">(%837:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %827:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%838:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.11.self_attn.Transpose.4">(%838:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%839:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.11.self_attn.View.3">(%839:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%839:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.11.self_attn.o_proj">(%839:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%840:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%840:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %823:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %816:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.11.mlp <CPU> [using_qnn:true, symbol:model.layers.11.mlp] {
        (%842:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%847:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.11.mlp.gate_proj">(%842:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%843:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.11.mlp.act">(%843:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%844:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.11.mlp.up_proj">(%842:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%845:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.11.mlp.Mul.0">(%844:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %845:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%846:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.11.mlp.down_proj">(%846:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%847:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%847:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.12 <CPU> [using_qnn:true, symbol:model.layers.12] {
        (%848:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %344:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %345:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%887:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %862:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %855:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.12.input_layernorm">(%848:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%849:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.12.self_attn (%849:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %344:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %345:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%879:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %862:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %855:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.12.Add.0">(%879:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %848:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%880:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.12.post_attention_layernorm">(%880:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%881:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.12.mlp (%881:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%886:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.12.Add.1">(%886:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %880:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%887:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%887:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %862:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %855:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.12.self_attn <CPU> [using_qnn:true, symbol:model.layers.12.self_attn] {
        (%849:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %344:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %345:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%879:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %862:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %855:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.12.self_attn.q_proj">(%849:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%850:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.12.self_attn.k_proj">(%849:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%851:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.12.self_attn.v_proj">(%849:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%852:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.12.self_attn.View.0">(%850:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%850:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.12.self_attn.Transpose.0">(%850:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%853:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.12.self_attn.View.1">(%851:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%851:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.12.self_attn.Transpose.1">(%851:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%854:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.12.self_attn.View.2">(%852:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%852:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.12.self_attn.Transpose.2">(%852:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%855:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.12.self_attn.q_norm">(%853:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%856:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.12.self_attn.k_norm">(%854:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%857:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.12.self_attn.q_rope">(%856:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%858:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.12.self_attn.k_rope">(%857:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%859:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.12.self_attn.CastType.0">(%859:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%860:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.12.self_attn.CastType.1">(%860:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%861:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.12.self_attn.Transpose.3">(%861:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%862:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.12.self_attn.Concat.0">(%344:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %862:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%863:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.12.self_attn.Concat.1">(%345:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %855:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%864:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.12.self_attn.Repeat.0">(%863:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%865:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.12.self_attn.Repeat.1">(%864:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%866:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.12.self_attn.MatMul.0">(%858:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %865:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%867:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.12.self_attn.Mul.0">(%867:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %868:tensor<[1], Int16PerTensor, CPU>[constant: [-0.31640625]][constant:[-0.31640625]]) -> (%869:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.12.self_attn.ReduceMin.0">(%869:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%870:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.12.self_attn.Add.0">(%870:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %871:tensor<[1], Int16PerTensor, CPU>[constant: [-0.7890613]][constant:[-0.7890613]]) -> (%872:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.12.self_attn.Equal.0">(%319:tensor<[1, 1, 32, 1024], Float32, CPU>, %873:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%874:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.12.self_attn.Where.0">(%874:tensor<[1, 1, 32, 1024], UInt8, CPU>, %869:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %872:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%875:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.12.self_attn.Softmax.0">(%875:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%876:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.12.self_attn.MatMul.1">(%876:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %866:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%877:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.12.self_attn.Transpose.4">(%877:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%878:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.12.self_attn.View.3">(%878:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%878:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.12.self_attn.o_proj">(%878:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%879:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%879:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %862:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %855:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.12.mlp <CPU> [using_qnn:true, symbol:model.layers.12.mlp] {
        (%881:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%886:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.12.mlp.gate_proj">(%881:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%882:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.12.mlp.act">(%882:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%883:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.12.mlp.up_proj">(%881:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%884:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.12.mlp.Mul.0">(%883:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %884:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%885:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.12.mlp.down_proj">(%885:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%886:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%886:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.13 <CPU> [using_qnn:true, symbol:model.layers.13] {
        (%887:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %346:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %347:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%926:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %901:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %894:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.13.input_layernorm">(%887:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%888:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.13.self_attn (%888:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %346:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %347:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%918:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %901:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %894:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.13.Add.0">(%918:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %887:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%919:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.13.post_attention_layernorm">(%919:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%920:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.13.mlp (%920:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%925:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.13.Add.1">(%925:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %919:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%926:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%926:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %901:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %894:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.13.self_attn <CPU> [using_qnn:true, symbol:model.layers.13.self_attn] {
        (%888:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %346:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %347:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%918:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %901:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %894:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.13.self_attn.q_proj">(%888:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%889:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.13.self_attn.k_proj">(%888:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%890:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.13.self_attn.v_proj">(%888:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%891:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.13.self_attn.View.0">(%889:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%889:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.13.self_attn.Transpose.0">(%889:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%892:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.13.self_attn.View.1">(%890:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%890:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.13.self_attn.Transpose.1">(%890:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%893:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.13.self_attn.View.2">(%891:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%891:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.13.self_attn.Transpose.2">(%891:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%894:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.13.self_attn.q_norm">(%892:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%895:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.13.self_attn.k_norm">(%893:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%896:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.13.self_attn.q_rope">(%895:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%897:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.13.self_attn.k_rope">(%896:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%898:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.13.self_attn.CastType.0">(%898:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%899:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.13.self_attn.CastType.1">(%899:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%900:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.13.self_attn.Transpose.3">(%900:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%901:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.13.self_attn.Concat.0">(%346:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %901:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%902:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.13.self_attn.Concat.1">(%347:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %894:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%903:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.13.self_attn.Repeat.0">(%902:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%904:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.13.self_attn.Repeat.1">(%903:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%905:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.13.self_attn.MatMul.0">(%897:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %904:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%906:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.13.self_attn.Mul.0">(%906:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %907:tensor<[1], Int16PerTensor, CPU>[constant: [-0.875]][constant:[-0.875]]) -> (%908:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.13.self_attn.ReduceMin.0">(%908:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%909:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.13.self_attn.Add.0">(%909:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %910:tensor<[1], Int16PerTensor, CPU>[constant: [-0.46484315]][constant:[-0.46484315]]) -> (%911:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.13.self_attn.Equal.0">(%319:tensor<[1, 1, 32, 1024], Float32, CPU>, %912:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%913:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.13.self_attn.Where.0">(%913:tensor<[1, 1, 32, 1024], UInt8, CPU>, %908:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %911:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%914:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.13.self_attn.Softmax.0">(%914:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%915:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.13.self_attn.MatMul.1">(%915:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %905:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%916:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.13.self_attn.Transpose.4">(%916:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%917:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.13.self_attn.View.3">(%917:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%917:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.13.self_attn.o_proj">(%917:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%918:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%918:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %901:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %894:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.13.mlp <CPU> [using_qnn:true, symbol:model.layers.13.mlp] {
        (%920:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%925:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.13.mlp.gate_proj">(%920:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%921:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.13.mlp.act">(%921:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%922:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.13.mlp.up_proj">(%920:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%923:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.13.mlp.Mul.0">(%922:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %923:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%924:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.13.mlp.down_proj">(%924:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%925:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%925:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.14 <CPU> [using_qnn:true, symbol:model.layers.14] {
        (%926:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %348:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %349:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%965:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %940:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %933:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.14.input_layernorm">(%926:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%927:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.14.self_attn (%927:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %348:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %349:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%957:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %940:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %933:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.14.Add.0">(%957:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %926:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%958:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.14.post_attention_layernorm">(%958:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%959:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.14.mlp (%959:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%964:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.14.Add.1">(%964:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %958:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%965:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%965:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %940:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %933:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.14.self_attn <CPU> [using_qnn:true, symbol:model.layers.14.self_attn] {
        (%927:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %348:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %349:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%957:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %940:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %933:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.14.self_attn.q_proj">(%927:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%928:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.14.self_attn.k_proj">(%927:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%929:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.14.self_attn.v_proj">(%927:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%930:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.14.self_attn.View.0">(%928:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%928:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.14.self_attn.Transpose.0">(%928:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%931:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.14.self_attn.View.1">(%929:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%929:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.14.self_attn.Transpose.1">(%929:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%932:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.14.self_attn.View.2">(%930:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%930:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.14.self_attn.Transpose.2">(%930:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%933:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.14.self_attn.q_norm">(%931:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%934:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.14.self_attn.k_norm">(%932:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%935:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.14.self_attn.q_rope">(%934:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%936:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.14.self_attn.k_rope">(%935:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%937:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.14.self_attn.CastType.0">(%937:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%938:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.14.self_attn.CastType.1">(%938:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%939:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.14.self_attn.Transpose.3">(%939:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%940:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.14.self_attn.Concat.0">(%348:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %940:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%941:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.14.self_attn.Concat.1">(%349:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %933:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%942:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.14.self_attn.Repeat.0">(%941:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%943:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.14.self_attn.Repeat.1">(%942:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%944:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.14.self_attn.MatMul.0">(%936:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %943:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%945:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.14.self_attn.Mul.0">(%945:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %946:tensor<[1], Int16PerTensor, CPU>[constant: [0.6328125]][constant:[0.6328125]]) -> (%947:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.14.self_attn.ReduceMin.0">(%947:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%948:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.14.self_attn.Add.0">(%948:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %949:tensor<[1], Int16PerTensor, CPU>[constant: [0.95703006]][constant:[0.95703006]]) -> (%950:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.14.self_attn.Equal.0">(%319:tensor<[1, 1, 32, 1024], Float32, CPU>, %951:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%952:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.14.self_attn.Where.0">(%952:tensor<[1, 1, 32, 1024], UInt8, CPU>, %947:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %950:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%953:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.14.self_attn.Softmax.0">(%953:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%954:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.14.self_attn.MatMul.1">(%954:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %944:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%955:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.14.self_attn.Transpose.4">(%955:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%956:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.14.self_attn.View.3">(%956:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%956:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.14.self_attn.o_proj">(%956:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%957:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%957:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %940:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %933:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.14.mlp <CPU> [using_qnn:true, symbol:model.layers.14.mlp] {
        (%959:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%964:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.14.mlp.gate_proj">(%959:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%960:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.14.mlp.act">(%960:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%961:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.14.mlp.up_proj">(%959:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%962:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.14.mlp.Mul.0">(%961:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %962:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%963:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.14.mlp.down_proj">(%963:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%964:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%964:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.15 <CPU> [using_qnn:true, symbol:model.layers.15] {
        (%965:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %350:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %351:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1004:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %979:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %972:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.15.input_layernorm">(%965:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%966:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.15.self_attn (%966:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %350:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %351:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%996:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %979:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %972:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.15.Add.0">(%996:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %965:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%997:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.15.post_attention_layernorm">(%997:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%998:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.15.mlp (%998:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1003:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.15.Add.1">(%1003:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %997:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1004:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1004:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %979:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %972:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.15.self_attn <CPU> [using_qnn:true, symbol:model.layers.15.self_attn] {
        (%966:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %350:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %351:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%996:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %979:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %972:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.15.self_attn.q_proj">(%966:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%967:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.15.self_attn.k_proj">(%966:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%968:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.15.self_attn.v_proj">(%966:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%969:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.15.self_attn.View.0">(%967:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%967:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.15.self_attn.Transpose.0">(%967:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%970:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.15.self_attn.View.1">(%968:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%968:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.15.self_attn.Transpose.1">(%968:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%971:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.15.self_attn.View.2">(%969:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%969:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.15.self_attn.Transpose.2">(%969:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%972:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.15.self_attn.q_norm">(%970:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%973:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.15.self_attn.k_norm">(%971:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%974:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.15.self_attn.q_rope">(%973:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%975:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.15.self_attn.k_rope">(%974:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%976:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.15.self_attn.CastType.0">(%976:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%977:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.15.self_attn.CastType.1">(%977:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%978:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.15.self_attn.Transpose.3">(%978:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%979:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.15.self_attn.Concat.0">(%350:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %979:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%980:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.15.self_attn.Concat.1">(%351:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %972:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%981:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.15.self_attn.Repeat.0">(%980:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%982:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.15.self_attn.Repeat.1">(%981:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%983:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.15.self_attn.MatMul.0">(%975:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %982:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%984:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.15.self_attn.Mul.0">(%984:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %985:tensor<[1], Int16PerTensor, CPU>[constant: [0.64453125]][constant:[0.64453125]]) -> (%986:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.15.self_attn.ReduceMin.0">(%986:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%987:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.15.self_attn.Add.0">(%987:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %988:tensor<[1], Int16PerTensor, CPU>[constant: [0.119140476]][constant:[0.119140476]]) -> (%989:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.15.self_attn.Equal.0">(%319:tensor<[1, 1, 32, 1024], Float32, CPU>, %990:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%991:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.15.self_attn.Where.0">(%991:tensor<[1, 1, 32, 1024], UInt8, CPU>, %986:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %989:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%992:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.15.self_attn.Softmax.0">(%992:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%993:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.15.self_attn.MatMul.1">(%993:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %983:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%994:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.15.self_attn.Transpose.4">(%994:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%995:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.15.self_attn.View.3">(%995:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%995:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.15.self_attn.o_proj">(%995:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%996:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%996:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %979:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %972:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.15.mlp <CPU> [using_qnn:true, symbol:model.layers.15.mlp] {
        (%998:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1003:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.15.mlp.gate_proj">(%998:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%999:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.15.mlp.act">(%999:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1000:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.15.mlp.up_proj">(%998:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1001:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.15.mlp.Mul.0">(%1000:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %1001:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1002:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.15.mlp.down_proj">(%1002:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1003:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1003:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.16 <CPU> [using_qnn:true, symbol:model.layers.16] {
        (%1004:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %352:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %353:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1043:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1018:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1011:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.16.input_layernorm">(%1004:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1005:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.16.self_attn (%1005:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %352:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %353:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1035:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1018:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1011:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.16.Add.0">(%1035:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1004:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1036:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.16.post_attention_layernorm">(%1036:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1037:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.16.mlp (%1037:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1042:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.16.Add.1">(%1042:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1036:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1043:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1043:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1018:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1011:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.16.self_attn <CPU> [using_qnn:true, symbol:model.layers.16.self_attn] {
        (%1005:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %352:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %353:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1035:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1018:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1011:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.16.self_attn.q_proj">(%1005:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1006:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.16.self_attn.k_proj">(%1005:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1007:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.16.self_attn.v_proj">(%1005:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1008:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.16.self_attn.View.0">(%1006:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1006:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.16.self_attn.Transpose.0">(%1006:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1009:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.16.self_attn.View.1">(%1007:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1007:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.16.self_attn.Transpose.1">(%1007:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1010:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.16.self_attn.View.2">(%1008:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1008:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.16.self_attn.Transpose.2">(%1008:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1011:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.16.self_attn.q_norm">(%1009:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1012:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.16.self_attn.k_norm">(%1010:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1013:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.16.self_attn.q_rope">(%1012:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1014:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.16.self_attn.k_rope">(%1013:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1015:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.16.self_attn.CastType.0">(%1015:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1016:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.16.self_attn.CastType.1">(%1016:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1017:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.16.self_attn.Transpose.3">(%1017:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1018:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.16.self_attn.Concat.0">(%352:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %1018:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%1019:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.16.self_attn.Concat.1">(%353:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %1011:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1020:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.16.self_attn.Repeat.0">(%1019:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%1021:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.16.self_attn.Repeat.1">(%1020:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%1022:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.16.self_attn.MatMul.0">(%1014:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %1021:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%1023:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.16.self_attn.Mul.0">(%1023:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1024:tensor<[1], Int16PerTensor, CPU>[constant: [-0.86328125]][constant:[-0.86328125]]) -> (%1025:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.16.self_attn.ReduceMin.0">(%1025:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1026:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.16.self_attn.Add.0">(%1026:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %1027:tensor<[1], Int16PerTensor, CPU>[constant: [-0.9999988]][constant:[-0.9999988]]) -> (%1028:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.16.self_attn.Equal.0">(%319:tensor<[1, 1, 32, 1024], Float32, CPU>, %1029:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%1030:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.16.self_attn.Where.0">(%1030:tensor<[1, 1, 32, 1024], UInt8, CPU>, %1025:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1028:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%1031:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.16.self_attn.Softmax.0">(%1031:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1032:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.16.self_attn.MatMul.1">(%1032:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1022:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%1033:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.16.self_attn.Transpose.4">(%1033:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1034:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.16.self_attn.View.3">(%1034:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1034:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.16.self_attn.o_proj">(%1034:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1035:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1035:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1018:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1011:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.16.mlp <CPU> [using_qnn:true, symbol:model.layers.16.mlp] {
        (%1037:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1042:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.16.mlp.gate_proj">(%1037:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1038:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.16.mlp.act">(%1038:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1039:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.16.mlp.up_proj">(%1037:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1040:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.16.mlp.Mul.0">(%1039:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %1040:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1041:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.16.mlp.down_proj">(%1041:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1042:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1042:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.17 <CPU> [using_qnn:true, symbol:model.layers.17] {
        (%1043:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %354:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %355:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1082:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1057:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1050:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.17.input_layernorm">(%1043:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1044:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.17.self_attn (%1044:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %354:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %355:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1074:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1057:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1050:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.17.Add.0">(%1074:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1043:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1075:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.17.post_attention_layernorm">(%1075:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1076:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.17.mlp (%1076:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1081:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.17.Add.1">(%1081:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1075:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1082:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1082:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1057:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1050:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.17.self_attn <CPU> [using_qnn:true, symbol:model.layers.17.self_attn] {
        (%1044:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %354:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %355:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1074:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1057:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1050:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.17.self_attn.q_proj">(%1044:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1045:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.17.self_attn.k_proj">(%1044:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1046:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.17.self_attn.v_proj">(%1044:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1047:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.17.self_attn.View.0">(%1045:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1045:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.17.self_attn.Transpose.0">(%1045:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1048:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.17.self_attn.View.1">(%1046:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1046:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.17.self_attn.Transpose.1">(%1046:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1049:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.17.self_attn.View.2">(%1047:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1047:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.17.self_attn.Transpose.2">(%1047:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1050:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.17.self_attn.q_norm">(%1048:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1051:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.17.self_attn.k_norm">(%1049:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1052:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.17.self_attn.q_rope">(%1051:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1053:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.17.self_attn.k_rope">(%1052:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1054:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.17.self_attn.CastType.0">(%1054:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1055:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.17.self_attn.CastType.1">(%1055:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1056:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.17.self_attn.Transpose.3">(%1056:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1057:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.17.self_attn.Concat.0">(%354:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %1057:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%1058:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.17.self_attn.Concat.1">(%355:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %1050:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1059:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.17.self_attn.Repeat.0">(%1058:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%1060:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.17.self_attn.Repeat.1">(%1059:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%1061:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.17.self_attn.MatMul.0">(%1053:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %1060:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%1062:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.17.self_attn.Mul.0">(%1062:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1063:tensor<[1], Int16PerTensor, CPU>[constant: [-0.33398438]][constant:[-0.33398438]]) -> (%1064:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.17.self_attn.ReduceMin.0">(%1064:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1065:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.17.self_attn.Add.0">(%1065:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %1066:tensor<[1], Int16PerTensor, CPU>[constant: [0.24121064]][constant:[0.24121064]]) -> (%1067:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.17.self_attn.Equal.0">(%319:tensor<[1, 1, 32, 1024], Float32, CPU>, %1068:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%1069:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.17.self_attn.Where.0">(%1069:tensor<[1, 1, 32, 1024], UInt8, CPU>, %1064:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1067:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%1070:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.17.self_attn.Softmax.0">(%1070:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1071:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.17.self_attn.MatMul.1">(%1071:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1061:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%1072:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.17.self_attn.Transpose.4">(%1072:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1073:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.17.self_attn.View.3">(%1073:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1073:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.17.self_attn.o_proj">(%1073:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1074:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1074:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1057:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1050:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.17.mlp <CPU> [using_qnn:true, symbol:model.layers.17.mlp] {
        (%1076:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1081:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.17.mlp.gate_proj">(%1076:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1077:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.17.mlp.act">(%1077:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1078:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.17.mlp.up_proj">(%1076:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1079:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.17.mlp.Mul.0">(%1078:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %1079:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1080:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.17.mlp.down_proj">(%1080:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1081:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1081:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.18 <CPU> [using_qnn:true, symbol:model.layers.18] {
        (%1082:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %356:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %357:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1121:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1096:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1089:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.18.input_layernorm">(%1082:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1083:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.18.self_attn (%1083:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %356:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %357:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1113:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1096:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1089:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.18.Add.0">(%1113:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1082:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1114:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.18.post_attention_layernorm">(%1114:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1115:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.18.mlp (%1115:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1120:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.18.Add.1">(%1120:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1114:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1121:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1121:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1096:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1089:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.18.self_attn <CPU> [using_qnn:true, symbol:model.layers.18.self_attn] {
        (%1083:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %356:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %357:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1113:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1096:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1089:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.18.self_attn.q_proj">(%1083:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1084:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.18.self_attn.k_proj">(%1083:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1085:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.18.self_attn.v_proj">(%1083:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1086:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.18.self_attn.View.0">(%1084:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1084:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.18.self_attn.Transpose.0">(%1084:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1087:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.18.self_attn.View.1">(%1085:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1085:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.18.self_attn.Transpose.1">(%1085:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1088:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.18.self_attn.View.2">(%1086:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1086:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.18.self_attn.Transpose.2">(%1086:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1089:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.18.self_attn.q_norm">(%1087:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1090:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.18.self_attn.k_norm">(%1088:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1091:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.18.self_attn.q_rope">(%1090:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1092:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.18.self_attn.k_rope">(%1091:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1093:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.18.self_attn.CastType.0">(%1093:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1094:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.18.self_attn.CastType.1">(%1094:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1095:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.18.self_attn.Transpose.3">(%1095:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1096:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.18.self_attn.Concat.0">(%356:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %1096:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%1097:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.18.self_attn.Concat.1">(%357:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %1089:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1098:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.18.self_attn.Repeat.0">(%1097:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%1099:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.18.self_attn.Repeat.1">(%1098:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%1100:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.18.self_attn.MatMul.0">(%1092:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %1099:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%1101:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.18.self_attn.Mul.0">(%1101:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1102:tensor<[1], Int16PerTensor, CPU>[constant: [0]][constant:[0]]) -> (%1103:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.18.self_attn.ReduceMin.0">(%1103:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1104:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.18.self_attn.Add.0">(%1104:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %1105:tensor<[1], Int16PerTensor, CPU>[constant: [0.5546863]][constant:[0.5546863]]) -> (%1106:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.18.self_attn.Equal.0">(%319:tensor<[1, 1, 32, 1024], Float32, CPU>, %1107:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%1108:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.18.self_attn.Where.0">(%1108:tensor<[1, 1, 32, 1024], UInt8, CPU>, %1103:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1106:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%1109:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.18.self_attn.Softmax.0">(%1109:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1110:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.18.self_attn.MatMul.1">(%1110:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1100:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%1111:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.18.self_attn.Transpose.4">(%1111:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1112:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.18.self_attn.View.3">(%1112:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1112:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.18.self_attn.o_proj">(%1112:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1113:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1113:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1096:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1089:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.18.mlp <CPU> [using_qnn:true, symbol:model.layers.18.mlp] {
        (%1115:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1120:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.18.mlp.gate_proj">(%1115:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1116:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.18.mlp.act">(%1116:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1117:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.18.mlp.up_proj">(%1115:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1118:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.18.mlp.Mul.0">(%1117:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %1118:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1119:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.18.mlp.down_proj">(%1119:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1120:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1120:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.19 <CPU> [using_qnn:true, symbol:model.layers.19] {
        (%1121:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %358:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %359:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1160:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1135:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1128:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.19.input_layernorm">(%1121:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1122:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.19.self_attn (%1122:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %358:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %359:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1152:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1135:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1128:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.19.Add.0">(%1152:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1121:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1153:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.19.post_attention_layernorm">(%1153:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1154:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.19.mlp (%1154:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1159:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.19.Add.1">(%1159:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1153:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1160:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1160:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1135:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1128:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.19.self_attn <CPU> [using_qnn:true, symbol:model.layers.19.self_attn] {
        (%1122:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %358:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %359:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1152:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1135:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1128:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.19.self_attn.q_proj">(%1122:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1123:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.19.self_attn.k_proj">(%1122:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1124:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.19.self_attn.v_proj">(%1122:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1125:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.19.self_attn.View.0">(%1123:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1123:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.19.self_attn.Transpose.0">(%1123:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1126:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.19.self_attn.View.1">(%1124:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1124:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.19.self_attn.Transpose.1">(%1124:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1127:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.19.self_attn.View.2">(%1125:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1125:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.19.self_attn.Transpose.2">(%1125:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1128:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.19.self_attn.q_norm">(%1126:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1129:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.19.self_attn.k_norm">(%1127:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1130:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.19.self_attn.q_rope">(%1129:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1131:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.19.self_attn.k_rope">(%1130:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1132:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.19.self_attn.CastType.0">(%1132:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1133:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.19.self_attn.CastType.1">(%1133:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1134:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.19.self_attn.Transpose.3">(%1134:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1135:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.19.self_attn.Concat.0">(%358:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %1135:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%1136:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.19.self_attn.Concat.1">(%359:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %1128:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1137:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.19.self_attn.Repeat.0">(%1136:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%1138:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.19.self_attn.Repeat.1">(%1137:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%1139:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.19.self_attn.MatMul.0">(%1131:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %1138:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%1140:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.19.self_attn.Mul.0">(%1140:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1141:tensor<[1], Int16PerTensor, CPU>[constant: [0.98046875]][constant:[0.98046875]]) -> (%1142:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.19.self_attn.ReduceMin.0">(%1142:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1143:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.19.self_attn.Add.0">(%1143:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %1144:tensor<[1], Int16PerTensor, CPU>[constant: [0.72265506]][constant:[0.72265506]]) -> (%1145:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.19.self_attn.Equal.0">(%319:tensor<[1, 1, 32, 1024], Float32, CPU>, %1146:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%1147:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.19.self_attn.Where.0">(%1147:tensor<[1, 1, 32, 1024], UInt8, CPU>, %1142:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1145:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%1148:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.19.self_attn.Softmax.0">(%1148:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1149:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.19.self_attn.MatMul.1">(%1149:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1139:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%1150:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.19.self_attn.Transpose.4">(%1150:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1151:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.19.self_attn.View.3">(%1151:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1151:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.19.self_attn.o_proj">(%1151:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1152:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1152:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1135:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1128:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.19.mlp <CPU> [using_qnn:true, symbol:model.layers.19.mlp] {
        (%1154:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1159:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.19.mlp.gate_proj">(%1154:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1155:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.19.mlp.act">(%1155:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1156:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.19.mlp.up_proj">(%1154:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1157:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.19.mlp.Mul.0">(%1156:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %1157:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1158:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.19.mlp.down_proj">(%1158:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1159:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1159:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.20 <CPU> [using_qnn:true, symbol:model.layers.20] {
        (%1160:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %360:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %361:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1199:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1174:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1167:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.20.input_layernorm">(%1160:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1161:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.20.self_attn (%1161:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %360:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %361:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1191:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1174:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1167:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.20.Add.0">(%1191:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1160:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1192:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.20.post_attention_layernorm">(%1192:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1193:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.20.mlp (%1193:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1198:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.20.Add.1">(%1198:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1192:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1199:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1199:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1174:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1167:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.20.self_attn <CPU> [using_qnn:true, symbol:model.layers.20.self_attn] {
        (%1161:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %360:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %361:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1191:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1174:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1167:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.20.self_attn.q_proj">(%1161:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1162:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.20.self_attn.k_proj">(%1161:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1163:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.20.self_attn.v_proj">(%1161:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1164:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.20.self_attn.View.0">(%1162:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1162:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.20.self_attn.Transpose.0">(%1162:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1165:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.20.self_attn.View.1">(%1163:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1163:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.20.self_attn.Transpose.1">(%1163:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1166:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.20.self_attn.View.2">(%1164:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1164:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.20.self_attn.Transpose.2">(%1164:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1167:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.20.self_attn.q_norm">(%1165:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1168:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.20.self_attn.k_norm">(%1166:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1169:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.20.self_attn.q_rope">(%1168:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1170:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.20.self_attn.k_rope">(%1169:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1171:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.20.self_attn.CastType.0">(%1171:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1172:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.20.self_attn.CastType.1">(%1172:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1173:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.20.self_attn.Transpose.3">(%1173:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1174:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.20.self_attn.Concat.0">(%360:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %1174:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%1175:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.20.self_attn.Concat.1">(%361:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %1167:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1176:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.20.self_attn.Repeat.0">(%1175:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%1177:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.20.self_attn.Repeat.1">(%1176:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%1178:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.20.self_attn.MatMul.0">(%1170:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %1177:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%1179:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.20.self_attn.Mul.0">(%1179:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1180:tensor<[1], Int16PerTensor, CPU>[constant: [-0.35351562]][constant:[-0.35351562]]) -> (%1181:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.20.self_attn.ReduceMin.0">(%1181:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1182:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.20.self_attn.Add.0">(%1182:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %1183:tensor<[1], Int16PerTensor, CPU>[constant: [-0.8124988]][constant:[-0.8124988]]) -> (%1184:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.20.self_attn.Equal.0">(%319:tensor<[1, 1, 32, 1024], Float32, CPU>, %1185:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%1186:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.20.self_attn.Where.0">(%1186:tensor<[1, 1, 32, 1024], UInt8, CPU>, %1181:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1184:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%1187:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.20.self_attn.Softmax.0">(%1187:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1188:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.20.self_attn.MatMul.1">(%1188:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1178:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%1189:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.20.self_attn.Transpose.4">(%1189:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1190:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.20.self_attn.View.3">(%1190:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1190:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.20.self_attn.o_proj">(%1190:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1191:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1191:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1174:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1167:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.20.mlp <CPU> [using_qnn:true, symbol:model.layers.20.mlp] {
        (%1193:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1198:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.20.mlp.gate_proj">(%1193:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1194:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.20.mlp.act">(%1194:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1195:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.20.mlp.up_proj">(%1193:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1196:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.20.mlp.Mul.0">(%1195:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %1196:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1197:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.20.mlp.down_proj">(%1197:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1198:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1198:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.21 <CPU> [using_qnn:true, symbol:model.layers.21] {
        (%1199:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %362:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %363:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1238:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1213:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1206:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.21.input_layernorm">(%1199:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1200:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.21.self_attn (%1200:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %362:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %363:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1230:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1213:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1206:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.21.Add.0">(%1230:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1199:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1231:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.21.post_attention_layernorm">(%1231:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1232:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.21.mlp (%1232:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1237:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.21.Add.1">(%1237:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1231:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1238:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1238:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1213:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1206:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.21.self_attn <CPU> [using_qnn:true, symbol:model.layers.21.self_attn] {
        (%1200:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %362:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %363:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1230:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1213:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1206:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.21.self_attn.q_proj">(%1200:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1201:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.21.self_attn.k_proj">(%1200:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1202:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.21.self_attn.v_proj">(%1200:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1203:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.21.self_attn.View.0">(%1201:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1201:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.21.self_attn.Transpose.0">(%1201:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1204:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.21.self_attn.View.1">(%1202:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1202:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.21.self_attn.Transpose.1">(%1202:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1205:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.21.self_attn.View.2">(%1203:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1203:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.21.self_attn.Transpose.2">(%1203:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1206:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.21.self_attn.q_norm">(%1204:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1207:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.21.self_attn.k_norm">(%1205:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1208:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.21.self_attn.q_rope">(%1207:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1209:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.21.self_attn.k_rope">(%1208:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1210:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.21.self_attn.CastType.0">(%1210:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1211:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.21.self_attn.CastType.1">(%1211:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1212:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.21.self_attn.Transpose.3">(%1212:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1213:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.21.self_attn.Concat.0">(%362:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %1213:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%1214:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.21.self_attn.Concat.1">(%363:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %1206:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1215:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.21.self_attn.Repeat.0">(%1214:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%1216:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.21.self_attn.Repeat.1">(%1215:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%1217:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.21.self_attn.MatMul.0">(%1209:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %1216:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%1218:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.21.self_attn.Mul.0">(%1218:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1219:tensor<[1], Int16PerTensor, CPU>[constant: [-0.85546875]][constant:[-0.85546875]]) -> (%1220:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.21.self_attn.ReduceMin.0">(%1220:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1221:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.21.self_attn.Add.0">(%1221:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %1222:tensor<[1], Int16PerTensor, CPU>[constant: [-0.4296869]][constant:[-0.4296869]]) -> (%1223:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.21.self_attn.Equal.0">(%319:tensor<[1, 1, 32, 1024], Float32, CPU>, %1224:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%1225:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.21.self_attn.Where.0">(%1225:tensor<[1, 1, 32, 1024], UInt8, CPU>, %1220:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1223:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%1226:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.21.self_attn.Softmax.0">(%1226:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1227:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.21.self_attn.MatMul.1">(%1227:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1217:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%1228:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.21.self_attn.Transpose.4">(%1228:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1229:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.21.self_attn.View.3">(%1229:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1229:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.21.self_attn.o_proj">(%1229:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1230:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1230:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1213:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1206:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.21.mlp <CPU> [using_qnn:true, symbol:model.layers.21.mlp] {
        (%1232:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1237:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.21.mlp.gate_proj">(%1232:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1233:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.21.mlp.act">(%1233:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1234:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.21.mlp.up_proj">(%1232:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1235:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.21.mlp.Mul.0">(%1234:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %1235:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1236:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.21.mlp.down_proj">(%1236:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1237:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1237:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.22 <CPU> [using_qnn:true, symbol:model.layers.22] {
        (%1238:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %364:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %365:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1277:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1252:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1245:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.22.input_layernorm">(%1238:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1239:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.22.self_attn (%1239:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %364:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %365:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1269:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1252:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1245:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.22.Add.0">(%1269:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1238:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1270:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.22.post_attention_layernorm">(%1270:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1271:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.22.mlp (%1271:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1276:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.22.Add.1">(%1276:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1270:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1277:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1277:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1252:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1245:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.22.self_attn <CPU> [using_qnn:true, symbol:model.layers.22.self_attn] {
        (%1239:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %364:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %365:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1269:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1252:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1245:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.22.self_attn.q_proj">(%1239:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1240:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.22.self_attn.k_proj">(%1239:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1241:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.22.self_attn.v_proj">(%1239:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1242:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.22.self_attn.View.0">(%1240:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1240:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.22.self_attn.Transpose.0">(%1240:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1243:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.22.self_attn.View.1">(%1241:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1241:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.22.self_attn.Transpose.1">(%1241:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1244:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.22.self_attn.View.2">(%1242:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1242:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.22.self_attn.Transpose.2">(%1242:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1245:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.22.self_attn.q_norm">(%1243:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1246:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.22.self_attn.k_norm">(%1244:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1247:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.22.self_attn.q_rope">(%1246:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1248:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.22.self_attn.k_rope">(%1247:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1249:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.22.self_attn.CastType.0">(%1249:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1250:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.22.self_attn.CastType.1">(%1250:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1251:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.22.self_attn.Transpose.3">(%1251:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1252:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.22.self_attn.Concat.0">(%364:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %1252:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%1253:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.22.self_attn.Concat.1">(%365:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %1245:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1254:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.22.self_attn.Repeat.0">(%1253:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%1255:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.22.self_attn.Repeat.1">(%1254:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%1256:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.22.self_attn.MatMul.0">(%1248:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %1255:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%1257:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.22.self_attn.Mul.0">(%1257:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1258:tensor<[1], Int16PerTensor, CPU>[constant: [0.66015625]][constant:[0.66015625]]) -> (%1259:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.22.self_attn.ReduceMin.0">(%1259:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1260:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.22.self_attn.Add.0">(%1260:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %1261:tensor<[1], Int16PerTensor, CPU>[constant: [0.9687488]][constant:[0.9687488]]) -> (%1262:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.22.self_attn.Equal.0">(%319:tensor<[1, 1, 32, 1024], Float32, CPU>, %1263:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%1264:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.22.self_attn.Where.0">(%1264:tensor<[1, 1, 32, 1024], UInt8, CPU>, %1259:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1262:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%1265:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.22.self_attn.Softmax.0">(%1265:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1266:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.22.self_attn.MatMul.1">(%1266:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1256:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%1267:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.22.self_attn.Transpose.4">(%1267:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1268:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.22.self_attn.View.3">(%1268:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1268:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.22.self_attn.o_proj">(%1268:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1269:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1269:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1252:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1245:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.22.mlp <CPU> [using_qnn:true, symbol:model.layers.22.mlp] {
        (%1271:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1276:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.22.mlp.gate_proj">(%1271:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1272:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.22.mlp.act">(%1272:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1273:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.22.mlp.up_proj">(%1271:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1274:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.22.mlp.Mul.0">(%1273:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %1274:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1275:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.22.mlp.down_proj">(%1275:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1276:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1276:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.23 <CPU> [using_qnn:true, symbol:model.layers.23] {
        (%1277:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %366:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %367:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1316:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1291:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1284:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.23.input_layernorm">(%1277:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1278:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.23.self_attn (%1278:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %366:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %367:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1308:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1291:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1284:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.23.Add.0">(%1308:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1277:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1309:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.23.post_attention_layernorm">(%1309:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1310:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.23.mlp (%1310:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1315:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.23.Add.1">(%1315:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1309:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1316:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1316:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1291:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1284:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.23.self_attn <CPU> [using_qnn:true, symbol:model.layers.23.self_attn] {
        (%1278:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %366:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %367:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1308:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1291:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1284:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.23.self_attn.q_proj">(%1278:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1279:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.23.self_attn.k_proj">(%1278:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1280:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.23.self_attn.v_proj">(%1278:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1281:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.23.self_attn.View.0">(%1279:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1279:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.23.self_attn.Transpose.0">(%1279:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1282:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.23.self_attn.View.1">(%1280:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1280:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.23.self_attn.Transpose.1">(%1280:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1283:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.23.self_attn.View.2">(%1281:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1281:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.23.self_attn.Transpose.2">(%1281:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1284:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.23.self_attn.q_norm">(%1282:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1285:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.23.self_attn.k_norm">(%1283:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1286:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.23.self_attn.q_rope">(%1285:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1287:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.23.self_attn.k_rope">(%1286:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1288:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.23.self_attn.CastType.0">(%1288:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1289:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.23.self_attn.CastType.1">(%1289:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1290:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.23.self_attn.Transpose.3">(%1290:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1291:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.23.self_attn.Concat.0">(%366:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %1291:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%1292:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.23.self_attn.Concat.1">(%367:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %1284:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1293:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.23.self_attn.Repeat.0">(%1292:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%1294:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.23.self_attn.Repeat.1">(%1293:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%1295:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.23.self_attn.MatMul.0">(%1287:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %1294:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%1296:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.23.self_attn.Mul.0">(%1296:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1297:tensor<[1], Int16PerTensor, CPU>[constant: [0.61328125]][constant:[0.61328125]]) -> (%1298:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.23.self_attn.ReduceMin.0">(%1298:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1299:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.23.self_attn.Add.0">(%1299:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %1300:tensor<[1], Int16PerTensor, CPU>[constant: [0.079589695]][constant:[0.079589695]]) -> (%1301:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.23.self_attn.Equal.0">(%319:tensor<[1, 1, 32, 1024], Float32, CPU>, %1302:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%1303:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.23.self_attn.Where.0">(%1303:tensor<[1, 1, 32, 1024], UInt8, CPU>, %1298:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1301:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%1304:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.23.self_attn.Softmax.0">(%1304:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1305:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.23.self_attn.MatMul.1">(%1305:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1295:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%1306:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.23.self_attn.Transpose.4">(%1306:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1307:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.23.self_attn.View.3">(%1307:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1307:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.23.self_attn.o_proj">(%1307:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1308:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1308:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1291:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1284:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.23.mlp <CPU> [using_qnn:true, symbol:model.layers.23.mlp] {
        (%1310:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1315:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.23.mlp.gate_proj">(%1310:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1311:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.23.mlp.act">(%1311:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1312:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.23.mlp.up_proj">(%1310:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1313:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.23.mlp.Mul.0">(%1312:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %1313:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1314:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.23.mlp.down_proj">(%1314:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1315:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1315:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.24 <CPU> [using_qnn:true, symbol:model.layers.24] {
        (%1316:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %368:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %369:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1355:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1330:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1323:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.24.input_layernorm">(%1316:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1317:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.24.self_attn (%1317:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %368:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %369:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1347:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1330:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1323:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.24.Add.0">(%1347:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1316:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1348:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.24.post_attention_layernorm">(%1348:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1349:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.24.mlp (%1349:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1354:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.24.Add.1">(%1354:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1348:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1355:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1355:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1330:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1323:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.24.self_attn <CPU> [using_qnn:true, symbol:model.layers.24.self_attn] {
        (%1317:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %368:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %369:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1347:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1330:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1323:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.24.self_attn.q_proj">(%1317:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1318:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.24.self_attn.k_proj">(%1317:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1319:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.24.self_attn.v_proj">(%1317:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1320:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.24.self_attn.View.0">(%1318:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1318:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.24.self_attn.Transpose.0">(%1318:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1321:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.24.self_attn.View.1">(%1319:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1319:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.24.self_attn.Transpose.1">(%1319:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1322:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.24.self_attn.View.2">(%1320:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1320:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.24.self_attn.Transpose.2">(%1320:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1323:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.24.self_attn.q_norm">(%1321:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1324:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.24.self_attn.k_norm">(%1322:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1325:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.24.self_attn.q_rope">(%1324:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1326:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.24.self_attn.k_rope">(%1325:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1327:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.24.self_attn.CastType.0">(%1327:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1328:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.24.self_attn.CastType.1">(%1328:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1329:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.24.self_attn.Transpose.3">(%1329:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1330:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.24.self_attn.Concat.0">(%368:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %1330:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%1331:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.24.self_attn.Concat.1">(%369:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %1323:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1332:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.24.self_attn.Repeat.0">(%1331:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%1333:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.24.self_attn.Repeat.1">(%1332:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%1334:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.24.self_attn.MatMul.0">(%1326:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %1333:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%1335:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.24.self_attn.Mul.0">(%1335:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1336:tensor<[1], Int16PerTensor, CPU>[constant: [-0.8828125]][constant:[-0.8828125]]) -> (%1337:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.24.self_attn.ReduceMin.0">(%1337:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1338:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.24.self_attn.Add.0">(%1338:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %1339:tensor<[1], Int16PerTensor, CPU>[constant: [-0.99609256]][constant:[-0.99609256]]) -> (%1340:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.24.self_attn.Equal.0">(%319:tensor<[1, 1, 32, 1024], Float32, CPU>, %1341:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%1342:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.24.self_attn.Where.0">(%1342:tensor<[1, 1, 32, 1024], UInt8, CPU>, %1337:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1340:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%1343:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.24.self_attn.Softmax.0">(%1343:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1344:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.24.self_attn.MatMul.1">(%1344:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1334:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%1345:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.24.self_attn.Transpose.4">(%1345:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1346:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.24.self_attn.View.3">(%1346:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1346:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.24.self_attn.o_proj">(%1346:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1347:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1347:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1330:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1323:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.24.mlp <CPU> [using_qnn:true, symbol:model.layers.24.mlp] {
        (%1349:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1354:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.24.mlp.gate_proj">(%1349:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1350:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.24.mlp.act">(%1350:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1351:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.24.mlp.up_proj">(%1349:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1352:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.24.mlp.Mul.0">(%1351:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %1352:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1353:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.24.mlp.down_proj">(%1353:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1354:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1354:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.25 <CPU> [using_qnn:true, symbol:model.layers.25] {
        (%1355:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %370:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %371:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1394:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1369:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1362:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.25.input_layernorm">(%1355:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1356:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.25.self_attn (%1356:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %370:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %371:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1386:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1369:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1362:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.25.Add.0">(%1386:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1355:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1387:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.25.post_attention_layernorm">(%1387:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1388:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.25.mlp (%1388:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1393:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.25.Add.1">(%1393:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1387:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1394:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1394:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1369:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1362:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.25.self_attn <CPU> [using_qnn:true, symbol:model.layers.25.self_attn] {
        (%1356:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %370:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %371:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1386:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1369:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1362:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.25.self_attn.q_proj">(%1356:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1357:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.25.self_attn.k_proj">(%1356:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1358:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.25.self_attn.v_proj">(%1356:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1359:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.25.self_attn.View.0">(%1357:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1357:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.25.self_attn.Transpose.0">(%1357:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1360:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.25.self_attn.View.1">(%1358:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1358:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.25.self_attn.Transpose.1">(%1358:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1361:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.25.self_attn.View.2">(%1359:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1359:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.25.self_attn.Transpose.2">(%1359:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1362:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.25.self_attn.q_norm">(%1360:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1363:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.25.self_attn.k_norm">(%1361:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1364:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.25.self_attn.q_rope">(%1363:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1365:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.25.self_attn.k_rope">(%1364:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1366:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.25.self_attn.CastType.0">(%1366:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1367:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.25.self_attn.CastType.1">(%1367:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1368:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.25.self_attn.Transpose.3">(%1368:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1369:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.25.self_attn.Concat.0">(%370:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %1369:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%1370:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.25.self_attn.Concat.1">(%371:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %1362:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1371:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.25.self_attn.Repeat.0">(%1370:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%1372:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.25.self_attn.Repeat.1">(%1371:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%1373:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.25.self_attn.MatMul.0">(%1365:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %1372:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%1374:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.25.self_attn.Mul.0">(%1374:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1375:tensor<[1], Int16PerTensor, CPU>[constant: [-0.29492188]][constant:[-0.29492188]]) -> (%1376:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.25.self_attn.ReduceMin.0">(%1376:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1377:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.25.self_attn.Add.0">(%1377:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %1378:tensor<[1], Int16PerTensor, CPU>[constant: [0.2812494]][constant:[0.2812494]]) -> (%1379:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.25.self_attn.Equal.0">(%319:tensor<[1, 1, 32, 1024], Float32, CPU>, %1380:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%1381:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.25.self_attn.Where.0">(%1381:tensor<[1, 1, 32, 1024], UInt8, CPU>, %1376:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1379:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%1382:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.25.self_attn.Softmax.0">(%1382:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1383:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.25.self_attn.MatMul.1">(%1383:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1373:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%1384:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.25.self_attn.Transpose.4">(%1384:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1385:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.25.self_attn.View.3">(%1385:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1385:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.25.self_attn.o_proj">(%1385:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1386:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1386:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1369:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1362:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.25.mlp <CPU> [using_qnn:true, symbol:model.layers.25.mlp] {
        (%1388:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1393:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.25.mlp.gate_proj">(%1388:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1389:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.25.mlp.act">(%1389:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1390:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.25.mlp.up_proj">(%1388:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1391:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.25.mlp.Mul.0">(%1390:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %1391:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1392:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.25.mlp.down_proj">(%1392:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1393:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1393:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.26 <CPU> [using_qnn:true, symbol:model.layers.26] {
        (%1394:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %372:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %373:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1433:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1408:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1401:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.26.input_layernorm">(%1394:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1395:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.26.self_attn (%1395:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %372:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %373:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1425:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1408:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1401:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.26.Add.0">(%1425:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1394:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1426:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.26.post_attention_layernorm">(%1426:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1427:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.26.mlp (%1427:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1432:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.26.Add.1">(%1432:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1426:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1433:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1433:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1408:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1401:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.26.self_attn <CPU> [using_qnn:true, symbol:model.layers.26.self_attn] {
        (%1395:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %372:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %373:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1425:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1408:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1401:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.26.self_attn.q_proj">(%1395:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1396:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.26.self_attn.k_proj">(%1395:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1397:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.26.self_attn.v_proj">(%1395:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1398:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.26.self_attn.View.0">(%1396:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1396:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.26.self_attn.Transpose.0">(%1396:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1399:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.26.self_attn.View.1">(%1397:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1397:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.26.self_attn.Transpose.1">(%1397:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1400:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.26.self_attn.View.2">(%1398:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1398:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.26.self_attn.Transpose.2">(%1398:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1401:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.26.self_attn.q_norm">(%1399:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1402:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.26.self_attn.k_norm">(%1400:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1403:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.26.self_attn.q_rope">(%1402:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1404:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.26.self_attn.k_rope">(%1403:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1405:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.26.self_attn.CastType.0">(%1405:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1406:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.26.self_attn.CastType.1">(%1406:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1407:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.26.self_attn.Transpose.3">(%1407:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1408:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.26.self_attn.Concat.0">(%372:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %1408:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%1409:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.26.self_attn.Concat.1">(%373:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %1401:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1410:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.26.self_attn.Repeat.0">(%1409:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%1411:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.26.self_attn.Repeat.1">(%1410:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%1412:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.26.self_attn.MatMul.0">(%1404:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %1411:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%1413:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.26.self_attn.Mul.0">(%1413:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1414:tensor<[1], Int16PerTensor, CPU>[constant: [0.9921875]][constant:[0.9921875]]) -> (%1415:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.26.self_attn.ReduceMin.0">(%1415:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1416:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.26.self_attn.Add.0">(%1416:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %1417:tensor<[1], Int16PerTensor, CPU>[constant: [0.89453006]][constant:[0.89453006]]) -> (%1418:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.26.self_attn.Equal.0">(%319:tensor<[1, 1, 32, 1024], Float32, CPU>, %1419:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%1420:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.26.self_attn.Where.0">(%1420:tensor<[1, 1, 32, 1024], UInt8, CPU>, %1415:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1418:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%1421:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.26.self_attn.Softmax.0">(%1421:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1422:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.26.self_attn.MatMul.1">(%1422:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1412:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%1423:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.26.self_attn.Transpose.4">(%1423:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1424:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.26.self_attn.View.3">(%1424:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1424:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.26.self_attn.o_proj">(%1424:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1425:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1425:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1408:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1401:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.26.mlp <CPU> [using_qnn:true, symbol:model.layers.26.mlp] {
        (%1427:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1432:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.26.mlp.gate_proj">(%1427:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1428:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.26.mlp.act">(%1428:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1429:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.26.mlp.up_proj">(%1427:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1430:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.26.mlp.Mul.0">(%1429:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %1430:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1431:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.26.mlp.down_proj">(%1431:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1432:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1432:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.27 <CPU> [using_qnn:true, symbol:model.layers.27] {
        (%1433:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %374:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %375:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1472:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1447:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1440:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.RMSNormOp <name="model.layers.27.input_layernorm">(%1433:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1434:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.27.self_attn (%1434:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %374:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %375:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1464:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1447:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1440:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.27.Add.0">(%1464:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1433:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1465:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.27.post_attention_layernorm">(%1465:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1466:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            graph.CallGraphOp @model.layers.27.mlp (%1466:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1471:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.27.Add.1">(%1471:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1465:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1472:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1472:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1447:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1440:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.27.self_attn <CPU> [using_qnn:true, symbol:model.layers.27.self_attn] {
        (%1434:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>, %319:tensor<[1, 1, 32, 1024], Float32, CPU>, %374:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %375:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>) -> (%1464:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1447:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1440:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.27.self_attn.q_proj">(%1434:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1435:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.27.self_attn.k_proj">(%1434:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1436:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.27.self_attn.v_proj">(%1434:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1437:tensor<[1, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.27.self_attn.View.0">(%1435:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1435:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.27.self_attn.Transpose.0">(%1435:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1438:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.27.self_attn.View.1">(%1436:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1436:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.27.self_attn.Transpose.1">(%1436:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1439:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.27.self_attn.View.2">(%1437:tensor<[1, 32, 1024], Int16PerTensor, CPU>) -> (%1437:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.27.self_attn.Transpose.2">(%1437:tensor<[1, 32, 8, 128], Int16PerTensor, CPU>) -> (%1440:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.27.self_attn.q_norm">(%1438:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1441:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RMSNormOp <name="model.layers.27.self_attn.k_norm">(%1439:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1442:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.27.self_attn.q_rope">(%1441:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1443:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.RoPEOp <name="model.layers.27.self_attn.k_rope">(%1442:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>, %379:tensor<[1, 32, 128], Int16PerTensor, CPU>, %380:tensor<[1, 32, 128], Int16PerTensor, CPU>) -> (%1444:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.27.self_attn.CastType.0">(%1444:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1445:tensor<[1, 8, 32, 128], Float16, CPU>)
            linalg.CPU.CastTypeOp <name="model.layers.27.self_attn.CastType.1">(%1445:tensor<[1, 8, 32, 128], Float16, CPU>) -> (%1446:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.27.self_attn.Transpose.3">(%1446:tensor<[1, 8, 32, 128], Int8PerTensor, CPU>) -> (%1447:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.27.self_attn.Concat.0">(%374:tensor<[1, 8, 128, 992], Int8PerTensor, CPU>, %1447:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>) -> (%1448:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.ConcatOp <name="model.layers.27.self_attn.Concat.1">(%375:tensor<[1, 8, 992, 128], Int8PerTensor, CPU>, %1440:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> (%1449:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.27.self_attn.Repeat.0">(%1448:tensor<[1, 8, 128, 1024], Int8PerTensor, CPU>) -> (%1450:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>)
            linalg.CPU.RepeatOp <name="model.layers.27.self_attn.Repeat.1">(%1449:tensor<[1, 8, 1024, 128], Int8PerTensor, CPU>) -> (%1451:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.27.self_attn.MatMul.0">(%1443:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>, %1450:tensor<[1, 16, 128, 1024], Int8PerTensor, CPU>) -> (%1452:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.27.self_attn.Mul.0">(%1452:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1453:tensor<[1], Int16PerTensor, CPU>[constant: [-0.061767578]][constant:[-0.061767578]]) -> (%1454:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.ReduceMinOp <name="model.layers.27.self_attn.ReduceMin.0">(%1454:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1455:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.AddOp <name="model.layers.27.self_attn.Add.0">(%1455:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>, %1456:tensor<[1], Int16PerTensor, CPU>[constant: [-0.60546756]][constant:[-0.60546756]]) -> (%1457:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>)
            linalg.CPU.EqualOp <name="model.layers.27.self_attn.Equal.0">(%319:tensor<[1, 1, 32, 1024], Float32, CPU>, %1458:tensor<[1], Float32, CPU>[constant: [0]][constant:[0]]) -> (%1459:tensor<[1, 1, 32, 1024], UInt8, CPU>)
            linalg.CPU.WhereOp <name="model.layers.27.self_attn.Where.0">(%1459:tensor<[1, 1, 32, 1024], UInt8, CPU>, %1454:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1457:tensor<[1, 16, 32, 1], Int16PerTensor, CPU>) -> (%1460:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.SoftmaxOp <name="model.layers.27.self_attn.Softmax.0">(%1460:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>) -> (%1461:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>)
            linalg.CPU.MatMulOp <name="model.layers.27.self_attn.MatMul.1">(%1461:tensor<[1, 16, 32, 1024], Int16PerTensor, CPU>, %1451:tensor<[1, 16, 1024, 128], Int8PerTensor, CPU>) -> (%1462:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>)
            linalg.CPU.TransposeOp <name="model.layers.27.self_attn.Transpose.4">(%1462:tensor<[1, 16, 32, 128], Int16PerTensor, CPU>) -> (%1463:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>)
            linalg.CPU.ViewOp <name="model.layers.27.self_attn.View.3">(%1463:tensor<[1, 32, 16, 128], Int16PerTensor, CPU>) -> (%1463:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.27.self_attn.o_proj">(%1463:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1464:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1464:tensor<[1, 32, 2048], Int16PerTensor, CPU>, %1447:tensor<[1, 8, 128, 32], Int8PerTensor, CPU>, %1440:tensor<[1, 8, 32, 128], Int16PerTensor, CPU>) -> ()
        }
    }
    graph.SubGraphOp @model.layers.27.mlp <CPU> [using_qnn:true, symbol:model.layers.27.mlp] {
        (%1466:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1471:tensor<[1, 32, 2048], Int16PerTensor, CPU>) {
            linalg.CPU.LinearOp <name="model.layers.27.mlp.gate_proj">(%1466:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1467:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.SiLUOp <name="model.layers.27.mlp.act">(%1467:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1468:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.27.mlp.up_proj">(%1466:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1469:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.MulOp <name="model.layers.27.mlp.Mul.0">(%1468:tensor<[1, 32, 6144], Int16PerTensor, CPU>, %1469:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1470:tensor<[1, 32, 6144], Int16PerTensor, CPU>)
            linalg.CPU.LinearOp <name="model.layers.27.mlp.down_proj">(%1470:tensor<[1, 32, 6144], Int16PerTensor, CPU>) -> (%1471:tensor<[1, 32, 2048], Int16PerTensor, CPU>)
            cf.ReturnOp (%1471:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> ()
        }
    }
    linalg.CPU.LinearOp <name="lm_head"> [using_qnn:true] (%1473:tensor<[1, 32, 2048], Int16PerTensor, CPU>) -> (%1474:tensor<[1, 32, 151936], Int16PerTensor, CPU>)
    //        
    //      o o    
    //            
    //       
    //             
    //        
}
 
